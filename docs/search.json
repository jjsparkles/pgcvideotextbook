[
  {
    "objectID": "chapter5.4_transcript.html",
    "href": "chapter5.4_transcript.html",
    "title": "Chapter 5.4: Meta-Analysis (Video Transcript)",
    "section": "",
    "text": "Title: Genome-wide association study design and interpretation\nPresenter(s): Gina Peloso, Broad Institute\nHost:\nOkay, I think we’re gonna get started. Good morning, today is the third in the primer series. We had one talk focused on complex trait genetics, one talk focused on Mendelian genetics. Today, we’re going to have a talk focused on common variants genetic association studies, and the subsequent three talks are going to focus on rare coding variants association studies of different forms. So, this will be the only session dedicated to genome-wide association studies, a tool that has been widely used in the past 10 years and quite foundational for much of the work that many folks are doing. Gina did her PhD work at Boston University, did a post-doc at the Broad Institute and Mass General, and is now an assistant professor in the Department of Biostatistics at Boston University and an affiliate of the Broad Institute and, today, talking about genome-wide association study design and interpretation. Thanks very much, Gina.\nGina:\nThanks. So, with genome-wide association studies, we are trying to get at testing the association of phenotypic variation with genotypic variation, and this is particularly useful when looking at complex traits. So, traits that have both a genetic and an environmental component, and these complex traits not only have a genetic component, but they have many genes that are contributing to the trait variability and some of these genetic effects can be very subtle. So, with genome-wide associations, we are testing the type of genetic variation called single nucleotide polymorphisms, or SNPs. Here is a pictorial of 10 chromosomes, and you can see that, for these base pairs, most of the individuals have the same exact allele, but in the middle here, there is an allele that varies among individuals where it could be a C or a G allele, and this is a single nucleotide polymorphism. And you can see that it’s a common variant, it’s seen in four of the 10 chromosomes, so it’s seen in many individuals and this is the type of variation we’re going to be testing with genome-wide association studies.\nGWAS are really getting at the common disease-common variant hypothesis. Here on the upper left-hand corner, you see that variants affecting Mendelian disease are considered very rare. So, along the x-axis is allele frequency, and then along the y-axis is the penetrance or the effect size of a variant. For Mendelian diseases here, they’re very, very driven by very rare variants of high effect. On the opposite side of the graph here is common variation with very subtle effects and low effect sizes, and this is what we’re getting at with genome-wide association studies, this type of variation down here.\nSo, genome-wide association studies have been performed for the past 10 years or so, and it really began with the HapMap project. And the goal of the HapMap Project was to describe variation in the human genome for common populations, those of Yoruba descent from Nigeria; individuals living in Beijing, China; individuals from Tokyo; as well as individuals from the CEPH Cohorts who were of Northern and Western European descent. And so, they queried a small number of these individuals for variation that varied among these individuals from these four population groups, and they took that variation from the HapMap Project and put it on to commercial genotyping arrays that can be then genotyped in many, many individuals. And so, what the goal of GWAS is to look across these genome-wide set of SNPs for an association to a particular outcome. Now, it can be said that GWAS have been very successful in identifying regions of the genome associated with a range of diseases, and this diagram was downloaded from the GWAS catalog and has been updated just last week, and it shows all the genetic association studies that have been identified by GWAS at a genome-wide significance level, and it’s impressive that the GWAS catalog contains 2,554 studies performed and has identified 25,037 unique SNP-trait associations. So, it’s been very successful in terms of identifying locations in the genome associated with disease.\nSo, these are the basic steps for performing a genome-wide association study. You have to think about your study design and optimally designing your study, and we’re going to talk about sample collection as well as genotyping. And then one of the main steps that has been really fine-tuned within GWAS and has made GWAS so successful is robust quality control done on the data for GWAS. And then finally, after you have performed your quality control, you can go on and do the interesting part and look at the association between your trait of interest and these genetic markers across the genome.\nSo, first, you have to come up with your outcome, what are you interested in studying, and decide whether you want that outcome of interest to be studied as a disease case-control sample, where you’re collecting cases and controls; for example, type 2 diabetes studies, schizophrenia studies, and obesity studies have collected extremes or individuals with disease and those without disease and performed GWAS comparing these two groups. You can also do GWAS on quantitative traits. I particularly work around cholesterol levels, and you can compare the distribution of a phenotype versus the allele frequency distributions, but importantly, before you embark on a GWAS, you want to make sure your trait has a heritable component and confirm that actually there’s a genetic effect that is contributing to the trait, so that you have an ability to find genetic markers associated with that trait.\nSo, you might think, what sample size do I need to detect effects of a certain magnitude? And this is a pretty old figure from 2005, so just at the start of GWAS, and what it says is along the x-axis is the frequency of the disease susceptibility allele and now along the y-axis is the sample size required. And this was for 80% power to detect an effect at about a 1x10-6 α level. And what you can see is that if you want to detect smaller and smaller effects, you need larger and larger sample sizes. Here we have an odds ratio of 1.2, and actually in GWAS, we’re detecting odds ratios much smaller than that. So, with an odds ratio of 1.2 and the most informative marker, so an allele frequency of about 0.5, you need about 4,000 subjects. In GWAS, we’re kind of detecting effect sizes in the order of 1.02. Alright, so you’re all the way up here and needing individuals in the tens of thousands to be able to attack such subtle effects.\nSo, what sample size you need kind of will determine a route you’re going to use for performing genome-wide association studies. So, there are two kinds of routes you can take for doing this. You can do a single-study analysis where you’ve collected both the phenotype of interest and you’ve done a genotyping array, and you analyze it in-house. These are really great when you have large effects, you’re looking for large effects, and you have unique phenotypes that are not traditionally collected on many individuals. However, power is limited in these studies because there’s only so many subjects that you can collect within a single study. So, a common method and what’s been traditionally done over the last ten years for common traits is multi-study or meta-analyses of studies contributing to the same results. This is great when you have commonly collected phenotypes. This will give you a larger sample size and therefore more power to detect subtler effects of the genotypes.\nSo, there are over 40 genotyping arrays that have been developed in the past ten years. This isn’t showing all of them, it didn’t come up right, but there are both Illumina and Affymetrix chips, and they vary widely in their content. So, this is going to come into play later when we talk about when you need to combine data across multiple studies because when you have multiple studies, they might not have all the same genotyping chip done on them.\nOnce you have the phenotype collected and you’ve gotten those individuals’ genotypes, you have to do quality control.\nBecause the ability to detect a true genetic association is only as good as the quality of your underlying data. And because a large number of markers are tested for association in a GWAS, even a low error rate can be detrimental to a GWAS association study. So, take this example: we have 1 million markers tested for association, which is pretty typical in a genome-wide association study, and let’s assume that approximately 0.1% of those markers are poorly genotyped and that inaccurate calling results in spurious association. Okay, so if inaccurate calling leads to spurious association, that can mean that up to a thousand markers might be unnecessarily taken forward for replication because of false positive associations due to poor genotyping. So, quality control steps are essential in analyzing genetic data, and they are taken to remove both individuals and markers with high error rates. It’s assumed that many thousands of individuals have been genotyped to maximize the power to detect an association, so removing a handful of individuals will really have little effect on the overall power of this study. Also, given that a very large number of markers are genotyped, the removal of a small percent of the SNPs should not markedly decrease the overall power of the study. That being said, every marker removed from a study is potentially a disease-associated locus that you’re not testing. So, removing a marker may potentially have more of an impact than removing one individual. Of course, we’re going to talk about genotype imputation where we might be able to recover these markers back.\nHere are some standard QC metrics that are done in genome-wide association studies. You have both sample QC and SNP QC. In sample QC, we look for high missingness rates; deviations from heterozygosity, which can indicate some contamination; gender checks to make sure we have the right individuals; duplicates; cryptic relatedness; unexpected relatedness (and that depends on our study design whether we have family information or not; if we do have families within the data set, we can look at Mendelian errors and see if there is a problem that could indicate that we have the wrong individual); and then we usually exclude population outliers. For SNP QC, we look at missingness; deviations from Hardy-Weinberg equilibrium, and that’s because it might be a problem with the genotyping that’s causing those deviations from Hardy-Weinberg (if you have a case-control study, you can look at the differential missingness between cases and controls); and then SNPs with a high number of Mendelian errors could indicate that there’s a problem with genotyping that SNP. Now, QC is usually done on individuals first and then on markers, and this approach is used because there are more markers contributing to the sample-level statistics than there are samples contributing to the marker-level statistics. So, for looking at the sample-level statistics, a couple of bad markers are going to be drowned out by all the good markers and won’t affect the sample-level statistics as much, as there are usually fewer samples than markers, so they have a higher weight in the sample-level statistics.\nAudience question: So the question was: when we look at relatedness, do we have information about the relatedness, how the subjects are related? Yes, you want to look at both. If you have a family study, you would look at the expected relationships versus the observed relationships based on their genome-wide identity by state matrix. So, you can compute the expected proportion of sharing of alleles and then compare that expected proportion to what you think is the case. Now, for family studies, you would compare that to a family structure, but if you think you have a set of unrelated subjects, you would expect those IBS (identity by state) estimates and the proportion of sharing between individuals to be relatively low.\nGina: Okay, so one of the confounders of genome-wide association studies is population structure. Population structure occurs when there are subgroups within your data that differ with respect to trait distributions as well as marker frequencies, and this can cause spurious association results. Population structure is one of the few confounders in genetic association studies, and that’s not to say you shouldn’t be adjusting for covariates in your tests for association. You still want to adjust for covariates because they increase the precision of your outcome. But population structure really causes a confounding effect that needs to be adjusted for.\nLuckily, there are techniques for adjusting for this effect. You can use a well-matched design, making sure you’re selecting individuals from the same regions, so there’s less of a concern. But even within samples of European ancestry, there is observed population structure. Here is a plot of a sample of all Europeans, and it is calculating principal components of genetic relatedness. Principal components are weighted aggregated scores of independent genetic variants. You can calculate them, and you get this plot when you plot principal component one versus principal component two. Here, I’ve labeled individuals by the Italian or non-Italian, and you can see that based on these two principal components, which is a weighted score of SNPs, is distinguishing between being Italian or not Italian. So, while this sample is all Europeans, there are subtle differences that can be detected, and these can cause spurious associations. You can use these principal components as covariates within a statistical model.\nAudience question:The question is if genomic inflation λ statistic larger than 1.05, why does that indicate population stratification? Having a λ statistic greater than 1.05 doesn’t necessarily indicate that there is population structure, but it could indicate there’s population structure. It could be because there are lots of markers showing spurious association and, when we talk about QQ plots, it could draw the association off the line if there are many markers that are affected by population structure.\nGina: The last way that you can control for population structure, and one of the ways that is more routinely done in today’s GWAS is using mixed models with kinship relationship matrices. A matrix of IBS sharing among individuals to be able to get at subtle differences in between individuals. So, using a mixed model is a third way to control for population structure.\nHere are some resources for best practices on QC of genome-wide association studies that provide a lot more detail on these resources about what QC thresholds should be used.\nSo I have alluded to that we can improve power by increasing the sample size by combining studies from multiple different cohorts and combining them together to increase our power. The problem with this is that different studies have used different commercial chips with different sets of SNPs for their genotyping, and we can take the set of common SNPs and analyze the set of common SNPs that are on each of the genotyping arrays. But that set of common SNPs across platforms is really restrictive.\nThis brings us to imputation. What imputation does is it fills in the genotypes or the SNPs based on LD and haplotypes from a reference sample to get a fuller range of SNPs in your study. Take, for example, we have this reference sample that has been genotyped on many SNPs, and then we have our sample from a commercial array that has a proportion of those SNPs. What we can do is use imputation software to fill in these missing SNPs based on the reference haplotype and leveraging LD between the SNPs.\nThe imputation panels that we have used over the last 10 years started with HapMap, which was the original backbone for imputation to be able to fill in missing SNPs. The goal to define variation greater than 5% in those four collected sample groups. Then, around 2008, 1000Genomes came along. 1000Genomes is actually 2,500 individuals, so it’s a much larger set of individuals than HapMap. HapMap had, for Europeans, 30 trios, and that gave a 120 independent chromosomes to form the backbone, so you really could only get at very common variation. When we went to 1000 Genomes, you could go a little further down the allele frequency thresholds and get down to perhaps 1% or so variation imputed well, because you had more copies seen in the reference.\nAnd today, if you were to do imputation, you would go to the Haplotype Reference Consortium (HRC). The Haplotype Reference Consortium has leveraged all the sequencing studies that have been performed over the last couple of years and aggregated that data to create a new reference panel of haplotypes to use as the backbone of your imputation. The Haplotype Reference Consortium has approximately 60,000 haplotypes available, and you can go down to a minor allele frequency or a minor allele count of approximately 5. As you go down this page, you increase the sample or the number of haplotypes used for the backbone as well as the allele frequency that you can impute the variance to.\nSo here, there’s multiple imputation software that could be used to go from one of these panels to impute those SNPs into your samples that have a chip already genotyped. I’m not going to comment on them.\nOnce you do the imputation, you get post-imputation measures of quality. So, you don’t want to keep all your variants. Some of the SNPs that are imputed aren’t going to be imputed well. So, if you have an imputation quality score close to 1, that means you’ve gotten good imputation quality and you can move on to analyzing those SNPs. However, sometimes it doesn’t work; it could be in LD-poor regions, and so imputation quality can be lower and we exclude SNPs that have low imputation quality.\nOnce you’ve done imputation and excluded variants that have poor imputation quality, you can readily combine information across studies. Here, it shows two different studies, one done on an Affymetrix chip and one on an Illumina chip. You can see that if we try to look at the overlap of SNPs between the two chips, there is a very small overlap between the two. But after imputation was applied to fill in missing SNPs based on the haplotypes, you can see that there is an overlap of these SNPs between the two and many more variants that can be analyzed in the combined study.\nSo, after we do quality control and imputation, we can move to the exciting part, where you want to be, and do analyses. Basically, with genome-wide association studies, we are doing simple linear regression, where we’re comparing the trait value between two groups, and we’re doing this over a massive number of SNPs. So basically, we have a simple model here, where we have Xi equal to 1 if individual i has the A allele at SNP i and trait, and you’re regressing the SNP on a trait for that individual. Basically, you are testing the difference in the mean trait value for individuals that carry the A allele versus those who do not carry the A allele. So, I just described simple linear regression, but for quantitative traits, we’re comparing each marker, the trends in the trait, so the trends and the genotypes, and we can use linear regression and adjust for covariates in that linear regression. For case-control studies or any dichotomous outcome, you’re comparing the marker frequency in cases to the marker frequency in controls, and there are a couple of different tests you can do, depending on your counts and ultimately your research question. And you want to make sure you control for possible covariates. A lot of times, we control for age and sex. Controlling for covariates (a covariate not a confounder) is to increase precision of your outcome, whereas we want to control for population structure using principal components analysis because it is a confounder of association.\nSo basically our null hypothesis is there’s no association between SNP i and the outcome. So, we’re testing β is equal to zero versus the alternative that our β estimate from the regression is not equal to 0. We get effect sizes, so we’re looking at how much of an effect the genotype has on the outcome, standard errors, and p-values. So, you’re getting an effect size, the standard error, and a p-value for each of the hundred thousands to 2.5 million SNPs in your GWAS. So, you’re testing each of those SNPs individually against the outcome.\nNow, we might see a significant result for many reasons. We might see a significant result because there is an actual effect of that SNP on the outcome, but we also might see a significant result because of just chance, some bias, and unadjusted confounding. So, we use statistical tests to determine whether our observed difference between groups is likely due to chance.\nAnd we rely on the p-value, which is the probability of the observed result or something more extreme given that the null hypothesis is true. And if this probability of the event is small enough, we say that the difference is simply not due to chance and we have an actual effect, and we can call the result statistically significant. A point to note is that we never accept the null hypothesis; we just fail to reject it because we might not have enough evidence or power to accept it. We never accept the null hypothesis.\nSo, you might ask yourself, what is small enough? Alright, we’re testing many variants across the genome for association to a trait. So, the simplest way is to adjust your overall α level for the number of tests you’re performing. And so, we do that traditionally with a Bonferroni correction, where you divide your overall αlevel by the number of comparisons, and so you get a new α level, and you compare your p-values that have been generated to this new α level. In GWAS, we’ve had a threshold of significance of approximately 5x10-8, and that accounts for approximately 1 million independent tests that have been understood to be within common variation within the European population. So that’s very key. We’re looking at this number 5x10-8, is really for common variation in Europeans. If you have a sample that is of another ancestry, you might have more independent variants, and therefore your level of significance should actually be lower.\nSo, there are two ways, coming back to combining data across multiple studies, there are two ways to combine data across multiple studies. You can do a combined analysis where you take the individual raw-level data from each of the contributing cohorts and create a huge dataset with all the data together and do the analysis that way. But this is often not feasible because of patient confidentiality and being able to share data across institutions. So, a common approach in genome-wide association studies is doing meta-analysis, where you generate the association statistics within each study, and then you combine those test statistics across studies using meta-analysis.\nThe most popular type of meta-analysis is inverse variance-weighted meta-analysis, also called fixed-effects meta-analysis. It’s been implemented in many software for doing these tests across all the variants in the genome, and it’s a weighted average of the effect sizes from each study, taking the precision of the effect into consideration so that larger studies are given more weight, and smaller studies are given less weights, and these weights are inversely proportional to the standard error, which has your sample size within your study.\nThere are just some equations here. You have the effect sizes, standard errors within each of the case studies. You can then get a weight for the case study and get a pooled effect estimate across each of the studies and a pooled standard error across all the studies. Then you can meta-analyze; you can get a meta-analyzed Z value by taking that pooled effect size divided by the pooled standard error to get a Z-score and then convert that to a p-value for your meta-analysis, which is distributed as a normal distribution under the null, the Z.\nHere are some best practices for using imputation-driven meta-analysis that provides a lot more details on how to operationalize this.\nSo, whether you’ve done a single study or whether you’ve done this meta-analysis, you have a lot of association statistics that you need to look through and understand, and so two different plots are traditionally created to be able to summarize the many tests that are done within a genome-wide association study.\nThe first is the QQ plot or quantile-quantile plot, which gives a visualization of the overall distribution of p-values. What you have on the x-axis is the expected chi-squared statistic, and on the y-axis, you have the observed chi-squared statistic. Under the null hypothesis, you would expect this to follow the 45-degree line. And actually, this is the expected -log10 p-value and the observed -log10 p-value. You would expect these to fall along the line under the null hypothesis. When they do, you can think that you have no unaccounted-for confounding or issues with the association statistics. This is the genomic control λ value, which is the median chi-squared statistic divided by the expected chi-squared statistic, and that is really getting at the bulk of the distribution. The most points you have on this graph are lying right over here because your p-values are normally distributed or uniformly distributed between 0 and 1. Right, you should have a uniform distribution. And so, if this is plotting the -log10 p-values, that means the majority of those points are right down in this lower piece of the QQ plots, and the lambda value is getting at what the median of this distribution looks like. We expect this λ value to be close to 1, and it’s really dependent on the sample size. As you get more samples, you’re more likely to have a larger λ value.\nAudience question: So the question was, why in this QQ plot, do the points dip below the diagonal?\nGina: Here, it’s just random noise. So, you can see, I drew a confidence interval around that 45-degree line, and they all are pretty much within that confidence interval. So, it is probably just random noise. It could be also that we are not powered enough for the association study. So, a lot of early rare variant association tests have seen, you know, points below the 45-degree line, or they have seen the observed distribution below the 45-degree line, and that’s just because they’re not powered enough to detect association well.\nAudience question: This is just simulated real data, so I’m saying it’s random noise. The question was why here were there more points below the diagonal? And this was a simulated graph, so just random, right?\nGina: So that is right. The bulk of the distribution is down here because most of the uniform distribution of our p-values.\nAudience question:So the question is, is the z-score always normally distributed in a GWAS? And if we have adequately done our study, we should be normally distributed. You might not have the power to detect very significant effects, and the Z of β over the standard error is going to be normally distributed for a standard normal.\nGina: So, you can have inflation in your test statistic, and you see that on your QQ plots. So here, I simulated some inflation, and you can see that the QQ plots are deviating from the line pretty early. And so, this could be because there’s population structure that’s not accounted for, some unrelatedness that’s not accounted for, some technical bias, or poor-quality genotypes. But when you see a few points that are deviating from the line, it indicates that there is a problem with your GWAS. In particular, when it’s deviating from the line early on.\nOkay, so let’s take a real example of these QQ plots. Here is from a study of LDL cholesterol, and here you see all the SNPs. Here’s my line right down here. The 45-degree line is all the way down here; you can’t even see it. And that’s because we have really significant results. We’re getting p-values at 10-600, highly significant. But we know we have known variation that we know is associated with LDL cholesterol. And so, if we zoom in on the lower part of the distribution, you can see that at this lower end of the distribution that the observed values are following the 45-degree line and that what we have up here is true associations. The green is genome-wide significant. I removed so when we remove things we’ve already found, we find that it’s behaving pretty normally. But this is including variants that have been shown to be genome-wide significance.\nHere is the second way we summarize the gross distribution of p-values. Here’s for LDL cholesterol. It’s called Manhattan plots because it’s supposed to look like the skyline of Manhattan. And what you see are peaks where we have associated loci, and we have the peaks and multiple. Each of the points on this graph is an individual p-value, an individual association. So, this peak here on chromosome 5 gives you a bunch of variants that are looking like they’re associated with LDL cholesterol. And this is because of the LD. The top variant here is in LD with the next variant, and there’s decay of LD as you go down this line. And so, you would expect to see this in a genome-wide association study, especially when you have imputed data. You want to not see just a single point setting up here and showing significance. If I just have a single point up here in the peaks, there might be a problem with that association because it’s saying this SNP is associated, but nothing in LD with that SNP is associated. So, something to be cautious of.\nYou can really dive down into particular regions with regional association plots. So this is done with LocusZoom, a software tool. You can Google it, but it will take your genome-wide association results and look at a particular locus you’re interested in. Here I’m plotting the SORT1 locus, which has been robustly associated with LDL cholesterol. And what I can see here is: using publicly available LD information, LocusZoom is getting LD information, and you can see that my top variants here are highly correlated – that the reds and oranges and greens are highly correlated with the other SNPs in the region, indicating that this is probably one SNP that’s really associated in the region and these other SNPs that are showing associations are just in LD with that top variant.\nOn the other hand, this is a regional association plot of the CETP region, and here you can see that there are some variants here that are highly significant but do not look like they are correlated with the top variant, indicating that there might be multiple signals in this region. And you would want to do some fine mapping or conditional analyses to get at the multiple signals within this region.\nA key point is that association is not causation. Okay, to take away from these GWAS, the variants or the SNPs that we’re analyzing can have a functional effect on the traits. They could cause an amino acid change, they can change expression of a gene or be involved in the regulation of the gene. But they could also be in LD with a functional variant. So, with GWAS, you’re really getting at different regions of the genome, loci that are associated with disease, and not particular variants that are causal for disease.\nThere are many tools to perform GWAS, developed by individuals at the Broad and Broad affiliates, particularly PLINK and EIGENSOFT, as well as METAL, which has been developed for meta-analysis by the Abecasis group. And also LocusZoom, which were the plots I was showing for the regional association plots.\nSo, after you’ve done your GWAS, there’s a lot more that can be done. So, you shouldn’t think of GWAS just getting a new set of significant loci associated with your trait. You want to start thinking about secondary analyses that you can use and gain further information using genome-wide association studies. And so you could do risk prediction. Can you use a score of genetic variants that have been shown to be associated with your outcome to predict disease? So can we use genetics and GWAS to predict disease? Pathway analyses – are the associated loci linked to a particular biological pathway? Can we learn new ideology about the disease based on the associations we’re finding in GWAS? We can also do Mendelian randomization, where we leverage genetic markers to get at causality of biomarkers.\nOther secondary analyses include estimating the variance explained by sets of SNPs, and this is done through the GCTA software. Now that many GWAS have been performed across a wide range of phenotypes, you can look at pleiotropy. Does a SNP relate to multiple traits? You could fine-map your genome-wide association regions to get at the independent SNPs within a locus. And then LD score has been a technique also developed by individuals at the Broad to be able to distinguish between confounding and polygenicity in genome-wide association studies. And this has been a good tool when you have increasing sample sizes in the GWAS being performed now for common diseases.\nIn summary, GWAS have been successful at locating regions of the genome with associations to complex traits. Many of the loci are non-coding with no gene function and need to be further investigated through functional studies and follow-up. And I think there’s more genetic variance to be found as we grow in sample size. We’re getting more significant genome-wide association signals; they just happen to be of a lower effect size. And we can really leverage non-European individuals. Most GWAS have been done in European individuals, and non-European individuals will gain us additional information. So that’s all I have for today. Thank you very much."
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "Chapter 3: Technologies",
    "section": "",
    "text": "Chapter goals:\n\nUnderstand the basic concept of SNP chip genotyping (also called beadchip, bead array, microarray), and the high level data handling process.\nGain introductory knowledge of Next Generation Sequencing (NGS), and implementation of this technology to sequence DNA."
  },
  {
    "objectID": "chapter3.html#sec-section1",
    "href": "chapter3.html#sec-section1",
    "title": "Chapter 3: Technologies",
    "section": "3.1 SNP array genotyping",
    "text": "3.1 SNP array genotyping\n\nThis video by Dr. Mészáros is focused on understanding the SNP chip technology: how SNP chips work and why they are useful.\n\nTitle: SNP Chips (Introduction to genomics theory)\nPresenter(s): Gábor Mészáros, Genomics Boot Camp\nLength: 28:05\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nPowerpoint slides for this video are available here."
  },
  {
    "objectID": "chapter3.html#sec-section2",
    "href": "chapter3.html#sec-section2",
    "title": "Chapter 3: Technologies",
    "section": "3.2 Next Generation Sequencing",
    "text": "3.2 Next Generation Sequencing\n\nThis section gives an introduction to Next Generation Sequencing. The first video by Dr. Kiel gives a very brief introduction to the concept of DNA sequencing, and how nucleotide bases are “read” from the DNA. The second video from ClevaLab gives a more detailed view of Next Generation Sequencing, how it differs from Sanger sequencing, and the different steps involved in sequencing a DNA sample with NGS methods, including library preparation, Sequencing by Synthesis and the sequencing reaction, cluster generation from fragments, indexing, filtering, mapping, and demultiplexing the reads.\n\n\nHow to sequence the human genome\nTitle: How to sequence the human genome\nPresenter(s): Mark J. Kiel\nLength: 5:04\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nNext Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nTitle: Next Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nPresenter(s): ClevaLab\nLength: 7:37\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section1",
    "href": "chapter2.html#sec-section1",
    "title": "Chapter 2: The Genome",
    "section": "2.1 Organization of the genome",
    "text": "2.1 Organization of the genome\n\nThe following video from HMX Genetics gives a brief overview of the human genome: What is DNA? How is DNA organized? How is DNA regulated? What are genes?\n\nTitle: An Introduction to the Human Genome\nPresenter(s): HMX Genetics, Harvard University\nLength: 5:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section2",
    "href": "chapter2.html#sec-section2",
    "title": "Chapter 2: The Genome",
    "section": "2.2 Genetic variation",
    "text": "2.2 Genetic variation\n\nThis section introduces a brief overview on genetic variation. The video below from Precision Health gives an introduction to two types of genetic variation: Single Nucleotide Polymorphisms (SNPs) and Copy Number Variants (CNVs).\n\nTitle: Genetic Variation and Mutation\nPresenter(s): Precision Health\nLength: 4:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section3",
    "href": "chapter2.html#sec-section3",
    "title": "Chapter 2: The Genome",
    "section": "2.3 Evolutionary signatures",
    "text": "2.3 Evolutionary signatures\n\nThis section describes how genetic variation is introduced into populations. The first video by Dr. Pamment gives a brief overview of the different events that can occur to introduce genetic variation, including mutations, crossing over, independent assortment, and random fertilization. The second video from Dr. Schaffner gives a greater in-depth look at genetic variation introduced through natural selection and gives the specifics of those selection processes.\n\n\nOrigins of Genetic Variation\nTitle: Origins of Genetic Variation\nPresenter(s): Jessica Pamment, Professor, DePaul University\nLength: 4:47\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nNatural Selection & Human Genetic Variation\nTitle: MPG Primer: Natural selection & human genetic variation\nPresenter(s): Stephen Schaffner, Computational Biologist, Broad Institute\nLength: 54:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.html#sec-section4",
    "href": "chapter2.html#sec-section4",
    "title": "Chapter 2: The Genome",
    "section": "2.4 Linkage disequilibrium",
    "text": "2.4 Linkage disequilibrium\n\nThis section of videos from Dr. Mészáros dives deep into the concept of linkage disequilibrium: what it is, how it affects and why it is important to genetics and genetic analyses, advanced methods of calculating linkage disequilibrium, and the different types of LD measures.\n\n\nWhat is linkage disequilibrium?\nTitle: What is linkage disequilibrium?\nPresenter(s): Gábor Mészáros, PhD\nLength: 12:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nMeasuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium?\nPresenter(s): Gábor Mészáros, PhD\nLength: 11:31\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n(Advanced) Measuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium? (ADVANCED)\nPresenter(s): Gábor Mészáros, PhD\nLength: 9:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nComputing linkage disequilibrium\nTitle: Compute linkage disequilibrium (Part 1)\nPresenter(s): Gábor Mészáros, PhD\nLength: 10:18\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter10.1_transcript.html",
    "href": "chapter10.1_transcript.html",
    "title": "Chapter 10.1: A Career in Psychiatric Genetics (Video Transcript)",
    "section": "",
    "text": "Title: How does Genetics Affect our Mental Health?\nPresenter(s): Cathryn Lewis, Alex Curmi\nYou have found the “Thinking Mind” podcast [Music].\nAlex: Professor Catherine Lewis, thank you for coming on the show.\nCathryn: Pleasure, Alex. Thank you for the invitation.\nAlex: I’m sure there’s a lot that we’re going to get to talk about in terms of genetics, psychiatry, psychiatric conditions, psychology, and how we can, using the art of maths and statistics, come to unravel some of these mysteries. But one of the great things about talking to interesting guests such as yourself is the ability to learn about Career Development and career paths, and to learn that career paths aren’t always linear. And maybe you could tell us a bit about how, how do you become a professor of genetics, statistics, epidemiology,\nCathryn: yeah, yeah, So my background is, it’s not in Psychiatry or psychology, but in maths. I did a first degree in very pure maths, um, and because I was passionate about maths when I was at school and really enjoyed it. But as the studies went on, I realized that that sort of esoteric theoretical side of Academia was not really what I was interested in. I wanted things that were much more practical, that were much more aligned with solving problems in everyday life. And so I moved on to do a master’s degree in statistics and did a lot of data analysis. Statistics is a fabulous area to study because it gives you access not just to the math side, but so many different areas that people use statistics in. And I was lucky enough during my PhD to start analyzing genetic data and realized that I’m was really interested in it and also that it was a blossoming, developing area of medical research. And so that launched me in my research career.\nUm, I should say that I’ve worked across different medical areas during my career. I started working in cancer, looking for genes for breast cancer, and I’m a very, very minor author in the paper that found the BRCA1 gene with high-risk variants for breast cancer, for example, and I’ve worked in autoimmune disorders, and really applying my statistical techniques where they can be useful. But what I’ve focused on in the last 15 years is working in the genetics of mental health disorders and of Psychiatry. And like many things, this was a very serendipitous chance. I remember presenting a poster at a conference on autoimmune disorders, and a psychiatrist came up to me and said we could do this in schizophrenia, and I started working with him in schizophrenia. And then Peter McGuffin, who was head of the social genetic and developmental Psychiatry Center where I am now am, saw the potential to bring my skills into mental health genetics and approached me, and 15 years later, I’m, um, you know, fully embedded in the genetics of Psychiatry.\nAlex: Have you found that serendipity is an important part of forging a career path?\nCathryn: Um, for me, it certainly has been. I know people have different approaches to planning their career; mine has always been quite short-termist. I’ve always, you know, I’ve always really enjoyed what I’ve done, and I’ve taken decisions based on, “Oh, that sounds like a really good project to be involved in,” “Oh, I’d really like to work with them.” So, I’ve never had a, you know, a long-term goal of what I would achieve, and I know that’s not the sort of advice that,\nAlex: it’s not the standard, right?\nCathryn: It’s not... it’s worked very well for me, and I’ve really enjoyed the flexibility of academic careers to hop around and to build up experience and to learn, you know, from different projects and different people - things that really produce a richness to the next, uh, the next thing that you do. And I should also say, perhaps, that I worked, I worked part-time for 10 years, years while my children were young, and again, really enjoyed that flexibility to continue with the bits of the role that were important to me.\nAlex: So you had the kind of experimental attitude towards your career, trying different things, and you were guided by your sense of fascination, which is actually career advice I give to people all the time because I think we’re conditioned by our educational system really to meet goals that are set by other people. And I also think that fascination is a kind of sense you can develop. The more you’re led by it, the more you can actually sense how actually interested or engaged am I in this particular thing. But so many people do things that they’re not engaged with for their whole lives, so it’s nice to hear that you’ve been able to escape that particular trap,\nCathryn: yeah, yeah, now that’s really good to hear that perspective on it and I agree with that kind of fascination, and of course, that’s what scientific research and exploration are all about. But maybe I can put in a plea, you know, I come from a maths background, and a lot of science now is about large numbers, it’s about AI, it’s having the skills to not only do molecular work (you know, in the lab or working with people) but to be able to analyze that data and put things together. And I would recommend anyone that gets the chance to develop their skills further in maths or statistics, that really does open doors to the breadth of different things that you can get involved in. And you know, particularly, young people, if they get the chance to study maths, you know, at a level beyond the age of 16 where it’s compulsory in the UK, you know, even downstairs dream that really opens doors having those skills and that showing your ability to be able to think analytically. And as a mathematician, of course, I’m biased, I do think mathematical skills open doors in many fields.\nAlex: Absolutely. So maybe you could tell us a bit about statistics. How does statistics help us to unravel some of the mysteries, particularly in Psychiatry, that we’re trying to figure out?\nCathryn: So one of the things that statistics enables you to get a handle on, is the risk of something happening, and that’s what we’re very interested in as individuals. You know, starting something very basic that we think about a lot is family history of psychiatric disorders. So we know, you know, we know from scientific studies, from our personal experience, that mental health disorders tend to run in families. We very often see that someone who’s diagnosed with depression, one of their parents or a more distant relative also has depression. And looking at families, we’ve been able to get risks of people developing a disorder themselves given their family history.\nBut I think people are not always very good at interpreting risks appropriately, and we see this very much so in newspapers, for example. We see that something is told that it increases our risk of something really bad happening by five times, and that’s quite scary to see, to know that, “Oh, I’ve drunk too much coffee, I might be at five times the risk of something else happening.” But that five times is a relative risk, that’s compared to it not happening. The other piece of information that we are not good at looking at is the absolute risk. So if the chance of something happening is very rare, say happening one in ten thousand times, then of five times, it’s still very unusual that it happens. And I think we’re very bad at interpreting risks because we need to put together that relative risk, which is often quite scary, with the absolute risk, which can bring that back to being quite reassuring.\nAlex: yes, so, so relative risk is like how, how much more likely are you to develop a condition given a particular state of affairs? That’s relative risk…\nCathryn: Exactly.\nAlex: Risk is just in the population, who’s going to develop this? How likely is it to develop a condition,\nCathryn: yeah, exactly. And so, to bring that back to mental health, so in schizophrenia, for example, we know that someone who has a parent who’s been diagnosed with schizophrenia is, you know, maybe eight or ten times more likely to develop schizophrenia themselves, and that’s a really scary number, and I know there’s a lot of concern, quite justifiably about that, but to put that in context, if the population risk of schizophrenia is about one percent, then being at an eight-fold increased risk of developing schizophrenia is a risk of eight percent. So yes, that eight percent is higher than the population risk, but that still means that there’s a 92 percent chance that that doesn’t happen, so it’s actually doesn’t develop schizophrenia.\nAlex: so really, we don’t… when we see genetic data, particularly I find an issue is when scientific data is filtered through the media because the media obviously are incentivized to produce something eye-catching or sensational. But leaving that aside, we don’t have good intuitions about how to evaluate risk as we see it as reported by scientific data, it sounds like.\nCathryn: Yes I completly agree with you, and there is, you know, as you say, the newspapers, journalists have to, you know, sell their news, and so they tend to report the things that are most likely to be eye-catching, you know, more sort of clickbait. And so they tend to report the relative risk rather than the more reassuring absolute risk.\nAlex: The other thing, I guess, the very traditional view of genetics is that our DNA is quite has a very high determination on what happens. You know, if you’re, if you have the particular genes for condition, you’re gonna get that condition. That’s how we tend to think of it classically. But we know more and more that there are some conditions which have a huge amount, a huge variety, of genetic influence, so much more than one gene. Say if you have a condition like Huntington’s disorder, that’s one gene, and if you have the gene, you’re going to develop that condition. But most things, is it fair to say that most things aren’t like Huntington’s?\nCathryn: It’s very fair to say so. So some traits are exactly like Huntington’s, like cystic fibrosis or variants in the BRCA1 gene for breast cancer, where those genetic changes on their own put someone at a very high risk of developing a genetic disorder, and that’s the sort of genetics that we’re taught in school, and it leads us to think about genetics as a black and white, yes, no, you know, we’re at risk or we’re not at risk.\nAlex: You have the fat gene, exactly, exactly.\nCathryn: Well, and can I say we all have the fat gene, we all carry that gene, but what differs between us is the changes in that gene, so specific changes at the level of the DNA variants that we carry, and that’s what makes a difference between us.\nAlex: Tell me more about that.\nCathryn: So, our genes are made up of a long string of DNA variants, so the base pairs A, C, G, and T, and about 99% of our genome is exactly the same. You know, you, me, and everyone listening to this podcast will have exactly the same base pair at exactly the same place in our DNA. But one percent of our genome may differ between us, and our genome is over three billion base pairs long, so one percent still gives us quite a lot of opportunities to differ, and some of those changes have a very big impact on our risk of different diseases, different conditions. But most of those changes that we see in our DNA have a very modest effect on disease risk, and much of my research is in the genetics of depression, and for the last 10 years, we’ve been carrying out studies to identify the genetic attributions to depression. And we do this in a classic epidemiological design, that’s a case-control study. We collect, we perform recent studies with groups of people who’ve been diagnosed with depression, and we compare their DNA to people either from the general population or people that we know have not been diagnosed with depression to date. And we step along the genome looking at multiple different places on the genome, and we look at the frequency of that change, and we say, okay, this site in the genome, people either have a C allele or an A allele, there. What’s the frequency of that A allele in people with depression, and with people without depression? And then we do a very simple statistical test and say, does that differ? And we do that at several million places through the genome, and by carrying out really careful statistical tests, we can identify specific changes that seem to increase people’s risk of developing depression,\nAlex: so specific, very specific genetic differences that are more correlated with someone having depression.\nCathryn: Exactly, but the actual risk conferred by each of those individual changes is very, very low. We’re not in the single gene Mendelian disorder territory that we’ve been with Huntington’s disease. We’re now in a polygenic genetic model, which is what seems to be underpinning depression, schizophrenia, bipolar, or all and also most physical disorders that really have an impact on people’s lives. We’re not looking for single genetic changes; we’re looking for genetic changes spread across the genome, each of which have a very modest effect on people’s risk of developing a disorder. But when we put them together across the genome, that gives us much more information on risks.\nAlex: So really, the role of statistics in all this is to churn through huge amounts of data and figure out what are the patterns, what are the meaningful patterns that we can extract from this data?\nCathryn: Exactly, and this really is big data analysis. And it’s big data in two ways. Our genome is very big, and we need to look, you know, across the whole of that genome. And also, because the risks conferred by each change are very modest, we need really large studies here to be able to identify those changes. So in depression genetics, 10 years ago, we carried out a very large international study through the Psychiatric Genomics Consortium that pulled together internationally 10,000 people with depression and 10,000 people without depression. And we were really excited by this. It was the largest study that had ever been performed in depression genetics. And we found absolutely nothing.\nAlex: Wow.\nCathryn: And this was a massive study; there were millions of research funding that had gone into this. And of course, it was, you know, 10,000 people, you know, their time and their energy, their participation in the study, and that was hugely disappointing at the time. And we stepped back and we thought about this and we looked at what had happened in other studies like in schizophrenia and bipolar and in studies in heart disease and rheumatoid arthritis that have been more successful. And we started to think, well, what is it that’s different about depression? And we realized a couple of things. That first of all, you know, depression is much more difficult to diagnose and other many of those other disorders where genetic studies were being successful 10 years ago. And so there was probably a lot of heterogeneity in those studies that we weren’t taking account for. And also, the genetics of depression… genetics is a much more modest contribution to risk in depression than it does in schizophrenia. So that when we look at twin studies, the heritability of depression is less than 40 percent compared to up to 80 in schizophrenia. So that means that in addition to genetics contributing, there’s much more scope for other risk factors to contribute. And we think of this, I think of this in the context of the biopsychosocial model. So we can investigate the bio part, but we need to acknowledge, particularly in depression, that there’s much, much more going on from the psychosocial part of this.\nAlex: This is such an interesting conversation because obviously you’re coming from the big data side of things, and I’m a clinician. And it’s so interesting because from what you’re telling me, it makes sense in light of my clinical experience. That, from my anecdotal clinical experience, it makes sense to me when you’re telling me that there’s a huge or a bigger psychosocial influence in depression with schizophrenia because with depression, for example, you can see very straightforwardly and intuitively how lifestyle factors, whether someone has a job, whether they have a good relationship, whether they have friends, whether they have whether they have meaningful work can influence whether or not they become depressed. And one so, let’s, let’s continue on the polygenic front. So, are we close to figuring out, for example, how many genetic changes might be involved in a condition like schizophrenia, or is that still mysterious, the total number of genetic changes involved?\nCathryn: We are making substantial progress. So, I talked about the depression study where we’d found absolutely nothing.\nAlex: You know, why did you guys find nothing, do you think?\nCathryn: Because um, because um, statistically, studies are always a mix of signal and noise, and you need to get enough signal to stand out against the noise. And that study, even though it had 9,000 people who generously contributed to our research, it just wasn’t enough. And so, having stepped back and thought about this, it had… we had the confidence to say we just need to continue, this is about sample size and uh, and so we went out, we increased the sample size substantially, we collaborated with a lot more people, and our latest genetic study, which is still unpublished, we’ve been talking about it in conferences, we now have identified 500 genetic variants across the genome which we believe play a role in depression.\nAlex: How many people did it take?\nCathryn: That’s a really good question. It’s taken half a million people with depression to find that. And some people, thinking of this in the biopsychosocial model, some people say, “Well, why do you bother? If you really need that many people to find these very modest genetic changes, you know, well, shouldn’t we be focusing our time and our energy and our money on doing different things to solve depression?” And I think my pushback on that is that we need to be doing everything that we can in our depression and all mental health disorders. You know, it’s a major problem for society, for those people who’ve been diagnosed, and the families that care for them. And we need to be doing everything we can, taking every angle that might help us diagnose, treat, maybe even prevent these disorders. And genetics allows us access to that underlying biology, and that’s really difficult to get a handle on in mental health disorders. Exactly, and we have no biomarkers; you know, we can’t do a blood test and identify people who are at high risk or use that for a diagnosis. And so, genetics, I’m hoping, will give us a handle on that underlying biology. It’ll tell us more about why people get depressed, what the pathways are for that, what are the changes in the brain, in the body, that can lead towards depression. And then, of course, that gives us perspectives on how we could help prevent that. You know, does it give us new new drug targets that we might be able to develop\nAlex: or other interventions like brain stimulation is becoming more involved?\nCathryn: Yeah, exactly, yeah.\nAlex: It’s really interesting. So, I talked to this week, I talked to Nolan Williams, who is a psychiatrist at Stanford, and he is an expert in brain stimulation. And he actually said from his work, which is doing very intense brain stimulation treatments, magnetic stimulation, he’s found out, by doing that and studying people in MRI scans, that there’s a subset of people with depression where the depression seems to be correlated with, if not caused by, one area of the brain activating before another area of the brain - the anterior cingulate cortex activating before the dorsolateral prefrontal cortex (I know that’s a mouthful, I’m sorry). And through stimulation, you can reverse that temporality, where one area gets activated before the other and regulates it, and that seems to have this amazing effect on people’s clinical presentation; they get a lot less depressed.\nSo absolutely, the biology is foundational. Do we know, do we have a sense of how many, or how many changes have we found genetic variances with conditions like schizophrenia, bipolar, and is there an overlap between those two?\nCathryn: For schizophrenia and bipolar disorder, we’ve found over 200 variants for each of those. But for all of these, we know that that’s the tip of the iceberg. You know, there is a lot more to find. There is overlap between them, there’s overlap between all psychiatric disorders, and the model, the way that we think about this currently is that there are probably some genetic changes, some genes that are involved that are relevant to the brain in general and across all different diagnoses. And then there are likely to be other variants that are unique to maybe subgroups of disorders or single disorders and conditions themselves. And so, but working across diagnoses and working transdiagnostically as well, genetics again should help us to do that and show where the similarities and where the differences lie.\nAlex: Because really, what this genetic picture (I’m not sure if you’re aware of this) is very challenging to our diagnostic framework as clinicians because we put diagnoses in categories or buckets. We say, this person has a psychotic condition such as schizophrenia, or this one has a mood disorder such as bipolar when it seems to be somewhere in between those two, we call it schizoaffective disorder or depression, or what have you. And this picture where it’s very continuous, where it’s loads of different genes and somewhere can be at any point on a spectrum, and then those genes, which I’d like to talk to you about later, have to interact with environmental stimuli. That paints a picture where people really aren’t in one category or another, but everything is kind of a spectrum. Would you agree with that notion?\nCathryn: Yes, and probably a multi-dimensional spectrum, and none of this is to be easy, and how might genetics help with that? Well, I mean, you’ve talked about people not fitting in with a particular diagnostic group, and I know people’s diagnoses change across life, and their symptoms change. And I would hope that genetics might be able to give us a perspective on that because our genetics stay constant all through life. And that’s one of the real values of genetic tests, is that the variants that we’re talking about, we inherit those from our parents, and they remain stable all through life. And so if we could get some information on how genetics might help predict prognosis, how might people’s mental health play out across their lifetime, what does their genetics indicate they might be at highest risk of developing, I would hope that that sort of information might become useful diagnostically. It’s never going to be the whole answer; nothing is going to be the whole answer in psychiatry; it’s all too complex. But we need to be taking, we need to be working together. I, as a geneticist, need to be working with you as a psychiatrist. We need to be working with people who do brain stimulation and do neuroimaging and working with psychologists and sociologists and looking at this in its entirety to see how we can make progress.\nAlex: I mean, as a clinician, I think having more genetic information about a given patient would be incredibly helpful. So if, because we get wrapped up in diagnostic mysteries all the time, and it can be very puzzling, and of course, people change subtly from day to day if they’ve had enough sleep, what medication they’re on, but if we can get some hard data, for example, tests which are sophisticated enough to say, “This person has 150 of the 200 changes that are associated with bipolar affective disorder, and they have 50 of the ones associated with schizophrenia,” for example, I can really see that that would provide a huge value to clinicians and to patients themselves. Because a lot of the trouble of being unwell is the huge amount of the mental suffering of being unwell is the mystery. And obviously, like you’re saying, we don’t want to take a too hard line approach where genetics is everything, but anything that can help us shine the light on these complexities can be very therapeutic I think.\nCathryn: That’s a really good point, and we’re starting to do that through polygenic scores. Now, these individual variants that we have, the 500 variants for depression, over 200 for schizophrenia and bipolar, individually, they give us tiny amounts of information. We can’t use them on their own. But when we put them together across the genome, we can construct polygenic scores, which does exactly what you’ve just said. It looks at how many of those risk variants does someone carry. Now, you know, we all carry some of those risk variants, and most people will carry an average number, so their genetics of depression won’t tell them very much; they’ll have an average loading for depression. But some people might carry high polygenic scores for depression because, by chance, they’ve inherited more of those risk variants from their parents than other people. And so that’s something that we’re starting to look at: how these scores allow us to look at the spread of risk in the population. So, so far, the amount of risk that they explain is quite low. For example, in schizophrenia, where perhaps we have the strongest genetic profiles to date, if you look at the people who have the highest polygenic scores, so the 10% of the population who are at the highest risk and the 10% who are at the lowest risk, the difference in their risk of schizophrenia is about 16-fold, to use a relative risk.\nAlex: So the 16 times more likely.\nCathryn: yes, exactly. So the 10% of people with the highest risk are 16 times more likely to develop schizophrenia than the 10% with the lowest\nAlex: which is fairly substantial because the risk of schizophrenia is about one percent at baseline, so that has a 16 percent,\nCathryn: but that does leave out the middle 80 percent of that distribution and another way of looking at that is to say, well, what might we want to do about this? Another perspective on this is to say, well, if we could identify people who were at the highest one percent of risk, so those one in a hundred people who have got the very highest genetic loading for schizophrenia, and we compared their risk to everyone else, what would that be? How big would that be? Is it really worth knowing, identifying that the very, very highest? And there, there’s a six-fold increase in risk, so that’s much more modest. It’s really saying that even though we know quite a lot about the genetics of schizophrenia, it’s not in itself giving us enough signal yet that it would be useful.\nAlex: But did you say, so the one percent, the top one percent, have a six-fold increase compared to everyone else? Which proportion had the 16-fold?\nCathryn: Uh, so the 16-fold risk was looking at the top and bottom ten percent. Okay, okay. So they’re looking at the real extremes, and this is, you know, it’s statistically valid, but it’s a statistic that says, “Right, if we look at the real extremes, what does that tell us?” And that’s really useful information for research studies, we might use that a lot to say to compare, you know, people who’ve enrolled in research studies who have the highest and lowest risk. But that’s not so useful for you as a clinician. And as a clinician, I think, you know, focusing on the people at the very highest risk and saying, “Does this tell me very much?” and at a population level, I think it doesn’t. But where it might be useful is, you know, where someone’s showing symptoms. So maybe someone’s had their first episode of psychosis or in an ultra-high-risk group, you know, including genetics together with all the other information you have on your patients, that I think where it might be useful.\nAlex: And it could help to advise individuals or their doctors on what lifestyle factors to encourage or avoid. So something we talk about a lot on this podcast is the association between cannabis and psychosis, which is a bit of a tricky one to explain in that the majority of people who use cannabis won’t encounter any issues, which is important when you’re thinking about legal policy and things like that. But a substantial minority or a significant minority can develop a psychotic illness, and this has been studied epidemiologically by Marta de Forte in a huge multi-centered trial where there’s a clear correlation between cannabis use and development of psychotic symptoms. And I think even anecdotally, many, many people know that one person when they were growing up who smoked a lot of cannabis and ended up developing a mental health condition of some kind. So do you think these polygenic risk scores, particularly when they’re more sophisticated, when we know more of the genetic variants, we’ll be able to tell us, “You need to stay away from this, that, or the other?”\nCathryn: I think that would be really useful, and we’re certainly heading in that direction. Um, and I have the pleasure of working with Marta de Forte and hearing about her wonderful research. And I think that’s really useful or potentially useful in a public health message. Um, but the reason that it’s useful there is because we have the genetic signal and we have an intervention. You know, we know that, well, with a really high genetic loading, maybe you really shouldn’t smoke cannabis. So knowing the genetics is not enough; we need to know what we do about that, and I think the genetics of schizophrenia and cannabis is the perfect example of that.\nAlex: Or someone at high risk of depression, say as a teenager, you could teach them certain skills, cognitive-behavioral skills around mental flexibility, because mental flexibility is really important for treating and preventing depression, for instance.\nCathryn: Yeah, so exactly. And also maybe the potential of exercise, which many people would say has a role in both preventing and hastening recovery from depression. Um, and it’s very easy for us sitting here to come up with these potential examples of where it might, where genetics might be useful. But, you know, we’re not, we’re not very good at assessing risks; we’re not very good at dealing with genetic information because as a society, we’ve never really had to do that before.\nAlex: It’s counterintuitive.\nCathryn: It is. And I think I worry slightly about if we’re going to start telling teenagers, “Well, you shouldn’t smoke cannabis because you have a high genetic risk of developing schizophrenia or psychosis.” That’s a really difficult message to give a teenager, and I think…\nAlex: it’s not easy marketing.\nCathryn: We’ve got a huge way to go to make sure that our communications of genetics are done in a way that’s helpful to people, that is not scary, that’s going to be advantageous, that’s not going to increase stigma, but it’s going to be helpful to everyone involved.\nAlex: And I suppose in a way that’s accurate because, you know, we’re having a whole long phone conversation, just to unpick the different pieces of this, and we have historical examples, obviously, of genetics and a genetic argument being misrepresented in order to justify an ideological goal or a political goal, things along those lines. So, there’s one issue with our species, I think, is we develop technology a lot faster than we develop an ability to understand it ethically. If we develop atomic weapons and then we have to figure out how we’re going to think about this in an ethical framework, and I think the same could be said with genetics because maybe I could ask you, do you think there’s a potential dark side to genetics and improving our genetic understanding, that it could be used for harmful means in the future?\nCathryn: I think the potential is certainly there, and we need to be really careful about how we talk about genetics and increasing people’s genetic literacy, so that they understand that genetics is not just a yes-no, that people are at high risk of a disorder or not, but it is a continuous measure. Measuring someone’s genetic liability to depression, to schizophrenia, you know, it’s like measuring someone’s height; it’s not a yes-no, it’s a, you know, people are most people are average risk, some are at high risk, and some are at low risk. But we’ve also seen, I think, areas where genetics can be misused. So, we’ve seen the genetics between different population groups around the world being used as a justification for discrimination, being jumped on by the far right to justify their perspectives. Another area that we’re seeing that worries many of us in genetic research is these polygenic risks for disorder being used in places that are not appropriate. And we see, we’ve talked about the schizophrenia risk being useful, but not on its own; it needs to be in context with everything else. But there are some companies that are offering genetic risks as part of embryo screening in in-vitro fertilization, and they do that because we have the technology, and they’re applying that. But I think we have to be really careful in those cases that what we’re that what, the way that we’re applying our genetic information is really underpinned by sound ethical and statistical concerns.\nAlex: Do you think, in the case of those companies, is the issue that they’re falsely representing how useful the technology is, how accurate it is, or is it just that it is actually has proven utility in screening for certain conditions, but they’re just kind of marching ahead without any kind of thought as to the ethics?\nCathryn: I wouldn’t accuse any of those companies of marching ahead without considering the ethics; that would be unethical of me. But from what I have read about the science and the risks conferred, it is very difficult for me to see that making a decision about which embryos to implant, on the basis of, for example, a polygenic score for schizophrenia, is justifiable. And of course, the concerns downstream are, you know, the impact on stigma, the impact on this idea that we could remove mental disorders by a simple genetic test. It’s really worrying, it’s completely inappropriate, and I think we need much broader societal discussions.\nAlex: And it sounds like there’s still too much mystery in this whole process to make those claims and to market these things as sort of… I can easily fantasize about how the marketing has a higher degree of certainty.\nCathryn: Exactly, and I think that there is so much noise. I mean, to put this in context, the risks that we have for schizophrenia account for about seven percent of overall understanding of risks of schizophrenia. So we can estimate what’s going on in that seven percent, that leaves 93 percent that we’re not including in that calculation, in that assessment.\nAlex: Um, how does the environment interact with our genetics to influence a particular presentation?\nCathryn: That’s a really good question, and we’ve this concept of gene-environment interaction has has been a part of research studies in for decades now, and our understanding is that for most examples, that it’s not an interaction but it is an additive, a combined effect and so what I mean there is that the genetics and the environment combine together to cumulatively give a risk of disorder, but they don’t interact on a statistical level. Yes, so what I mean by interacting here is that, well, let’s look at this the other way around, that genes and environment seem to contribute independently, so someone’s genetic risk to a disorder will give them a certain amount of risk, their environmental component will give you another risk, and we can just add those together. If there’s an interaction, that’s saying that the environment, the way that the environment contributes depends on the genetics, and that’s not what we’re seeing in most cases. So, you know, our intuitive understanding, I think, or a useful thought was, well, if people have really high genetic risk and a poor environment, maybe that will make their risks soar, you know, maybe the combination of two bad things will really lead to a much, much higher risk.\nAlex: You think of a compound,\nCathryn: yeah, but that’s not what we’re seeing, we’re seeing that there’s an additive effect, and the evidence for that is still quite modest in mental health disorders, but that’s what we’re seeing in heart disease, say, where we’ve got in a much better handle on traditional risk effects, you know, from obesity, from cholesterol, and we combine that with the genetics, and they do seem to be additive. So, I always, as a statistician, push back when people talk about interactions because what we seem to be seeing is more of an additive joint effect.\nAlex: that maybe doesn’t fit, maybe there’s a that doesn’t fit with the what we understand about some of the biology because my understanding is that certain genes can be switched on or off depending on environmental triggers, but that concept, the idea that genes can be switched on or off, does that fit with your statistical understanding?\nCathryn: I think it does just because these models, you know, are complex, and I don’t think that that model as you’ve described it would conflict with an additive effect. But we have a long way to go, I mean, you know, we’ve talked about the rapid progress we’ve made in genetic studies over the last 10 years, and as our understanding improves, as we know more about the genetics and the biology, we may, you know, we may well find interactive effects, but what I think we can say is that they’re not, you know, they’re not the overwhelming signal we’re finding.\nAlex: because I think maybe the way I was using interaction was different from the way you I think you meant interacting mathematically and I meant interacting as in the environment, the environmental stimuli switching on a gene or off.\nCathryn: Yes, you’re right, yes, the statistical and this biological terms are slightly different and emphasize that fact we need to work together, we need to combine all these different perspectives.\nAlex: So we’ve talked a lot about psychiatric conditions, you know pathological states, but let’s talk about a non-pathological but still psychological state like personality. Do does genetics have an influence on our personality?\nCathryn: Genetics does have an influence on our personality, and what a question that we should really be asking is how big is the genetic influence? Um, and the influence on personality is fairly high, and of course, that has an impact for mental health as well. So neuroticism as a personality trait is highly correlated with depression.\nAlex: And what is neuroticism? I should point out to our listeners. Okay, there’s two things I want to point out. First thing is, there’s the way personality is used technically. I mean, that’s the way we use personality colloquially, and we run into this when we, for example, talk about this or personality disorders. When we’re talking about personality technically, we mean a set of traits that are stable across the lifetime that influence someone’s interaction with themselves, the world, and other people. That’s my elevator pitch definition of personality. Um, I think that’s really important to keep in mind. That was the second thing I was going to point out, which I can’t remember now. Um, neuroticism is used in many different contexts in Psychology and psychoanalytic literature. What do we mean when we talk about neuroticism in this context?\nCathryn: So in this context, neuroticism through a series of questions that, um,\nAlex: so I think of it as a sensitivity to stress. So how much emotional dysregulation do you experience per unit stress? Like you take person A and person B, they both lose their jobs, person A will have a particular emotional response and person B will have a particular emotional response, and it’s quite likely they’ll be different. People are different, they respond emotionally differently to roughly equal units of stress. But neuroticism doesn’t have a great name for PR, so that’s a problem that’s happened, but, um, yeah, how, so how, on average, what kind of influence is genetics having on our personality?\nCathryn: So genetics does have an influence on personality, and we see that from studies all around the world, from twin studies and genetic studies. You know, we know there’s a big contribution, um, in exactly the same way as there’s a contribution towards mental health disorders, and it’s exactly the same sort of model that it is polygenic. You know, that we’re starting to identify individual variants that contribute to personality traits, to well-being, to whatever way we want to measure someone’s personality. But again, each of those individual variants has a very small effect, um, and we’re again starting to identify those polygenic profiles that that contribute to someone’s personality.\nAlex: I think people will have an inherent resistance to the idea that there is some genetic influence on personality because it has that flavor of biological determination, and personality is really important. So if you take a trait by conscientiousness, that’s people conscientiousness is highly correlated with life success, and that subdivides into industriousness and ordinariness. So industriousness is literally how hard you work. So obviously, you could see how that would have a correlation with success. If people were to find out that whether genetically or otherwise they have a particular, because you can do a test and find out what your personality is or what traits you’re high in or low in, is that is that a, is that a prison sentence? Are you bound by that? Or are there things that you can do to change your personality traits?\nCathryn: I want to push back on that phrase prison sentence, absolutely not. None of the genetics that we’re talking about in this context, these polygenic models, none of them are deterministic. You know, they are probabilistic, and they combine with the environment, with your family, you know, with society that you’re in. They can give you a certain inherited predisposition that might push you down a certain road to start with, but that genetics is always going to be mixed with everything else that you’re exposed to.\nAlex: What are some tips or maybe principles you’ve discovered for building a successful career in science? Which is a very difficult thing to achieve. What are some things you’ve learned along that along that road?\nCathryn: So building a successful career, so many different things. I think flexibility and resilience is one of it. Science itself is serendipitous, you know what, whether the studies you’re involved in, you know, get funding, have a successful result, are fashionable, and are popular in the scientific environment, all of those depend quite a lot on luck. It depends a lot on who we work with, you know, having supportive mentors and supervisors who can assess your strengths and see, you know, how you can develop best and what direction you could perhaps be moving in that would help you thrive. Um, it is really important. But also that ability to bounce back, you know, to be working in that personal environment, that professional environment where when your paper is rejected by a journal for the third time or your grant application doesn’t get funded again, you know, that being able to think “right, what’s Plan B here? where do I go next? and how can people help me achieve what I want to achieve?”\nAlex: So your attitude towards failure is very important.\nCathryn: Yes, I mean, you know, I sit here as an academic, as a professor, but working in a university is only one of the places where excellent science takes place. A lot of my PhD students and a lot of my postdocs, you know, don’t stay in academia but move into industry, into the pharmaceutical industry, into biotech, into policy work, into government and third sector organizations. There are so many different fascinating careers from a scientific basis where our trainees can thrive, and I encourage that breadth of discovery across all the options available.\nAlex: And the other thing I’ve observed about science from the outside, I’ve never done research myself. Is that it requires this very delicate balance of curiosity and open-mindedness on the one hand, and then being skeptical and detail-oriented on the other hand. And those two things don’t often go together, but from what I’ve observed, the very best scientists seem to be able to merge both, or at least the teams and labs. The best teams and labs have people who are good at both. Has that been your experience, or has it been something different?\nCathryn: I like that perspective of looking on science. Um, and you’ve talked about one person having both of those. For me, much of this has been about teamwork, you know. So you get a wonderful discovery, you know, you press the button on the computer and you get your highly significant p-value, and you jump up and down an enthusiasm with this in the team meeting. And then, you know, the good scientists that work with you will be saying, well, what about this, and did you account for this, and have you controlled for this? And it will be that balance of that excitement, that moderation, that checking, that doing the sensitivity analyses.\nAlex: Where do you see yourself? Are you the enthusiast or are you the moderating influence or are you both?\nCathryn: I’m a statistician. I am a skeptic, and I’m a skeptic about every single number that people present to me, and they have to justify me that and show me that, you know, whatever they do to their data, whatever perspective they have on that study, that effect does not disappear, then I will believe it.\nAlex: Okay, we should work together then because I’m definitely an enthusiast.\nCathryn: It would be a great partnership.\nAlex: Um, you were a listener of this podcast, and I am constantly trying to make it better, and I’m constantly asking, you know, what is it that you get from listening to it and how could it be better?\nCathryn: Um, I’m a huge fan of the podcast. I’ve learned so much, um, both within my field and well outside my field that has really helped me, you know, develop as a scientist and working in mental health, and really getting an appreciation. Um, I mean, as we’ve talked about, I’m a statistician. I often come at things from a numeric perspective rather than a people perspective, and listening to podcasts like this has really made me appreciate the real diversity of work that goes on and the value of people with lived experience, and how, you know, getting people involved in research, whatever their experience is, is essential for people like me to do good research. So, I would say just carry on that diversity.\nAlex: I need your moderating more [laughter]\nCathryn: More genetics, more biology, but it’s that diversity that, for me, it’s the real power of your work,\nAlex: Professor Lewis. Thank you so much.\nCathryn: My pleasure. Thank you for having me, Alex."
  },
  {
    "objectID": "software_correlation.html",
    "href": "software_correlation.html",
    "title": "SNP Heritability and Genetic Correlation",
    "section": "",
    "text": "LAVA\nTitle: LAVA Tutorial\nPresenter(s): Josefin Werme\nLevel: Intermediate\nLength: 16:08\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nGCTA-GREML\nTitle: Estimating SNP-based heritability with GCTA-GREML\nPresenter(s): Jian Yang\nLevel: Intermediate\nLength: 18:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nGenetic Architecture\nTitle: Tracking 12 years of genetic architecture estimates of schizophrenia\nPresenter(s): Naomi Wray\nLevel: Beginner\nLength: 15:26\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.2_transcript.html",
    "href": "chapter1.2_transcript.html",
    "title": "Chapter 1.2: Epidemiology (Video Transcript)",
    "section": "",
    "text": "Till Andlauer:\nMost of the work of the psychiatric genomics consortium is about genome-wide association studies, or GWAS in short. But, what is a GWAS?\nSo first you need to consider that complex disorders, like most psychiatric disorders, are polygenic. So we don’t have single causal mutations that confer risk as is the case for monogenic disorders, but many many genetic variants with small individual contributions to disorder risk.\nGWAS typically analyze single nucleotide variants or polymorphisms, SNVs or SNPs in short, such a SNP is a position in the genome where the genotype can vary. In this example, the majority of people in a given population carry a “G”, and the minority an “A”, so “A” is the minor allele. The frequency of the minor allele can also differ between patients suffering from a disease and healthy controls. So that’s what GWAS is all about. So you take a group of patients and a group of healthy controls, and you determine the genotype of hundreds of thousands of SNPs using microarrays.\nThen you compare the frequency of alleles for each of these variants between cases and controls using logistic regression. You could also conduct the GWAS for a quantitative trait, for example body mass index or brain volume, and analyze associations using linear regression.\nGWAS results are presented as a Manhattan plot. Here you see one on Depression from the PGC. All the analyzed SNPs are shown on the x-axis, ordered by chromosome. And on the y-axis you see the minus log10 association p-value. Thus, the smaller the p-value, the higher the tower in the Manhattan plot. In this GWAS, you can see 44 such towers reaching above the red line. This red line is the genome-wide significance threshold, a p-value of 5 x 10-8, which corresponds to Bonferroni correction for multiple testing of 1 million variants.\nAnd why do you get these towers? Because of linkage disequilibrium. Nearby SNPs are correlated. They get inherited together more often than expected by chance. Thus, clusters of correlated variants show similar associations leading to the towers in the Manhattan plots.\nNow, published GWAS results typically don’t come from a single analysis; instead separate GWAS are conducted in dozens of cohorts, and the results of each of them is combined using meta-analysis. And that’s what the PGC does. In this manner, over the last years, hundreds of genetic loci associated with psychiatric disorders have been identified. And the list is constantly increasing. However, a lot of the work only begins after the GWAS has been conducted, and that is trying to annotate the function of the identified SNPs. There are many books and articles that provide you more information about GWAS and this book on psychiatric genetics is a good example."
  },
  {
    "objectID": "chapter5.3_transcript.html",
    "href": "chapter5.3_transcript.html",
    "title": "Chapter 5.3: Association Testing (Video Transcript)",
    "section": "",
    "text": "The Biometrical Model & Basic Statistics\nTitle: Biometrical Model and Basic Statistics\nPresenter(s): Benjamin Neale\nHello, I’m Ben Neale. I’m one of the course directors for the International Statistical Genetics Workshop, and I’m here today to talk to you about the biometrical model and basic statistics. Some really core theoretical underpinnings of how we think about twins and families, heritability estimation. All of this stretches back to an intellectual tradition that goes, you know, to Mendel and Galton and Fisher, all people that we’ll talk about over the course of, say, from the mid-1800s. You know, forward to today.\nNow a number of the scientists that I’m going to refer to are also eugenicists. I’m not going to talk about eugenics here. I know that can be very triggering for many. If you would like to learn more about eugenics or understand the relationship between polygenic inheritance and polygenicity and eugenics theory, I’d refer you to Lea Davis’ talk in the 2021 workshop about precisely that. But nevertheless, let’s move on and focus on the science in this particular session.\nSo a natural starting point when thinking about Human Genetics or, really any genetics, is to start with Mendel and start with this idea of inheritance of physical characteristics, and that was what Mendel was really interested in - working out and understanding how parents and offspring share some kinds of traits. In Mendel’s case, he was doing breeding experiments using pea plants, and here we’ve got the little picture of this sort of toy example of a yellow smooth-skinned pea crossed with a green wrinkly-skinned pea. And if you cross peas of those particular phenotypic characteristics, all you end up with in the first generation are smooth peas that are yellow. That is denoted here by a kind of imagined genotype of AABB - “AA” “BB” for yellow and smooth and then little “aa” “bb” for green and wrinkly. This particular phenotype is operating in a recessive fashion. If you think about green or wrinkly, that is to say, you have to be homozygous or identical at the genotype. You have to have two copies of the genetic variant - one from your mom and one from your dad  to express the green wrinkly phenotype for the green or the wrinkly, they are on independent chromosomes. This is the law of independent assortment that Mendel was also getting at. You can juxtapose that against the yellow smooth pea, which will behave in a dominant gene action form where that is to say the big A or big B means that if you have just one copy of that particular genetic variant that is governing the phenotype that you’re looking at, the trait of yellow or the trait of smooth-skinned pea, you end up with an F1 generation that has all the same phenotypes - all yellow peas and all smooth-skinned. And then if you cross within that F1 generation, so you take the F1 generation and you mate across the different plants in that fashion, you end up with 3/4 of the time being a yellow pea and 1/4 of the time being a green pea, and 3/4 of the time being a smooth-skinned pea and 1/4 of the time being a wrinkly-skinned pea. Now this wrinkly, smooth, yellow, green is operating either in a dominant fashion for the yellow or recessive fashion for the green. And it’s a really single genetic variant that’s governing this trait in this particular toy example of a pea plant.\nAnd Mendel experimentally showed this. He bred a bunch of peas and did the analysis to get to a place where we arrive at Mendel’s laws of independent assortment and segregation. Half of your genetic material comes from your mom, and half comes from your dad. Now, Mendel did this when there wasn’t really much of an appreciation of statistics, but we’re going to spend a lot of time talking about statistics in this course and throughout the rest of this lecture. So as a result, his sums came out exactly at the proportion, so they stopped the experiment when they got to the place where they thought the answer was because there wasn’t really a notion of randomness in the context of this particular experiment. But nevertheless, those kinds of laws - Mendel’s laws that I’m sure you learned about in your introduction to biology course - are still governing how genetic variation is transmitted from parent to offspring in humans and basically every other species in the population.There are lots of different mechanisms of reproduction in biology, and so you can get into all of that complexity. But we’re going to focus our attention on the way things work in humans during this course because I think we’re interested in understanding a little bit more about trait variation in the population or populations that we’re studying.\nNow, when thinking about Mendelian genetics, you can also think about this sort of interesting intermediate case, where you have white flowers and you have red flowers, and you cross them. And then in the first generation after that, you see nothing but pink flowers. So you see no flowers like either the white flower side or the red flower side. You only see pink flowers. And then if you take that pink flower generation and cross it with another white flower, then what you end up with is a 50/50 mix of pink flowers and white flowers. Whereas if you cross within just the pink flowers, you end up with a quarter white flowers, half pink flowers, and a quarter red flowers.\nNow, this is a sort of co-dominance, the sort of intermediate circumstance for Mendelian genetics, right? So there’s this idea that the phenotype is not one of two forms but actually maybe slightly more on a continuum of white to red, and you get the halfway point in that sort of space when you cross with red flowers and white flowers. So it could be that the two genetic variants are, in a sense, equally balancing one another rather than necessarily purely expressing or not expressing their particular phenotype.\nNow, that’s a complicated case, but let’s take something that’s also sort of intuitive and important. And let’s talk about height. If we think about height in the population, this guy over here, Francis Galton, has been doing a great many different things scientifically. But perhaps one of his most important contributions was this observation that parents who are tall or parents who are short tend to have kids that are tall or have kids that are short. And actually, not only do they tend to have tall parents have tall kids and short parents have short kids, but the kids don’t seem to be quite as short as the parents are when you look at the mid-parental height (which is this line from A to B on this picture). That’s the distribution of heights for parental pairs, taking the midpoint of those. And then if you look at the kids of those parents with the shortest stature, they don’t have kids that are short. They have this C to D line, so they actually tend to regress a little bit towards the mean. So that’s actually where we get our term regression from. So when we do a linear regression, it’s actually from this kind of drawing a line concept that Galton was doing when he was writing about height and stature and parents and offspring.\nBut there’s a natural question that arises when you think about something like height. If you think about your own height, do you think there’s like a single genetic variant that tells you how tall you are? I don’t. That doesn’t seem to make so much sense for something that’s like genetics. Why would you have a single variant that ends you up at, say, 6’4 inches, like my height? No, it’s actually a little bit more complicated than that, right? Just intuitively, this idea that there’s this kind of continuous variation in height in the population. There’s the kind of distribution of height sort of almost looks like a normal distribution, and so how do we square this inheritance of discrete physical characteristics that Mendel observed with this idea of continuous variation?\nNow, Galton was also heavily influenced by Charles Darwin. You know they were relatives. There was a lot of sort of intellectual curiosity around trait variation and kind of genetics and biology, more generally, right? Like this was all happening again in that kind of mid-1800s, so a huge amount of change in the scientific literature, and a lot of data being collected about things like people’s heights and their kids and their families, and these ideas of how do we take something like height? How do we take a continuous trait like height and integrate that with what we understand about discrete inheritance of characteristics from Mendel? Because that was really Mendel’s main point, is that there was something - some discrete thing being transmitted from parent to offspring.\nSo in 1915, there’s a really beautiful paper by East looking at Corolla length in Nicotiana longiflora, or the tobacco plant. And the experiment that East did was he concentrated the longest Corolla length and the shortest Corolla length plants and then made those the founding parental generations and then crossed those and ended up with a distribution. This F1 distribution was sort of in the midpoint, you know, sort of between those two distributions of Corolla length from the long Corolla length and the short Corolla length tobacco plants, and he had this F1 distribution. And then he mated within that F1 generation to get the F2 generation. Same sort of thing as with the peas or the flowers, and what you can see quite distinctly is that the distribution spreads out quite a bit from F1 to F2. And that observation, like the increase in the variability of the distribution from F1 to F2, points to the idea that there is maybe more than one genetic factor being inherited here. That there’s maybe something that’s adding a little bit of jumbling up and actually, you know, what’s actually happening that we can know and appreciate now and obviously indicate is that amongst the tall parents, you’ve homozygosed, or you’ve taken the two alleles that are the long form of whatever it is, particular allele it is. And in the shorter length plants, you’ve homozygosed, the short form. And so when you do the F1 generation, you get a lot of heterozygous sites. So that is to say, instances where you have one of the long form and one of the short form of the particular genetic variant that is having an impact on corolla length in the tobacco plant. And as a result of them all being homozygosed, they’re actually very similar genetically in F1. And then when you go to F2, you then have the binomial resampling, so some of the heterozygous sites are now turning into the homozygous long form or the homozygous short form in that F2 generation.\nNow, not content with simply looking at the increase in the distribution as you get to the F2 generation, East then went and sampled from different points across that distribution and then. In a sense, created an F3 generation, so just take a bunch of plants with a similar sort of height and then see what happens there. And there you can see that the mean more or less tracks with the mean of where the sampling was happening in that subsequent F3 generation. And so that’s the idea that there is actually a genetic contribution to the phenotypic variance in Corolla length in the tobacco plant, and that you could see it through these kinds of breeding experiments.\nThis is very important work, and it kind of articulated this idea that polygenic inheritance was, you know, perhaps the most natural explanation, but there wasn’t really a clear mathematical formulation that made everything sort of airtight and coherent. And for that to hit the scene was a paper that Ronald Fisher wrote in 1916, and actually submitted to the Royal Society in London in 1916. It was rejected, and then it kind of got passed around in the academic publishing press of the day into this paper in 1918 where it was described in the Royal Society of Edinburgh instead.\nAnd as a result of the sort of observations that Lee is talking about, and many others were making throughout the scientific literature at the time, Fisher wrote this treatise on the correlation between relatives on the supposition of Mendelian inheritance, and this paper is an extremely rich, dense text. It has a huge number of ideas that are still relevant today, a century on. So this is more than a century ago, the theoretical underpinnings of quantitative genetic theory were really articulated and laid out in a very, very clear and precise mathematical framework by Fisher. And in, you know, just something like 30 pages or so, there’s all kinds of ideas about how to think about genetic variance, the definition of genetic variance, the definition of variance that we use today, the idea of partitioning variance indeed, prior to this paper, partitioning variance, and even like ANOVA-like ideas hadn’t really been invented, and that’s Fisher’s clear contribution here. How do we try and disentangle really complicated ideas? What can we maybe partition? Now, again, this is a statistical tool. This is a scientific tool. It is a model description of the world. It is not a complete and rich description of the world, and whenever we talk about partitioning variance, it’s important that it is a statement that genetic variation influences a phenotype. But it is by no means the only mechanism by which a particular phenotypic value can arise, and indeed the environment may matter a great deal. Changes in the environment can have massive changes on phenotypes, and that’s not really captured in a kind of idealized toy model that Fisher was talking about in the context of the biometric model. But nevertheless, additive genetic variance, dominance, worries about a sort of mating epistasis, how to think about multiple alleles, how to think about all kinds of different forces that would shape the genetic landscape of a phenotype in a population were really given some deep thorough mathematical treatment by Fisher, and this paper is so important that it’s still the sort of primary way that we think about the definition of heritability. Indeed, the definition of heritability goes back to precisely this paper.\nOK, so how did Fisher get there? How did he reconcile this idea of discrete inheritance from Mendel with continuous variation like height? And what Galton was doing with the looking at the parents and kids and measuring their height and showing that they were the same? Well, what Fisher did was he invoked something called the central limit theorem, and what the central limit theorem states is that if you have a bunch of independent factors that sort of are summing together to create some outcome, then a normal distribution will emerge. And we can see this if we create a sort of toy example of thinking about coin tosses. And what we’re going to do when we think about coin tosses, we’re going to think about just the binomial chance. And in a sense, if you think about your parents, they have genetic variation. They have a lot of places where they’re heterozygous, where they have one form of a genetic variant and another form of a genetic variant, and it’s a random toss of the coin. Which form of that genetic variant you yourself get, and so thinking about that coin toss, well if there’s just one coin like there was for like wrinkly or smooth in terms of the peas or yellow skin or green skin for the peas, then that might you know just be a binomial chance exactly like this. But what happens when we start to add coins? Well when we start to add coins we see different distributions of outcomes in those coin tosses coming. And if you keep adding coins, what you see is, and you know, a distribution emerge, and that distribution that emerges is this normal distribution that was written about by de Moivre in the 18th century and Gauss in the 19th century, and this normal distribution here. That’s like if you have an infinite number or a very large number of outcomes, you end up with that normal distribution, but it’s worth remembering that if you just look at ten coin tosses as your outcomes, you’re already getting pretty close to a normal-ish distribution in the population.\nOK, so how do we relate the normal distributions to something like diabetes or schizophrenia? Well, we invoke something called the liability threshold model that was really articulated in advance of the Fisherian idea of the quantitative genetic theory, the biometrical model that I’m talking about. So Pearson, working with Alice Lee in 1901, sort of articulated this idea that if you have an underlying distribution of liability, of you know some sort of risk for a phenotype like schizophrenia or diabetes, then if you’re above that threshold, then suddenly you’ll have, you’ll present with that illness or that disease or that binary trait. And if you’re below it, then you won’t have that trait. And so you can actually think about some discrete binary phenotype as really having an underlying continuous distribution. Now, Pearson sort of elevated Alice Lee in the early 20th century. It was not often the case that women co-authored scientific papers because of gatekeeping by many different male scientists and institutions, and a lot of institutionalized sexism. But Pearson, I think, was a little bit more of a let the sort of people who actually created the ideas get the credit, and so he advocated for Alice Lee to be recognized in the scientific papers, and I think there’s a lot of contributions that have gone unspoken in history, and so it’s nice to sort of recognize one where there was actually the co-authorship extended to the key intellectual partner for developing these ideas. Now, the way that Pearson and Lee got to this set of ideas was actually thinking about horses and thinking about horse coat color, which you could see as discrete characteristics ranging from like a Black Horse all the way to a White Horse. But if you lined up all of those horses from the lightest shades to the darkest shades or the dark shades, the lightest shades, then there might be some underlying distribution of horse coat color that was maybe slightly more normally distributed in the population, and that was the whole idea. That was how they got to this notion that there could be some hidden distribution that we can’t see, but that we just see this kind of binary outcome at the end. And this liability threshold model is still a very powerful tool for modeling discrete outcomes, particularly when they are multifactorial, when they have lots of contributing causes, just as we saw with the central limit theorem ideas with Fisher.\nOK, so Fisher didn’t just define, you know, variance and partitioning genetic variance and the analysis of variance in the 1918 paper. He also set forth a model to describe how genetic action might operate in the population, and these are slides that Manuel Ferreira made many years ago that I’m still using because I just find them so exquisitely clear. And so what we’ve got here are three genotype classes: “aa,” “Aa,” and “AA,” and they are attached to different means in the population: the white circle, the yellow circle, and the red circle have different means in the population conditional on what your genotype is that you have. And that’s because this genetic variant, if you hold like everything else constant, has some impact on the phenotype. It has some mild change that will maybe make you slightly taller or shorter, thinking about Galton’s example dataset. And you can see that the genetic effect here that Fisher wrote down is “little a,” and “little a” is like really not the best naming convention, as it’s a really difficult collision of terms, but it is what it is. That’s the way it’s written. So here’s how we’re teaching it. And so the kind of genotype mean in the population is “minus a” for the “aa” [genotype], “d” for “Aa” [genotype], and this is in a context where D or the dominance deviation, which is to say how far away from the midpoint value of the two homozygote classes you are, the “aa” or “AA” where is that “Aa” genotype in relation to that midpoint. It’s well, in this instance, it’s dominance equals zero, and so the “Aa”  has a mean of 0, and then “AA” has a mean of “plus a,” and those are the sort of genotype conditional means. So here’s now a picture of normal distributions layered on top of the genotype distribution, so we see the red “aa” distribution has some mean trait value. The blue “Aa” distribution has some mean trait value, and the green “AA” has some trait value in there, you know separated by an “a’s” worth of distance, and they are now a source of variation in the trait in the overall population, which is actually quite a big source of variation in this particular example.\nNow, what happens if we have some dominance deviation, right? So maybe everything isn’t purely additive. Maybe additivity doesn’t explain the universe perfectly well. Well, in that circumstance, we’ll see this “d” now move the genotype mean of “Aa.” Note that the midpoint is still term 0, is the midpoint between the two homozygote classes, and Fisher did that to make the algebra a little tidier, and I think we all appreciate the tidiness of the algebra when we get there. OK, so this is the dominance deviation, so this is allowing for non-additivity in the genetic effect, and there are a lot of supporters of non-additivity in the Biological Sciences because there are a lot of recessive-acting phenotypes, sort of like we saw with the yellow-green pea color, and the green peas are sort of a recessive mode of action, and so that necessity is an important observation in biology, and so this non-additivity has got a long strong intellectual tradition in this space. OK, so that’s what happens under additivity. That’s what happens under dominance. Now let’s talk a little bit about some of the statistics that are used, and these are really just your elementary first-level statistics. Very, very basic statistics ways of describing distributions of traits, and so here we’ve got some idealized simulated trait on the X-axis on the right, and then this red line of mean, it’s got a mean of zero that, you know, I’ve artificially fixed to be 0, based on R, and then the frequency is the count of the number of individuals with the trait value, and here we have it in normal units on the X-axis, and the mean is just simply defined as the sum of the observations that Xi, those individual trait values, divided by the total number of individuals or n. Pretty pretty simple. Pretty basic statistics. Hopefully, you all remember how to calculate a mean.\nNow let’s talk about the variance. The measure of spread in the distribution. Well, the variance is now summing up the deviations from the mean and collecting those and aggregating those over the entire distribution. And Fisher used the deviation from the mean squared because he found that that was the most consistent estimator when thinking about trying to define the measure of the spread. So what that means is that it has the least variability of an estimator of the spread of the distribution, which is why we favor X minus the mean quantity squared for each individual divided by now “n - 1”. And the reason that we have an “n - 1” there is that we’ve had to give up a degree of freedom to the mean, and I’m not going to go much further into that. But degrees of freedom are a bit fiddly like that, but this is a way to make the estimator unbiased.\nOK, so the covariance of a distribution is now thinking about not just one trait but two traits, which is to say that we’ve got trait one on the X-axis here and trait two on the Y-axis. And these traits have some relationship to each other. There’s some, you know, you can see this red line. That’s the regression line that I fit on this particular dataset, and they are a bit correlated or have some covariance. Now, all the covariances is it’s just a way of again summing up the deviations from the mean. But now instead of doing it in one dimension, we’re doing it in two dimensions. So we’ve got the Xi minus Mu of X, which is the individual trait value of X for the i-th individual minus the mean of that for the overall dataset. And then we do the same thing for the Y trait as we did for the X trait, and then we multiply those deviations for each individual together, and we divide that by the number of paired entries that we have minus one, and that gives us just the covariance, and so this is just in the scale that X is on or Y is on when X and Y are standard normals. When they are, have, you know means of 0 and variance of 1. This turns into the correlation, but there’s also a way to turn the covariance into correlation as well.\nOK, so how much mean and variance? Well, we can think about the contribution of the QTL to the mean, and really, that’s just a way of making sure that the mean squares with how we’ve defined our genotype classes, and so we take the number of individuals with a given Trait value times the frequency of individuals with that trait value. And you might remember how there were different means for the different traits that we saw in the picture a little while ago, with the “AA,” “Aa,” and “aa”. Really well, those mean points we’re going have to come up with a grand mean for our total dataset.\nNow. This might be something like cholesterol levels in the population or something like height. You know any continuous phenotype. And really, it can have any shape, be any distribution, and it will still have some mean. Now, here we’ve got our “AA,” “Aa,” “aa” genotypes. The effect of the QTL, the quantitative trait locus. That’s the genetic variant that is having an impact on the trait or phenotype, and that effect here is for the “AA” genotype, just a. For “Aa,” it’s D, and then for “aa,” it’s -a. That’s the conditional mean. That’s the mean of the phenotype conditional on carrying that genotype, and then there are the frequencies of those genotype classes. Another bit of notation introduced. So we have a tendency to define the frequency of the genotype as “p” and “q” for the other allele. So one allele, one form of the genetic variant gets the frequency of “p,” the other form of the genetic variant gets a frequency of “q,” which equals “1-p,” and you end up with “p^2,” “2pq,” and “q^2” for the frequency of the genotype classes, if the Hardy Weinberg rules follow, and the Hardy Weinberg is basically just a way of saying that there isn’t an unexpected amount of correlation amongst your parents in terms of their genotype. So if you have random mating in the population more or less, then you’ll have that sort of frequencies be p^2 for “AA,” 2pq for “Aa,” and q^2 for “aa.”\nNow the mean is just going to be taking those conditional genotype means times the frequency of the genotype and summing those all together. And so that’s what we see down here in the mean of X, and that gives us a grand mean that we’re going to use in the context of our calculation of the variance. OK, so when we calculate the variance again, we’re looking at the squared deviation from the mean. Now note, this is in the population, and so we don’t have to worry about the “n-1”. It’s funny because it’s thinking about everybody rather than thinking about an estimate, and that little estimator thing is just a little bit of nuance around statistics. But again, we have this (xi minus mu)^2, so the ith individual’s trait value of X minus the mean squared times the frequency of that genotype class. And so here we just work through some more algebra. So the variance is taking this effect mean for “AA” multiplying it, you know, taking off the grand mean that we calculated on the previous slide, and multiplying it by the frequency of that genotype in the population. So we have (a - mu)^2 * p^2 + (d - mu)^2 * 2pq + (-a - mu)^2 * q^2. And that’s how we define the variance of the QTL. That’s what Fisher partitioned as the variance of the QTL, all the way back in 1918. Again, before there was the structure of the genome, before the nucleic acids were really understood, before we had any real notion of what was actually going on with DNA itself. But we understood that there were genetic variants operating in the population, that they were a source of variation, and this source of variation could be a tractable, quantifiable thing. This VQTL, this variance of the QTL. And the heritability of trait X at this locus is just simply taking the variance of the QTL divided by the total variance of the phenotype. Now this is all worked out for just one genetic effect, but remember, we could have many, many genetic effects, and so if we sum up all of those genetic effects together, then that might get us to the sum total of the variance of QTL, which is VA, and then divide that by the sum total of the phenotypic variance. Just the variance of the trait in the population more generally. And that’s how you get to your heritability.\nOK, so let’s work through a bit more of the algebra, so we’ve got this variance. Here we get this (a - m)^2 * p^2 + (d - m)^2 * 2pq + (-a - m)^2 * q^2, and we can actually partition the variance of the QTL into a part that is additive and a part that is dominance, and so that’s what’s been done here through rearranging is taking the VA of the QTL, and that’s the main effect that if you take a genotype and just run a regression, the genotype against a phenotype, you will end up with this additive genetic variance as your estimator. Fisher made it really convenient and really nice for us in the derivation of the math and kind of thought it through that way, and then if you add that second term and you encode a dominance deviation from that additivity, then you can get this VD of the QTL, or the dominance contribution to variance. So that’s just the deviation from the purely additive model, all predicated on where that heterozygous class goes.\nOK, now that’s one genetic effect, but remember for something like height, things are a bit more complicated, right? We don’t have a single genetic effect necessarily. We actually maybe have many, many genetic effects, and so we can kind of develop those ideas a bit further and say that there might be some distribution to the genetic effects. This isn’t what you know. This is what Fisher implied in his work. He said that let’s assume that there are polygenes. We can assume a distribution of those single nucleotide polymorphisms, those specific genetic effects that you’re going to learn about a little bit later on in the course. We can then use that to generate an estimate of the heritability, and that’s exactly what the tool GCTA does. It’s also what we did in the context of the LD score regression. So this has just been an introduction to heritability, additive genetic variance, dominance genetic variance, means, variances, and covariances. These are the most fundamental basic building blocks that the rest of the two weeks will be built on, and I hope you’ve enjoyed this introduction to how we think about partitioning phenotypic variance and a little bit on why we do it so as to try and understand a little bit more about the world around us. Thank you.\n\n\n\nHypothesis Testing, Effect Sizes, and Statistical Power\nTitle: Hypothesis Testing, Effect Sizes, and Statistical Power\nPresenter(s): Brad Verhulst\nHello and welcome to the 2022 Boulder Workshop, where we’re going to discuss hypothesis testing, effect sizes, and statistical power. My name is Brad Verhulst from Texas A&M University, and I’m going to walk you through some of the important components of these concepts.\nSo, the first thing that we’re going to start with is statistical power, and just to quickly define it, statistical power is the probability of correctly rejecting the null hypothesis. Importantly, statistical power depends on 4 components. The first thing is sample size – this is often what we’re trying to calculate. The second is our α level, and typically we set α at 0.05 or the p-value of α equals 0.05 or less. The third thing is our β level, our power level, the probability that we’re going to reject the null hypothesis if the null hypothesis is actually false, and the final thing that we’re going to do is we’re going to look at effect sizes. And so, in order to do this, we’re going to start right at the beginning and think about how statistical power relates to the basic components of hypothesis testing.\nHypothesis testing:\nSo, when we’re thinking about hypothesis testing, we really have three steps. The first step is to define what the null hypothesis is. Oftentimes, this is a hypothesis of no difference. So, Group A is equal to Group B, or the parameter of interest that we’re looking for, say, our heritability coefficient is equal to 0. Of course, at that point, we then define what would be considered sufficient evidence to reject the null hypothesis. Say p is less than 0.05, for example. In a genome-wide significance framework, we might want to say p is less than 5x10-8, and that would be our threshold for rejecting the null hypothesis. The final thing that we do is we go and collect data and then we actually conduct our analysis and see where the parameters fall.\nSo, when we’re thinking about hypothesis testing then, we can imagine a distribution of our test statistic under the null hypothesis. The next thing that we need to do is define this evidence that we’re going to use to reject the null hypothesis. So, in a standard situation where α is set to 0.05, anything that falls in this blue shaded region, we would claim is inconsistent with the null hypothesis, and therefore we will reject it. Now, even if it is part of the null hypothesis, we will observe that approximately 5% of the time, i.e., a p of 5%. The second thing that we really need to think about is the distribution of the test statistic under our alternative hypothesis. Now, most of the time, if we’re going to conduct a study, we’re not thinking, “Oh, nothing’s going to happen.” We’re thinking, “Oh, something’s going to happen.” And what we believe is going to happen is that our test statistic is going to fall in this distribution – in the alternative hypothesis distribution. And so, the probability that we’re able to reject the null hypothesis given that we’re in the alternative hypothesis, drawing our statistic from the alternative hypothesis distribution, is going to be our power, and this red shaded area is the β component here – 1 minus our power – which we can quite easily say is sometimes, even if our statistic is drawn from the alternative hypothesis, sometimes that statistic still won’t reach our level of evidence that we’re required to reject that it’s part of the null hypothesis distribution.\nThe second component here that we really need to talk about is effect sizes. So, effect size is a measure of the strength of a phenomenon in the population. And most of the time when we’re thinking about effect size, we’re thinking about effect size as independent of the sample size or other components of statistical power. In a lot of cases, this helps us communicate the result in everyday language, especially if the scale is meaningful on a practical level. For example, we might want to say that people who take Zyban or Bupropion smoked 5 fewer cigarettes per day, or people who take Liponox will lose 28.16 pounds in eight weeks. That would be an effect size measure. Effect sizes are agnostic to whether the effect is real or not, so it could be a real effect or it could be a false effect. And the effect size doesn’t really care whether it’s one or the other, because they’re not associated with p-values as they don’t incorporate any components of sample size into them.\nWhen we’re thinking about effect sizes and we’re thinking about the distribution of the null and the alternative hypothesis, the difference between the mean of the null and the mean of the alternative hypothesis distribution is what we’re really thinking about when we’re thinking about the effect sizes. So if we’ve got a regression coefficient of 0.2, this difference between these of 0 and 0.2 under null and alternative hypotheses, respectively, would be our effect size. So, what are the conventional sizes of that we think of when we’re talking about effect sizes? Well, most of the stuff comes from Cohen’s classic 1988 book where he provided some standards for interpreting effect sizes, and it’s really important to be cautious when we’re interpreting effect sizes, because in a rather counterintuitive way, large effect sizes are not necessarily more theoretically interesting and instead tend to be rather obvious. So, when Cohen wrote his book, he noted that, well, a small effect, something with an R2 around 1%, is likely to be something that we need to do some sort of statistical analysis to detect. We really do need to do some sort of modeling of our data in order to extract the association; it’s not going to be observable just from walking around. A medium effect size or an R2 of about 0.1 is going to be apparent upon careful inspection – might not be completely obvious – but it’s going to be apparent if you were to really look carefully at the world around you. And then we’ve got large effect sizes, or an R-squared of about 0.25, and this is really going to be obvious at a superficial glance. Things like men tend to be taller than women or something completely obvious like that on average. That doesn’t mean that all men are taller than women, but that on average, we wouldn’t really need to do a statistical test in order to get this.\nOkay, so now that we’ve defined several of the components of statistical power, let’s talk about what that really means. So statistical power is typically used to do two types of power analyses. The first type of power analysis is called an a priori, or a prospective power analysis. We typically do an a priori power analysis in order to figure out how many responses are necessary to fairly test our null hypothesis. This is typically done for grant applications or things of that sort, where we have to justify a sample size that we’re planning on collecting. A second type of power analysis is called a post hoc or a retrospective power analysis, and in this type of power analysis, what we’re going to do is we’re going to explore whether the effects that we’ve observed can be reasonably expected to reject the null if it’s actually false. If we were to, say, add more people, how much power did we have in our test or is our test based on about 20% power, 50% power, etc., etc.? And knowing how much power you had to test your null hypothesis is really an essential element in understanding the likelihood that you’re going to replicate your results.\nSo, if we think about the likelihood that you observe a true effect, you reject the null or you fail to reject the null, we can think of a situation where we know what the truth is. So, we can say that the null hypothesis is true or the null hypothesis is false. Of course, in reality, we never know what the truth is, but we can set this up as kind of a straw man. And then if the null hypothesis is true, but we reject the null hypothesis, we’re committing a Type I error, or a false positive. If, by contrast, the null hypothesis is actually false and we fail to reject the null hypothesis, then we’re committing a Type II error, or a β error – in this case, it’s a false negative. Of course, if the null hypothesis is false and we reject it or if the null hypothesis is true and we fail to reject the null hypothesis, then we’re in a good state of affairs.\nSo what is a Type I error? A Type I error is a false positive. The rejection region that we’re focusing on here is this red-shaded region in this figure, and if our test statistic falls in this region, we will reject it even if the effect isn’t true. Basically, in this case, we just sort of got lucky. So, given that our α is fixed probably by our discipline or at least exogenously from the experiment, this is the basic significance level that we’re trying to test.\nBy contrast, a β error is the probability of failing to reject the null hypothesis when it’s actually false. In this case, our statistic is drawn from the distribution on the right-hand side here, but it just happened to be really far out in the lower tail of our effect size distribution, and what that means is that it doesn’t exceed the necessary α threshold to reject the null hypothesis. In this case, this is a false negative.\nSo, when we think about the standard conceptualization of statistical power, we’re really thinking about those four elements. We’ve got our effect sizes, our sample sizes, and the α and β levels. So, if we take this simple equation and we rearrange it, we can show that we’ve got our α level, our β level, and the difference between α and β is equal to the square root of our sample size times our effect size. And this works pretty well for a lot of cases when we’re looking at differences of means or we’re looking at sort of correlations or something like that.\nOnce we get into things like twin models where we’re looking at differences between distributions of correlations, things get a little bit more complicated, so instead of thinking about the standard power calculations, twin modeling tends to use two possible methods for calculating statistical power. The first method is a simple Monte Carlo simulation method where you simulate a model under the alternative hypothesis numerous times, say 1000 times, and then you count how many times you observe a test statistic for your parameter of interest that exceeds the critical value that you’re looking for, so 0.05 for example. And the proportion of times you get this significant result is your statistical power. It’s pretty simple to do. The downside of it is it can be very time-consuming. Also, with models that are complex, this can take a lot of time, and you can end up with a lot of model failures that may or may not affect your statistical power.\nAn alternative method is to use what we’d call non-centrality parameters. Because we’re working with parametric tests, we know or we assume that the distribution of the test statistic follows, say, a standard chi-squared or a standard normal distribution, and we can leverage this assumption to more directly calculate statistical power without doing just an absolute ton of replication. So, we can do it once and then calculate power from there, rather than doing it 1000 times and looking at the proportions. And because of this time difference, the number of times you have to do it, this can be done relatively quickly.\nSo, we’re going to focus on non-centrality parameters. So, the non-centrality parameter is the sum of the mean of the test statistic distribution under the alternative hypothesis with a given set of degrees of freedom. That’s a little complicated, but basically what we’re talking about is the difference between the distributions that we’ve been discussing already: that effect size distribution. So, there are two points that are especially important for calculating statistical power using non-centrality parameters. The first is: as the effect size gets larger, the mean of the test statistic distribution gets larger and, therefore, the NCP gets larger, and as the NCP gets larger, we have more statistical power. The second component is: as sample sizes increase, the standard deviation of the null and the alternative distributions get tighter and, therefore, the NCP, the non-centrality parameter, gets larger as well. In both cases, as effect sizes get larger or as sample sizes get larger, we are increasing our statistical power.\nOkay, so when we’re calculating power with non-centrality parameters in twin models, we’ve got four basic steps that we’re going to follow. The first thing that we’re going to do is we’re going to simulate twin data that corresponds with the alternative hypothesis. Say we want to test the power to detect an additive genetic variance component of 0.4 – how much power would we need to do that? And so that would be something that we want to test. Of course, the level of your effect sizes, for example, how big your additive genetic variance component is, should be based on the literature as far as possible. Of course, if you’re doing something really novel, you might not know how heritable it is, and so you’ll have to take a guess.\nThe second step is to fit the full and the reduced models to the simulated data to obtain a chi-squared value from the likelihood ratio test. So, if we’re going to test that the heritability or the additive genetic component, what we want to do is we want to simulate the data in step 1, and in step 2, we’d run the ACE model and, in step 2, we’d also compare that against a reduced model the CE model. And we would be able to tell based on that how significant that A parameter was.\nOnce we’ve got this chi-squared value from the likelihood ratio test for the full and the reduced, we can calculate the average contribution of each observation to the chi-squared. So, in order to do that, we take the difference that we have observed from the likelihood ratio test and simply divide it by the total number of observations. In this case, if we had MZ twins, we divide it by the total number of MZ plus the total number of DZ twins: twin pairs. And this will give us what I call the weighted non-centrality parameter.\nAnd then we can go ahead, and in step 4, we can actually calculate that we can use this weighted non-centrality parameter to calculate the non-centrality parameter for a range of statistical sample sizes. And then we can just basically multiply it by any given sample size to get what our Chi-squared value would be for that particular value of n.\nSo, if we had a chi-squared value, for example of 10 with 1000 observations, the weighted non-centrality parameter would be 10, or our chi-squared value divided by 1000, which is our number of observations to give us 0.01. Therefore, on average, each observation contributes about 0.01 to the NCP. Because the NCP scales linear with sample size, if we had 2000 observations, we would simply multiply this 0.01, this weighted non-centrality parameter, by 2000, and we would get a chi square value of 20. If we had 500 observations, we would multiply this weighted non centrality parameter of 0.01 by 500 and we get a chi square value of 5, and it’s really that easy.\nSo all of the stuff that I’ve told you today comes from this paper that I wrote in 2017, for the Boulder Workshop, and I’m going to walk you through quickly a power analysis based on the script that we’ve put together. All of the functions can be found in this powerFun.R script, and as a quick note, you’re going to need to have the powerFun.R script in your current working directory or the powerScript.R will not be able to find the functions - because all the functions are in here. So what we can do then is walk through a couple of examples to show how this script works and show how you can calculate power using the non-centrality parameter in twins and everything’s pretty well boxed up. But of course, the devil’s in the details, so we’ll go through some of those details now.\nSo, the first thing that we’re going to do is we’re going to want to require the necessary R packages, and there’s two in particular that we’re going to use OpenMx and MASS. So OpenMx is going to help us to specify and fit our twin models, and MASS is going to help us to simulate the data. Remember that’s the key step at the beginning. And then what we’re going to do is we’re just going to source all of these functions and running these three lines of code will allow us to start playing with some of the possible power analyses that we’re going to want to look at.\nSo, I’ve set out a series of power analyses that we might find interesting as twin modelers. So, the first question that we might want to know is what is the power to detect A, or the additive genetic component, in the univariate model as C, the shared environmental component, increases. So, what I’ve done here is I’ve specified 3 models, one where the additive genetic path coefficient is 0.6 for each of the models and then the common environmental path coefficient goes from 0.3 to 0.5 to 0.7. And note here that we’re assuming that the sample size for the MZ and DZ twins is equal. We’ve set it arbitrarily to 1000, but this is actually something that it’s much more important to get the proportion of twins right – so, in this case, we’re having equal proportions – than to actually specify a specific number. Sometimes when you are fitting rather esoteric models, you might want to bump this up to say, 10,000 or 100,000 twins in each group in order to get more precise estimates of the average non-centrality parameter or the weighted non-centrality parameter.\nSo if we run these three lines of code, we’ve basically set ourselves up to get all of the information that we’re going to need, and we can look inside this object, say “modA1.” And what we’ll see here is 3 bits of information. First, we can see on the left-hand column of the top table the estimates that we specify. So, we wanted a 0.6, 0.3, and then R just computes what the rest of the path coefficients would be. So 0.6 and 0.3 would leave an E component of about 0.74, and you can see how closely these are being estimated to what we’re asking for. And we can see what the standard errors are, and this kind of tells us what the results from our twin model would have been under this situation. We want to make sure that these estimates here match what we’re seeing or what we’re asking for in our function. If they deviate too far, then our simulation didn’t work, and we probably have to do it again, perhaps with a larger sample size. The two key pieces of information here that we want to know are going to relate to the A and the C parameters. So, our weighted non-centrality parameter of A here is going to be this value here, 0.0121, etc., and the value of C is going to be 0.00105 or so. And we can see that this value here for A and this value here for C correspond, or at least proportional, to the non-centrality parameters, giving us some suggestion that what we’re seeing is going to be useful.\nSo now that we’ve actually calculated the weighted non-centrality parameters for A, we can then just plot them. And so the powerPlot function that is cooked into this powerFun functions will allow us to plot all of the various non-centrality parameter power analyses that we’re interested in doing, and if we just run those four lines of code, it’s going to give us a legend as well. And what we can see here is that on the x-axis, we’ve got the sample size that we’re looking for, and on the y-axis, we’ve got the power to reject the null hypothesis or the power to detect a significant parameter. Now, if we look at the black line here, that was our first situation where A was equal to 0.6 and C was equal to 0.3, so we can see here that as we increase the power, we finally get that magic value of power equals 0.8 when we get about 625 individuals. By contrast, if we increase C from 0.3 to 0.5, which is this red line here, we can see that the power to detect the A component increases much faster, and if we increase it again to 0.7, it is increasing even faster. Basically, what we can see here is that the power to detect A depends on the assumptions that we have about the power to detect C, which is very interesting, and it has a big effect on this. If we look in our R console window, what we can see is our 80% power to detect an A of 0.6 when C is 0.3, we would need about 645 twin pairs, split evenly between MZs and DZs. If we had a C of 0.5 and an A of 0.6, what we’d have is about 400 twin pairs necessary, so 200 MZs and 200 DZs. Now if we have C of 0.7 and an A of 0.6, which is explaining pretty much all of the variation with either the additive or the common environment additive genetic or common environment, what we see here is we only need about 51 MZ and 51 DZ twin pairs, which is an astronomically low number of twin pairs.\nOkay, so, the next question is what’s the reverse? How does the power to detect C vary as we increase A? So instead of increasing C from 0.3 to 0.5 to 0.7, we increased A, and we kept pretty much all of the other parameters the same, and so all we need to do here is run this and if you wanted to test different values here, you could just replace any of these values and we can run that. And then we can similarly plot this using the same functions. Basically, what we can see here is that the power to detect C depends much less on our value of A than the power to detect A depended on our value of C. So, what we can say here is we really have to have some expectation of both A and C included in our power analysis.\nOkay, so up to this point, we’ve looked at the power to detect A and C when the sample sizes were equal. In this next section, what we’re going to do is we’re going to look at two different sample sizes and how they affect power. So, we’re going to say we’ve got a 5:1 ratio, so 5000:1000 ratio of MZ to DZ twins, and then we’ve got an equal ratio, and then we’ve got a 1:5 ratio of MZ to DZ twins. And just for simplicity, we’ll keep it at 0.6 for the A and the C component, but this doesn’t really. This demonstration doesn’t really depend upon the values of A and C. What we’re looking for is the proportions of the various twin types that we have. And so again, all we need to do is quickly run this. These three lines of code and then we can plot the power. And if we were to quickly plot that again, what we would see is the plot would shape up much nicer.\nOkay, so what we can see in the power analysis where we vary the proportion of MZ to DZ twins from 5:1 to equal to 1:5 is that the best power comes, or the smallest sample sizes come, when we have about equal numbers of MZ and DZ twins. A lot of times people think, “Hey, you know, in order to get more power, I should have more MZ twins.” But that’s actually not the way that we would see this shaking out to detect additive genetic variance. Of course, to detect common environmental variance, it’s actually much more beneficial to have more DZ twins than MZ twins, and if we have MZ twins, we are dramatically underpowered to detect the common environmental components of variance, which is an interesting and important finding that we need to keep in mind when we’re doing our twin analysis.\nOkay, the next thing that we have to keep in mind is whether we’re using continuous or categorical data. And that’s for the current analysis, look at binary data, so case-control data. And what we can do here is we can think of how the prevalence of our case-control trait affects the power to detect a significant genetic component. Now, let’s say that the genetic component is again a path coefficient of 0.6 and the common environment is a path coefficient of 0.6. So, we’ve got equal A and C here and we’ve got equal proportions of MZ and DZ twins. So, all we need to do to start out with is specify all of the various different coefficients that we might be interested in running and then we can plot it again. And as we can see here, as the prevalence goes from about 0.5 down to 0.05, the power drops dramatically. So, for our common traits, we can get away with many fewer twin pairs, say about 1,100 for a prevalence of 0.5, whereas for a prevalence of 0.05, we’re looking at over 4,000, and we need to keep this in mind when we’re doing any types of twin models.\nOkay, the final set of power analysis that I want to talk about is the power to detect genetic correlations between our variables. So what we’ve done here is we’ve specified an increasing set of A and C or A for the first trait and A for the second trait. And what we’re looking at here is increasing the genetic correlation between these two traits from 0.1 to 0.3 to 0.5, so very small, moderate, and actually quite large genetic correlations. And we’re keeping the C correlations proportional to the A correlations, and then we’re going to keep the sample sizes approximately equal again. All right. So, if we run these 9 different scenarios, what we’ve got here is all of the results that we need and now we can easily plot those and we can take a look at what comes out. So, for both, because both variable 1 and variable 2 were simulated to have the same A, C, and E components, we can see here that the power to detect those variance components is equal for both traits. What we can see is that we need about just less than 600 twin pairs to detect an A of 0.3, about 200 just over 200 for an A of 0.4, and just about 100 to detect an A of 0.5. Importantly, here, the power to detect our genetic correlations varies pretty dramatically with the amount of heritability that we have in our traits. So, for modestly heritable traits, we can only really reliably detect a genetic correlation of about 0.5. So with an A of 0.3, we’re looking at about 1,200 people, 1,200 twin pairs in order to get our correlation power up to the 0.8 level. By contrast, if our A is about 0.4, we need about 500 people to get that significant genetic correlation power to 80%, whereas we need about 1,400 to get it for the modest genetic correlation of about 0.3. And then by the time that our a parameter gets to about 0.5, we have even more power. And of course, when we have a very small genetic correlation, we never really achieve sufficient power with reasonable sample sizes.\nOkay, so just to recap, we’ve gone over the elements of statistical power, hypothesis testing, and effect sizes, and we’ve gone over a variety of different methods of calculating them, and I’ve shown you a demonstration of how you can use some of the functions that we’ve put together over the years to calculate power for a classical twin design.\nThank you very much. My name is Brad Verhulst, and you can ask me all of the questions that you need in the workshop practicals where we’ll go through a little bit of this information as it relates to estimating twin models. Thank you very much."
  },
  {
    "objectID": "software_cnvs.html",
    "href": "software_cnvs.html",
    "title": "CNVs",
    "section": "",
    "text": "This video by Dr. Howrigan explains copy number variation: what is a copy number variant, and how is it detected in genetic data. Additionally, examples of CNV analyses demonstrate what a CNV file format looks like, as well as output from CNV analyses, and how to perform CNV burden and association testing on that data.\nThe paper referenced in this talk:\nMarshall CR, Howrigan DP, et al. Contribution of copy number variants to schizophrenia from a genome-wide study of 41,321 subjects. Nat Genet. 2017 Jan;49(1):27-35. doi: 10.1038/ng.3725. Epub 2016 Nov 21.\n\nTitle: How to Run Copy Number Variation (CNV) analysis\nPresenter(s): Daniel Howrigan\nLevel: Intermediate\nLength: 20:03\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to PLINK1.9 and PLINK2.0 website."
  },
  {
    "objectID": "chapter10.6_transcript.html",
    "href": "chapter10.6_transcript.html",
    "title": "Chapter 10.6: GDPR for Dummies: A Survival Guide for Genetics Research (Video Transcript)",
    "section": "",
    "text": "Title: GDPR for dummies: A survival guide for genetics research\nPresenter(s): Heidi Beate Bentzen\nHeidi Beate Bentzen:\nMy name is Heidi Bentzen. I’m a researcher at the University of Oslo. My background is in law, and I will give you a GDPR for Dummies survival guide for genetics research.\nGDPR in the European Union, there used to be a personal data directive. That meant each member state was free to figure out how to implement the rules. This meant that we were left with very varying interpretations across member states. So, to resolve this issue, the EU decided, “We’ll put a regulation in place instead,” and that is the GDPR, the General Data Protection Regulation. What this did was to harmonize data protection law in the EU member states. It also specifically regulates genetic data. The GDPR has two purposes. One is to manage data flow. In our response, in sorry, start over. The GDPR, um, sorry, I don’t know what’s going on right now. Sorry, I’ll start this slide over again.\nHost: Okay, that’s fine. Go ahead, and it’s still recording, so we’ll just edit this piece out and then okay.\nHeidi: Thank you.\nHost: Right over. No problem. Start when you’re ready.\nHeidi: The purpose of the GDPR is to manage data flow in a responsible, uniform, and predictable manner. Its objectives are to protect fundamental rights and freedoms of natural persons, and in particular, their right to data protection. But it’s also to ensure that the free movement of personal data within the European Union shall not be restricted or prohibited. So the rules in the GDPR are strict, but for a very good reason. You will know that the data is being processed just as securely at a hospital in Spain as with a private company in Finland, that the individual’s fundamental rights are respected not only in Sweden but also in Greece. So the GDPR is not about restricting data processing; it is about managing the data processing responsibly.\nOne of the fundamental objectives of the European Union as such is to strengthen its scientific and technological basis, and this entails also free circulation of scientific knowledge. It has therefore declared that there should be as few legal obstacles to research collaboration as possible, and this, of course, necessitates movement of personal data. At the same time, the Charter of Fundamental Rights in the European Union guarantees protection of personal data, so the GDPR tries to balance these two objectives. So, therefore, the GDPR is built as a tool to facilitate research while promoting the individual, protecting the individual research participant. So it’s a template for collaboration in a knowledge society.\nAnd we see this very clearly also in the text of the GDPR itself, which speaks very highly and favorably about scientific research, and it mentions myriad exceptions to the main rules if the processing is for scientific research purposes. So does the GDPR apply to the data processing you are doing in the European Economic Area? Well, the tentative answer is most probably, but it only relates to data, not samples. Blood samples are not considered personal data, but the labeling on them and the data obtained from the samples through laboratory analysis are. And if the data is anonymous, it is exempt from the GDPR, but to determine whether it is anonymous, you need to use a kind of anonymization tests, which means that you need to look at the factual circumstances. You need to consider all the means reasonably likely to be used to try to identify someone, and you need to consider this dynamically, so if you want to process the data for 10 years, you also need to consider, well, what would it cost then? Which technology would we have available nine years from now to potentially identify this person? Looking at the re-identification literature, we know that we can usually, in genetics research at least, usually not promise research participants that they will not be identified. In scientific research, data is typically processed pseudonymously, and pseudonymized data is considered personal data subject to the GDPR.\nNow, how do you ensure that the data processing is compliant? Well, let me help you: GDPR cheat sheet. I’m going to run you through a GDPR cheat sheet, so I will teach you about the seven principles the GDPR is built on. So all the other rules you see on the GDPR will somehow just be specifications of these seven principles. So if you know these, you’re pretty, you’re pretty well compliant already.\nThe first principle is lawfulness, fairness, and transparency. It is that personal data should, it shall be processed lawfully, fairly, and in a transparent manner in relation to the data subject. Let’s look at lawfulness first. One thing we should do here is to keep different types of consent apart. So we often talk about informed consent when we’re speaking about research. Now, informed consent is a term used in the ethics literature, and it typically refers to a research ethics instrument as informed consent to research participation. What we are talking about in terms of GDPR is something a little different. We are talking about the need for a lawful basis for data processing, and that can, but does not need to be, consent. For instance, let me give you an example: if you’re running a clinical trial, there will be laws, um, mandating you to process certain data for safety purposes. The basis for the data processing for that portion should be law, and it should not be consent. So here you see the difference. When you can, however, combine these two types of consents in one document, but they should be clearly separated within.\nWith the genetics research, we have a bit of a problem in the sense that it’s so dynamic. Well, it’s not really just a problem, it’s very, very nice, but it also means that it’s challenging to keep the consents valid because the information you have provided a couple of years ago might be very out of date at this moment. So there are currently several initiatives looking into alternative consent forms, so I’m just mentioning this to you. One option is, for instance, dynamic consent, which is electronic and it allows for continuous information updates and it can also allow for very easy re-consent processes. With genetics, it has the additional benefit that if you make a finding that you’re unsure if you should report back, then you can ask the research participants if we make certain classes of findings, would you like to know or not to know? And so you will know how they feel about it.\nAs you saw, transparency was the other part of the first principle, and the principle of transparency entails that personal data shall be processed in a transparent manner in relation to the research participant. Let me give you an example. AI tools for assistance with medical diagnosis can be remarkably efficient. So, one example is facial image analysis for the diagnosis of genetic disorders, and that is so efficient that it can even reveal information that can’t be picked up by experienced health personnel, or, and it can even detect carrier status. So, in this study, for instance, it had a 99% top 10 diagnostic accuracy. The tool mentioned here is a 10-layer deep convolutional neural network. Neural networks are, as you all know, notorious for being difficult to explain. However, we found that it’s actually not that hard to fulfill the transparency requirement, as many will have you believe. Here, for instance, it was accomplished by a three-layered explanation: one very easy explanation about pattern recognition, a more detailed explanation with photos and examples of transmitted data as you see in the slide, and finally, for those particularly interested, a link to the Nature Medicine paper explaining the algorithm and, prior to the publication of that article, to the preprint.\nFairness is the last element of the first principle. AI, for instance, can be both sexist and racist, and the principle of fairness entails that the data processing should be fair in relation to the data subject. So this means, for instance, that one should strive to avoid bias and once you disclose any potential bias in the data, and this is then an example of how this was done from the article I mentioned on the previous slide.\nThe second principle of the GDPR you need to know about is purpose limitation. Purpose limitation consists of two dimensions: one is specification and the other is compatibility. Specification means that personal data shall be collected for specified, explicit, and legitimate purposes, and compatibility means that the data shall not be further processed in a manner that is incompatible with those purposes.\nHere you see one of the places where scientific research really enjoys a privileged position, because for scientific research, you still need to specify the purpose and make sure it’s explicit and legitimate, but you are allowed to do further processing even if it’s incompatible with the original purpose.\nThere are other sides to the purpose limitation principle you need to be aware of, though, and particularly for genetics research. You are often building large genetic databases, and those are tremendously useful for medical research, but they also have enormous misuse potential. And it’s crucial to understand the impact of scandals, and this instills a responsibility for you to think about how to protect the database from unintended third-party access. And this is a task that goes well beyond information security, and I’ll explain to you through an example or two how, the type of issues we typically face here. For instance, a few weeks ago, the Norwegian Supreme Court denied law enforcement access to a scientific research biobank. A baby had died, supposedly of formic acid poisoning, and the father is a suspect in the case. So, tissue sample from the scientific research biobank is necessary evidence, and without it, it will not be possible for the police to prove its case. The Supreme Court stated that the biobank access can decrease trust in scientific research and associated biobanks and denied access.\nHowever, looking across the border here to Sweden, there was a case in which the former Minister of Foreign Affairs, Anna Lindh, was murdered and the biobank acceded to the police request without the court order and handed out samples of a named suspect, uh, who turned out to be the actual killer. However, what happened? The consequences for the Swedish newborn screening biobank, which was the one that was used in this case, was that 2000 people immediately withdrew their consent to the biobank when they heard about this, and an additional 50 people each month did so. So, this shows that if you start allowing for other purposes for others to use the same data, you can actually destroy the main purpose of the data collection, and this is why I’m telling you to watch out for this. We also see this a lot in other disputes, for instance, related to biological kinship, we see it after mass casualty events, we see it with immigration issues and reprioritization issues.\nThe third principle of the GDPR is data minimization. The data must be adequate, relevant, and this is the difficult part for genetics research, it must be limited to what is necessary in relation to the purposes for which they are processed. So, that means that you need to make sure that the purpose of the processing aligns with the data you are generating. So if you’re only looking at one gene, you need to be able to explain why you’re doing a whole genome sequencing. So make sure that you have a purpose for the processing that aligns with the tests you’re running and the data you are generating and processing. Why do we have this rule at all? I mean, we’re looking into big data analysis and the more data, the better, right?\nWell, it’s the Orwellian argument. Indiscriminate data retention is not considered okay. The data processing needs to be proportional, and this is, for instance, why you see that we don’t have universal forensic databases. This is a basis in human rights law and, therefore, does not just apply to the EU but more generally. And we saw that, for instance, when there was a suggestion to establish a universal forensic database in Kuwait and that was stopped by their supreme court. But this explains why there is so much pressure in the healthcare and research sector from third parties to our databases. They simply want access because they’re not in a position to establish similarly good databases themselves.\nThe fourth principle of the GDPR is that personal data shall be accurate and up-to-date, and I don’t think I need to explain this much more because as researchers, you very much appreciate this aspect.\nThe fifth principle of the GDPR is integrity and confidentiality. A lot of this relates to information security. That personal data shall be processed in a manner that assures appropriate security, protection against unauthorized or unlawful processing, and against accidental loss, destruction, or damage. If you look at the specific rules of the GDPR, you will, for instance, also see that you may, in some instances, need to or want to conduct a data protection impact assessment. And that is where you assess what risk there is to the research participants and how you can mitigate those risks.\nThe sixth principle is storage limitation. This means that personal data shall be kept in a form which permits identification of data subjects for no longer than is necessary for the purposes for which the personal data are processed. But personal data may be stored for longer periods if it’s only processed for scientific research purposes. So again, a very positive exception in the scientific research field. This does, of course, necessitate that you have appropriate technical and organizational measures in place, but still, you see that data retention can be lawful outside the regular bounds if the processing is for scientific research.\nSo, the last main principle of the GDPR is accountability, and this means that the controller, that’s the one deciding the purpose of the processing, they shall be responsible for a purpose and means of the processing, I should say, they shall be responsible for compliance with all the previous principles. And I can also remind you that if you look back at this list again and you think about transparency and accountability, together, those comprise the two elements of trustworthiness. So you see here that what the EU is aiming for is simply trustworthy data processing. So they want the flow of the data, they just want it to be managed responsibly and in a trustworthy manner towards the research participants.\nSo as you’ve seen then, the GDPR is built to function as an instrument for facilitating responsible scientific research, and it functions as such. Both within the EU and for collaborations such as with countries the European Commission has decided offer an adequate level of data protection. For instance, Japan. So when Japan and the EU mutually decided that they offer adequate levels of data protection, this created the world’s largest area of safe data flows. Other countries that similarly to Japan have a decision in place from the European Commission that they offer an adequate level of protection, so the data can flow freely between the EU and those countries include, as you see on the top of this slide, Andorra, Argentina, commercial organizations in Canada, Faroe Islands, Guernsey, Israel, Isle of Man, Japan, New Zealand, Switzerland, and Uruguay. And there are also adequacy talks ongoing with South Korea.\nIf I switch away now from the GDPR and see, well, how did we actually get the GDPR at all? Well, it has a mother, and the mother is the Council of Europe Convention 108, which now also exists in a modernized version. That is the only internationally legally binding instrument in the field open to any country, and there are currently 55 countries worldwide bound to that convention. It’s all the countries you see on this slide, so those are the 47 countries who are members of the Council of Europe, and in addition, it’s Argentina, Cape Verde, Mauritius, Mexico, Morocco, Senegal, Tunisia, and Uruguay. So, in all of these 55 countries, the elements of the GDPR you will also see in these countries’ data protection legislation because they build on the same convention, which is the Council of Europe Convention 108.\nAnd finally, with the exception of human rights and the Council of Europe Convention 108 being possible to enter into for any country worldwide, most legislation related to processing of data for research is regional. However, with colleagues from across the globe, we have identified the functions that governance of genomic data should fulfill as the basis for the design, implementation, and the evaluation of governance frameworks. So we acknowledge that different governance functions may be in tension with each other. For instance, access to data versus introducing oversight and restrictions to ensure appropriate data uses. So we’ve used the governance framework of six large-scale international genomic research projects from across the globe: Africa, Asia, the US, and Europe, to illustrate governance choices as well as their approaches to important trade-offs and how these are reflected in their governance functions. This may be work that shows the core global elements to consider in genomics research. Thank you so much for your attention, and please get in touch if you would like me, at any point, to answer any questions. Thank you."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "Chapter 1: Introduction",
    "section": "",
    "text": "Chapter goals:\n\nGain a basic understanding of psychiatric genetics and genome-wide association studies.\nUnderstand the recent history of psychiatric genetics.\nGain insight into the current state of psychiatric disorder research."
  },
  {
    "objectID": "chapter1.html#sec-section1",
    "href": "chapter1.html#sec-section1",
    "title": "Chapter 1: Introduction",
    "section": "1.1 What are psychiatric disorders?",
    "text": "1.1 What are psychiatric disorders?\n\nThe video below from the Broad Institute gives a very brief overview of Psychiatric disorder genetics.\n\nTitle: Psychiatric Disorders\nPresenter(s): Broad Institute\nLevel: Beginner\nLength: 2:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section2",
    "href": "chapter1.html#sec-section2",
    "title": "Chapter 1: Introduction",
    "section": "1.2 Epidemiology",
    "text": "1.2 Epidemiology\n\nDr. Andlauer gives a brief overview of genome-wide association studies and genetic epidemiology.\n\nTitle: What is a genome-wide association study?\nPresenter(s): Till Andlauer\nLevel: Beginner\nLength: 3:15\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section3",
    "href": "chapter1.html#sec-section3",
    "title": "Chapter 1: Introduction",
    "section": "1.3 History",
    "text": "1.3 History\n\nIn the videos below, Dr. Kendler gives brief overview of some of the prominent players in psychiatry and psychiatric genetics, starting from the late 1700s through the mid-1900s. Part 1 highlights the work of Robert Burton, Prosper Lucas, Jenny Koller, Ernst Rüdin, and Eugen Bleuler, and discusses the darker periods of the history of psychiatric genetics during the late 1920’s and 1930’s.\nPart 2 is a deeper dive into a single episode in the history of Psychiatry: “The Beginnings of the Debate Between the Mendelians and the Biometricians in Psychiatric Genetics”, focusing on four individuals: David Heron, Karl Pearson, Aaron Rosanoff, and Charles Davenport during 1913 to 1914.\n\nTitle: History of Psychiatric Genetics (Part 1)\nPresenter(s): Ken Kendler\nLength: 21:44\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: History of Psychiatric Genetics (Part 2)\nPresenter(s): Ken Kendler\nLength: 43:41\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter1.html#sec-section4",
    "href": "chapter1.html#sec-section4",
    "title": "Chapter 1: Introduction",
    "section": "1.4 Psychiatric genomics: State-of-the-science",
    "text": "1.4 Psychiatric genomics: State-of-the-science\n\nBelow are videos describing the current state of psychiatric research. These are state-of-the-science videos for psychiatric disorders, which were recorded for the PGC by early career researchers, with accompanying papers in Psychological Medicine (Vol 51, issue 13).\n\n\nThe Psychiatric Genomics Consortium\nTitle: PGC: Future Perspectives\nPresenter(s): Patrick Sullivan\nLength: 33:33\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nADHD\nTitle: Insights into Attention-Deficit/Hyperactivity Disorder from Genetic Studies\nPresenter(s): Joanna Martin, Christie Burton, Isabell Brikell, Nina Roth Mota\nLength: 14:24\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nAlzheimer’s Disease\nSee Dementia\n\n\n\nAnxiety Disorders\nTitle: Genetic contributions to anxiety disorders: Where we are and where we are heading\nPresenter(s): Helga Ask, Rosa Cheesman, Daniel Levey, Kristin Purves, Heike Weber\nLength: 11:32\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nAutism Spectrum Disorder\nTitle: Genetic contributions to Autism Spectrum Disorder\nPresenter(s): Alexandra Havdahl\nLength: 16:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nBipolar disorder\nTitle: Genetics of Bipolar Disorder\nPresenter(s): Brandon Coombes, Kevin O’Connell\nLength: 16:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nDementia\nTitle: Genetic Risk for Dementia\nPresenter(s): Malia Rumbaugh\nLength: 58:02 (Frontotemporal dementia starts at 17:55)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nEating Disorders\nTitle: Genetics of Eating Disorders\nPresenter(s): Hunna Watson, Alish Palmos, Avina Hunjan, Jessica Baker, Zeynep Yilmaz, Helena Davies\nLength: 21:30\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nMajor depression\nTitle: Major Depressive Disorder: Introduction and General Epidemiology\nPresenter(s): Kim Kendall, Evelien Van Assche, Till Andlauer, Karmel Choi, Jurgen Luykx, Eva Schulte, Yi Lu\nLength: 32:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nObsessive Compulsive Disorder\nTitle: Genetics of Obsessive-Compulsive Disorder: What we know in 2020\nPresenter(s): Christie Burton\nLength: 15:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nPost-traumatic Stress Disorder\nTitle: Posttraumatic Stress Disorder: From Gene Discovery to Disease Biology\nPresenter(s): Frank Wendt\nLength: 15:33\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSchizophrenia\nTitle: Genetic Architecture of Schizophrenia\nPresenter(s): Kaarina Kowalec, Niran Okewole, Sophie Legge, Marcos Santoro, Sathish Periyasamy\nLength: 29:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nShared genetic architecture\nTitle: Shared Genetic Architecture across Psychiatric Disorders\nPresenter(s): Andrew Grotzinger\nLength: 14:04\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSubstance Use Disorder\nTitle: The Genetics of Substance Use Disorders: A Review\nPresenter(s): Joseph Deak, Emma Johnson\nLength: 17:38\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nSuicidal Thoughts and Behaviours\nTitle: Insight into the Genetics of Suicide\nPresenter(s):\n\nHilary Coon, Professor, University of Utah\nAnna Docherty, Assistant Professor, University of Utah\nDouglas Ruderfer, Assistant Professor, Vanderbilt University\nNiamh Mullins, Assistant Professor, Mount Sinai School of Medicine\n\nLength: 58:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nTourette Syndrome\nTitle: Genetics of Tourette Syndrome\nPresenter(s): Matt Halvorsen\nLength: 15:23\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "ch8_gusev_transcript.html",
    "href": "ch8_gusev_transcript.html",
    "title": "Chapter 8: PGC Day TWAS Primer (Video Transcript)",
    "section": "",
    "text": "Author: Sasha Gusev (alexander_gusev@dfci.harvard.edu)\nLength: 16:39\nI am Sasha Gusev this is my first time at cgsi so thanks everybody for having me and giving me the opportunity to give this tutorial as with the other ones please feel free to interrupt or ask questions throughout and I’ll try to sort of break things down in a way that’s accessible.\nI’m going to be talking about genome-wide Association studies and specifically trying to make sense of genomewide association studies as a way to understand human disease and complex traits and so just to sort of start it at the at a very basic level this is the output of a genomewide association study or GWAS.\nThe procedure is very straightforward - you collect a lot of genetic data on individuals with the disease and without the disease or with a quantitative trait and then you test each genetic variant (and that’s what each of these dots is here) for association with the phenotype. The variants that are significantly associated are above a predefined threshold here, and if they replicate we treat those as genetic variants that are causal for the disease and this is sort of a study design that I think initially almost seemed too simple to work but now over time and with very large sample sizes has produced thousands if not hundreds of thousands of associations for nearly every complex trait that it’s been applied to when there were sufficient sample size.\nIn fact the challenge is now that these Association studies are almost producing too many results and what we would rather have than sort of this figure which is a real plot from geological prostate cancer is something more like this which is a systemic or systematic understanding of the disease of which genes are involved in the disease how they interact what contexts they’re relevant in and so forth and so uh whereas initially there is sort of a challenge of just fleshing out this side of the plot getting these associations um I think a key challenge now is in Connect going from this side of the plot over here to an actual understanding of the disease and one of the sort of most basic pieces of getting to that understanding is connecting variants to the genes associated variants to the genes that they likely operate through and then operate on the trait and so we can break it down into this very simple structure we have a variant we want to know its Target Gene and the effect that it has on that disease and so in particular we can break this down even further and first just ask whether we can identify variants that influence the expression of genes in a systematic way and this is something that was observed some time ago is that in fact if you take gene expression and you you basically kind of run a G was but on expression as your outcome gene expression measured in the past through microarrays or now through rna-seq and test variants typically near the gene in CIS with the gene for association with expression across individuals you will find that the expression of many genes is often highly heritable and so there’s an estimate here in 2011 that the CIS Locus for an average Gene contributed to between 37 and 24 of the variance of expression and again once you have a heritable phenotype in a population you can sort of apply the GEOS Paradigm to that phenotype and instead we call that an eqtl analysis and I’m sure you folks have seen work from the gtex Consortium over many years applying ettl studies and identifying thousands of variants associated with the expression of many genes in many tissues and in fact again this is one of those cases where as the sample sizes have grown this study design has actually yielded a very large number of associations that are almost like too difficult too many to fully process and I think the most recent Gtech study showed that if you sort of relax the significance threshold for these associations nearly every Gene has at least one eqtl in some tissue and in fact I think that if you continue as the sample sizes have grown even further we see that genes then start to have secondary eqtls and tertiary eqtls and this sort of curve uh does not is not even hitting diminishing returns so that’s the piece about identifying genetic variants that influence gene expression and then there’s been a lot of work in trying to understand how these eqtls connect to disease and I’ll highlight a couple studies in particular which basically asked in a couple different ways whether an eqtl is more likely to be a GEOS variant or is more likely to be associated with a complex trait and so the results on the left show that eqtls specifically as you get more confident about them being the causal eqtl are more enriched for heritability across many complex traits from gwas and then this figure on the right from gamazon at all showing that if you just sort of try to partition the amount of disease heritability that could be explained by eqtls those estimates are also quite High across a large number of complex traits again ranging from maybe 10 percent up to 35 percent so there’s this sort of incidental evidence that eqtls are enriched for disease heritability and may therefore give us an instrument to understand um the likely causal genes and eventually go back to that sort of big system-wide understanding of the phenotype so that’s the first part of the arrow the other part of this of this network is we want to understand how this genetic mechanism of gene expression actually goes on to influence the trait and for which traits and this is where the approach of a transcriptomide Association study or a tiwas comes in and I’ll just start with a very basic sort of thought experiment of what would we um want to do if we had the ideal data set how would we in sort of with infinite resources try to relate gene expression genetics and disease together and I think one way that we could do this is we could estimate expression in the hundreds of thousands of individuals that we have genetics and case control status in here like this represents case control status and then we could ask what genes are genetically correlated meaning the effect sizes on expression are also shared with the effect sizes on disease we could do this for every single Gene across the genome and that would give us an estimate of the genes that in principle could be linked to this phenotype and the the hurdle here is that we we uh very rarely or pretty much never have data at this scale what we typically have is a relatively small study of genotypes and measure gene expression usually as in the case of the G tax in a sort of healthy relatively healthy population that was convenient to sample and then we also have very large disease studies that also have genotypes but no gene expression measured and so the basic Insight of the transcript and mind Association study or tiwas is kind of thinking about the fact that we have what is shared across these two these two studies is the genetics and we know previously that gene expression is itself a heritable trait and if it’s a heritable trait then in principle it should be a predictable trait and so what we want to do is use the genetics to predict expression into this study over here where we haven’t measured it and then use the predicted expression as a sort of proxy to estimate the relationship between the genetic component or the predicted component of expression um and the uh in the in the phenotype um and again this is all I’m sort of presenting everything in the context of a single Gene but the idea is to use this methodology and scan across every Gene in the genome and identify the set of genes that are significantly uh genetically correlated or uh For Whom the predicted expression is significantly associated with the phenotype uh and so right then we do the test so the first question is can we actually predict gene expression in this way and the fact that we’ve observed significant eqtls or individual variants that affect expression basically tell us that tells us that that we can and in work that we’ve done and others have done we’ve shown using a number of different prediction schemes that I sort of won’t go into but that are various forms of penalized or Bayesian regression that you can in fact predict gene expression with a substantial degree of accuracy and in particular when you use models that incorporate all of the genetic variation around the gene you typically have substantial gains in the predictive accuracy so even though the single topic ETL explains a large fraction of the CIS effect or of sort of the total heritability near the gene um there is a very large number of genes for which additional variants contribute substantially to the predictive accuracy and so simply going from a single snip Paradigm to a sort of locus wide Paradigm increases our predictive accuracy and that’s going to translate into better Association statistics in the in the eventual guas study um now one additional constraint is that we typically don’t really even have this design where there’s individual level data in both studies what we actually have more frequently is this design where we have individual level data for the gene expression study and then we have summary statistics for the gys and the summary statistics are basically for every snip the marginal the marginal sorry I think this turned off dead battery I think I know what that means um is there a does anyone know if there’s a backup or can you guys just hear me like this I don’t know if this can you hear does that does that work okay let me try to decouple one sorry okay all right I’ll try to work with this and then let me know if you can get another one for that one cool okay so what we typically have is these summary statistics oh sorry do you need anything else it’s perfect okay let’s do it great all right that’s easier thank you okay so summary statistics uh this is what we actually have and they are the marginal Association statistics for every variant and what we want to know from this kind of data is what would the gene the predicted Gene trade Association have been if we could get to the individual level data and measure it and so this is really where the the uh the tus methodology comes in again because this is the type of data we have most of the time um and I’ll just sort of sketch out how this uh this parameter is estimated and the basic idea is that we think about what we would want to do with individual level data and then we kind of move terms around and try to identify pieces that can be estimated from the summary level data and so we start with predicted expression over here x are the genotypes that we use for the prediction W are the weights that we’ve trained in the gene expression data that gives us this this term G that’s the predict expression and then what we want to know is the association between y the phenotype and G the predicted expression so specifically we want to know this orange beta T was so we can kind of plug in the terms into a basic ordinarily squares regression and then decompose these terms and you can start to see pieces here that you can actually estimate from summary level data and in particular you’ll see that this covariance between the genotype and the phenotype actually corresponds to this these G was summary statistics that we get the association between each snip and and the phenotype and then this term down here the covariance between the Snips themselves is also something that we in principle can get from reference panels because it doesn’t rely on knowing the phenotype and so these two pieces we can get externally we plug them back in and now this is a summary based estimate of the beta tus that only requires the z-scores the reference LD and then these weights which we have we sort of assume that we have a priority and then I won’t go into the details of how we derive the variance for this Statistics it’s very very similar and the final Association statistic that we get looks like this where again in the numerator you have you can think of this as a weighted sum of the gyc scores that’s weighted by the uh the predictors of expression and then in the denominator we have essentially the variance of that predicted expression that accounts for the correlation across these Snips so Snips that are correlated are going to add to the variance and Snips that are independent or not so this is basically the the score and I think this is also kind of a useful framework to think about how you can go from Individual level data to estimates of quantities we’re interested in with summary level data uh when we apply this technique to summary based data and individual based data it works really well correlation is nearly perfect and again we didn’t really make any assumptions going through it uh going through that previous derivation except for the fact that the LD is well matched to the Target population and also there’s sort of a hidden assumption that the effect sizes can’t be so enormous that we need to account for changes in the environmental variants and those assumptions are very easily satisfied in most studies so now thinking about when does this approach actually uh lead to associations we ran some simulations where we considered three different study designs under the model where there is a causal Gene and we’ve observed the predictors of that causal Gene so you could imagine in that scenario just running your standard G was to try to identify the association um you could imagine testing only the top snipped the top eqtl that’s associated with expression or you can imagine running a full tus test and when we do that we see that in this scenario because we’re testing fewer features we’re only testing each gene instead of each snip then the power of the tuos or the eqtl only approach is higher than the guas approach so this is one case where not only are we getting a parameter that we’re interested in on its own we also have some increase in power because the multiple testing burden is effectively is effectively lower furthermore if we expand the model and say additionally consider genes with multiple causal variants where now the to us approach of applying a penalized model to the entire Locus is giving us more signals more predictive accuracy than the top eqtl we see that the power of these single snip approaches drops but the power of the tiwas locus-wide approach remains effectively the same and so again this is another scenario where when we have many causal variants for expression that all lead to disease then we can substantially boost power uh and where the truth is is is is sort of in between or maybe a little bit off the page there’s going to be some loci where we don’t have the measured expression uh at all so these uh expression based approaches will just fail there’s going to be some loci where there’s only a single variant for the Gene and will be up here and there’s going to be some loci where there are many causal variants for the Gene and the tiwas will then maximize power relative to other approaches uh so this is all in simulations under very specific presumed models we can also ask how well does this approach perform in real data and this this has actually been quite a challenging question to answer because as it stands we have very few well-established causal genes for disease so I showed you that plot at the beginning that had over 200 known associations for prostate cancer but the number of well-established really definitively established causal genes for prostate cancer for that study is extremely small and that’s sort of the case for most complex traits so we don’t actually have kind of working in a regime where we don’t really have a ground truth there was a study that was done in this pre-print by weeks at all from The Phoenician lab which I thought was an interesting attempt to to try to get at a ground truth and the basic idea was that if we look at data in their case they looked at data from the UK biobank where you had associations both with common sort of standard gwas and also a rare variant coding variant based set of tests and you identify a Locus where there’s both common non-coding associations and rare coding variant associations you can assume maybe it’s not a safe assumption but they assume that the rare coding variant is telling you the right causal Gene and so under this model they basically have a kind of ground truth which is what is the rare coding variant tell you the causal Gene is and then they can ask how various other approaches do based on just the blue stuff just the common variant associations for identifying that causal Gene and so now they have a ground truth um they can plot Precision recall curves and they used this approach to evaluate a bunch of different methods listed here and then also to propose their method which is uh conceptually quite different I won’t go into it but it’s sort of like an ensemble that integrates many many different features at the locus to make the predictions but I think the the what’s relevant here is how these other approaches perform and what you can see is that there’s quite a lot of heterogeneity in their performance uh the tiwas is here in blue and as one point it has the highest uh recall in this model relative to the other approaches aside from their sort of Ensemble based approach and then additionally they integrated each of these method methods together with their model and in that scenario the tiwas had the highest Precision together with their approach but again I think an important takeaway here is that this is far from a solved model with a clear optimal method the TOs provides you an estimate of a certain statistical quantity but this is biology in biology is complicated and so lots of different approaches have different trade-offs for what they’re able to identify and at what levels of precision and recall and then you know one thing I should mention that maybe people are noticing is that if you take a very simple model of just what is the nearest gene or what’s the distance or how far away is the potential causal Gene that actually performs really well and in fact it performs about as well as the method that they that they develop developed and also when combined has very good Precision so again I think that there’s many explanations for this one is that in fact it may be that the nearest Gene oftentimes is the correct Gene it may also be the case that this specific model tends to emphasize genes that are close to the association statistics but again I think it’s also important to keep in mind that probably some hybrid of all of these methods to that also consider proximity is going to eventually be this sort of optimal solution okay uh so that’s kind of where we stand with um with with 2s applications um I you know coming back to this figure you can sort of wonder why the Precision uh of tiwas is relatively low compared to these other methods and I think again there’s an important set of caveats which were sort of highlighted in this paper from Mike Weinberg at all a couple years ago nature genetics which essentially come down to the fact that tiwas is an association study it’s not a causal inference technique and as an association study it’s going to be susceptible to tagging and correlation in the same way that genome-wide Association studies are and so in this paper they proposed a number of sort of alternative models which could still identify a significant T was hit one alternative model is that you can have co-regulate Mission at a Locus where the same genetic variant or set of genetic variants Drive the expression of multiple genes both the causal and non-causal genes and this is a real phenomenon that it’s not that uncommon that you will identify loci with multiple genes with very high cyst genetic correlation and high genetic correlation to the trait another case is you can imagine some part of the genetic effect on a non-causal gene is tagged because of LD between variants with the effect on the causal Gene and this would produce a false positive Association or it would sort of induce some effect on both the causal the non-causal Gene and the causal Gene and then likewise you can imagine a scenario where the effect on the causal Gene was missed in the gene expression study because it was not sufficiently powered or it didn’t get the right context and so this would lead to a false negative association and so again I think this is important to keep in mind that this is a test that is expected to tag the causal mechanism when these assumptions are met but in the real world these assumptions should also be sort of interrogated uh the other I think important limitation and one that’s potentially solvable uh is to consider is the fact that as we’ve seen with other genetic predictors the uh uh these expression predicted expression models do not uh generalize well to other um to different populations and really because most of the data has so far been collected in individuals of European ancestry this is particularly a problem for generalizing to data from non-european populations um or in admix populations with low European ancestry and so this paper showed models that were trained in European individuals that had high accuracy predicted into held out European industry individuals and had significant drops in accuracy when predicting into individuals of African ancestry and again I think that they’re potentially interesting ways to address this problem you know probably the the most basic is just to start collecting more data in other populations we should definitely be doing that but also there’s methodological approaches that could potentially leverage all of the training data that we have available or think about the differences between populations to improve the prediction of these of these models okay I’m gonna drink so I also wanted to talk a little bit about methods I think these are all methods that we did not develop and had no hand in but that I think are interesting approaches to moving beyond just that beta tus that I described for the association between expression and disease and I’ll sort of walk through them briefly you know to to give you guys a flavor of methodologically what else can be done in this space there’s a great method called utmost that came out a couple years ago in nature genetics which uh thought about how um gene expression data that’s measured in multiple tissues in the same individuals could potentially be used to improve these predictive models so everything that I’ve been talking about so far sort of assumed that there was a population with some single modality of gene expression but you could imagine and this is exactly what the how the gtex was designed that you’ve measured multiple tissues for every individual this Y is now a matrix instead of a vector for a given Gene and then the approach that utmost proposes is to actually try to learn the expression for each tissue together with all of the other tissues observed and so again you see some similarities here these bees the what they’re learning are sort of the W’s that I talked about earlier now are being learned for all tissues at once and they do that by using again a form of penalized regression where they have a penalty within each tissue where they want the weights to generally be sparse and then they also have a penalty across the tissues where they don’t want to see a lot of differences between tissues they sort of assume that if a snip is important for one tissue it should also be important for another tissue and this approach particularly for tissues that had relatively small sample sizes substantially increased the prediction accuracy basically by borrowing signal from other tissues that were available and that’s sort of shown here in in purple is the increase in prediction accuracy and held out data um another approach thinking about this sort of multi-tissue framework is instead of learning weights using multiple tissues we may be interested in testing multiple tissues where each set of Weights were learned individually and so there was this work a method called multi-skin by barbaridol in 2019 which essentially showed that if what we’re interested in is this relationship here between now we have many G’s for a single Gene we have the predictive model from one tissue a second tissue a third tissue and we want to know if there’s an association for any of these features in a joint model so a multi-degree of Freedom test for Association what we actually have again because we don’t have the individual level data we’ve actually observed is these marginal tiwas individual tus statistics um but if we know the correlation between these statistics then we can actually approximate the relationship or the effect under the three degree of Freedom or end degree of Freedom a test from these marginal effects the multi-scan paper also did some clever stuff where you have many tissues with highly correlated expression and you don’t want to just throw them all into this model by using principal components analysis to First reduce the dimensionality of the expression down then just test the leading components of expression for Association in this P degree of Freedom test and this also in practice showed that it produces a much larger number of significant Gene trait associations again now we’re sort of saying that if there’s a little bit of signal here and a little bit of signal here and here then that can add up to a lot of signal across the the three degrees of freedom um the other I think interesting method or any other interesting method in this space is uh is now thinking about how to integrate together um many tiwas associations across a given Locus and so you’ve probably seen methods for GEOS fine mapping that try to identify the set of causal variants or variants that contain the causal variant with some predefined probability the same kind of methodology or the same sort of concept can be applied to tiwa statistics and so instead this work of Mancuso at all in 2019 reformulated this problem in terms of having multiple tiwas associations at a single Locus and then fine mapping these down to the likely to the set of likely causal genes and this is actually sort of starting to address some of the caveats that I outlined earlier when you have co-regulation of multiple genes or you have some tagging across multiple genes this is now an approach to put probabilities on which genes out of many are likely to be causal whereas which are likely to just be tags and to sort of estimate posterior probabilities of causality for a given Gene so just in the last couple of minutes I wanted to mention a bit about what else can be done with this framework and so everything so far that I’ve been talking about has involved gene expression or transcription but really the idea is that any molecular trait that is heritable and that can be predicted from data that we’ve measured is amenable to this sort of approach and this way of integrating with gwas and in particular we can go back to this model which maybe we’ve we’ve solved now in some sense and and sort of we can observe that this is this is also an over simplification in that most of the time for non-coding variants what we expect is that there’s some regulatory element that sits in between the variant and the express Gene that maybe excuse me is the modifier or is the mediator of this gene expression so really there’s probably an enhancer or a transcription factor or a combination of those features um through which this snip has an effect on the expression of the gene which then goes on to have an effect on the trait and with sufficient data we can actually start modeling these regulatory elements and the genetic predictors of these regulatory elements and we have some recent work to that end which we call a regulome or a system-wide Association study so we’re sort of padding out the the the the letters of the alphabet here but the idea is that instead of learning predictors of a given Gene you can learn predictors of a some biochemical activity including transcription Factor binding chromatin state or chromatin accessibility and and additionally in this regime we can also leverage some allele specific information of variants that are inside these Peaks that we suspect to be modifying their activity and so we can boost power even further because because we can we can sort of Leverage signal within each individual in addition to Across the individuals and again we’ve shown that this approach is fairly robust that you can identify a very large number of predictive models and when we’ve applied this approach specifically to cancer gwas phenotypes so again this is cancer risk we’re just talking about the predisposition to develop cancer we see going back to this plot that I started with we’ve now characterized each of these loci where we see that there were this Inner Circle here is the number of lots that it had a significant tiwas association with a gene but then actually when we incorporate these epigenetic features we see a much larger number of loci that additionally have associations through chromatin accessibility in this case many of which are do not actually exhibit a direct transcriptomic Association and so this is actually sort of interesting and somewhat mysterious in that we’re able to identify loci where there seems to be a genetic regulatory effect that we don’t see have a downstream Cascade on expression we do capture most of the low side that have the the the the tiwas association those were able to characterize but then we have this number of additional loci and we’ve sort of started to think about what those loci could be telling us one observation is that if you look at uh um if you look at the sort of distribution of evolutionary constraint across the genome you will see that as in regions with higher evolutionary constraint we see fewer tiwas models that can be built probably because selection is making it more difficult to detect or is decreasing the observed effect on expression making it harder to pick up the eqtls but we actually see more of these arwas or chromatin was models observed in those loci so this is maybe this Gap could potentially explain that sliver in the previous figure of these are low side that are very difficult to pick up in the expression framework but are not as difficult to pick up when we look directly at the intermediate chromatin phenotype and a sort of related observation that we’ve made is if you look at genes in terms of their tissue specificity so as we move from here to here from the left to the right these genes are more specifically expressed in prostate tissue we’re looking at a prostate cancer GEOS again we see that the tus models there’s fewer of them they’re harder to fit for more tissue specific expression but for the chromatin based models and the transcription Factor based models there’s more of them and they’re easier to fit and so again this could be pointing towards a phenomenon where more tissue specific expression has lower power for the sort of eqtl and transcriptome-based models but higher power for these epigenetic based models and so with that I’ll conclude um I hope I’ve been able to uh convince you at the very least that gene expression is a complex heritable and predictable trait and this predictability is something that we can leverage to integrate that trait into other data sets or we don’t have it measured and specifically we derive this tiwas statistic which is a measure of the cisgenetic correlation between the gene expression and the disease as I noted at the end this is not just limited to transcription other molecular phenotypes can be used within the same framework and and again I want to sort of emphasize the caveats that go along with any kind of Association is that it’s not a a causal inference and in fact causal inference in this space is I think a really interesting and sort of ongoing open problem how do we disentangle all of those different arrows that I was showing earlier and then also just to remind you that all the prediction here has been within the cislocus of the gene that’s where we have power at the current sample size but there’s a whole world of trans effects which we haven’t uh really we sort of just barely scratched the surface in understanding and so that’s something that I think as studies get larger and as we have more experimental data we’ll also be able to fold into this framework and so with that I’ll take your questions thanks thank you"
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "Chapter 5: GWAS analysis",
    "section": "",
    "text": "Chapter goals:\n\nUnderstand what are the main analysis steps in a Genome-Wide Association Study (GWAS).\nUnderstand the GWAS quality control metrics and considerations for each step.\nUnderstand what imputation is and why it is useful for GWAS.\nGain understanding of the statistical background to GWAS association testing and considerations for those association tests.\nUnderstand how meta-analyses are important and useful for GWAS, and various methods for implementing them."
  },
  {
    "objectID": "chapter5.html#quality-control-introduction",
    "href": "chapter5.html#quality-control-introduction",
    "title": "Chapter 5: GWAS analysis",
    "section": "Quality control: Introduction",
    "text": "Quality control: Introduction\nTitle: Quality control\nPresenter(s): Katrina Grasby (katrina.grasby@qimrberghofer.edu.au) and Lucía Colodro Conde (Lucia.ColodroConde@qimrberghofer.edu.au), from the 2021 International Statistical Genetics Workshop hosted by the Institute for Behavioral Genetics at the University of Colorado, Boulder.\nLevel: Beginner friendly\nLength: 16:34\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#running-quality-control-on-genotype-data",
    "href": "chapter5.html#running-quality-control-on-genotype-data",
    "title": "Chapter 5: GWAS analysis",
    "section": "Running Quality Control on Genotype Data",
    "text": "Running Quality Control on Genotype Data\nTitle: How to run Quality Control on Genome-Wide Genotyping Data\nPresenter(s): Jonathan Coleman (jonathan.coleman@kcl.ac.uk)\nLevel: Beginner friendly\nLength: 16:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#considerations-for-genotyping-qc",
    "href": "chapter5.html#considerations-for-genotyping-qc",
    "title": "Chapter 5: GWAS analysis",
    "section": "Considerations for Genotyping QC",
    "text": "Considerations for Genotyping QC\nTitle: Considerations for genotyping, quality control, and imputation in GWAS\nAuthor: Ayşe Demirkan (a.demirkan@surrey.ac.uk)\nLevel: Beginner friendly\nLength: 33:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-introduction",
    "href": "chapter5.html#imputation-introduction",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Introduction",
    "text": "Imputation Introduction\nTitle: Haplotypes and Imputation\nPresenter(s): Dr. Gábor Mészáros, University of Natural Resources and Life Sciences, Vienna.\nLevel: Beginner friendly\nLength: 19:25\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-steps",
    "href": "chapter5.html#imputation-steps",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Steps",
    "text": "Imputation Steps\nTitle: Imputation\n\nPresenter(s): Dr. Sarah Medland, Queensland Institute of Medical Research.\nLevel: Beginner friendly\nLength: 16:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#imputation-deep-dive",
    "href": "chapter5.html#imputation-deep-dive",
    "title": "Chapter 5: GWAS analysis",
    "section": "Imputation Deep-Dive",
    "text": "Imputation Deep-Dive\nTitle: An Introduction to Genotype Imputation\nPresenter(s): Dr. Brian Browning, University of Washington.\nLevel: Intermediate/Advanced level.\nLength: 44:37\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#the-biometrical-model-basic-statistics",
    "href": "chapter5.html#the-biometrical-model-basic-statistics",
    "title": "Chapter 5: GWAS analysis",
    "section": "The Biometrical Model & Basic Statistics",
    "text": "The Biometrical Model & Basic Statistics\nTitle: Biometrical Model and Basic Statistics\nPresenter(s): Benjamin Neale (bneale@partners.org)\nLevel: Beginner friendly\nLength: 34:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.html#hypothesis-testing-effect-sizes-and-statistical-power",
    "href": "chapter5.html#hypothesis-testing-effect-sizes-and-statistical-power",
    "title": "Chapter 5: GWAS analysis",
    "section": "Hypothesis Testing, Effect Sizes, and Statistical Power",
    "text": "Hypothesis Testing, Effect Sizes, and Statistical Power\nTitle: Hypothesis Testing, Effect Sizes, and Statistical Power\nPresenter(s): Brad Verhulst: bverhulst@vcu.edu\nLevel: Intermediate\nLength: 35:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter5.2_transcript.html",
    "href": "chapter5.2_transcript.html",
    "title": "Chapter 5.2: Imputation (Video Transcript)",
    "section": "",
    "text": "Imputation Introduction\nTitle: Haplotypes and Imputation\nPresenter(s): Dr. Gábor Mészáros, University of Natural Resources and Life Sciences, Vienna.\nHi everyone, welcome back to the Introduction to Genomics lecture series. This time, we will be talking about haplotypes and imputation before we move on to the new material. So, here is the quick summary from the previous lectures.\nWe talked about SNP markers that are widely used. There is a number of ways to express these genotypes, but we are always talking about biallelic SNPs. These biallelic SNPs are being genotyped with the species-specific SNP chips. We talked about how to determine the positions on the genome, and we talked about physical maps. Also, we talked about recombination events that are of major biological importance, and they introduce variability to the populations.\nHaplotypes\nThis graph is also from the last time, so we have an individual here, and there is a recombination event. The previous “capital A,” “capital B,” and “capital C” haplotype is changing to “capital A,” “lowercase b,” and “lowercase c” haplotype because of this recombination event.\nDuring this lecture, we will look a bit more closely at these haplotypes and also show how to use them or what is the use for them in the context of genotype imputation.\nSo, when we look at the genotypes, what we see, in reality, is paternal and maternal chromosomes together that are joined during fertilization to the set of alleles. So, we see certain genotypes at certain loci. For the sake of example, let’s say that we have these four individuals, and at four loci, we have these genotypes. What we see here are summaries only.\nNow, of course, we can ask the question: What are the actual sets of alleles on each chromosome? For the first individual, it is easy because its genotype consists entirely of homozygotes. So, basically, “A A,” “B B,” and “C C.” So, we know that on both chromosomes are actually the very same haplotypes of “capital A lowercase b” and “capital C.”\nIn the second individual, we have one heterozygote already. So, while here is also just actually one option how to divide the haplotypes, the actual haplotypes on both chromosomes are different from each other. Because on one chromosome, there is “capital A capital B” on “lowercase c,” and the other chromosome has “lowercase a capital B” and “lowercase c.” So basically, one of each of these alleles goes into one chromosome, and the other to the other chromosome.\nOf course, it becomes more interesting the more heterozygous we have on our genome, because this actually creates options on how the haplotypes could be distributed. So we have these three loci here, and two of them are heterozygous: the loci B and C. If we look at the pairwise combinations of these alleles, then we could arrive at actually two solutions: either this one or this one.\nIf you look at the alleles in these haplotype pairs, then the genotypes will end up always with this summary genotype. But also, if you look carefully, the haplotype pairs, the first two haplotype pairs are different from the second two haplotype pairs.\nOf course, the more heterozygotes we have, the more complicated it gets. So, for example, for the three heterozygotes, we have even more combinations. So, I put question marks here. So if you want, you can work this one out yourself. Just pause the video here and try to work out what are the actual haplotype possibilities in case that we have three heterozygous loci. So, what are the haplotype combinations that are possible that end up with these summary genotypes?\nAfter you’ve done it, you can unpause the video and see if you were right or just continue watching and get the answer in three, two, one… Go!\nSo, these are the actual possibilities. You see that there are actually four haplotype pairs that are possible based on these three heterozygous genotypes, and each of these haplotype pairs is different from one another. So, basically, how you solve this? First, you take the first from each pair: “capital I capital B capital C,” and then remains “lowercase a lowercase b lowercase c.” So this would be the first one. Then you go with the pairwise combinations. So you flip one locus at a time until you get to all combinations.\nPhasing\nOf course, we have many more than just three heterozygotes on the genome. So, there is a question: how do we solve these questions in practice, where we have tens of thousands of loci and also thousands of heterozygous genotypes? Now the answer is, of course, with computers. Fortunately, there is sophisticated software that solves these questions for us and delivers the haplotypes we could analyze further. This computation process is called phasing.\nSo, the phasing is a task of the process in the computer to assign alleles to the paternal and maternal chromosomes. It looks for haplotypes, or these so-called “phases,” in large-scale genotype data and solves these complex problems of assigning correct haplotypes.\nOf course, this is easier if the so-called trios are genotyped. So they basically are the offspring and the father, mother, and their child, or even if we have multi-generational trios that include families, including grandparents and great-grandparents, are genotyped. So if everyone is genotyped, this process is somewhat easier. In reality, however, we don’t have this ideal situation. Many times only parts of the populations are genotyped, so it is harder to work out the actual haplotypes. Fortunately, this is also possible, and haplotypes could be determined also for samples of unrelated individuals, for a population. “Unrelated” here is in quotation marks because there is usually some kind of relationship between the individuals within a population.\nSo, as I mentioned before, there are specific software solutions for all of this, which actually divide the genotypes into smaller segments and try to derive these haplotypes from these smaller segments and merge them back properly.\nImputation - general definition\nNow, when we determine these haplotypes or these phases in a population, these are really useful for a number of purposes, and one of these purposes is the so-called genotype imputation. I mentioned multiple times that the SNP genotyping is fairly reliable, but occasionally, we see missing genotypes. So, actually, with this genotype imputation process, we can make an educated guess on how to fill in these missing genotypes so we get the full information.\nSo, the imputation process is nothing else than filling in missing information. There are two major ways how we can use this method. The first one is the imputation of sporadically missing SNPs, and the other one is imputation between SNP chips. For example, we can extend a lower density SNP chip, for example, a 50k SNP chip, to a higher density. For both of these approaches, I will give examples in the following slides.\nImputation of sporadically missing genotypes\nOut of the two methods, the imputation of sporadically missing SNPs is more straightforward. So, as we established, some of the SNPs could be missing due to genotyping error, and because of these genotyping errors, we might be forced to remove individuals from our analysis. Or, for example, if we need complete data in a sense that all SNPs should be known, then this is also a problem for us. But this situation could be fixed by imputing these sporadically missing SNPs.\nLet’s say that we have an established haplotype in a population that looks like this: [haplotype diagram not provided]. And when we have another animal or individual that is genotyped, and there is a genotyping error, but the haplotype looks like this: [On slide]. So it’s basically totally the same as before, so all the other loci for this haplotype are exactly matching, but these genotypes are missing. Based on this comparison, if every other SNP fits, we have a very good idea what should be filled in the place of the question marks. So we have complete data also for this individual.\nImputation between different SNP densities\nThe imputation between SNP chips works on a similar logic, but it’s somewhat more complex. So let’s say we have SNP chips of two densities, and this is a smaller example. So you see that there are 16 columns here. This will be our larger SNP chip, and the second SNP chip would be a smaller one that consists of eight SNPs. Each line here would be an individual, and each column would be a locus, and these loci are either homozygous for one (that is a 0), heterozygous (that is a 1), and homozygous for the other (that is a 2).\nNow, the usual arrangement with these smaller and larger SNP chips, so that contain more or fewer SNPs, is that one SNP chip or the smaller SNP chip is a subset of the bigger one. So, basically, all the SNPs from the smaller SNP chip appear on the bigger one as well, but there are other SNPs that are on the larger SNP chip but unknown for the smaller one. This shows the starting situation here when we genotyped nine individuals with the smaller SNP chip.\nNow, let’s say that these individuals are from a population that are fairly unrelated, but we also know that even in unrelated individuals, there are short stretches of sequences that are identical by descent. These local patterns of IBD (Identical by Descent) could be described, and also the length of these segments determined, which, of course, varies based on the recombinations. If we identify these segments or these haplotypes, we can use them to our advantage. So, for example, these would be the haplotypes that occur in our population, and also, for the sake of this example, these are also color-coded.\nSo if we return to our original example for the nine individuals that are genotyped with the lower density SNP chip, we could see that each of these individuals could be described as a combination of certain haplotypes. And because these haplotypes are already known, we actually know what we should put into the place of the question marks. This is then also done, and the information is being filled in to these gaps that were previously unknown.\nSo what we basically do is we take the information from the higher density SNP chip, make the haplotypes for the population, and we use the information from these haplotypes to fill in the information also for the other individuals that were genotyped with a lower density SNP chip in case this lower density SNP chip is a subset of the higher density SNP chip.\nHere I would also underline that these haplotypes do not come from nothing but actually, we need a sufficient number of individuals that are actually genotyped with this higher density SNP chip in this population. So we can determine the actual haplotypes that occur in this population, which can be further used for this genomic imputation as described here now.\nImputation accuracy and practical use\nWhy is this useful? Well, the lower density SNP chips tend to cost less. So if genotyping costs are an issue or we want to genotype a really large number of individuals, we can use, well, just this lower density SNP chip and go for the imputation process. Of course, for this, we need haplotypes that were determined based on individuals’ genotypes by the high-density SNP chip. This imputation is a so-called “in-silico,” so basically, it’s done with computers, which also means that it is with no additional costs other than the computation cost for the whole process.\nThere are different options and possibilities for software for this process, and to my knowledge, all or most of them are also free or open access. Based on this software, we can do the imputation that will be done with a certain accuracy. So actually, the whole process is not 100 percent accurate, but actually works surprisingly well. The imputation accuracy, in general, depends on the size of the reference set and the data quality. What I mean with this is that we need to determine actually the haplotypes that occur in this population or the population of interest. So of course, we need to have a representative sample genotype with the higher density SNP chips in order to determine the haplotypes that occur in the population. So we could use these haplotypes further on in the imputation process.\nIn general, the imputation works really well for the common SNPs, which occur reasonably frequently within a population. This also means that, unfortunately, the imputation works less well for these so-called rare SNPs that occur very infrequently, because there is just no way for the imputation process to pick it up from the haplotypes that are available for this population. So the general advice is that if someone is interested in very specific rare alleles, then the imputation process is perhaps not the best solution. In that case, genotyping the individuals with the actual higher density SNP chip is advisable.\nBut overall, the imputation works really well. So I put there that the imputation accuracy could be more than 95%. So I just put their numbers so you have a bit of an idea that we are talking about very high values, actually especially in the simulation studies. In my experience, the imputation accuracy is lower than 99%, and the people start to get unhappy. So it’s, actually, in the papers, especially in simulations, the imputation accuracy is much higher than 95%. In real data, well, it could be variable, as I mentioned. This really depends on the reference and the data quality.\nAlso, there is a range of possibilities how to evaluate the actual imputation accuracy, but it is mostly done with the so-called masking procedure. So it’s a very similar process that I described also in this presentation. So there are the genotypes obtained from a higher density SNP chip, and basically within this process, some of these genotypes are deleted, and then the imputation software is used to fill these missing markers in. But of course, we know what is the actual genotype for these higher density SNP chips.\nSo then, basically, the values that were filled in by the software and those that were obtained from the actual genotyping are compared, and this is the basis of how actually the imputation software is also being evaluated, how good of a job it does. But as I mentioned, these software do a surprisingly good job, and we already arrived at the end of this segment, and we end, as always, with a short summary.\nSummary of the lecture\nWe talked about the so-called haplotypes that are a series of SNPs, and these haplotypes clarify which combination of alleles come from which parent. Of course, if we want to do these computations on the large scale or in real genotypes, we need to use computers for it, and there is a range of specialized software programs that do the job for us. The approach itself is called phasing, and these phases or haplotypes could be used in various ways, but one of the uses is the so-called imputation process, which is nothing else than filling in the missing SNPs to our data.\nHere we also have options. If we want to fill in sporadically missing SNPs that were not genotyped for some reason, so some kind of genotyping errors or missing SNPs could be filled in or imputed. Or we have a different option when we can actually extend smaller SNP chips to a larger one based on haplotypes and information from these larger and denser SNP chips, perhaps even saving some money in the process because these lower density SNP chips tend to cost less. And if we are not interested in some very specific rare alleles, and we are fine with the imputed version of these genotypes, we can use these for our research.\nSo we end here today. Let me know if you have any questions or comments down in the comment section below. Also, thank you for your time you spent on this video, and I wish you a very nice day.\n\n\n\nImputation Steps\nTitle: Imputation\nPresenter(s): Dr. Sarah Medland, Queensland Institute of Medical Research.\nSarah Medland:\nHello, my name is Sarah Medland and I’ll be talking to you today about imputation. So there are three main reasons why we might impute data. The first of these being meta-analysis or combining our data with that of another cohort. Secondly, fine-mapping. And I’ll give an example of that in just a moment, and Thirdly is to combine data from different chips.\nSo imagine a situation where you have a large cohort which is being genotyped half on chip A and half on chip B. If we were to put the data from these two chips together and analyze them, we would end up with a mixture of power distributions. So we would have some SNPs that are on both chips and they would be the most powerful SNPs in the analysis compared to those that are on one chip or the other. If we were to take this forward for analysis and look at our QQ and Manhattan plots, we would have a very hard time interpreting those results because of that mixture of powers. So if we were to find an association and go in and look at the region, we could expect that the distribution of p-values wouldn’t follow what we would expect based on the LD or the correlation structure within that region. So because we have this differential in power, the SNPs that would be on both chips would have the highest power and potentially higher p-values than those that are on one chip or the other. So to get around this, what we could do is bring those two data sets together, take them forward for imputation, and end up with a data set that has a fairly constant N and a single power distribution that’s not dependent on whether or not a SNP was present on the chip or not. We can also use imputation to correct for sporadic missingness, and genotyping errors, and also impute in types of variation that we haven’t directly genotyped such as structural variants.\nHere’s an example of fine mapping. In this situation we have run a GWAS, but we’ve only used genotyped SNPs and we have this variant that we’re finding on chromosome 19. When we go in and have a look, it appears to be floating, so it’s not really supported. We have nothing really in this region to back it up particularly well. So looking at that, it’s very hard to work out if that could be a true finding or not, and one of the things we might do is fine mapping, which is to go in and impute other content in that region and see whether there is additional support that we’re not observing in our genotyped SNPs. So when we go ahead and do that in this case, we can see this actually is the true effect. It’s well supported by SNPs in the region, it’s just that these variants were not genotyped on this chip.\nSo when we’re talking about imputation, what are we actually talking about here? We usually start with a genotype data set that has missing or untyped genotypes. We have a reference set of haplotypes, so a public reference set, and those references are compared to our genotypes. We try and identify which haplotype best represents each segment of data, and then we infer in the missing content. So to put this another way, we start with the genotype sample, which has some genotypes but is missing others. We have our set of reference haplotypes. What we’re going to do is compare our genotype samples to our reference haplotypes. Try and work out which haplotypes best represent which segments of data, and then infer in the missing genotypes. This is done in a probabilistic way, and we can assess the accuracy of this imputation as we go.\nSteps to Imputation\nOK, so there’s a couple of steps and things we need to think about when we’re setting up for imputation. So firstly we need to have really well QC’ed data and this would be similar to the QC that you were shown in the QC in GWAS session from yesterday. Secondly, we need to decide which of our references we want to use, and we are in the situation where we now have quite a lot of references, so it’s worth thinking carefully about why you are using a particular reference and what you’re trying to do with your analysis.\nThe most common references to use at the moment are the 1000 Genome and the HRC references the 1000 Genome reference is a multi-ethnic reference, whereas the HRC is a predominantly European reference. The HapMap and 1000 Genome references can be downloaded and used locally. The other references are mainly only available from custom imputation servers. Although there is a wide difference in the size of the references, so for example 1000 Genome reference yields around 20,000 markers, whereas, sorry 20 million markers, Whereas the HRC yields around 40 million and TOPMed yields around 300 million. At the end of the day, if you have a cohort of predominantly European individuals, you’ll likely to end up with between 8 and 10 million usable markers for your analysis.\nPhasing\nSo once we have QC’ed and decided on our references, the next step is to phase our data. Phasing in this case means we estimate the haplotypes within our data. So we take our genotype data. We try and reconstruct the haplotypes using reference data and so for example, in this situation here we have three genotypes and there are four potential haplotypes that we can arise that from that data. We don’t do this manually. We use software that’s been specially designed to do it, and the most common software packages at the moment, are Eagle and Shapeit. They use hidden Markov model and Markov chain Monte Carlo methods to reconstruct the haplotypes. And then these are used to provide scaffolds to infer or impute the data.\nImputation Programs\nFor our imputation as well we use customized programs and the most commonly used ones at the moment are minimac or impute. There are others that are available. An important point is to never use Plink for imputation, although Plink has an imputation option, it’s really not very well designed and I wouldn’t recommend using that. So the two most commonly commonly used imputation programs are minimac and impute. So minimac comes from the work of Gonçalo Abecasis, Christian Fuchsberger, and colleagues. And has a number of downstream analysis options, including SAIGE which will use later in the week, BoltLMM and Plink2. Impute is now up to impute version 5. This comes from Jonathan Marchini and colleagues, and it incorporates the Positional Burrows Wheeler Transform (PBWT), so it’s a fast and efficient way of undertaking imputation. Once again, it has a number of downstream analysis programs that have been written specifically for the output of this program.\nImputation Cookbook\nSo how would you actually go about doing your imputation? If you are in this situation where you have to do imputation locally, I would seriously recommend using what we call a cookbook and there are a number of these available online. So here’s a link for minimac3 imputation cookbook for 1000 Genomes. If you are in the situation where you can use an imputation server, I strongly recommend that you do that and there are a couple of these available. So there’s one at University of Michigan, which is probably the most heavily used one, there’s one at the Sanger in the UK and a new one, the TOPMed imputation server for those wanting to impute TOPMed data. Here’s a little shot of each of those front pages. In the practical, we’re not going to walk through how you impute data, because there’s a really good set of imputation practical sessions available on the Michigan imputation server site, these are from the American Association of Human Genetics meeting in 2020, and you can walk through each of those if you’re interested in learning how to run imputation on the server.\nData QC\nThe main points are that as I said, we need to QC the data well so we exclude SNPs with excessive missingness, low minor allele frequency, Hardy–Weinberg issues and Mendelian errors. We should also drop strand-ambiguous or palindromic SNPs. And you need to be careful that your data is on the right build and alignment. So depending on which reference you’ve chosen, if you’ve chosen the TOPMed references, you need to have your data on build 38. If you choose the others the build should be on build 37. So you need to output your data in the format expected for the phasing program, and it’s really important that you check the naming convention for the references and the program that you want to use. So do the SNPs, use RS numbers, or are they in a position reference?\nIf you are using an imputation server, once you have your data QC’ed and ready to go, it really is as simple as uploading your data, picking the options that you want to use, and then submitting the job. After the imputation you have about a week to get your imputed data off the server and then it’s all wiped.\nOK, so once we’ve done our imputation, if we use the Michigan imputation server, then our data is going to be in a format called a VCF format. So in this format, each line in the file represents a variant and each block of data represents an individual. The file contains our imputed data in three different formats. The first of these before the semicolon is hard-call or best-guess genotype and this refers to the number of copies of the alternate allele that someone has. The second of these is the dosage format, which ranges between zero and two, and once again this is a count of a number of doses of the alternate allele that someone has. The third format, which is not used very often is what we call a genotype probability format, and so it’s the probability that an individual is has an AA, AB, or BB genotype for each of the SNPs in our file.\nTo go along with these, we have a series of info files that contain the information about the imputation accuracy and the frequency of the variants in the sample. So we have our SNP identifiers and you can see some of these get quite long. We have our two alleles, our frequencies, our r-squareds, or imputation accuracies, a column telling us whether it’s genotyped or not. And for those SNPs that are genotyped, we have a leave-one-out imputation accuracy. So this gives us an idea of how accurately these genotyped SNPs have been imputed or would have been imputed if they weren’t already genotyped.\nRsquared\nSo one thing to keep in mind is there are subtle differences between the way that the different programs create their R-squared metrics. In both cases, effectively the R-squared is the ratio of the observed variance to the expected variance. But there are small differences in how they are calculated. There’s also a difference in that the IMPUTE info measure is capped at one. Whereas the MACH or Minimac r-square measure is allowed to go above one as an empirical estimate.\nThe two programs to get fairly good agreement though and they should line up fairly well if you were to impute the same set of data both ways. So the R-squared or the info is telling us about the level of certainty we have in the data. So if we had an R-squared of one, it’s indicating there’s no uncertainty. R-squared zero means complete uncertainty, and for example, an R-squared on .8 on 1000 individuals will give us the same amount of power as if that SNP had been genotyped on 800 individuals? So you can think of it that way.\nAfter we’ve done our imputation, it’s a good idea to do some QC and check that it’s worked. So some things that you can do is to look at the minor allele frequency compared to the reference and see how that looks. You can also look at the R-squared across the chromosome and see how that looks. So here’s some toy examples of a relatively good imputation, but you can see are r-squared sort of varies quite markedly across the genome, but generally follows a fairly well defined distribution. Whereas this is particularly and deliberately a bad imputation, and you can see that we’ve got a very different distribution here.\nGWAS\nAfter doing the basic imputation, it’s a good idea to run a GWAS for a trait that is fairly well powered. Ideally something continuous. Have a look at the Lambda and look at your Manhattan QQ plots. And then run the same trait using GWAS using only the observed Genotypes and plot the imputed versus the observed variant results and see what you get.\nConsortiums\nLastly, quite often when we’re running imputation, we’re actually running it for a consortium or a meta-analysis, and they will give you instructions about which reference panels to use and what to do. They will probably ask you to analyze all variants, regardless of whether they pass QC or not. It’s important to think about this, especially if you are using those TOPMed references which have something like 300,000,000 variants, as only around 8 to 10 million of those will typically be useful for GWAS, and so you’ll be running analysis and uploading results for many, many SNPs that won’t be very useful for anyone. So if you’ve got any questions, feel free to ask them at the start of the practical session or to post them to the slack. Thanks very much, bye.\n\n\n\nImputation Deep-Dive\nTitle: An Introduction to Genotype Imputation\nPresenter(s): Dr. Brian Browning, University of Washington.\nBrian Browning:\nThis talk is going to look at genotype imputation, which is a standard technique. We’ll cover just an overview of it, and we’ll look at some of the models used, which then in the research talk that I’ll have on Thursday, we’ll make use of some of the information in the tutorial. And I’ll finish up with maybe a little bit of discussion of programming because that’s something that’s very relevant to this audience.\nSo, imputation is just estimating missing data. You can use the other data in the dataset, you have external datasets, and if you’ve played any word games, you’ve done imputation. Classic examples are hangman. Start with three characters, the last two characters are “ATT,” about a third of the letters in the alphabet can fit in there, and hangman gives a good illustration of a general principle of imputation. The more context you have, the better you can fill in or estimate that missing data. If I give you some additional characters for the sentence “The dog chase ___,” you can do a much better job filling it in. Instead of a third of the characters in the alphabet, there’s one “A,” “C” probably springs to mind first, it could also be an “R,” but your probability distribution becomes much more pointed.\nWhat is genotype imputation?\nGenotype imputation is the filling in of genotypes. So, it originally started where you actually were imputing genotypes. Now, for computational reasons, we work on the haplotype and imputing allele level. So, your reference data consists of reference haplotypes to phase reference haplotypes per reference sample, and the sample you’re imputing has two haplotypes too, but it’s missing a lot of data. It’s typically genotyped on a SNP array, and you have just a couple of—just get back there—you know, you might have a marker here, or on one haplotype, there’s a G, and a marker here, or the other haplotype, there’s a C, and on the second haplotype for the sample, an A and a C. And based on these reference samples, your SNP array data, and a probabilistic model, you want to make inferences about what all these dots are.\nApplications to GWAS\nImputation has been around for a long time. Imputation of sporadic missing genotypes has been around for a long time, but imputation came into prominence in 2007 where a group from Oxford with the Wellcome Trust Case-Control Consortium and, sort of simultaneously, a group in Michigan, Gonzalo Efficaciousness Group, developed methods for imputing ungenotyped Type 2 markers using reference data. The initial application was defining new trait-associated loci, and in the initial study, it actually didn’t produce much, although it did produce some power. The idea is that you have a SNP array in which you genotype three hundred thousand, five hundred thousand, or a million markers, but there are a lot of other markers in the genome. If you can impute them, then you can test additional markers. So, it should give you a little bit of an increase in power, and it will—in the second application—was for fine mapping.\nSo, your genotyped markers showed that you have an association in a region, but there may be other markers that weren’t genotyped that give you a stronger signal. That can be valuable for replication studies. So, you impute the additional markers in the region. You might find a marker that’s more highly correlated with the trait you’re interested in, and then that’s the marker that you’d want to take forward, definitely including in your replication study because it should have a greater chance of replicating the association if it’s real.\nThese first two applications are nice; I don’t think they’re necessarily game-changers, but they’re useful—very useful. But the real killer application is meta-analysis. So, there are lots of different SNP arrays out there with different numbers of markers by different vendors. SNP arrays from different vendors tend to not have a lot of overlap, and when you want to do meta-analysis, it’s very difficult to do meta-analysis when your datasets are genotyped on different markers. It’s like this Gordian knot. Well, imputation just slices through that knot. You take a reference panel, you impute all your individual datasets so that they all have the same markers that are in the reference panel, and now they’re on the same set of markers, and you proceed.\nSo, that’s been very—when you see these studies in Nature and Nature Genetics where they have several hundred thousand samples and they have scores and scores of associations, it’s imputation that made that work in a straightforward way because there are a lot of different datasets, and they had to use imputation to get them all on the same marker sets to do the meta-analysis. So, the meta-analysis, I think, has been very, very successful.\nImputation output\nWhat you get out of imputation is not necessarily like Hangman where you’re guessing the most likely letter. It’s a probabilistic output. So, we think based on the reference data and the observed data in the sample that on this allele of this haplotype, there’s a certain probability that the allele is the A allele or a certain probability that it’s the B allele. So, throughout this talk, all these methods extend to multi-allelic markers, but first, just to remove that complexity, we’ll assume diploid allelic markers. Now, I’ll typically refer to the alleles as A and B. My background is with human data, so whenever I refer to some physical characteristics of data, I’ll be usually thinking of human data. So, my apologies to people from an animal background. It’s just that’s not my background. So, my examples are from the human domain.\nHere’s a haplotype at a marker. You might have, for the A allele, a probability of 98%, and for the B allele, 2%. And on the other haplotype, you also have a probability distribution that essentially gives you all the information you need for whatever you want to use. The advantage of probabilistic output is that you’re capturing the uncertainty in the imputation rather than hard genotype calls where you’ve erased that uncertainty. You can get called genotypes if you want by just taking the genotype that has the highest probability. To get posterior probabilities at the genotype level rather than the allele level, you can assume Hardy-Weinberg equilibrium, and it pops out. Also, with probabilistic genotypes, you can use them in the standard frameworks for testing. So, if you do linear regression analysis, often the predictor at a marker is the number of copies of the minor allele—zero, one, or two. That same framework works with imputed data; it’s just instead of an integer number of copies, you have the expected number of copies, which is sometimes referred to as the expected dose of the allele. Like in this example, the B allele dose turns out to be 0.88. That’s what you’d plug into your regression analysis.\nMeasuring imputation accuracy\nThere’s more uncertainty in imputation, and you need a way of measuring it. There are two ways—one is really obvious, and the other is a little less so. The most obvious way is genotype discordance. The less obvious way is the correlation in allele dosage. The first groups that were developing imputation devised these methods; the Michigan group and the Oxford group devised something similar. Michigan devised the correlation metrics I’ll talk about here. So, R squared, even though it’s a bit more complicated, has some advantages. A big advantage is that it’s normalized for allele frequency. For example, if I tell you that I can impute the alleles at a marker with 99.9% accuracy, it’s tough to interpret that without more information. If the marker allele frequency is 30%, 40%, 50%, then 99.9% accuracy is really good. If the marker allele frequency is 0.1%, then 99.9% accuracy is really, really bad. You could use the dumb imputation strategy of always imputing the alleles to be the major allele, destroying all the information at the marker, and still achieve 99.9% accuracy. So, you have to know the allele frequency. Correlation automatically builds it in. If you’ve taken an introductory statistics class, you know that when you compute a correlation, variances are in the denominator. These variances capture the allele frequencies. This way, correlation can be interpreted without having to know the allele frequency. The squared correlation metric is looking at the expected correlation between the imputed allele dose (imputed number of copies of alleles in samples) and the true number of copies of alleles in samples. It has a couple of other factors that may not be obvious, but they’re useful. R squared can be estimated if the true genotypes are unknown. You can estimate your accuracy from the imputed data itself without knowing the truth. This assumes that your posterior genotype probabilities are well-calibrated. There’s a derivation of something similar that illustrates one way to derive this, given in the reference I’ve cited in the American Journal of Human Genetics. The second surprising feature is that R squared gives information about relative power. There’s an interpretation in terms of power that’s useful for R squared.\nIt turns out that allelic tests have similar power whether you use imputed genotypes for in-samples or true genotypes. This has been known for a long time. The best explanation I’ve seen is a box in the American Journal article that I’ve cited. If you’re looking at a marker that you’ve imputed where the estimated R squared (which we’ll take to be the true R squared) is 0.8, and you have a total of a thousand samples—half cases and half controls—if you test that imputed marker, the power should be roughly the same as using the true genotypes, which are correct for 80% of your samples (800 samples). This gives you a way to interpret what that R squared might mean. The general rule with imputation is that you can impute high-frequency markers really well and low-frequency markers not very well. Where’s the cutoff? What determines the threshold of what you impute? There are two primary factors—one we can change and one we can’t. The one we can’t change is the effective population size; we’re stuck with that. The more diverse the population is, the bigger the effective population size, and the shorter the shared haplotype segments are. The shorter these shared haplotype segments are, the harder it is to impute. We can’t change that. But what we can change, if given money, is the reference panel size. The bigger the reference panel size, the lower frequencies we can impute. We often think in terms of minor allele frequency; for many applications, that’s more natural. However, for imputation, the thing I find useful to think about is minor allele count.\nImputation accuracy (MAF VS MAC)\nSo, this is the same data, and these plots are sort of nice for getting a sense of the potential for what imputation accuracy can be for different frequencies. This is simulated data from a Northwest European population. Reference panel sizes vary from 50 to 200 thousand samples, and the target data was on a million SNP chip. On the left-hand plot, we’re plotting the squared correlation. This is using the true simulated truth versus the imputed data for different minor allele frequencies. On the right-hand plot, it’s the same data but broken up by minor allele count. On the left-hand side, you can see that you really need to know the frequency of what you’re imputing to understand the accuracy. On the right-hand side, when expressed in terms of minor allele count, it’s much more stable. With this simulated data, around 10 markers (10 copies of the minor allele) in the reference panel, you’re getting a squared correlation accuracy around 0.74. With around 20 markers in the reference panel, 20 variants in a reference panel, you’re getting around 80% imputation accuracy. This is simulated data; it’s going to be a bit better than current reference panels because current reference panels are predominantly, if not totally, from low-coverage sequence data. Low-coverage sequence data has its Achilles’ heel in estimating low-frequency genotypes, so it has a very high error rate for low-frequency genotypes. However, as we move into high-coverage reference panels obtained from high-coverage sequencing, this kind of performance should be practical in outbred populations like European populations.\nOne of the inferences from this is that as you double the reference panel size, if you were able to impute markers with at least 20 copies of the minor allele with a certain reference panel size, when you double it, that should still be true. It’ll even get a teeny bit better. So, every time you double the reference panel size, the frequency of variants that you can impute, other things being equal, cuts in half. It’s sort of a linear relationship.\nWhy impute when you can sequence?\nNow, if you’re from a sequencing background, a natural question to ask is why impute when you can sequence? Imputation has errors; sequencing is more accurate, especially high-coverage sequencing. Why go to the trouble of imputing? So, this slide just sort of breaks down the pros and the things that imputation is competitive with, high-coverage sequencing, and things that it’s not competitive with. The easiest thing to do is estimating allele frequencies, and that’s what we do. We do Association testing, which is where imputation has been used most widely. That’s its strong point. So, with 50,000 Northwest European reference samples, if you impute down to 20 copies, that’s imputing down to a minor allele frequency of 10 or 2 times 10 to the minus 4th. So, you can go very relatively low and do very well with imputation if your goal is to estimate minor allele frequencies.\nIf your goal is a little bit harder, it’s much harder to estimate a genotype than to estimate a minor allele frequency, then it gets more problematic. In my simulated data, for five percent minor allele frequency and above, it does very well. You can actually estimate the genotype with about 99.9% accuracy in that range. But it’s not true for less than five percent; the accuracy slips, and imputation at a genotype level, not a minor allele frequency level, but a genotype level is just not as accurate. Now, we could improve that by going to ever larger reference panels, but there is a break, and it’s much higher than the break for estimating allele frequency. There’s a much higher break for estimating genotypes’ threshold. And of course, for de novo mutations, I don’t care how big a reference panel is, you aren’t gonna be able to impute them. So, it’s true, there are things that genotype sequencing does much better than imputation, so why would you do it? Money, alright?\nHigh-coverage sequencing costs around a thousand dollars, or at least that’s the last time I checked. I think it’s still in that range, and that may even require an order in bulk. Yeah, I’m curious if somebody has data on that; I’d like to hear what the current costs are for genotyping as a service, chip typing. If you’re a good negotiator, you can get a pretty good deal on chip typing. You need big data sets; you need to play the metrics off against Illumina and get them to go against each other, but you can get it down to $50 a sample.\nImputation with the current methods is about half a cent a genome for 10,000 reference samples, and these are order magnitude figures. About five cents a genome for a hundred thousand reference samples, and if you have a million reference samples (which we won’t for a few years, probably), 50 cents a sample. So, essentially, a hundredfold less than the chip typing cost. And there’s a lot of data out there with chip typing costs or with GWAS chip data available. Compared to sequencing, then, the cost difference is a factor of two thousand. You may not have a thousand dollars to sequence a sample, but you probably have five cents. Alright, so yeah, there’s a trade-off depending on what your application is. If you’re poor, especially if you’re interested in Association testing, imputation gives you a lot of bang for the buck.\nHidden Markov model (HMM)\nSo, the next part of the talk is I’d like to go over the models, the standard model, the most widely used model for imputation. There’s been some very nice, clever work developing other approaches, matrix completion approaches, summary statistic approaches. Oh, just, you know, in the interest of summarizing, I’m gonna talk about the one I’m most familiar with and also the one that, from what I’ve seen, has the greatest accuracy.\nSo, the basic methods are based on hidden Markov models, where you have a Markov process and you can’t observe the underlying states of the process; it’s hidden. What you do have is observed data, and I’ll go through the parts of the model, and then we’ll use this model in the research talk on Thursday. So, I’ll describe the Lee and Stevens model. Once the field of imputation moved into what’s been called pre-phasing, where imputing onto haplotypes, the Lee and Stevens model becomes the model of choice, because it’s computationally tractable at the haplotype level without having to do a lot of shortcuts. You can do the full Even Stevens model, and it gives you very accurate results. The reference for that seminal model is given on this slide.\nThe hidden Markov model has a number of components. I’ll go through those components, and all of them will typically be based on a slide like this. I’ll go over that in some detail. So, the first thing it has is model states, and there’s going to be a model state for every pairing of a haplotype and marker. So, the markers, these are on the reference haplotypes, are given as columns of the matrix. The haplotypes, these are reference haplotypes, not the haplotypes you’re imputing from the reference samples, are given here. We’ll label these h1, h2, h3, h4, and so on. The states of the model then just become the elements of the matrix, these circles. And for reasons that will become apparent in the slide or two, we want to label those states with the allele that the reference haplotype carries. So, we’ll use blue for the reference allele, and the yellow coloring will represent the alternative allele.\nThe number of states in your model is just going to be the number of rows times columns. So, it’s the number of haplotypes times the number of markers.\nInitial state probabilities\nThe next component of the model, after you’ve defined the states for the Lee and Stephens hidden Markov model, is the initial state probabilities. These are the probabilities before you’ve seen any observed data. And the way the model, the process, the Markov process works is you start at the first marker and you work your way through to the last marker. So, the initial state probabilities are, there’s only nonzero probabilities in the first column for the first marker. So, all those, for each haplotype, all the states at the first marker, they will have equal probability. So, the probabilities sum to one. There’s no reason to prefer one state to the other. And at every other marker, at every other column, those states have probability 0 to be at the beginning.\nState transitions\nThen there are state transitions. Just to keep the slide from getting too cluttered, I’ve only shown one so far. The state transition I’ve shown is just what you can think of as the primary state transition. So, the primary state transition just goes when you go from one marker to the next, you stay on the same haplotype with probability close to one. That’s what happens. But actually, it’s a little bit more flexible and complex. With a small probability, you can jump to a random haplotype. That’s what I’ve tried to show for just a single marker right here. So, with a probability close to one, you stay on the same state, and there’s no, what we call, no recombination. With a small probability, the remaining probability, you jump to a random state, and that random state can also include the state you’re on. What that is modeling is historical recombination. In the past, there’s, you know, for a while you’ve inherited the same haplotype your imputing on matches or has inherited the same sequence of genotyped alleles as a certain reference haplotype. And then, because of a historical recombination, it switches to another reference haplotype. So, the probability of that small probability of transitioning to a random haplotype is proportional over a short distance, is approximately proportional to genetic distance. The bigger the genetic distance, the higher the recombination rate, and so you have a greater probability of transitioning to a random haplotype. I won’t show this anymore, but just be aware that the actual state transitions can go from any marker at the next marker, but I’m only going to show the primary ones where you stay on the same haplotype. Alright, the next component of the hidden Markov model is you have to have some way of relating your observed data to the des Markov process, and that comes from what are called emission probabilities. And this is where the labeling, where we labeled each reference haplotype at each state with the allele that the reference haplotype carries. So, if you’re in a particular state, let’s just take the first state marker at the first haplotype, it will, with probability near one, emit the blue allele, and that’s shown in these equations here. So, if you’re in a blue state, the square, I use a square to represent the allele on the observed data, it will be epsilon, here’s a small value, it will be close to one. With a small probability, it will emit the other state. You’ll have a mismatch between your observed data and the state you’re in. And the same principle holds for yellow. If you’re in the yellow, you’ll emit a yellow allele with high probability, and with a small probability, I’ll admit the other allele. This works at any state where you have observed data. These open squares mean you have missing data, like you would have if you were performing genotype imputation. Then, here’s another marker that’s genotyped in your sample you’re imputing. And so, for example, not knowing anything else, you just intuitively expect it’s more likely you’d be in a state on haplotype four, one, or two because the emission probabilities are higher there because the allele matches, than in states H3 and H5. At the states where you have missing data, there, the emission terms, they’re constant. It doesn’t matter what the underlying state is. You have no observed data, and you can treat it as one, and it drops out. Now, let me back up. Once you have the state posterior beliefs, you can get the imputed allele probabilities. So, the key thing you’re trying to understand is given this imputed data at a particular marker, at any imputed marker, I’m sorry, given the observed haplotype, the observed alleles on that haplotype, you want to understand what in each of these hidden states, what its probability is. Once you have that, you’re essentially done with the imputation problem. If you want to know what the probability of the blue allele at the third marker is, once you have these state probabilities, you just add up all the state probabilities for blue haplotypes, for the blue states, and that gives you the probability, posterior probability, of the blue allele. The posterior probability of the yellow allele at the third marker, the probability that that’s yellow, you just add up the state probabilities for all the states for the reference haplotypes where the reference haplotype carries the yellow alleles. So, the key is those state probabilities, and that’s the next slide.\nCalculating HMM state probabilities\nSo, this standard way of breaking this up, it’s really beautiful math. I love the math that you use to get these state probabilities. Is you break it up into what’s called a forward probability and a backward probability. So first, this is the state probability. Because little m is a marker, H is your haplotype, so it’s the probability of being in the state at marker M, haplotype H, and the O is your observed alleles. Capital M is the total number of markers. So given all the observed alleles at the genotype markers, you want to be able to compute, conditional on that, what the state probability is. And from what we talked about in the last slide, as soon as you know the state probabilities, you just sum them up to get your imputed allele probabilities. So you break it up using the end of conditional independence assumptions into two parts that are called a forward and a backward probability. So there are a couple of things to, I think, note about this equation. One is there’s a forward probability for every state in your model. So remember, the number of states is the number of rows times columns, number of haplotypes times the number of markers. So there’s a forward probability for each of those states, and there’s a backward probability for each of those states, for each marker, and for each haplotype. The forward probability only includes the observed data up through marker m. So if we are imputing a forward probability for a state at marker m, the forward probability only includes the observed data up to marker m. The backward probability includes the observed data from the next marker all the way to the end. So there’s that kind of division going on. And then, let’s see, then the backward probability, I’m guessing it comes from the way they’re computed. It turns out you’d compute the forward probabilities by making a forward pass through your data.\nComputing forward probabilities\nSo, the way it works is you start with the state probabilities for marker 1, which you get just from the initial state probabilities. They’re all equal. Given your state probabilities for marker 1, there’s an update equation which I’ll just flash on the screen in a few minutes. There’s an update equation that gives you all the state probabilities at marker 2. Once you have all the state probabilities of marker 2, there’s an update equation that uses these probabilities to determine all the state probabilities at marker 3. And you just march through your data, one marker at a time. And at each step, you use the state probabilities at the preceding marker to give you the forward probabilities at the next marker. I said state probabilities, but I meant the forward probabilities at the preceding marker. And you keep marching.\nComputing backward probabilities\nAs you might guess, the backward probabilities work the same way, but in reverse. You start at capital N, the last marker in your data set. You start with the backward probabilities there, which turn out to be all one initially. Then from that, you get the backward state probabilities at marker N-1, at the preceding marker, and you keep marching backward. So, you end up eventually at marker 6, and given the backward probabilities at marker 6, you can get them at marker 5. Given all the backward probabilities at marker 5, the backward update equation gives you the ones at marker 4, and so on. And you march back. So, you do a forward pass and a backward pass through the data, and it’s imaginatively called the algorithm.\nForward update\nThe forward-backward algorithm will use this equation in the research talk, and I just wanted to give a high-level look at the equation. This is an example of the forward update equation. You don’t have to memorize it, but just understand the different components of it. So, remember, the forward update equation gives you the forward probabilities at marker n+1 given the forward probabilities at marker M. You’ll notice there’s an N+1 there. Then, you sum over all the reference haplotypes and all the states at marker M. Here is the forward probability at marker M, and then you end up multiplying that by a transition probability. This is the state probability of being in the reference state for reference haplotype H Prime and transitioning to the state at the next marker with reference to haplotype H. Then you multiply by an emission probability. So, given the state you’re in, what’s the probability of the observed allele at that state? The backward state update, I won’t go over it, has the same format. It’s a little bit different, but it’s the same format. You’re summing over a triple product of the backward state probability at the state you’re coming from, a transition probability, and an emission probability. One thing that we’ll use in the research talk on Thursday is we’ll use very strongly the fact that when you’re at an imputed marker, this term effectively drops out. And that’s it. We’ll use that to find some faster ways to compute imputation. The last part of the talk, I wanted to talk about programming. This is for people with a CS background. This, some of this, a lot of this, all of this, perhaps you’ve seen, but there are probably people in here that are like I was a number of years ago, coming into computational genetics with little or no programming background. So, there are certain things that I thought might be helpful to just go over that may save you some time, make your life a little simpler, because a lot of our work involves writing code.\nSimplicity\nSo, I find when writing software that the chief challenge for me is complexity. If and when it gets more complex, my mind has a hard time grasping, and it becomes more error-prone. And as much as if you’re like me, you get a buzz from writing really fast code, it’s pretty exciting to write code that’s really fast. But that’s really not the first objective. The first objective is to write clean, simple code. Don’t worry about optimization. You’re just trying to write this simple code so that when you look at it, it’s easy to understand what it does. It operates in a logical way, and the structure of it makes sense. It’s just easy for the mind to absorb and grasp without having to really study it.\nThere is a place for optimization, but it’s not the first thing you want to do. There’s a famous quote that started with Donald Knuth. He took this statement from scripture and then changed it for computer science. He wrote, “Premature optimization is the root of all evil.” And I think the idea is that if you optimize too soon, you can end up optimizing the wrong thing or doing unnecessary optimizations that don’t actually improve the code. Or if your code isn’t simple to begin with, you have a hard time finding the right optimizations. Optimization is not free. Yes, it can speed things up, but it has a cost. If your simple code was fast enough, you wouldn’t need to optimize. So when you optimize, by definition, you are introducing complexity, and that complexity has costs. You’re going to have more bugs in it because it’s more complex. It’s going to be harder to find the bugs because it’s more complex. It’s going to be harder to extend your code to add new features to it because it’s more complex. It’s going to be harder to maintain the code because it’s more complex. It’s harder for other people to come look at your code and understand what you’re doing because it’s more complex. So there’s that cost, and you have to weigh that cost against the expected benefit. Is a big increase in complexity worth getting a 5% reduction in runtime? In most cases, not if your code is fast enough for practical purposes. Getting a tenfold or hundredfold reduction in runtime may not be worth it for the increased complexity if it’s already fast enough for your purposes. If it runs in a second, weigh it up and understand how much complexity you’re adding and what the trade-off is before you even add it, because optimization is not free.\nThe second general principle that I find useful is modularity, which just carries the idea that you want code units of code where the input is very simple, what it does is simple to understand, at least at a high level, and the output is very simple. A module of code that you can treat conceptually as a unit without having to really think about it very deeply. Now, when you write the code, you may not have to think about it deeply, but once it’s working and doing what you want, you can just treat it as a building block. And ideally, you want your program to be very loosely interacting modules, so that when you’re working on a particular module, because it operates very independently, you can give that your whole concentration. You don’t have to have the whole program, the whole complex program, at your fingertips in your memory. You can just focus on the individual part you’re working on, because it’s loosely coupled.\nThe classic example of this would be UNIX utilities at the command line. If you’ve used a UNIX system, so UNIX utilities, there are utilities for sorting, counting lines, counting words, extracting columns, extracting lines that meet a certain criteria, changing characters, replacing words. And there are all these UNIX utilities, and you can do a lot of your programming or a lot of your data manipulation without actually sitting down and writing code. You just take the UNIX utilities and string them together in a series of filters or in a pipeline. UNIX utilities are a classic example of doing some units of code that do one thing, do it fairly well, and that you can work with as a unit without understanding how they’re implemented. And that kind of approach is really useful for writing complex projects.\nAnother general thing when you’re writing is if you can be aware when your classes or your methods, or which are called functions depending on the language you’re working in, do too much. One of the pieces of advice I read early on when I was learning programming is to be aware of functions that extend more than the length of your computer screen. In my experience, that’s good advice. When it doesn’t fit on the screen, I’m more likely to make errors because I can’t see the whole function without scrolling. And the extra length usually indicates additional complexity. So, you know, all rules have exceptions, but generally, if the function extends more than the screen, I want to see if there’s a way to make the code cleaner, simpler, easier to understand by breaking it up into parts.\nClasses, if you work in an object-oriented environment, which for complex problems can be very useful, for me, your threshold may be different, but my threshold is, once my file, I work in Java predominantly, once it’s more than two or three hundred lines, I notice that I have a harder time really understanding what’s going on in the class. And when it gets long, if I can, I try to find ways to see if there’s a way I can break things up that would make it simpler to understand. There’s not always, but I try. So, to get that simple code involves what’s called refactoring, which is just cleaning up your code without changing how it behaves. So, when do you refactor? How do you know what parts of the code you want to spend some effort cleaning up?\nJust from experience, anything I’ve just written, a scan, they need to be cleaned up. I never get a really right design the first time, unless it’s just a trivial piece of code. I may not realize it when I write it that day, it has problems. But there’s an acid test for discovering if you have code that’s hard to understand. It’s when you come back to it after a couple of months, and for some reason, you have to go back to that code and you’re looking at it. It’s very humbling because you can’t understand what you wrote. You wonder what you were thinking, why did I do this, is this a bug, isn’t this more convoluted than doing it the other way? All those thoughts that when you look at the code with fresh eyes, it’s usually then that the problems become obvious. Did I spend time refactoring when I first wrote the code? I’m just a little too close to it. It’s hard for me to see the blemishes. But when you come at it with fresh eyes, they’re really obvious.\nAnd it’s often during that painful time you’re spending trying to understand what your code is doing again, that also can, at the same time, be very easy to see ways that you could have done this differently. You could have combined things, you could have changed the organization to make it easier to structure. You could divide long methods into shorter methods. You can combine code that’s essentially duplicate code so that instead of having to maintain two components of code, it’s just one. So, all those things become obvious when you look at the code again with fresh eyes.\nSo, refactoring, if you’ve had the experience like, you know, I’ve had many, many times of just having a not very fun day trying to understand what you wrote in the past, refactoring just means the next time you look at the code, it won’t be so bad. And it’ll end any poor soul that’s not you. If you have a hard time understanding your code, just imagine what somebody else coming to your code is going to have, what difficulties they’ll have.\nTesting and debugging\nThen, for testing and debugging, one tool that I find really useful is regression testing. So, this is not linear regression from statistics; it’s just testing to make sure your code is still working correctly. And this can be set up in a very automated way. You have some test data sets - the test data sets you originally used to convince yourself that your code was behaving well and doing what it was supposed to do. Save those test data sets, write some scripts, and save the output that you had from a previous version. And when you continue working on your code, maybe doing refactoring, and then you reach a stable point in the code, you can check and see whether your code is still producing the same output.\nYou can check it at a qualitative level to see if it’s still producing the same accuracy. You can also just use the `diff` tool in UNIX to check whether the output files have changed at all. This is a fantastic way to catch bugs that you’ve introduced. You had working code, you made some changes, and you didn’t realize it, but you broke something. It makes it very systematic and easy to find those types of bugs.\nAnother useful tool is a version control system. I use a tool called Git. It allows you to go back and see previous states of your program. You may have started off on development and then found out that this, I made a mistake, I need to backtrack. Or maybe there’s a bug that you never caught before and it was introduced in the past. It allows you to essentially do a binary search to find just the point where you introduced the bug and determine exactly what changes you made at that point. So, you can very precisely nail down the place where the bug was introduced.\nIf you’re new to Git, there are a lot of tools online for that. There’s a Udacity course on Git, a short Udacity course. There’s also a manual online. If you search for something called “Git Book,” you should find the online book that explains Git. If you want to learn Git, it’s not easy to learn Git initially. I don’t think these version control systems are easy to learn initially, but they’re a powerful tool once you learn them. It’s like the UNIX environment - it’s hard to learn but very powerful.\nThe last is, it pays to remember your previous bugs. Bugs are not uniformly distributed in your code. Typically, they tend to cluster, and the reasons for that are, you know, fairly natural. It could be a complex section of code - complex code is more likely to harbor bugs. They might be spatially correlated in your code. It could be something in your life, where you weren’t at your best the day you wrote that code. You didn’t get enough sleep, you got a letter from the IRS, whatever it was, just something that threw you off a little bit where you’re not up to your full capabilities.\nSo, it pays to remember where your past bugs are. When you find a bug, if you can remember the type of bugs you had and where they tended to occur in the code in the past, that can help you. It can give a little more preference to those parts of the code when you’re trying to track that bug down, and it can save you a lot of time.\nThank you, thank you for your attention. That’s the end of my remarks."
  },
  {
    "objectID": "chapter1.3_transcript.html",
    "href": "chapter1.3_transcript.html",
    "title": "Chapter 1.3: History (Video Transcript)",
    "section": "",
    "text": "History of Psychiatric Genetics: Part 1\nTitle: History of Psychiatric Genetics (Part 1)\nPresenter(s): Ken Kendler\nKen Kendler:\nIntroduction\nI’m pleased to be able to talk to you about the history of psychiatric genetics. In this short seminar, all I can really expect to do is to provide a brief introduction and a few highlights. I’ve decided to focus on the earlier history of psychiatric genetics, as this is likely less familiar to most of you, and rather than only giving lots of dry facts and dates, to try to illustrate this with particular prominent individuals, to provide when I can photos of them, to let you see what their books or articles look like. I would say that the goal I’ve set myself to show you bits and pieces of the rich history of our discipline of psychiatric genetics so that I might whet your appetite for further self-study.\nRobert Burton\nSo let’s get started. In a really prequel stage the modern discipline of psychiatric genetics as we understand it started very close to the time of psychiatry itself, that is somewhere in the couple of decades between 1780 and 1800. But we do have one giant figure, Robert Burton, who was writing in the early 17th century, in this case at the date of 1621, a little more than a decade after the death of William Shakespeare, and he wrote what was a classic that has been read up until this century on depression. The title is the “Anatomy of Melancholy”, and I want to point to you to a small part where he describes the synopsis of his first partition. As you can see my pointer here, that he describes all the causes of melancholy, dividing them into supernatural or natural, and in secondary as primary he describes about the stars and chiromancy, but here particularly he notes that old age is a risk factor for melancholia temperament, and here we note “parents”, it being an hereditary disease. So we have an observation from decades, and actually more than centuries before modern medicine developed and psychiatry developed, with the understanding as people had observed that one form of psychiatric illness, melancholia, clearly passed from parents to their children.\nThis is a pre-print of an article that is coming out of the American Journal of Psychiatry, and it reflects a rather detailed review of the pre-history of psychiatric genetics from these years of 1780 to 1910. What I next want to do is just highlight this overview. As you can see from the abstract here, I reviewed 48 representative text published during this time period, nearly all of those textbooks had sections on the etiology of mental illness or actually they more frequently called it insanity. In that section they almost always made some comments about the importance of genetics in the etiology of insanity. So here is a review of the main points of that review itself:\nFirst, since the beginning of psychiatry as a medical discipline, roughly around 1780, most expert authors who were textbook writers viewed heredity as among the strongest risk factors for insanity. Second, during this time period most writers concluded that it was a predisposition to illness, rather than the illness itself, which was transmitted in families. Third, and this is related to the concept of predisposition, they were well aware of the apparent probabilistic nature of the transmission of insanity. They noted that it often skipped generations, with an affected parent or aunt a grandparent or aunt or uncle, but the parent as well and the child is affected, and also noted that it seemed to often impact only on a few of many individuals within the same siblings. Fourth, authors discussed at length the question of whether there was what they described as “homogeneity” or “heterogeneity” of the familial transmission of the various forms of insanity. The general conclusion was that heterogeneous transmission was the rule. That is, that in the relatives of the insane patients, almost all who would have been hospitalized in asylums at this time. The clinicians who were writing this material viewed a wide variety of psychiatric, and sometimes neurologic, disorders. Homogeneous transmission, that is the idea of like begets like, was an exception. It’s interesting that two kinds of syndromes are mentioned repeatedly across these textbooks as being homogeneously transmitted, as one is much of what we would call cyclic psychosis or manic depressive illness, and the other was suicide, with a number of authors noting suicides running in families with high levels of homogeneity. Fifth, many writers noted that odd and eccentric personality features were common in the relatives, and especially parents, of their insane patients, and this obviously is a prequel to later developments of schizophrenia spectrum conditions within our own time period. Finally, inheritance as they used it, and as Burton used it, was commonly understood more broadly than we would today. It includes prior environmental parental experiences, that is the inheritance of acquired characteristics was accepted for most of the 19th century and even well into the early decades of the 20th century, but also some authors noted that parent-offspring transmission could arise from psychological effects of the behavior of the parents or intrauterine effects.\nProsper Lucas\nNow, let me turn to viewing a highlight of a series of important authors that we might wish to describe and i want to begin with Prosper Lucas. Prosper Lucas was a French Alienist and geneticist. He lived from 1808 to 1885. He wrote a major two-volume monograph on genetics in 1847, as you can see this “philosophical and roughly physiological traits of natural heredity” would be the translation. These are very long and extensive. It would have one of the great mid 19th century reviews of the field of genetics, and he reviewed them across medical, physical (that would be anthropometric traits), and psychiatric conditions. In addition, Prosper Lucas was in his day what we might call a statistical geneticist and developed a pre-Mendelian theory trying to explain the stochastic nature of inheritance. The importance of Prosper Lucas’ work is best illustrated in one of the great books in all of the history of science, Charles Darwin’s “Origin of Species”, in 1859. He noted Prosper Lucas’ book as quote, “the fullest and best on the subject”, and in his later works, particularly on “Variations in Plants and Animals”, Darwin frequently cited Lucas’ work more than a dozen times in that book itself.\nJenny Koller\nNow let’s skip a number of generations to the figure of Jenny Koller, the most prominent woman who contributed to this literature in the 19th century. A bit of background is in order. Throughout the entire 19th century, virtually all observations in psychiatric genetics were uncontrolled. The vast majority of statistics that were reported is a very simple one: that is the proportion of hospitalized psychiatric patients who were hereditarily burdened, and by which they mean has some well-documented history of psychiatric illness or occasionally neurological illnesses in their relatives. These percentages varied very widely from four or five percent to 80 percent, and that’s partly because what disorders they consider to constitute a hereditary burden was quite variable and which relatives they counted was also quite variable, but they had never used controls. The first study to do that was done by Jenny Koller in 1895. Here’s a picture of what Dr. Koller looked like. She was encouraged by her mother to consider medicine. She met with one of the few physicians in Switzerland at the time, she completed medical school there, but she was unable to get what would be the equivalent of an internship, as they were not offered for women in Switzerland, and so went to Paris, France for her training.\nHere is what her article looks like; this would be a translation. She was at that point working in Zurich. I’ll give a brief biography in a minute. The title is “Contribution of hereditary heredity statistics of the mentally ill in the canton of Zurich: comparison of the mentally ill with a hereditary burden on healthy people due to mental disorders and such like”. We can see that people wrote longer titles in those days for articles than we do today, and this was a relatively prominent journal based in Berlin actually, even though she was Swiss. We published in Neuropsychiatric Genetics earlier this year; Astrid Klee, who’s a professional German translator, and I a detailed review of this article, and in the appendix we present an entire translation into English. I want to just briefly summarize for you here just picking up on the abstract.\nThe first such study in the history of psychiatric genetics that is was published in 1895 that was the doctoral dissertation of a Swiss physician Jenny Koller. She was working under Auguste Forel at the Burghölzi hospital in Zurich, Switzerland, a hospital very famous in the history of psychiatry, and Forel was succeeded by Eugen Bleuler, who we will discuss later. She obtained histories of a range of mental and neurologic disorders in the parents, aunts, uncles, grandparents, and siblings of 370 hospitalized psychiatric patients and 370 controls who were not hospitalized. Interestingly, the rates of hereditary burden were only modestly higher in cases than controls. There was a great deal of surprise at the high level of mental illness occurring in the relatives of unaffected control subjects. However, Koller followed up trying to address two quite important questions: first, she examined which individual syndromes were actually more common in the relatives of the psychiatric patients and the controls, and she found only two categories, which what we roughly translate as major mental illnesses and roughly translate as eccentricities, that is abnormal behaviors, but she also considered syndromes of apoplexy, that is a stroke, nervous disorders, which included such things as headaches, and probably some other neurologic disorders, as well as anxiety and dementia and those three were not more common in the relatives of the psychiatric patients than the controls. Furthermore, the rates of mental illness and eccentricities were much more strongly elevated in the first degree relatives of parents, largely children, largely parents, and siblings in cases versus controls but the elevation for grandparents and aunts and uncles were much less. Again, this was the first systematic observations of the tendency of disorders to aggregate more in what we would now understand to be individuals more closely genetically related. In my view, Koller’s study represents a major methodological advance in psychiatric genetics, helping to define which disorders co-aggregated with major mental illness.\nErnst Rüdin\nLet me now turn to what will be a darker part of the history of psychiatric genetics. This is the figure Ernst Rüdin. I will again give you a brief biography. Those of you who can see will notice that he had a little lapel pin in it with the swastika and that will be part of our story.\nWell, Ernst Rüdin was born in 1874, died in 1952. He was Swiss. He graduated from medical school in Zurich in 1898, and his first job was also at the Burghölzi hospital after Jenny Koller to assistant Eugen Bleuler. He then moved to the psychiatric clinic of the university of munich in 1907, where he became an assistant Emil Kraepelin, under whom he completed his doctoral thesis. He was appointed a senior physician there in 1909. When Kraepelin’s lifelong dream the establishment of the German institute for psychiatric research, the first such multidisciplinary institute in the world for psychiatric illness, was established in Munich in 1917, Kraepelin appointed Rüdin as the head of the Department of Genealogic and Demographic Studies. Most of the leading members of the next generation of psychiatric geneticists, and that includes such figures as Eliot Slater, Eric Stromgren, and others from all across Europe, came to train in the department that that Rüdin had established in the 1920s and early 1930s. Rüdin himself had two major works in psychiatric genetics, although his school, the program that he established had many many more works.\nThe first was published in 1911 when he was 26 and in this journal and the title was “Methods and goals of family research and psychiatry”. It’s an extensive article, and outlines a methodologically sophisticated Mendelian-informed research program for psychiatric genetics, but you will notice in there a number of mentions about “volk” eugenic theories about the “volk” and the wellness of the genes of the “volk” himself. His second major work was this and along with his daughter Edith Zerbin-Rüdin, more than two decades ago, I published in Neuropsychiatric Genetics a summary and abstract of this. The title would be “Studies of the inheritance and origin of mental illness to the problem of the inheritance and primary origin of Dementia Praecox”.\nThis was the first large-scale systematic family study ever done of a psychiatric disorder. It included using these newly developed methods of Weinberg to age-correct the sample and to use a proband-wise correction method to get the correct results. The morbid risk that he calculated for narrowly and broadly defined schizophrenia in the sample was 5.4 and 7.7 percent. Those results are quite similar to modern findings; the study that I did in Russ Coleman early in my career found a rate of six percent, so very much in the range that Ernst Rüdin found. He also found other psychoses were common, with a morbid risk of 5.1 percent, lower rates in half siblings. He noted the risk of schizophrenia was significantly increased in parental diagnosis of alcoholism, a history of schizophrenia in second or third degree relatives, and by parental diagnoses of other psychoses. So overall, Rüdin found a familiar relationship between schizophrenia and other psychoses, a substantially lower risk of schizophrenia in parents versus siblings, and a segregation pattern of schizophrenia that was not did not conform with simple Mendelian disorders. Importantly to note, this was not a case-control family study as we might do now, actually the purpose of this study was to see whether segregation patterns consistent with simple Mendelian transmission could be found. Rüdin did note in this monograph that if there were two recessive genes, those at 0.25 squared would equal a recurrence rate of 0.0625, he said that might be consistent with the model.\nNow we need to talk about another key feature of Rüdin’s career. Rüdin as a teenager developed a strong interest in racial hygiene and was in 1905 among the founders of the German society for racial hygiene. After the rise of the National Socialists, that is the Nazis, in Germany in 1933, as director of the German Institute for Psychiatric Research (he had taken over the position from Kraepelin after his death), Rüdin collaborated extensively with the Nazi government, praised their racial policies, and received substantial funding both from them and later on from the SS. From 1933 onward, he played a major role in the development of the Nazi program of involuntary sterilization of the medically and psychiatrically ill. It is generally agreed that he was not actively involved in the Nazi T4 program, which was the systematic killing of the mentally ill and mentally handicapped, that began in 1939, but the historical documents are quite convincing that he was aware of these efforts and knew of colleagues who were working in them. And he certainly made no attempt to stop or to publicize them. Furthermore, as documented in high quality historical resources that have only been recently available in English, I would particularly recommend this book by Dr. Weiss “The Nazi Symbiosis: Human Genetics and Politics in the Third Reich”. It’s quite clear that Rüdin played an important role in the scientific legitimization of Nazi racial policies, both within Germany and internationally, and we know historically of course that the acceptance of these policies laid the groundwork for the future horrors of the Holocaust. In light of his activities from 1933 onward, there is a range of opinions in our field about how Rüdin’s substantial scientific contributions to the history of psychiatric genetics should be treated. Some have argued that any discussion of his work is inherently immoral, as it inevitably rehabilitates him, and dishonors the memory of those whose suffering he has contributed to. And i respect that tradition personally, although i don’t agree with it. Others have argued that his work can and should be studied for two important and complemented reasons: one, he has played an important role in the history of our psychiatric genetics, but second, as a case study of the potential tragic effects of the misuse of our science.\nEugen Bleuler\nLet me then go on and conclude with this figure Eugen Bleuler. He’s probably the best known of all the people that I’ve spoken about here. This is a a lovely photo of him as a young man. Bleuler is most famous for introducing the term “schizophrenia” to the world in a lecture in Berlin on 24 April 1908. He revised and expanded his schizophrenia concept in his seminal of 1911. This is his great monograph, “Dementia Praecox, or Group of Schizophrenias”, and was unfortunately only translated to English in 1950. But in 1917, that is only a few years after the monograph published by Rüdin, Bleuler wrote a very thoughtful critique of Rüdin’s family study of Dementia Praecox, and this is the German version of this. And this is now been published in Schizophrenia Bulletin, which represents a detailed review of Bleuler’s comment on Rüdin and, like the other works that I’ve been doing, here in the appendix, along with Astrid Klee, we publish a full English translation of Bleuler’s comments.\nI want to review here the four key points made by Bleuler, again working at a time very different from our own. First, Bleuler argued that understanding the transmission patterns of schizophrenian families requires definitive knowledge of the boundaries of the phenotype. And he argues that those remain unknown. Bleuler strongly suggest that Rüdin’s choice, which was Kraepelin’s concept of dementia praecox, was far too narrow. As we know, Bleuler was interested in such broader concepts as latent schizophrenia. What Bleuler argued was that clarifying the genetics of schizophrenia is inextricably bound up with the problem of defining the phenotype: you can’t do one without the other. Second, Bleuler argued for the importance of “erbschizose”, which might be described literally as “inherited schizoidia”. And he wondered whether either his famous “4 As” or other (and i quote here from our translation) “brain-anatomical, chemical, or neurologic characteristics” might underlie the genetic transmission of schizophrenia. This really is very similar to our concept of what might be an endophenotype. Third, Bleuler was quite interested in the nature of the onset of schizophrenia, suggesting that environmental adversities would often provoke quote “latent” illness to become manifest. It was important to argue that to identify such risk factors and actually incorporate them into the genetic models, a relatively advanced concept we’re still struggling with today. Fourth, although not optimistic that current knowledge would permit a resolution of the transmission models for schizophrenia, Bleuler finds single locus models for schizophrenia Dementia Praecox to be implausible, and at several points wonders whether polygenic models (he didn’t give the number it certainly was not conceived as the way Ronald Fisher would be), but a small number of genes might better apply.\nSummary\nSo this concludes the main part and I want to summarize. My goal has been to introduce you to a few of the many important figures and topics in the history of psychiatric genetics. I want to leave you with one ethical and two overarching scientific themes of this history. The ethical part: our history illustrates the misuses to which the science of psychiatric and behavioral genetics can be put, with tragic results, and therefore the need for our vigilance to reduce the chances of such developments in the future. From a more scientific perspective, I would suggest that the history of psychiatric genetics includes many themes, but two of which are overarching: the first is the close interrelationship between psychiatric diagnosis and genetics. How can you do the genetics of a disorder whose diagnostic boundaries are unclear? So inevitably there is an intimate relationship with these two fields, that is, to get the genetics right we need to get the diagnosis right, and the genetic strategy is one of the best ways to help us decide that what a valid diagnosis is. So we are in this large, long, iterative process. And the second key issue that runs through much of the 19th and early 20th century, is do psychiatric disorders run true in families or is there often heterogeneous transmission? And i would suggest to those of you acting working in the field is that both of these issues are very extant and we have solved neither of them. Thank you for your attention.\n\n\n\nHistory of Psychiatric Genetics: Part 2\nTitle: History of Psychiatric Genetics (Part 2)\nPresenter(s): Ken Kendler\nKen Kendler:\nMy name is Kenneth Kendler. Last year, I gave a presentation on the history of psychiatric genetics that focused on specific episodes and individuals important in the history of our field. Today, I’m going to give a deeper dive into a single episode. The title, as you can see, is “The Beginnings of the Debate Between the Mendelians and the Biometricians in Psychiatric Genetics.” I’ll be focusing on four individuals: David Heron, Karl Pearson, Abraham Rosanoff, and Charles Davenport. I will be covering a fairly short time period of the years 1913 to 1914, so think the years just leading up to the First World War.\nNow, it’s not often that psychiatric genetics work gets into the New York Times, but here, if you were to open up the magazine section of the Sunday New York Times on November 9th, 1913, you would see this big full-page spread titled “English Expert Attacks American Eugenic Work – Teachings Are Pronounced Fallacious and Actually Dangerous to Social Welfare by Dr. David Heron of the Galton Laboratory, London. Dr. Charles Davenport makes a vigorous reply.” Here’s a portrait of Charles Davenport, whom we will get to know somewhat better, and these are two buildings: that was the Eugenics Record Office at the Cold Spring Harbor Laboratory, which has developed into a molecular powerhouse in more recent years.\nI want to introduce you to the four key characters: Aaron Rosanoff, a Jewish immigré trained in medicine and psychiatry in the United States; we will be tracking his career in some detail later in the talk. Charles Davenport was a major American Mendelian in the early 20th century; he was at this point the director of the Eugenics Lab at the Cold Spring Harbor. There’s a great deal of literature on Davenport’s life. David Heron, much less well-known, was a Scottish-born statistician who trained as a Galton research fellow under Pearson at the Galton Research Labs of National Eugenics at the University of London, and he performed the first-ever quantitative genetic study of insanity in 1907. Karl Pearson, I think, does not need much introduction; he obviously was a major figure in the history of statistics and in these years was directing the Galton Labs at London.\nNow, all of the four individuals here were ardent eugencists. There are aspects of eugenics and arguments that played into this. I do not emphasize this in my presentation, but if you read the primary sources, you will hear more about that here. Now, what do these folks look like? This is a portrait of A.J. Rosanoff, not the best, but you can get an image of him. This is a picture of Charles Davenport, quite clear, and you saw a portrait of him in the New York Times as well. Um, here is, I could not find a picture, despite much effort, for David Heron, so I give you his first monograph - historically, a quite important document that I may cover in a future lecture. Our first study of the statistics of insanity and the inheritance of the insane diathesis, which was a quantitative study using Pearson’s tetrachoric correlation of insanity. And finally, a Karl Pearson, uh, here he is with his family, the son Egon, he will hear more of, and his daughter - I think a very nice photo of Pearson.\nSo, let’s give a little bit of historical background to, uh, what is going on at this time period. After the rediscovery of Mendel’s Laws in 1900, an intense and often personal debate ensued about the relevant values of biometrical genetics and Mendelian genetics as applied to plants, animals, and especially humans. As the traditional histories tell us, this debate was largely an English affair centered on two chief protagonists: it was Gregory Bateson on the Mendelian side and Karl Pearson as the lead biometrician. For those of you not familiar with these terms, the biometricians emphasize quantitative traits and thought that you should study relationships among relatives by calculating correlations, especially Pearson’s both product-moment and tetrachoric correlations. On the other hand, the Mendelians emphasized investigation of discontinuous traits and were interested in Mendelian patterns of those traits. Um, in essence, what we are seeing now is a small American skirmish as a part of this much broader controversy between these two fields, and here I will give you a bibliography at the end, but these are probably the two best reports about this overall background argument if you wish to follow them up further.\nNow, we really get started here with this monograph. This monograph sort of lits the fire that exploded in the controversy that we saw in the New York Times. Notice the dates of publication. This was initially printed in the American Journal of Psychiatry in 1911 and reprinted in the Eugenics Office. The key author there was A.J. Rosanoff. The title: “A Study of Heredity of Insanity in the Light of Mendelian Theory.” This was the first major pedigree study of psychiatric illness ever done, applying Mendelian rules. Uh, it was a couple of years ahead of Rüdin’s key publication in 1916.\nA little bit of background: with a supportive Davenport and his team of field workers from the Eugenics Record Office at Cold Spring Harbor, Rosanoff studied 72 pedigrees who were ascertained through psychotic patients admitted to King’s Park State Hospital in New York, which, in fact, is where Rosenhoff was working. They studied, quote, “insanity and Allied neuropathic conditions,” unquote, which he claimed in this publication was inherited as a Mendelian autosomal recessive trait. And of course, by that, we mean fully penetrant, models of incomplete penetrants were not being seriously considered at this time.\nNow, Rosanoff did not attempt to uncover the mode of transmission of specific psychiatric disorders, so this deviated certainly from the Rüdin School that looked especially at dementia praecox and certain forms of insanity, and many of the other subsequent genetic investigations we are used to. Rather, he focused on a very broad phenotype that he termed “the neuropathic constitution”.\nAnd what he tested: the Mendelian model by examining segregation rates and sib-ships from these pedigrees as a function of their mating type. He studied 206 matings with a total of 1,097 offspring, and any “well” parent with a neuropathic offspring was assumed to have a heterozygote genotype. So, just so you’re familiar, with these, D stands for the wild type, and R is the risk allele. So, a “DR” is a heterozygote, and he focused on six mating types, and you can read these out, of course. These are going to be given the recessive model that he assumed: affected by affected, affected by a heterozygote, affected by a well homozygote, heterozygote by heterozygote, well by heterozygote, and well-by-well. And from simple algebra understanding Mendelian predictions, you have these percentages expected in the offspring of those individual mating types, from zero percent to one hundred percent.\nNow, what’s the phenotype that Rosanoff studied, and this is obviously a little bit from the monograph itself. In studying any neuropathic defect, one must bear in mind that its clinical manifestations will vary with the personality of the subject and the conditions of the environment. It is indeed a notorious fact that most of the so-called clinical entities are remarkable for the variety of their manifestations. This fact has necessitated the introduction and clinical practice of the conception of neuropathic equivalence, and that will be a major topic that we will discuss further.\nNow, this is the key results from that monograph that I’ll go through, but I redo this in a clearer format. But, in effect, these are the mating types with the “b” and “b1” differing about whether you know that the person is a heterozygote because they have an affected offspring, with that assumption of the model and one in which you assume that, but you can actually read off the expected versus the observed, and in the next figure, you’ll be able to see that more clearly.\nSo, if you scan your eyes down these columns, you’ll note that there’s a fairly significant deviation here but for most of the other columns and somewhat there, there’s moderately good agreement between. Now, there was no chi-square test, even though that had been invented a number of years ago, no statistics observed, but from these pedigrees, Rosanoff and Orr reached the following conclusion, as is shown in the table: “the correspondence between theoretical expectation and actual findings is in some cases exact and in all cases remarkably close. It would seem, then, that the fact of the hereditary transmission of the neuropathic constitution as a recessive trait in accordance with the Mendelian theory, may be regarded as definitively established”.\nLet’s look at some of the pedigrees; they are all presented in the monograph, and I’m only going to give you a selection, but just to get a clue about what the meanings of the symbols are. So, the clear squares and circles are going to be normal progeny. Those with a little circle in them are a normal subject without progeny, and so they can’t, according to their system, know whether the person is a normal homozygote or a heterozygote. And then these are normal subjects with neuropathic progeny, which, by their definition, have to be heterozygotes. And then the affecteds who need to be homozygotes are the fully shaded ones, and then the ones about dying child another again, you can study this at your leisure if you so wish.\nSo, I’m only going to show, I think, four pedigrees, but working through those, although maybe a little tedious, will be helpful to get a sense of the data. So we always have the proband who is hospitalized at King’s Hospital in this primary sib-ship here. In this case, we have a manic-depressive insanity case hospitalized, then all the affected—let’s go through those first. This woman has a sister who is described as very nervous, which meets their criteria, or theirs, for a neuropathic trait. She has a mother with epilepsy, a maternal grandfather with epilepsy, and a paternal grandmother who is described as “hysterical when a girl had ideas someone was trying to poison her.” The only other people here are the obligatory heterozygotes by Rosanoff’s system, and again, because they had affected offspring, assuming their recessive inheritance, those are obligatory heterozygotes. So that’s the first pedigree.\nSecond pedigree, here again, number seven is the proband with manic-depressive illness, and this is, again, a heterozygote by ill individuals. So according to Mendelian theories, these should, on average, have 50% affected. We have, with this affected male proband, a sister who is described as “easily excited in a nervous temperament,” which is assumed to be neuropathic, has an affected father with recurrent melancholia with insomnia, having spent five months in a sanatorium, has a paternal uncle who is described as a “crank,” and a maternal aunt described as having convulsions, and a maternal uncle described as convulsions. Note that these also both meet criteria for neuropathic trait and are considered affected. And then an affected paternal grandfather with senile deterioration, presumably due to some kind of dementing illness like Alzheimer’s.\nOkay, third pedigree, this is obviously now four generations, so much richer. We have the core sib-ship here, and here we have, it looks like a double proband family, so both nine and ten, which is our recurrent melancholia with suicide attempts and manic-depressive insanity, the sister and brother, and they have one affected sister who is described as “attack of depression with suicidal tendencies.” This is a homozygote by homozygote mating, with the father having had fainting spells, and the mother having been diagnosed with recurrent attacks of depression. There’s a paternal grandfather who is described as “money mad, very cruel, very miserly, wealthy, left much of his money to a housekeeper.” And then we have a maternal great-grandfather who’s described as alcoholic. We have a couple of individuals here who are marked as heterozygotes because of their obligatory because they had offspring. So, number four had a daughter with fainting spells, not drawn on the pedigree, but the authors are willing to assume that she must be a heterozygote because of that. And this uncle had a feeble-minded and “queer” son, and again is assigned as an obligatory heterozygote. And then these parents, because they were intervening between these, must have been heterzygote to transmit the recessive trait.\nAnd this is the last and obviously a much broader pedigree. So here we have a very rich, dense pedigree in which number 13 is a woman who is the proband with dementia praecox. She has two affected older brothers, who are alcoholic, actually both of them, an affected younger brother with a highly nervous temperament, worries, and also with alcoholism, and number 11 worries over things, with blue spells. The father is described as neuropathic with a highly nervous temperament and irritable. One of the uncles is a nervous temperament, alcoholic. Another uncle highly nervous. And then a sister who has a daughter who’s nervous, so she is considered to be an obligatory heterozygote. And then we have a very dense pedigree here in the maternal line: fainting spells, shiftless, alcoholic, spending sprees, religious crank, alcoholic, and inferior make, a possibly epileptic, and then an affected maternal grandfather: alcoholic, cranky story. So those are examples of the pedigrees that Rosanoff uses to reach the conclusions that this neuropathic trait, which they consider to be indicative of insanity or vulnerability to insanity, is indeed inherited as an autosomal recessive.\nSo the next step is that in 1913, so about two years later, but there must have been some publication lag, David Heron and Karl Pearson write a three-part series of monographs entitled “Mendelism and the Problem of Mental Defect.” They mean this to include both what they would have called mental handicap and psychiatric illness, but Heron is the lead one, and the title of this is a “Criticism of Recent American Work.” And this is the monograph that directly prompted the New York Times article.\nAnd we will, um, I’m going to just read this briefly, and again, if you want to stop the video and read it in more detail, but this is now from that front-page article. I’ll just give you a quote of what I think was written by the reporters in the New York Times: “A spirited attack has been made by Dr. David Heron of the Galton Laboratory, University College London, upon the entire body of American eugenics in general and the work of the Eugenics Record Office at Cold Spring Harbor, under the direction of Dr. Charles B. Davenport, in particular. The criticism, which finds the work of the American investigator biased, faulty, and exceedingly slip-shod, is issued in the form of a monograph, the essential content of which is published below, together with a reply by that Dr. Davenport himself”. So, Dr. Heron got most of his monograph published in the New York Times, not bad, and you can read the rest as you will.\nSo let me try to summarize a fairly lengthy, and I’m going to be simplifying a number of issues and not dealing into everything, but I’m trying to touch the high points here. So, what did Heron say about the problems with Davenport’s work? And this is both his mental handicap work, which I’m not going through, but on virtually all of it applies to the study of the insane diathesis as well. Quote, “As a defender of the young science of human genetics, which was at this historical stage nearly inseparable from that of eugenics, Heron describes a critical problem of the defenders, which he sees himself and, of course, his mentor, Karl Pearson, of the young field.” And now this is a direct quote: “They see, unfortunately, dogma outstripping knowledge. They see fallacious methods of reasoning applied to problems which are essentially statistical by numerous writers who lack the necessary training.” So, we will see that we’re going to be into some sort of heavy, rather personal descriptions in the ensuing monograph.\nLet me try to run through substantively what was Heron critiquing in Rosanoff’s work. He begins by focusing on the fieldwork itself, and notice here, many of these also apply and uses examples of Davenport’s earlier study of feeble-mindedness. First, Davenport published lots and lots of monographs, and one of them was a field manual for interviewers. Again, he hired this team of mostly young women that would go out, identify pedigrees. We don’t know anything about the specifics of the assessments, but they did contact them based on personal contact, and that was an important methodological advance. So, I don’t want to play down some of these important advances, and Davenport did track down Huntington’s career pedigrees, as well as his work in these other areas.\nBut there are some rather concerning quotations from this field manual for interviews, and this is again a quotation with a highlight from Heron’s monograph itself, and again I will read this. “Some defects that the field worker will study,” I am quoting Heron’s quote of Davenport’s monograph from field workers. So, this is his instructions: “Some field workers will study such defects as albinism and feeble-mindedness, are known as recessive defects.” So he’s assuming that feeble-mindedness, a pretty broad phenotype, is recessive. For example, by hypothesis, feeble-mindedness is, for the most part, a recessive trait, and the hypothesis must be tested as follows. So he’s telling the field workers how do you test for a recessive disorder: “You must find a person suffering from feeble-mindedness who is a descendant of two normal parents (hypothesis: both of the parents are simplex, meaning heterozygous). The field worker must understand that each parent will probably have somewhere in his or her ancestry a feeble-minded person, and it is the business of the field worker to make a special search for such person or persons in the pedigree.”\nNow, you can see why Heron would cry foul. Heron, now quoting from his monograph, says, quote, “It is difficult to understand how the field workers would fail to be prejudiced by these instructions, which appear to be telling the field workers the pedigree structures they are supposed to find.” Heron concludes, quote, “What faith are we to place in data collected in this way when we find that the field workers are instructed to, quote, make a special search, unquote, for those individuals who are necessary for the support of the Mendelian theory, and when the data are, quote, indexed in such a way that no exceptions to Mendelian rules can appear,” he is quoting from another version that I didn’t want to burden you with, explaining how the field workers had to find pedigrees that don’t appear to deviate from the Mendelian expectations.\nSo, pretty hard and well-supported concerns about field workers. Next, he addresses the vagueness of the phenotypes that Davenport and colleagues used, and again, Davenport publishes, quote, “The Trait Book of the Eugenics Record Office,” which you can also look up, a 52-page listing of their various phenotypes, and these will test my vocabulary here, but this is just from the “Is” and the “Ls”, most of them here. This is quoting from Heron’s quote, these are the phenotypes that they were interested in having their interviewers in fact assess: “Impracticalness, inadventurousness, disheartedness, unconversationableness, unanecdoteness, ludicrousness versus absence of sense of humor, sublimity versus stolidity, sweetness versus bitterness, coolness in emergency versus loss of head, cooperativeness versus aloofness,” and it goes on for page after page after page.\nAgain, quote, “The use of the term ‘insanity’ in the titles of these two papers is very misleading” Heron says, “only a comparatively small proportion of the affected individuals are actually insane. These papers deal not with the inheritance of insanity but with the inheritance of what the authors call a ‘neuropathic’ condition which is so comprehensive that it is a matter of surprise that there are any ‘normal’ individuals at all.” You notice a touch of sarcasm in his tone. “It is, indeed, a fortunate circumstance that the Mendelian theory requires the presence of some normal individuals,” end quote.\nAnd here again is a quote, bear with me, but I think this is important. “The neuropathic condition is based upon appearances and conditions manifest from infancy to old age, and the following are some of the conditions with the author’s opinion justify classification as neuropathic,” and they (what Heron does) and the saying that we did is we go through pedigrees, but he went through many more of them, and he obviously is picking out the ones that seem to be particularly unusual. “Died in infancy of convulsions, senile deterioration, died of marasmus, sister of mercy in Australia said to have died of homesickness, quick-tempered, very queer lives alone, boards out cats, restless, fidgety, nervous, cold headache, sick headache, worrier, rambler, neuralgia, insomnia, dictatorial, selfish, not very bright, high-strung, odd, very quiet disposition, lost interest in life, etc.”\nNow, I did my own research here. I picked, by random, 10 of Rosanoff’s pedigrees, and they had 73 affected individuals in those 10 pedigrees. And I classify them using their own description: 21 - their main definition was nervous, 13 were described as eccentric or queer, seven alcoholic, six high-strung, four cases of dementia praecox (which all were probands), four cases of manic-depressive insanity (all probands), four imbecile or feeble-minded (so those were probands), and one is microcephalic. So, I think Heron is right that in the description, the vast majority of individuals in these pedigrees are not severely mentally ill individuals, so terming this the genetics of insanity can be questioned.\nThen Heron turns, and this is the hardcore statistical part of the argument, to two ascertainment problems: Davenport and Rosanoff never correct for the proband through which they ascertain the pedigrees, thus biasing upward rates of illness in that sibship. Those of you that are not familiar should be aware that the early efforts to try to see, especially for recessive patterns from familial traits, that you find families with potential affected individuals, and this was mostly in the five to eight years right after Mendel’s work was rediscovered, kept getting ratios above 25. And it was Weinberg, a pediatrician, a German physician, who noted that you cannot count the affected individual. He developed what is his so-called program correction method, which, by the way, Rüdin adopts in his methods. But it does appear that neither Rosanoff nor Davenport were aware of this and the fact that counting the affected person, as they always do in the ratios, is going to bias upward in a substantial way. So that’s problem one.\nSecond, they don’t correct for the fact that larger sibships have many more affected and are likely to be ascertained, and that, we, in fact, one of the pedigrees we saw was doubly ascertained, and those have to be counted independently, which again Weinberg points out to get the correct findings. Now, interestingly, Heron doesn’t raise the problem that they don’t correct for age. So that an individual who is unaffected and 25 years old, unaffected and 50 years old, probably should be counted differently, by any criteria. You will find that if you’re trying to get Mendelian segregation ratios for age-dependent penetrant phenomena, you’re not doing correct for age, you’re going to distort things. But Heron was not apparently aware of that, although Rüdin was.\nSo here, “in summary, we believe that those who dispassionately consider the papers discussed in this criticism must conclude, with the present writer, that the material has been collected in an unsatisfactory manner, that the data have been tabled in a most slipshod fashion, and that the Mendelian conclusions drawn have no justification whatsoever…And when we find such teachings based on the flimsiest of theories and on the most superficial of inquiries”, and noticing the rather ad hominem attacks here, “proclaimed in the name of eugenics and spoken of as entirely splendid work, we feel it is not possible to use criticism too harsh, nor words too strong, in repudiation of advice which, if adopted, must mean the death of eugenics”, think also human genetics, “as a science”. Okay, so that is Heron’s response.\nNow, I’m going to go through much more briefly the comments of Karl Pearson, who wrote the other two monographs. Pearson was far more diplomatic in his comments. I’m only going to point out two things. One, he notes that especially the mental defect sections that really treating this as a dichotomy is probably not reasonable and that it’s better understood as a continuous trait. In fact, Pearson and the Galton lab did a number of correlational-based studies of estimates of intelligence and mental handicap. And then, he also expresses concern about the phenotypic assessments and, as we saw, the assumption which got counted in there of heterozygote status without evidence. That is the circular reasoning of counting the heterozygous implicatorially, which assumes the transmission pattern you’re trying to test.\nNow, let’s get into Davenport’s response. This is, in fact, from The New York Times directly, and then we’ll get into the monograph, but I think we can begin to get the tone of how Davenport responded here, quote, “A sweeping condemnation of American work in the science of eugenics is issued by the Galton Laboratory of University College under the direction of Dr. Karl Pearson.”\n“The specific criticism of American methods and American findings being presented by Dr. David Heron”, note the nationalism implied here, “this condemnation which finds the contribution of America based on worthless, slipshod research and filled with unwarranted conclusions is based chiefly on the Eugenics Record Office bulletins. Being myself responsible for the work of the Eugenics Record Office, which includes not only the actual laboratory work done at Cold Spring Harbor during the past three years, but the active collaboration of institutions and individual scientists throughout the country”, think Rosanoff, “I feel like it is my task to explode the false and injurious impressions sure to be created in the popular mind by the broadcast criticism issued from the Galton Laboratories”. So, again, pretty directly attacked.\nNow, let’s switch to a couple of months later. So Davenport didn’t really rest on his haunches here. He published a brief one-page response in Science magazine, which again you can read either here’s the reference here, but I am going to jump to some quotes from this. You can, again, see the general spirit of the vocabulary, and this is how he begins it: “The all-too-familiar blessings of Professor Karl Pearson about Mendelians have recently been continued by his understudy, Dr. David Heron, and directed toward American work in general and that of the undersigned in particular. Like my colleagues in this country, I should have remained silent under the attacks, knowing that discriminating men of science in this country, as well as in England, recognize their true animus that they lie outside the pale of science.”\nAnd then a couple of other quotes from that short Science article: “The numerous errors to which he calls attention fall, for the most part, into three categories based on misunderstandings so gross of the critics, on the critics’ part, as to render it difficult to believe they are not intentional. A critic who is guilty of such extensive, stupid, captious, and misleading criticism can hardly expect a scientific consideration of other points he raises.” So pretty strong language. “It will be futile, as a biologist, to attempt to show to the applied statistician his errors.” So, noting the contrast, biology, good, positive. We’re going to get to the biological root. Statisticians just deal with numbers, not real biological facts. Quote, “Genuine scientific criticism has always been useful in the advancement of science, but friends of Galton must regard it as a tragedy that the fortune of one of the largest-minded and most fertile men of science,” (small digression: Davenport liked Galton a lot, and in fact, Galton was surprisingly ambivalent in this Mendelian-Biometrician controversy, kind of switching from one side to the other because he liked some of the concepts of Mendelians), so he, Davenport, is praising Galton and derogatory toward his main pupil and mentee, Karl Pearson. So Galton “should be supporting a laboratory one of whose leading members (think Pearson) spends much time making elaborate researchers into his delusions concerning the blunder of others, instead of making positive discoveries in a field where so little is known and where the need for utilizable knowledge is so great.”\nOkay, so now let’s go to the last article I’m going to consider in detail, which is Rosanoff’s response. Rosanoff wrote a longer article in 1914, so again, moving pretty quickly given publication lag, and the American Journal of Insanity that in 1921 changed its name to the American Journal of Psychiatry that many of us know well. So here are some selective quotes:\n“In reading Dr. Heron’s pamphlet,” this is now Rosanoff speaking, “one is struck, first of all, by the unusual temper of the attacks, apparent on almost every page. The work of Davenport, Davenport and Weeks, Rosanoff and Orr, Goddard”, who worked mostly on mental handicap, “and some others are analyzed in a fashion are referred to without a single feature being found in them to be worthy of anything but unreserved condemnation from the critic.”\nHe quickly, however, and I think this is the most interesting part, gets to see what he sees as the heart of the dispute.\nNow, I find it intriguing that it is Rosanoff, the psychiatrist, and not Davenport, the real-deal geneticist, who makes these points. “The unfortunate position in relation to the scientific world of the English biometrical school, to which Dr. Heron belongs, may account in some measure for the temper of the attack. They have, by reason of a pride in their own tradition,” (think correlations) “that refuse the guidance of the light of Medelism, continuing to devote their time and labor to the investigation of the heredity of various traits in man, as well as in plants and animals, by purely statistical methods” (note that’s not used in a praiseworthy manner), “while biologists, note the contrast, the world over, were piling up evidence of observation, experiment, continuously adding to the support of Mendel’s theory. Eventually, it came about that the work of the Galton Laboratory has been valued by the scientific world for the development of refined statistical methods and not as a biological contribution to the subject of heredity”. So, Rosanoff really kind of calls it as he sees it, that is, the statistics versus biology controversy that I think, in many ways, fed this scientific and eventually personal controversy.\nHe goes on. Rosanoff then quotes Bateson, who, up until now, we haven’t heard his name, but in the English world, he is the leading member of the Mendelian group, very vigorously against Pearson but not about psychiatric illness. And now we have Rosanoff quoting from Bateson, “Of the so-called investigations of heredity by extension pursued by extensions of Galton’s non-analytic method and promoted by Professor Pearson and the English biometrical school, it is now scarcely necessary to speak. That such work may ultimately contribute to the development of statistical theory cannot be denied, but, as applied to the problems of heredity, the effort has resulted only in the concealment of that order which it was ostensibly undertaken to reveal.” In other words, statistical methods will never get to the true genetic factors; only the Mendelian approaches will be successful.\nFinal criticism, which focused on Heron’s lack of clinical training, suggesting that is a “universal agreement”. We’ll see. So this is it. It was the last. Rosanoff: “The burden of proof is upon the critic who, though a layman, assumes an attitude, in relation to his psychiatric issue [diagnosis], which is in opposition to a view universally held by psychiatrists.” That’s simply factually incorrect. In fact, most workers in the field at this point would have focused more on the genetic transmission of specific disorders, although some others did emphasize these very broad kinds of diathesis. So coming back to the quote, “And if he, furthermore, attempts to disqualify, on the basis of his attitude, work which in his opinion contains too large a margin of error, he must, in addition, take the burden of furnishing an acceptable measure of the error before his criticisms can be rendered valid; this our critic has not done.” So he’s basically saying he’s got no business criticizing my psychiatric judgment about the breadth of these phenotypes that he studied.\nAll right, so let me now summarize what I’ve taken you through. I’ve left out a lot of detail, but I think I’ve touched the high points. So first, it’s pretty obvious there’s a high level of acrimony, harsh, rather derogatory language on both parties. We’ve got a mixture of some substantive scientific critiques, but quite a lot of ad hominem attacks, not the best way to do science, I have to say. This is certainly started by Heron, but Davenport and to some extent Bateson give it back in measure, and Rosanoff, you know, still has, uh, takes his gloves off to some extent.\nThere are a couple of articles in the reference, one particular contemporary commentator who praises Heron’s science but castigates Heron for this derogatory approach. But it’s very interesting, I found an obituary of Heron written by Egon Pearson, that was the little curly-headed kid at the left of the Pearson picture, and he wrote, quote: “Recently when I asked Heron how he felt looking back about these years, and I think in his academic career, this controversy was pretty substantial, he replied to the effect that as a young man he enjoyed a good fight.” So I think we’d get the response from Heron as an older person that he obviously enjoyed some of the more pugilistic intellectual combat that he engaged in, and yeah, it got started with his monograph.\nI think, having studied this a fair bit, that there’s quite substantial evidence that Davenport used biased field methods, and this is true across a number of the monographs. So, I think there is circular reasoning in the way he trained field workers, training field workers to go out, especially, and find the pedigrees that validated their Mendelian assumptions.\nI think that the phenotypes used and the associated assessment methods simply strain credulity. I don’t have time to present you, and this is a trend of Davenport. Davenport wrote his most notorious monograph, which is called “Nomadism or the Wandering Impulse,” in which he determined was a sex-linked recessive with affected individuals, and this is again, I read from this pedigree table, having phenotypes including “Western Desperado, sailor, traveling salesman, itinerant tinker, canal boat captain”, as manifesting this sex-linked recessive trait called “nomadism”. And again, although this is not pointed out in this debate, several other psychiatrists, including Abraham Myerson, an important figure in psychiatric genetics during this time period in America, also strongly disagreed with Rosanoff’s very extended diagnostic pattern, saying these are completely impossible. So, Heron was not the only one challenging this.\nThere are methodologic errors for Davenport and Rosanoff that are really quite clear-cut, that is, their simple failure to correct for ascertainment biases. This has been published in German by Weinberg a few years before. What can explain that, given that Davenport, certainly his reputation, was based on studying human pedigrees, so he was not at that point on the cutting edge of the algebraic problems of how you get mendelian ratios, and one has to fault them, and again, that no age correction was performed. And here we see the much greater methodologic sophistication of the Louden School compared to what Davenport was doing. So if I had to judge (but you can all reach your independent opinions), Heron is considerably more right than wrong in his critique of Davenport and Rosanoff, and history, by and large, supports this judgment about the validity of the scientific work and particularly the claim of a clear demonstration of autosomal recessive inheritance of this so-called diathesis that they study.\nNow, I don’t think this forgives Heron’s tone, but it is a hard for modern readers not to share some skepticism of these methods. To take a step back, what led Davenport and Rosanoff to these misjudgments? My own view is that this was a case of what you might call Mendelian zealotry and over enthusiasm for Mendelian models that can distort scientific judgment, but again, you should reach your own conclusion. Does this have implications for the subsequent history of psychiatric genetics? Well, I think it does.\nFor example, in the late 1980s, this all followed the 1983 demonstration of linkage analysis by Jim Guzella using RFLPs. There was a rush of interest and enthusiasm of linkage analyses for psychiatric disorders, and these, of course, depend on Mendelian, or, you know, we call single major locus models of large effect Mendelian-like transmission. Some of us in the field (a lot’s participated, myself included) but who talked about the methodologic difficulties, the fact that we don’t have evidence from Mendelian-like segregating pedigrees for these, that we ought to be worried about false positives, and we certainly shouldn’t be studying very small numbers of pedigrees. Those of you old enough to know, there were three very high-profile false report results of Mendelian segregation using linkage analysis: two for manic depressive illness in 1987 and one for schizophrenia in 1988, and these are the three of them: Baron et al., Sherrington et al., and Janice Egeland, all very high-profile and very disturbing to the field. Gradually, the non-replications came in.\nSo, in my view, it is hard to underestimate the strength of the lure of Mendelian models then for Psychiatry Genetics. Psychiatrists are craving widespread scientific credibility; they want to demonstrate that the genetics is appropriate to find a full Mendelian model is so attractive to this. And this was a time again, in the 1980s and early 1990s, where twin research was also blossoming.\nWe had the first interview assess population-based studies, both with David Folker at IBG and Linda Neves at VCU. We had wonderful quantitative models fitting these liability threshold models, and I personally experienced tensions between the twin and the molecular researchers, including what you would see at earlier meetings of the World Congress of psychiatric genetics. And if I had to characterize it, if you were listening to each group critical of the other, you had the two following stereotypes: the Mendelians, mostly people doing linkage analysis, when they want to put down the Twin researchers, used this phrase, “you’re not doing real genetics, you’re just looking at statistical models,” which of course completely echoes what we saw 60-70 years earlier in the Dilemma that we’ve seen here between the biometricians and the Mendelians. So really, what goes around comes around. What were the Twin researchers saying of these people doing linkages, sometimes on three, five, or eight pedigrees? Quote, “you’re just a star-struck gene jock fitting models that we don’t know don’t apply.” So we have the same kind of ad hominem attacks between these two fields that have visited us again.\nSo the tension between Mendelian and biometrical genetics has been on the root of psychiatric genetics for several Generations now. And you would, give me a minute to say, I think there’s a deep irony in the evidence that has emerged from GWAS studies in recent years, that is the Pearson-Fisher model of a normal distribution of liability due to many genes of small effect has actually turned out to be right. So, the Mendelians were wrong in all their claims that “you’re just studying statistics” and the real biological understanding is only going to be through classic Mendelian models. And it’s very pleasing to see, in some important ways, through PRS scores through SEM GWAS, that we are now seeing a bit of a merger of these fields. Have we finally calmed down from this kind of internesting argument that we’ve had with one another across the generations?\nSo, before I conclude, what happened to the main players in this little story? Well, Charles Davenport gradually lost influence. He was an ardent eugenicist. He was influential in immigration restrictions of immigration from Southern and Eastern Europe during the 1920s and 1930s. He retired in 1934. The Carnegie Foundation, which was really supporting all of this, withdrew critical support for his Cold Spring Harbor laboratory in 1940. In part, because the enthusiasm of the Eugenics movement in the United States diminished substantially with the rise of the Nazis in Germany from the mid-1930s on. Davenport did not reduce his enthusiasm for that.\nDavid Heron, on the other hand, left academia in 1915 and worked for an insurance company in England. He went on to be the president of the Royal Statistical Society and had a distinguished career, but he was not doing this kind of work.\nAaron Rosanoff went on to a distinguished academic and administrative career in psychiatry. He led the first U.S. twin studies of schizophrenia and manic-depressive illness, so he reverted to more traditional diagnostic-based approaches, and he was sufficiently well-known to be appointed the state Commissioner of Lunacy in California in 1933. He also wrote a textbook, so he did relatively well for himself.\nPearson remained the Galton chair of eugenics at University College London until his retirement in 1933. He was at that point, although perhaps a little bit eclipsed by Ronald Fisher, one of the giants of statistical work. In June of 2020, UCL announced that it was renaming two buildings which had been named after Pearson because of connection with Eugenics (and for all of these authors you will read parts of here where they are very enthusiastic about eugenic pressures, and that’s a whole other talk).\nLast word in the New York Times, Heron got his last point. So this is now January 4th, so it’s like a month later, and you can see “Dr. Heron accused the head of the American Eugenics record office of callousness, inconsistency, and misinformation, and says that the following of his advice would mean the death of eugenics as a science”.\nSo here is, I am writing a paper on this, so this is an extensive bibliography on this page, and last, so again you can freeze your frame if you want to look at these in detail, and this is a rich area for those of you that have historical interests. I appreciate your attention, thank you."
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "Chapter 4: Study designs",
    "section": "",
    "text": "Chapter goals:\n\nUnderstand the different types of epidemiological study designs and when their use is appropriate.\nUnderstand different sources of error that can occur in epidemiological studies and how to adjust/correct for that error.\nGain insight into different types of genetic study designs and how to select study designs that are appropriate to your research goals."
  },
  {
    "objectID": "chapter4.html#sec-section1",
    "href": "chapter4.html#sec-section1",
    "title": "Chapter 4: Study designs",
    "section": "4.1 Epidemiological study design",
    "text": "4.1 Epidemiological study design\n\nThis video by Drs. Appuhamy and Clark gives an overview of different epidemiological study designs, covering the basics of the study types: ecological study, case series, cross-sectional study, case-control study, cohort study, interventional study, systematic review, and meta-analysis.\n\nTitle: Epidemiological Studies: A Beginners guide\nPresenter(s): Ranil Appuhamy, James Clark, Let’s Learn Public Health\nLength: 9:42\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.html#sec-section2",
    "href": "chapter4.html#sec-section2",
    "title": "Chapter 4: Study designs",
    "section": "4.2 Confounding, Chance, and Bias",
    "text": "4.2 Confounding, Chance, and Bias\n\nThis section includes a video from Cochrane Austria that provides a fairly comprehensive overview of potential sources of error for clinical and genetic studies. The video discusses \"what is confounding?\", different types of confounders, and how to deal with confounding, chance and random error, and different types of biases: selection bias, performance bias, measurement bias, and attrition bias. \n\nTitle: Confounding, chance, and bias\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nLength: 9:50\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.html#sec-section3",
    "href": "chapter4.html#sec-section3",
    "title": "Chapter 4: Study designs",
    "section": "4.3 Genetic study designs",
    "text": "4.3 Genetic study designs\n\nThis section explores different types of genetic study designs, starting first with a video from OpenLearn on twin studies and how studies of twins allow for detection and distinguishing of genetic- versus environmentally- affected traits. The second video from Dr. Thomas\n\n\nTwin studies\nTitle: What are “Twin studies”?\nPresenter(s): OpenLearn from the Open University\nLength: 2:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nChoosing the Right Study Design in Genetic Epidemiology\nTitle: Choosing the Right Study Design in Genetic Epidemiology\nPresenter(s): Duncan Thomas, University of Southern California\nLength: 34:31\nLink to video here.\nLink to video transcript here."
  },
  {
    "objectID": "chapter4.1_transcript.html",
    "href": "chapter4.1_transcript.html",
    "title": "Chapter 4.1: Epidemiological study design (Video Transcript)",
    "section": "",
    "text": "Title: Epidemiological Studies: A Beginners guide\nPresenter(s): Ranil Appuhamy, James Clark, Let’s Learn Public Health\n[Music]\nHello, and welcome to this video on epidemiological studies. We’re going to have a quick look at epidemiological studies, what they are, a few different types of studies, their advantages, and disadvantages.\nWhat is a study?\nFirst of all, what is a study? Well, simply put, a study is a scientific process of answering a question using data from a population. It can be any question. For example, does smoking cause cancer? Is there more disease in one area compared to another? Or, what food is responsible for causing an outbreak? So, the first thing to do in any study is to have a study question. What are we trying to find out? Then, we need to figure out what the best type of study is that would help us answer the question. Once we’ve decided on a study type, we need to do the study, collect and analyze the data using a suitable statistical method. Then, we need to interpret the results to make sense of it all, and finally, we need to report the results of our study. Importantly, all studies need to be done in an ethical way. Now, let’s have a look at a few different study types.\nECOLOGICAL STUDY\nAn ecological study is the type of study where measurements like disease rates and information about exposures are made on a group of people. The groups can be as small as people in a house or as large as people in an entire country. It’s important to remember that results and conclusions from ecological studies apply to a group and not to individuals. Ecological studies are useful for comparing the health of populations in different places, such as measles rates in Australia and New Zealand, or at different times. They’re also useful for generating questions and highlighting issues that can lend themselves to future investigations or studies.\nCASE SERIES\nA case series describes the characteristics of a group of people who have the same disease or the same exposure. The aim of this is to understand the demographics, clinical presentation, prognosis, or other characteristics of people who have a particular disease or describe something unusual. For example, in the early 1980s, the occurrence of an unusual pneumonia in men led to the recognition and identification of HIV.\nCROSS-SECTIONAL STUDY\nA cross-sectional study takes a selected population and measures health information at a given point in time, giving a snapshot of their health. It usually involves asking participants a series of questions using a questionnaire. Health surveys that collect health information about people in a population are an example of a cross-sectional study. Because these studies commonly measure how many people have a disease at a particular point in time, they’re also called prevalence studies. It’s important to make sure that the selected population is representative of the total population.\nCross-sectional studies are relatively inexpensive and easy to conduct compared to other studies. They can provide information on multiple exposures and outcomes and are a good way of assessing the health needs of a population. However, because the information is collected at a single point in time, it cannot be used to determine whether a particular exposure caused the disease or not.\nCASE CONTROL STUDY\nA case-control study starts off with cases, which are people with a disease. It uses a comparison group called controls who are similar to cases but do not have the disease. Then, both groups are asked about their previous exposures to different risk factors. Now, for each of the risk factors, the odds of being exposed if they were a case is compared to the odds of being exposed if they were a control. This is called an odds ratio. An odds ratio of more than one means that people with the disease are more likely to have been exposed to that risk factor than people without the disease. This suggests that it could be a possible cause of the disease. An odds ratio of less than one suggests that it’s a protective factor, and one suggests no association. Case-control studies are commonly used in foodborne outbreak investigations. For example, we can compare the odds of eating different kinds of food between people who fell ill after consuming a meal and those who didn’t. If the odds ratio is greater than 1 for a particular type of food, then it’s possibly the cause of the illness. A major advantage of a case-control study is that they’re often quick and cheap to do. Also, because they start off with cases, they can be used to study uncommon diseases. However, because these studies involve small numbers, they’re not good to study rare exposures. One of the challenges in a case-control study is to find suitably matched controls. Also, because studies ask about exposures in the past, people might not be able to recall their exposures accurately.\nCOHORT STUDY\nLet’s have a look at cohort studies now. In a cohort study, a group of people is followed over a period of time to see what happens to them, and information about risk factors is collected. We can then compare the occurrence of an outcome like disease in those who are exposed to a particular risk factor to those who are not exposed to that risk factor. The main measurement used in cohort studies is called the relative risk. A relative risk is the ratio between the risk of disease in the exposed group compared to the risk of disease in the unexposed group. A relative risk of greater than 1 means that the exposure is associated with an increased risk of the disease. If it is 1, it indicates that the risk is the same, and if it’s less than 1, it indicates that the risk is lower. A well-known cohort study is the British doctor study done in the 1950s, where a group of doctors were followed up for many years. This study provided valuable scientific evidence of the harmful effects of smoking, especially the link between smoking and lung cancer. One of the main advantages of cohort studies is that the time sequence of events can be determined. This is useful when trying to determine what caused a disease. Another advantage is that information about several different outcomes and risk factors can be collected at the same time. This allows for some analysis to be conducted on the data. A disadvantage of cohort studies is the high cost, and they can involve a large number of people being followed over a long period of time. They’re generally not suitable to study rare diseases. A challenge in cohort studies, especially ones that are conducted over a long period of time, is ensuring that people who started the study stay until the end of the study. If many people drop out, it will affect the results of the study.\nINTERVENTIONAL STUDY\nNow, let’s look at another type of study, an interventional study. In an interventional study, an intervention is done on a group of people, and the outcome is studied. Examples of interventions can be giving a medicine, a vaccine, or health advice. The outcome can be things like a change in disease status or change in behavior. A randomized controlled study is the best study design for an interventional study. Say we want to study the effects of a new drug. We start off with a study population, and people are then randomly allocated to be in the intervention group where they receive the drug or in the control group where they don’t receive the drug. Then, after a suitable time period, the outcomes in the two groups are assessed and compared to see what effect the drug had. Outcomes can be things like a change in disease symptoms or death rates. Ideally, neither the participants nor the investigators should know which group received the intervention. This is called double blinding and ensures that neither of them can influence the outcome of the study. The main advantage of a randomized controlled study is that it can provide good evidence that the intervention led to an outcome. Randomization ensures that both groups have an equal chance of receiving the intervention and that they have similar characteristics. This way, the effect of the intervention can be determined without other factors influencing the outcome. The disadvantages are that these studies.\nSUMMARIES\nFinally, sometimes it’s challenging to keep up with all the studies that have been conducted on a particular topic. Fortunately, there are studies that summarize other studies. There are two main ways that this is done: a systematic review and a meta-analysis.\nA systematic review systematically identifies all the relevant studies on a particular topic, assesses the quality of each study, synthesizes and interprets the findings, and presents an impartial, unbiased, and balanced summary of the evidence. A meta-analysis, on the other hand, uses data from all the studies that have addressed the same question and have a similar study design. It then combines the data from these studies to perform a statistical analysis and produce a single summary result.\nAnd that’s an overview of some of the commonly used epidemiological study types. We’ve looked at what a study is, different study types, their advantages, and disadvantages."
  },
  {
    "objectID": "chapter6.html#polygenic-risk-scores-introduction",
    "href": "chapter6.html#polygenic-risk-scores-introduction",
    "title": "Chapter 6: Polygenic Scores",
    "section": "6.1 Polygenic Risk Scores: Introduction",
    "text": "6.1 Polygenic Risk Scores: Introduction\n\nThe goal of this section is to give a general introduction into the purpose of polygenic risk scores (PRS) and how they are calculated for populations. The first video from the Center for Personalized Medicine describes what a polygenic risk score is and a broad idea of its applications. The second video from Dr. Andlauer expands upon this introduction by introducing the concepts of LD clumping and thresholding when calculating PRS.\n\nTitle: Polygenic Risk Scores\nPresenter(s): Center for Personalized Medicine, Oxford University\nLevel: Beginner friendly\nLength: 4:03\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Polygenic Risk Scores\nPresenter(s): Till Andlauer\nLevel: Beginner friendly\nLength: 4:36\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter6.html#polygenic-risk-scores-in-detail",
    "href": "chapter6.html#polygenic-risk-scores-in-detail",
    "title": "Chapter 6: Polygenic Scores",
    "section": "6.2 Polygenic Risk Scores: In detail",
    "text": "6.2 Polygenic Risk Scores: In detail\n\nThis video by Dr. Campos gives a more detailed look into how PRS are calculated and considerations for running PRS analyses, including a brief introduction and description of LDPred2 software for calculating PRS.\n\nTitle: Polygenic risk scores\nPresenter(s): Adrian Campos, Adrian.Campos@qimrberghofer.edu.au\nLevel: Intermediate\nLength: 22:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter6.html#polygenic-scoring-methods",
    "href": "chapter6.html#polygenic-scoring-methods",
    "title": "Chapter 6: Polygenic Scores",
    "section": "6.3 Polygenic Scoring Methods",
    "text": "6.3 Polygenic Scoring Methods\n\nThe following two videos explain different methods of calculating PRS and evaluate their performance. The first video by Dr. Pain compares different available PRS methods on their performance and required computational time (ends at 12:30) in the UK Biobank. Towards the end, Dr. Pain introduces their new software to calculate standardized individual polygenic risk score based on genetic ancestry. In the second video, Dr. Ni compares different PRS methods on their performance in the cohorts in the Psychiatric Genomics Consortium (i.e., schizophrenia cohorts). Dr. Ni gives toy examples to explain the different parameters they used to evaluate how well each PRS method performed.\n\nTitle: Polygenic Scoring Methods: Comparison and Implementation\nPresenter(s): Oliver Pain\nLevel: Intermediate\nLength: 21:14\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Polygenic risk scores: PGS comparison\nPresenter(s): Guiyan Ni\nLevel: Intermediate\nLength: 13:52\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_gwas_transcript.html",
    "href": "software_gwas_transcript.html",
    "title": "Software Tutorials: GWAS (Video Transcript)",
    "section": "",
    "text": "Genome-wide Association Studies in PLINK\nTitle: Genome-wide Association Studies\nPresenter(s): Hailiang Huang\nHailiang Huang:\nHello everyone. My name is Hailiang Huang. I’m a statistical geneticist at the Broad Institute. I’m very happy today to do this GWAS tutorial with you.\nSo the first thing—let’s clone the GitHub I have created for you - the GitHub repository that I have created for you with the data and tutorial materials. Please type this in your command line in the terminal: `git clone` and the location of the repository under my first and last name and GWAS-tutorial. It will take a while but it should be very fast. All right, this is done. Let’s get into the location - The folder you just cloned - and have a look at all the files. I would recommend you to open the ‘GWAS on the squad tutorial’ and your text editor, so that you can simply copy and paste everything from the text editor to your terminal. All right, I have opened this in one of my text editors and just noticed a few pieces of information here. The information here—so first, where you can download this GitHub repository. In case you have any questions, feel free to contact me. Additionally, for this tutorial, you have to install PLINK2, which can be retrieved here, and ‘R’, which can be downloaded from the R project website. Also, we use the ‘qqman’ package in R, so please have it installed in your R program.\nAll right, let’s get started. So first, let’s take a look at the data. This is one of the two data files. It has individual-level genotype information. If you use the ‘head’ command, you will see the first few rows in that file. The first two columns are the individual ID and the family ID. The next two columns are the IDs for the parents of that individual, as well as the sex and the diagnosis. After the first six columns, the rest of the columns have the genotype of that individual, which you can easily tell. In case the genotype is missing, we have ‘0’ to indicate the missing genotype.\nThe other file is called a map file. It basically has a list of SNPs (Single Nucleotide Polymorphisms), and each row here corresponds to one SNP. Every two columns here correspond to the SNP’s map, so that the program ‘plink’ will know which SNP this dataset refers to.\nAll right, so the first thing we typically do after we see this dataset is to convert it to binary format. What you saw is very convenient for you to get an idea of what the data is, but there is a format called binary format that is more efficient in terms of storage and analysis. All you need to do is indicate your input file, which can be done with the flag ‘--file’. You can use the command ‘--make-bed’ to convert this text file to binary format, and then indicate where you want this binary format to be written using the ‘--out’ flag.\nAll right, this is done. If you compare the size of the file before and after the conversion to the binary file, here is how you can do that. You can see that before conversion, you have a 77-megabyte file with the genotypes. After the conversion, you only have a 4.7-megabyte file. So, the conversion results in more than a 10-times space reduction.\nAll right, so let’s start some QC. The first thing we want to do is to remove the individuals that have a high missing rate. You can do that by using the ‘plink’ with the flag ‘--missing’. The ‘--bfile’ flag specifies the location where the input files are, and the ‘--out’ flag specifies where ‘plink’ should write the output files to. So, you use the ‘--missing’ command to calculate the missing rates. Here is what the output file looks like. It basically has a list of individuals with their missing rates. Here, you’ll find the ‘autofilter’ command filtering on the missing rate in the ‘F_MISS’ column. You can see that we have two individuals with missing rates greater than two percent. A high missing rate typically indicates problems in the sample quality, so it’s not a bad idea to remove the samples with a high missing rate.\nAll right, the next thing we want to do is to remove samples with a high heterozygosity rate. This can be done by calculating the heterozygosity using the ‘--het’ command. If you take a look, this is the output from that calculation. It’s basically a list of individuals with their heterozygosity rates. I have prepared an R script for you to group together individuals that fail either the missing rate test or the heterozygosity test. Due to time constraints, I won’t go into the details of that R script, but you can call that R script like this. In that script, we will create a list of individuals with extremely high missing rates or heterozygosity rates that are either too high or too low. All you need to do is use the ‘--remove’ command to exclude that list of individuals and inform ‘plink’ that you want to write the new data file to this location.\nAll right, so we have removed individuals that have a high missing rate. However, we also have to do that for the SNPs. To remove the SNPs that have a high missing genotype rate, we can leverage the other output file from the ‘plink’ command ‘--missing’, which is the ‘high missing SNP’ file. This file contains a list of SNPs with their missing rates. We decide to use five percent as a threshold. We write all these things we want to exclude in this file. Then we use the ‘plink’ with the flag ‘--exclude’ to remove such SNPs. So, ‘plink’ removes individuals, and ‘plink’ with ‘--exclude’ excludes the SNPs. All right, this is done. We now have the second iteration of QC with the SNPs with high missing rate removed.\nThe next thing we want to do is to find SNPs with differential missing rates—where the missing rate is different in cases versus in controls. We do so by using the ‘--test-missing’ flag. Take a look at what is inside. You have a list of SNPs with their missing rates in cases as affected and in controls as unaffected. In this case, we don’t want to use the p-value because the sample size is too small for the p-value to be helpful. We simply want to remove the SNPs with a five percent difference between cases and controls. This can be done by subtracting the difference of missing rates in cases versus controls and using five percent as a threshold. We write everything we want to exclude here, and we will remove that list of SNPs.\nAll right, the last thing we want to do on the SNPs is the Hardy-Weinberg equilibrium test. This can be done by using the ‘--hardy’ flag. This is a list of SNPs with their Hardy-Weinberg marker P-value. We want to only use the Hardy-Weinberg test for the unaffected samples, and we want to use a p-value of 10 to the power of minus six as a cutoff. We filter for such SNPs and use ‘plink’ to remove those SNPs.\nAll right, now you have a dataset that is relatively clean for individuals and for SNPs. What’s next? The next step is to remove individuals that are related to each other. But to do that, we need to create a dataset that is independent in terms of genetic variance. We can do so by using the ‘--indep-pairwise’ flag. This involves a bunch of parameters that go into the LD pruning process, including the window size, the step of the window that is moving, and also the threshold you want to use for identifying two variants that are in LD with each other.\nSo, after you run this command, you will have a list of SNPs that ‘plink’ would recommend you to keep as independent SNPs. This is in the file called ‘plink.prune.in’. All you need to do is generate a new dataset with only these variants by using the ‘--extract’ command. This will give you an independent set of markers in a new ‘plink’ binary format. Then, you want to use the ‘--genome’ flag to calculate the relatedness between samples. Take a look at what is in that output file, which is just a pair list of individuals with their relatedness. Typically, a relatedness measure higher than 0.2 indicates something concerning. We want to identify those pairs using this command, write them to a file, and then use the ‘plink’ ‘--remove’ command as we did before to remove such individuals.\nAll right, let’s take a look at how many variants and individuals we have removed. Initially, we had over 100,000 individual SNPs and 200 subjects before QC. After QC, we got rid of less than 1000 variants and eight individuals.\nAll right, now let’s assess how well we did in the QC. We use the ‘wc -l’ command to count how many variants we had before and after QC, as well as how many individuals we had before and after QC. It looks like the data was pretty good. The QC process removed roughly 1000 variants, which is quite normal given that we started with over a little over a hundred thousand. We ended up with around 99,700, which is not bad. We removed eight individuals from the initial 200, leaving us with 192, which is also acceptable. All right, so far, so good. What’s next?\nPrincipal Components Analysis (PCA)\nNext is to generate the principal components so that we can include them as covariates to control the population structure. This is how you do that: You need to create a new independent dataset, independent in terms of variance, and you want to generate that based on the latest file you have created after removing the related individuals. You do that similarly as you did before, using the ‘--independent-pairwise’ command. Then, you create a new file that only has the independent variants, like this. After that, you perform the principal component analysis using the ‘--pca’ command.\nAll right, this is the output from that command. The ‘.angle’ value and ‘.eigenvec’ files have what you need for the downstream analysis. So, let’s take a look. The ‘.eigenvec’ file contains a list of individuals with their principal components—many of them.\nRunning GWAS association test\nSo, what you do next is to perform your GWAS with these principal components. This can be done using the ‘--logistic’ flag because this is a case-control study. You want to use logistic regression. The ‘--hide-covar’ flag basically asks ‘plink’ not to write the coefficient for the covariates, as for this study, you are only interested in the variance. The ‘--covar-variance’ flag tells ‘plink’ which file has the covariates. Additionally, this is to calculate how many components you should include as covariates. Here we do one to ten. All right, let’s do it.\nAll right, it’s done. So, the ‘.logistic’ file has a list of all the variants with their odds ratio, standard error, confidence interval, and the p-value. This is exactly what you need from a GWAS. We could use this script I wrote in R to do some visualization. That script also calculates the genomic inflation factor, which is only 1.06. It looks like the genome has been very well controlled. Let’s look at the two figures it has generated: the Manhattan plot and the QQ plot.\nAll right, so this is a Manhattan plot. And this is the QQ plot. You can tell that the genome has been well controlled; there’s no significant inflation. Unfortunately, for this particular study, we don’t have a genome-wide significant signal for now. We do have one signal surpassing the suggestive significance threshold, which suggests that we may need more samples to identify the same significant locus. All right, that is a very quick tutorial, and I hope it helps for your research. Thank you for your attention.\n\n\n\nGenotype QC\nTitle: How to run quality control on genome-wide genotyping data\nPresenter(s): Jonathan Coleman\nJonathan Coleman:\nHello, I’m Joni Coleman, and in this brief presentation, I’m going to discuss some key points concerned with running quality control on genome-wide genotype data, which is a common first step in running a GWAS. I’m going to provide a theoretical overview, addressing the overarching reasons why we need to do QC, highlighting some common steps, and discussing a few pitfalls the data might throw up. I’m not going to talk about conducting imputation or GWAS analyses or secondary analyses. Nor am I going to talk at great length about the process of genotyping and ensuring the quality of genotyping calls. Similarly, I won’t go into any deep code or maths. However, if you are starting to run your own QC and analyses, I recommend the PGC’s RICOPILI automated pipeline as the starting point. There are also some simple scripts on my group’s GitHub that may be useful as well. They follow a step-by-step process with codes and explanations. We’re currently updating this repository, so look out for some video tutorials there as well.\nSo here is our starting point. I’ll be using this graph on the top right several times throughout this talk. This is a genotype calling graph with common homozygotes in blue, heterozygotes in green, and rare homozygotes in red. Hopefully, your data will already have been put through an automated genotype calling pipeline. And if you’re really lucky, and an overworked and underappreciated bioinformatician, may have done some manual recalling to ensure the quality of the data is as high as possible. But in point of fact, the data you’ll be using won’t be in this visual form but rather as a numeric matrix like the one below that lists SNPs and individuals. This might be in the form of a PLINK genotype file or its binary equivalent, or it’s in some similar form that can be converted to the PLINK format.\nAnd where we want to go is clean data with variants that are called in the majority of participants in your study and won’t cause biases in downstream analyses. That should give a nice clean Manhattan plot from GWAS like the one below, rather than the Starry Night effect of this poorly QC’d Manhattan plot above. However, something I’d like to emphasize across this talk is that QC is a data-informed process. What works for one cohort won’t necessarily be exactly right for another. Good QC requires the analyst to investigate and understand the data.\nRare variants\nOften, the first step is to remove rare variants. This is because we cannot be certain of variant calls. Consider the variants in the circle on the right: are these outlying common homozygotes or are they heterozygotes? We cannot really tell because there aren’t enough of them to form a recognizable cluster. Typically, we might want to exclude variants with a low minor allele count, for example, five. There are many excellent automated calling methods to increase the amount of certainty you have in these variants. But it’s also worth noting that many analytical methods don’t deal well with rare variants anyway. Again, the demands of your data determine your QC choices. It may be more useful for you to call rare variants, even if you’re uncertain of them, or you may wish to remove them and be absolutely certain of the variants that you retain.\nMissingness\nNext, we need to think about missing data. Genotyping is a biochemical process, and like all such processes, it goes wrong in some cases and a call cannot be made. This can be a failure of the genotyping probe, poor quality of DNA, or a host of other reasons. But such calls are unreliable and they need to be removed.\nMissingness is best dealt with iteratively. To convince you of that, let’s examine this example data. We want to keep only the participants, which are the rows in this example, with complete or near-complete data on the eight variants we’re examining, which are shown in the columns. So, we could remove everyone with fewer than seven SNPs, but when we do that, oh dear, we’ve obliterated our sample size. So instead, let’s do things iteratively. We’ll remove the worst SNP again. Variant 7 goes. And then we remove the worst participant. Bye-bye, Dave. Then we remove the next worst SNP, so that’s SNP two. And now everyone has near-complete data, and we’ve retained nearly all of our cohort. This was obviously a simple example. How does this look with real data?\nSo here we have some real data, and it’s pretty good data. Most variants are only missing in a small percentage of the cohort, but there are some that are missing in as much as 10% of the cohort. So, let’s do that iterative thing: removing variants missing in 10% of the individuals, and then individuals who have more than 10% missing variants, and then 9%, and so on, down to 1%. When we do this, the data looks good. Nearly all of the variants are at 0% missingness, and those that aren’t are present in at least 578 to the 582 possible participants. We’ve lost around 25 participants for about 22,500 SNPs. But what if we didn’t do the iterative thing and we just went straight for 99% complete data?\nWhen we do that, the distribution of variants looks good again—arguably, it looks even better—and we’ve retained an additional 16,000 variants. However, we’ve lost another 40 participants, which is about 6% more of the original total than we lost. Typically, participants are more valuable than variants, which can be regained through imputation anyway. But this, again, is a data-driven decision. If coverage is more important than cohort size in your case, you might want to prioritize well-genotyped variants over individuals.\nHardy-Weinberg Equilibrium\nSo, we’ve addressed rare variants, where genotyping is uncertain, and missingness, where the data is unreliable. But sometimes, calling is simply wrong, and again, there are many reasons that could be. We can identify some of these implausible genotype calls by using some simple population genetic theory. From our observed genotypes, we can calculate the allele frequency at any biallelic SNP we’ve called. So here, the frequency of the A allele is twice the frequency of the AA calls (those are our common homozygotes in blue), plus the frequency of AB calls (heterozygous in green). We can do the equivalent, as you see on the slide, for the frequency of the B allele. Knowing the frequency of the A and B alleles, we can use Hardy and Weinberg’s calculation for how we expect alleles at a given frequency to be distributed into genotypes, to generate an expectation for the genotype we expect to observe at any given allele frequency. We can then compare how our observed genotypes (i.e., the blue, green, and red clusters) fit that expectation, and we can test that using a chi-squared test.\nNow, Hardy-Weinberg equilibrium is an idealized mathematical abstraction, so there are lots of plausible ways it can be broken, most notably by evolutionary pressure. As a result, in case-control data, it’s typically best to assess it just in controls or to be less strict with defining violations of Hardy-Weinberg in cases. That said, in my experience, genotyping errors can produce very large violations of Hardy-Weinberg. So, if you exclude the strongest violations, you tend to be removing the biggest genotyping errors.\nSex check\nThe previous steps are mostly focused on problematic variants, but samples can also be erroneous. One example is the potential for sample swaps, either through sample mislabeling in the lab or incorrectly entered data in phenotypic data. These are often quite hard to detect, but one way to detect at least some of these is to compare self-reported sex with X chromosome homozygosity, which is expected to differ between males and females. In particular, males have one X chromosome. They’re what’s known as hemizygous. So, when you genotype them, they appear to be homozygous on all SNPs on the X chromosome. Females, on the other hand, have two X chromosomes. They are holozygous, and they have a normal X distribution centered around zero, which is the sample mean in this case. You could also look at chromosome Y SNPs for the same reason. However, Y chromosome genotyping tends to be a bit sparse and is often not of fantastic quality. So, there are benefits to using both of these methods. It’s also worth noting that potential errors here are just that—potential. Where possible, it’s useful to confirm these with further information. For example, if there isn’t a distinction between self-reported sex and self-reported gender in your phenotype data, then known transgender individuals may be being removed unnecessarily. The aim here is to determine places where the phenotypic and genotypic data is discordant, as these may indicate a sample swap. And this might indicate the genotype-to-phenotype relationship has been broken, and that data is no longer useful to you.\nInbreeding coefficient\nAverage variant homozygosity can also be applied across the genome, where this metric is sometimes referred to as the inbreeding coefficient. It’s called that because high values of it can be caused by consanguinity—related individuals having children together, which increases the average homozygosity of the genome. There can also be other violations of expected homozygosity, so it’s worth examining the distribution of values and investigating or excluding any outliers that you see.\nRelatedness\nExamining genetic data also gives us the opportunity to assess the degree of relatedness between samples. For example, identical sets of variants imply duplicates or identical twins. 50% sharing implies a parent-offspring relationship or siblings. And those two things can be separated by examining how often both alleles of a variant are shared. Specifically, we would expect parents and offspring to always share one allele at each variant, whereas siblings may share no alleles, they may share one allele, or they may share two. Lower amounts of sharing imply uncles and aunts, and then cousins, grandparents, and so on, down to more and more distant relationships. In some approaches to analysis, individuals are assumed to be unrelated, so the advice used to be to remove one member of each pair of related individuals. However, as mixed linear models have become more popular in GWAS, and mixed linear models are able to retain and include related individuals in analyses, related individuals, therefore, should be retained if the exact analysis method isn’t known. Again, it’s worth having some phenotypic knowledge here. Unexpected relatives are a potential sign of sample switches and need to be examined, confirmed, and potentially removed if they are truly unexpected. And once again, it’s important to know your sample. The data shown in this graph does not, despite what the graph appears to suggest, come from a sample with a vast amount of cousins. Instead, it comes from one in which a minority of individuals were from a different ancestry, and that biases this metric. I’ll talk a little more about that in just a moment.\nRelatedness can also be useful for detecting sample contamination. Contamination will result in a mixture of different DNAs being treated as a single sample, and this results in an overabundance of heterozygote calls. This, in turn, creates a signature pattern of low-level relatedness between the contaminated sample and many other members of the cohort. These samples should be queried with the genotyping lab to confirm whether or not a contamination event has occurred and potentially be removed if an alternative explanation for this odd pattern of inter-sample relatedness can’t be found.\nAncestry\nFinally, a word on genetic ancestry. Because of the way in which we have migrated across our history, there is a correlation between the geography of human populations and their genetics. This can be detected by running principal component analyses on genotype data pruned for linkage disequilibrium. For example, this is the UK Biobank data. You can see subsets of individuals who cluster together and who share European ethnicities, other subsets who share African ethnicities, and subsets who share different Asian ethnicities. In a more diverse cohort, you would be able to see other groupings as well. This kind of 2D plot isn’t the best way of visualizing this. For example, here, it isn’t really possible to distinguish the South Asian and admixed American groupings, and you don’t get the full sense of the dominance of European ancestry data in this cohort. Europeans, in this case, account for around 95% of the full cohort. But because of overplotting, i.e. the same values being plotted on top of each other in this 2D plot, you don’t really appreciate that. Looking across multiple principal components helps for that.\nAncestry is important to QC. Many of the processes I’ve talked about rely on the groups being assessed fairly being fairly homogeneous. As such, if your data is multi-ancestry, it’s best to separate those ancestries out and rerun QC in each group separately.\nConclusion\nSo, that was a brief run-through of some of the key things to think about when running QC. I hope I’ve gotten across the need to treat this as a data-informed process and to be willing to rerun steps and adjust approaches to fit cohorts. Although we’ve got something resembling standard practice in genotype QC, I think there are still some unresolved questions. So, get hold of some data, look online for guides and automated pipelines, and enjoy your QC.\nThank you very much for listening. I’m doing a Q&A at 9:30 EST. Otherwise, please feel free to throw questions at me on Twitter, where I live, or at the email address on screen, which I occasionally check. Thank you very much.\n\n\n\nTractor: GWAS with admixed individuals\nTitle: Tractor: Enabling GWAS in admixed cohorts\nPresenter(s): Elizabeth Atkinson\nElizabeth Atkinson:\nHi, thanks so much for your interest in using Tractor. I hope this tutorial helps get you started in doing ancestry-aware GWAS on admixed cohorts.\nSo, here’s just a brief outline: I’ll start with some motivation, go into the overview of our statistical model, and then talk about implementation of the actual Tractor code.\nTo start off with our motivation, I want to reiterate a point that is now thankfully becoming common knowledge in the GWAS community. That is, that the vast majority of our association studies are actually conducted on European cohorts. And if we look further at this breakdown of the small kind of wedge of the pie of who’s not European, we can see that there’s actually only a few percent from recently admixed populations, such as African-American and Hispanic Latino individuals—groups that collectively make up more than a third of the US populace. And just to make sure we’re all on the same page, an admixed individual refers to somebody whose ancestry is not homogeneous but rather has components from several different ancestral groups. There are actually many more admixed individuals out there whose samples have already been collected and genotyped or sequenced alongside phenotypes, but they’re not making it into this figure due to being intentionally excluded for being admixed. So, there’s really a pressing need for novel methods to allow for the easy incorporation of admixed people into association studies.\nAdmixed people are generally removed due to the challenges of accurately accounting for their complex ancestry, such that population substructure and stratification can seep in and bias your results. And in the context of GWAS, this means the potential for false positive hits that are due to ancestry rather than a real relationship to the phenotype. So, in a global collection such as this example from the PGC, even controlling for PCs, which is the standard way that admixture is attempted to be kind of controlled for, there’s still so much variability in the data that there’s a lot of concern over false positives. It’s because of this that researchers will often sort of draw a circle around people who are deemed homogeneous enough to be included in the study, and everyone else is excluded. So, this nearly always ends up resulting in European individuals being included, as they generally represent the bulk of samples that have been collected. So the main motivation of the project I’m talking about today was to try to rectify this issue and develop a framework to allow for the easy incorporation of admixed people into association studies.\nSo, with Tractor, we are handling this concern by directly incorporating local ancestry. Local ancestry tells us the ancestral origin of each particular haplotype tract in an admixed individual at each spot in their genome. So, in this three-way admixed individual, the y-axis are the autosomes, the position along them is on the x-axis, and because humans are diploid, the top and bottom half of each chromosome is painted differently. You can see that each tract in this individual is colored according to the ancestry from which it derived.\nSo, the intuition behind Tractor is that to correct for population structure, we effectively scoop out the tracks from each component ancestry to be analyzed alongside other ancestrally similar segments. So, we do this by tracking the ancestry context of each of the alleles for each person at each spot in their genome. Note that here I’m only showing this red ancestry, but the same thing is going to be happening for the blue and green ancestry.\nThe statistical model built into Tractor tests each SNP for an association with the phenotype by splitting up the ancestry context of the minor alleles. So, in the two-way admixed example, we have our intercept, of course, and then we include terms for how many copies of the index ancestry there are for this person at that spot in the genome. For example, if you have zero, one, or two African alleles at this location, as well as how many minor alleles fall on your ancestry A versus ancestry B backbone. So those are the first three x’s in these terms. This is what corrects for the fine-scale population structure. If there were differing allele frequencies for the ancestries at this spot in the genome, as we expect there to kind of routinely be due to the different demographic histories of modern-day human populations, then we’re no longer going to be confounded by this, as we’ve sort of deconvolved them. So, the minor alleles you’ll see on each ancestry backbone, is kind of properly scaled to their background expectations. Note that while I’m showing a two-way admixed example here, this model can also scale easily to an arbitrary number of ancestries by addition of terms, as well as allowing for the ready inclusion of all of your necessary covariates.\nSo, I already went over how our model corrects for the fine-scale population structure at the genotype or haplotype level, which is what’s allowing the inclusion of admixed people in a well-calibrated manner in GWAS. But it also has some other nice benefits I’d like to briefly mention. We’ve built in an optional step to recover long-range tracks that we find to be disrupted by statistical phasing by using our local ancestry information. With respect to GWAS, our novel local ancestry-aware model can improve your results in a variety of ways, most notably through boosting GWAS power, generating ancestry-specific summary statistics, and helping to localize your GWAS signals to be closer to the causal SNPs. So, we really hope that this framework should advance existing methodologies for studying admixed individuals and allow for a better-calibrated understanding of the genetics of complex disorders in these underrepresented populations.\nNow, I’ll shift to giving a brief walkthrough of the steps involved in implementing the Tractor pipeline. There are three different steps, the first one being optional for efforts that require complete haplotypes. I’m going to go through each of them kind of individually, but the first three steps are implemented as Python scripts, and step three, the GWAS model, is sort of the recommended implementation in the cloud-native language, Hail. There’s a lot more documentation and description of these steps on our GitHub page, so please check out the wiki there for more information, as well as a Jupyter notebook to kind of walk through the steps of the pipeline.\nSo, before running local ancestry-aware GWAS, you need to call your local ancestry. We recommend the program RFMix for this, which is described in their paper ‘Maples et al. 2013,’ as well as their GitHub repo, which I’m linking to here. The success of Tractor really relies on good local ancestry calls, so it’s really important to ensure that your local ancestry inference performance is highly accurate before you try to run a Tractor-GWAS. Just to make sure you know we’re all ready to go, I wanted to show an example command for launching an RFMix run on one chromosome using some publicly available data. This is sort of the basic parameter settings that we’ve come up with as being kind of ideal for our Tractor pipeline. Again, more details on the Wiki page.\nBut what I’d like to spend more time on is the actual Tractor scripts. This first step in our pipeline is what’s going to detect and correct for switch errors from phasing by using our local ancestry calls. You can see our supplementary information and Extended Data Figures 1 and 2 in our manuscript for some additional context around what we mean by this. This step uses RFMix ancestry calls as input and is implemented with the script ‘UnkinkMSPfile.py,’ which tracks those switch locations and corrects them. So, the output will consist of two files: a text file that documents the switch locations for each individual (so that’ll be sort of your input MSP file name suffixed with ‘switches’), and a corrected local ancestry file (again, the same input name but this time suffixed with ‘unkinked,’ as in unkinking a garden hose – we’re kind of straightening out those switch errors). Here’s your kind of example usage – you should just need to point to the MSP file name without this ‘msp.tsv’ suffix. I’m going to call this script. So this is one of the default output files from RFMix.\nNext, we also need to correct switch errors in our phased genotype file, and we’re expecting VCF format for this. This is what’s going to actually recover the fuller haplotypes and improve our long-range track distribution. This step is implemented with the script ‘UnkinkGenofile.py’ and expects as input the phased VCF file that went into RFMix, as well as that switches file that we generated in the previous step. So, the switches file is basically used to determine the positions that need to be flipped in your VCF file. Tractor also expects all of your VCF files to be phased, and it’s recommended to strip their info and format annotations prior to running, just to ensure good parsing. Again, Step One is optional and won’t affect your GWAS results, but can be useful for efforts that require complete haplotypes.\nAlright, so in Step Two, we extract the tracks that relate to each component ancestry into their own VCF files and calculate the ancestry haplotype and alternate allele counts for each component ancestry for your admixed cohort. This step is implemented with the script ‘ExtractTracts.py’ and expects as input the stem name of your VCF (you can have it just be unzipped or gzipped is also allowed), as well as your RFMix MSP files. So, you can input those ‘Unkinked’ files from the previous step if you chose to correct switch errors, or you can go straight from your original data if you did not. This script now accepts an arbitrary number of ancestry calls, which is very exciting, so you can specify the number of component ancestries with the ‘--num_inks’ flag – the default is two, you can change it to however, you know, multi-way your population was. If your VCF file is g-zipped, also include the flag ‘--zipped’; if not, leave that flag off. So there are multiple files output from this step, including the counts for each individual at each position for these three different pieces of information I’m listing here. So, firstly, you’ll get the number of copies of each ancestry at that spot – so if you have zero, one, or two copies of, you know, ancestry A, B, C, however many ancestries are in your data. Secondly, you’ll get the number of alternate allele counts on each of those ancestral backgrounds. And thirdly, you’ll get VCF files for each of those ancestries, containing only genotype information for that ancestry’s tracks. So, these will be sort of VCF files that will contain a bunch of missing data – will kind of blank out the other ancestries – and it’ll also contain some half-calls for instances where there’s, you know, sort of heterozygous ancestry at a spot.\nAlright, so our ancestry dosage output can now be used in our local ancestry-aware GWAS model. The recommended implementation of the Tractor joint analysis model uses the free, scalable software framework called Hail. Hail can be run locally or on the cloud. I’m going to be showing a cloud implementation here, and it can be useful to build and test your pipeline in a Jupyter notebook when applying it to your own cohort data. So, for this reason, we supply an example Jupyter notebook written in Hail on our GitHub, which can be adapted for user data. The specific commands to load in and run linear regression on our ancestry dosage file will be as follows:\nSo, here is the specific suite of commands in Hail that will read in and format our Tractor dosage files, just to make it a little nicer running things downstream. This is a little bit more involved in terms of loading files in than a standard VCF or MatrixTable. This is because we need to run GWAS on entries rather than rows. So, for each ancestry and dosage haplotype file, we’ll be kind of annotating that information in, rather than just sort of one piece of information per variant. And this will become a little bit clearer in the following slides, but that’s why there’s like, you know, a couple extra steps in this reading in dosage files, proportion.\nSo, next we’ll join our ancestry and haplotype dosage files together onto a Hail Matrix table. We do this by basically annotating the haplotype counts and any of the other ancestry information we have onto our first ancestry – ancestry zero. And here, we’re creating a new combined Hail Matrix table called ‘mt,’ which will have all of this information in it. Basically, we’ll have our original ancestry zero dosage information and then ‘structs’ for each additional term that we’re annotating on. So, here I’m showing examples for two-way and three-way admixed example. In the two-way example, we’re annotating structs for the ancestry one dosages, as well as our hap counts for our ancestry zero – our index ancestry haplotype counts. These are all three default outputs from our earlier Tractor steps. So, if you want to have more multi-way admixed populations included, you just need to add additional terms for each new ancestry – one new dosage term for each additional ancestry and N-1 terms for the haplotype counts.\nSo, now we can load in and annotate our Matrix table with our phenotype and covariate information, and from here on, we are now utilizing standard Hail commands. The only thing to be careful about with loading in your phenotypes is to make sure that you’re keying by the same ID field that will match the names in your genotypes file. And again, note that I’m doing this in the Google Cloud example, so I’m pointing to the location of this file in my Google bucket.\nFinally, we can run the linear regression. For a single phenotype, for example, total cholesterol or TC, as shown here, along with our relevant covariates. So here, in addition to our intercept of one, we have our hap counts term and our two terms for our two-way admixed population – our ancestry zero and one dosages. And then we also are putting in all of our covariates that we want. In this case, I’m using sex, age, blood dilution level (which is kind of a phenotype-specific covariate), and then an estimate of the Global African ancestry fraction that we got prior to this from using the program Admixture. So we recommend putting in an estimate of your Global ancestry in addition to these sort of local ancestry-specific terms, just to account for if there’s a relationship to the phenotype with their overall ancestry proportions. And we find that the direct measure of global ancestry is a bit more accurate than PCs, so this is sort of the recommended strategy.\nDoing that, we are basically annotating each of the variants in our Matrix table with the results of the GWAS. The results are again saved as a ‘struct’ here named TC (our phenotype), and this will contain pieces of information for each of the summary statistics that we listed here, including the beta effect size, the standard error, and the p-value. Within each of these, there’s going to be an array of index-able values that’s in the order of your terms. So, the index of zero is going to be your intercept; in the example I listed prior to this, one is going to be your haplotype dosage; two is your ancestry zero dosage; three would be ancestry one; and then covariates would be four, until however many covariates that you have. So you can pull out summary statistics for every term in your model to allow for kind of easy comparison. They’re all going to be annotated into kind of one big ‘struct’ that’s just index-able by what position they were in your model.\nTo run multiple phenotypes in batch, you can make a list of phenotypes first and then cycle through them, annotating The Matrix table with multiple ‘structs’ that will each be named, you know, the names of your phenotypes. So, again, all the information can be stored just in that one Matrix table.\nIn our Jupiter notebook, we also provide some code to generate Manhattan and QQ plots. You can select the term you would like to plot by its index, as we talked about in the last slide, and you can use the Hail commands `plot.manhattan` and `plot.qq` to visualize the results. So, here we’re pulling in our TC struct for the p-value. We want the third term, or the index of two.\nAnd here is what those plotting commands will produce. With those, you know, the settings, by launching those commands, you’ll end up with figures that look exactly like this.\nGreat! So with that, I hope that this was helpful. Thanks so much again for your interest in Tractor, and feel free to leave questions on our GitHub repo or email me with any thoughts. I also have many people to thank, of course, for this project, including my K01 funding from NIMH. And I’ll also direct you to our paper published several months ago in Nature Genetics, which has a much more thorough description of our method and some other considerations. Again, our code is freely available on GitHub, alongside a Wiki tutorial to help you get started with Tractor there as well. So, thanks very much, and I hope that you enjoy using Tractor.\n\n\n\nGenotype Calling and Imputation with MoChA\nTitle: Genotype calling/imputation on the cloud: MoChA pipleine\nPresenter(s): Giulio Genovese\nGiulio Genovese:\nHello, everybody! Today, I’m going to talk about the MoChA Pipeline and how you can use it to perform phasing and imputation of DNA microarray data in the Cloud.\nSo, as an introduction—what is phasing and imputation? Well, for every genome-wide association study where we work with DNA microarray data, we always have to go through three main steps. The first step is to call genotypes from microarray intensity data. Then, we have to retrieve the haplotype structure of the samples by phasing. And then, we have to impute the missing variants that were not present in the microarray by comparing the haplotypes that we phased with haplotypes of another reference panel of samples for which we have more information. Phasing and imputation, in particular, are two steps that require a fair amount of computations and that parallelization of computations—they have high computational requirements. There have been many pipelines out there to perform these two steps, but today, I’m going to talk about the MoChA Pipeline, which consists of two different scripts. One is the MoChA wedl, and one is the impute wdl. The first one performs calling genotypes, and basically, the second one performs imputation.\nThese scripts are written in WDL, which is a language for workflows and allows you to define very complex segments of tasks. And in WDL, in particular, corresponds to a set of tasks and a description of the order in which tasks have to be performed. And in writing the task, we have to define which input file we need, which commands we’re going to run on those files, and which output file, so you expect. And the nice thing about writing pipelines in the WDL (Workflow Description Language) language is that they can be run on any computational environment.\nIn particular, you can do this thanks to the Cromwell Workflow engine, which is an engine developed by the Broad Institute. The idea is that you can input your WDL pipeline into the Cromwell server, and the Cromwell server will take care of dispatching all the required jobs to different nodes in a highly parallelized fashion. This could be nodes on an HPC cluster like an SGE, SLF, SLURM cluster that is common in academic environments, or maybe in a Cloud computing cluster like Google Cloud, Amazon Web Services, or Microsoft Azure. And this is all transparent to the user, who doesn’t have to worry about the technicalities of the cluster implementation when writing a WDL pipeline.\nSo, what features does the MoChA pipeline for phasing and imputation have that might not be available in other similar pipelines? Well, the first feature is that it follows a minimalistic approach. The user always has to provide a minimal amount of necessary inputs to run the pipeline. It can take various input file formats, but in particular, it can process raw Illumina input files, also known as IDAT files, without requiring to be pre-processed in alternative ways. It uses very state-of-the-art software for phasing and imputation (SHAPEIT4, IMPUTE5). It allows you to use the reference genome of your choice. So, you can request the GRCh38 reference that you want to use. And once you have set up a Cromwell server, you can actually even run the pipeline in Terra, if you want, it’s very easy to use.\nAnd it scales very nicely and seamlessly to a cohort of biobank size. In particular, you can run it on the UK Biobank, and the phasing part will run for less than $100, and phasing imputation overall will run in less than 40 hours if you run it in Google Cloud. The reason I actually developed this phasing pipeline was to support analysis related to mosaic chromosome alterations. In particular, when you run this pipeline, you also want to get as an output a list of constitutional mosaic chromosomal alterations in the samples you process.\nRunning the pipeline\nSo, how complicated is it to run the MoChA pipeline? Hopefully, I can convince you that it’s not that complicated. Running a pipeline mostly means that you need to define a set of input files. Let’s say that we’re going to run the pipeline in IDAT mode, and we’re going to provide a set of IDAT files, which are again Illumina intensities. We’ll need that and we’re also going to have to provide a table with a list of samples and a table with a list of batches. And then, in particular, I’m going to have to provide a configuration file in JSON format indicating in which way the pipeline needs to be run. Once we input that into the Cromwell server together with the MoChA WDL script, the output we expect is going to be a set of VCF phased files and an output table with the list of these files.\nThis is an example of what the input for a run is going to look like. Here, we have a sample table where each row is a sample. On the bottom left, we have a batch table where each row is a batch. For each sample, we have to define which batch it belongs to and which green and red IDAT file it corresponds to. And for each batch, we have to define the three different manifest files from Illumina, which are the BPM, CSV, and EGT files. These inform the software about which variants have been tested and about what the cluster’s original type looks like. The configuration file, which is in JSON format, is going to define just a minimum set of parameters. Like the name of the cohort, the mode, which in this case is the IDAT mode because we are inputting IDAT files. In this case, because the manifest files are in GRCh37 coordinates, we will ask the pipeline to realign the manifest files. This comes in very handy when Illumina does not provide a manifest file for GRCh38. And then we can provide information about how much parallelization we want to achieve in phasing. So, we can define the window size, the maximum window size for phasing, which in this case would be 50 centimorgans. But we can decrease that if we want to parallelize the computation even more. And then, after that, we need to provide a file with URL addresses for where to find all the files for the dimension of reference for the Illumina manifest files, for the IDATs, and also for the two tables. And that’s it. Once we have these three files, we can just submit the MoChA workflow, along with the configuration file, into the Cromwell server with a command like that.\nAnd then, if everything goes well, we would obtain, as an output, a table with a list of VCF files. The VCF files that will be generated will be divided across batches. So, for each batch, we’re going to get a VCF file with the phased genotypes and probe intensity. And then, also, a VCF file with just phased genotypes useful for imputation. We’ll also get some summary plots that will visualize the call rate across samples and maybe some statistics that will indicate possible contamination. Also, other summary plots indicating probe intensities for the sex chromosome and others that I’m not sure. The cost that I’ve observed, if you run it in Google Cloud, is about one dollar for five thousand samples for GSA-like arrays.\nSimilarly, when running the imputation pipeline, it’s not that different, especially after you’ve run the MoChA WDL pipeline. You’re going to have to input a set of phased VCF files, one per batch. You’re going to input a table listing the batches, and again, a configuration file listing some important options. Again, this will be input into the Cromwell server, together with the impute WDL script, and we’re going to expect a bunch of imputed VCF files out if everything runs correctly.\nThis is an example of what an input would look like. And again, a batch table file in WDL format, where each row contains information for one batch. And then, a configuration file here on the right, where again, we can define parameters like the name of the cohort, which chromosomes we want to impute. If we don’t want to impute all chromosomes, if you omit this, all 23 chromosomes will be included. And the information about, for example, which kind of information we want in the output of the imputed VCF files. Do we want the dosages? And then we can ask for dosages. Or do we want genotype probabilities? Or maybe we don’t want either and we just want the genotypes. By default, the imputation pipeline will use the 1000 Genomes high coverage reference panel. But you can also input a reference panel of your choice if you want. The only requirement is that you have access to the reference panel, and that it’s structured in VCF format, split across the 23 different chromosomes. Again, a simple command like this will submit the job to a running Cromwell server, where we indicate that we’re going to run the impute WDL script together with the JSON file with the configuration.\nIf everything goes well, we’re going to get as an output, again, a table with a list of VCF files. We’re going to obtain one VCF file for each batch and for each chromosome that we have asked to impute. The pipeline, as I said, it uses IMPUTE5 to run. Optionally, you can use Beagle5. You should need to run it in a commercial setting if you don’t have a commercial license for IMPUTE5. But IMPUTE5 is free for academic research. The cost, if you use the 1000 Genomes Project reference panel, is about one dollar for a thousand samples when you run it in Google Cloud.\nApplications\nNow, I’m going to show you very quickly some applications of the MoChA pipeline. First of all, this is a list of biobanks across the world where we were able to run the scripts. Thanks to collaborators that have access to the data on some of these biobanks. I personally have access to the UK Biobank, the Massachusetts General Brigham Biobank data. You might notice that due to different restrictions with different biobanks, we actually have to run the pipeline on a very different arrays of computational environments, from LSF and SLURM clusters to Google Cloud and European clusters. But this is a testament to the fact that the pipeline structure can run on almost any biobank.\nMoving on, there will be more pipelines in development. There is actually a polygenic score pipeline that can be run right on the output of the imputation pipeline if you’re interested in computing polygenic scores. There is also a work-in-progress Association pipeline that will run REGENIE in a very distributed fashion and will take again as an input the output of the MoChA imputation pipeline.\nPart of the reason why I’ve been developing this set of pipelines is that about five years ago, I started to work with Po-Ru (Loh) on a project to detect chromosomal alterations in the UK Biobank. To our surprise, we found that there was a lot of interesting biology to be discovered by doing so, and I published a paper. After that, I started to realize that there was actually quite some interest in running this kind of analysis with mosaic chromosomal alterations in biobanks, but the technical skills to actually do that were really prohibitive for many users. So I decided to try to package the analysis that people were doing into an easy-to-reproduce framework, and that led to the development of the MoChA pipeline.\nHowever, although it is designed to scale to biobank-sized datasets, it can be applied also to smaller cohorts. This is an example of work from Max Sherman that applied the framework on autism samples. He was able to show that mosaic chromosomal alterations are more common in autism probands compared to their unaffected siblings, even if this explains only a very small fraction of autism probands.\nAnd to conclude, the MoChA-WDL pipeline supports versatile computational environments. It follows a minimalistic design and can input IDAT files. It can be used on GRCh38 even with old cohorts , and it can scale to biobanks of any size. If you’re interested in CNV analysis, it actually provides germline and somatic outputs for further study.\nWith that, I would like to acknowledge the many people who have helped in developing this framework. First, I want to thank my supervisor, Steven McCarroll, who has supported me for many years. For Po-Ru, who has run many of the initial analyses in the UK Biobank, showing that this framework is actually important. And then, the many, many, many collaborators and users of the pipeline who have provided incredibly important feedback. With that, I’ll conclude, and understand that there will be a Q&A session after this. I’ll be available for questions if you have any. Thank you for listening, and I hope this pipeline might be useful to you.\n\n\n\nSAIGE: Family-based GWAS\nTitle: Genetic association studies in large-scale biobanks and cohorts: Scalable and Accurate Implementation of GEneralized mixed model (SAIGE)\nPresenter(s): Wei Zhou\nWei Zhou:\nHi, I’m Wei Zhou. In this presentation, I’m going to give a quick tutorial on a program called SAIGE, which is for scalable and accurate implementation of generalized mixed models. It was developed for conducting genetic association studies in large-scale biobanks and cohorts.\nWe will firstly go through several challenges of GWAS in large-scale cohorts and biobanks, mostly for binary phenotypes, followed by our solutions. We have implemented SAIGE for single-variant association tests because the single-variant association tests are usually underpowered for rare variants. More recently, we have implemented the region or gene-based association tests called SAIGE-Gene for rare variants in the same SAIGE R package. Lastly in this talk, we’re going to use some examples to demonstrate how to run the SAIGE R package.\nHere, three main challenges of GWAS in large-scale biobanks and cohorts are listed, including sample relatedness, large-scale data sizes, and unbalanced case-control ratio of binary phenotypes.\nFirst, let’s take a look at the sample relatedness. Sample relatedness is an important confounding factor in genetic association tests that needs to be accounted for. For example, in the UK Biobank data, almost one-third of individuals have a third-degree or closer relative in the cohort. However, linear and logistic regression models assume individuals are unrelated.\nTherefore, instead, we use mixed models for GWAS with related samples. Mixed models use the random effect, denoted as “b” here, to account for sample relatedness. “b” follows the multivariate normal distribution, whose variance contains the genetic relationship matrix (GRM). Each of the diagonal elements in GRM corresponds to the relatedness coefficient of a sample pair, which can be estimated using the genetic data.\nIn linear and logistic mixed models, “b,” which is the random effect, is included to account for the sample relatedness through the GRM.\nIn 2015, Loh et al. developed a linear mixed model method called BOLT-LMM. BOLT-LMM uses several asymptotic approaches to achieve the scalability of the mixed models, and it is the first linear mixed model method feasible for GWAS in biobank-scale data.\nSo, can we use the linear mixed model for binary phenotypes? In this figure, the variance against the mean of residuals is plotted for the linear model and logistic model. The linear model assumes constant variance, as the orange line shows, while the true mean-variance relationship for binary traits is represented by the green line. Therefore, for binary phenotypes, instead of using a linear mixed model, we would like to use the logistic mixed model.\nIn 2016, Chen et al. developed an R package called GMMAT in which the logistic mixed model has been implemented for GWAS on binary phenotypes while accounting for sample relatedness. So, we use logistic mixed models to account for sample relatedness in GWAS binary phenotypes.\nWhen we apply the logistic mixed model as implemented in GMMAT to one example phenotype, colorectal cancer in the UK Biobank, we found that it would take more than 600 gigabytes of memory and 184 CPU years to finish one GWAS in the UK Biobank.\nThis brings out our next challenge: large-scale data. To be able to run logistic mixed models in biobank-scale data for binary phenotypes, we need to optimize implementation.\nTo make logistic mixed models computationally practical for large-scale datasets. We use similar approaches that have been previously used in BOLT-LMM, which is the first linear mixed model method that is scalable for large-scale biobanks. Several approaches have been used to reduce the computation memory and time. We have successfully reduced the computation memory from more than 600 gigabytes to less than 10 gigabytes, and the computation time becomes nearly linear as the sample size increases.\nThe program optimization allows us to apply the logistic mixed model for binary phenotypes in the UK Biobank. Here again, we have the same example for colorectal cancer. After we apply the logistic mixed model to this phenotype, we still see many spurious signals, as we can tell from the Manhattan plot.\nSo, what is missing here?\nWe later find out the reason is the unbalanced case-control ratio of binary phenotypes. Unbalanced case-control ratio is widely observed in biobanks. This plot is showing a distribution of case-control ratio of 1,600 binary phenotypes in the UK Biobank, and around 85 percent of them have case-control ratio lower than 1 to 100, which means there are 100 times more cases than controls.\nPrevious studies have shown that unbalanced case-control ratios can cause inflated type 1 errors of the score test, which is represented by the blue line in the plot. The y-axis is for type 1 error rates, and the x-axis is for the minor allele frequency of the testing genetic markers. The score test is widely used in GWAS because of its relatively low computational burden. As we can see from the left to the right, as the study becomes more unbalanced, the inflation becomes more severe. Also, as the minor allele frequency becomes lower, the inflation becomes more severe.\nThe reason is that when the case-control ratio is unbalanced, the score test statistic does not follow the normal distribution. In the plot, the dotted line is for the normal distribution, and the black solid line is for the score test statistic. So, when we try to get a p-value based on the normal approximation, we’ll see inflation.\nTo solve this problem, Dey et al. in 2017 proposed to use the saddle point approximation to approximate the empirical distribution of the score test statistic, instead of using the normal approximation. And they have implemented this in the SPAtest R package. So, for our third challenge, we’re going to use the SPAtest to account for the unbalanced case-control ratio.\nIn summary, SAIGE can work for GWAS with related samples based on the mixed models, and it can work for large-scale data with those optimization strategies. It can account for unbalanced case-control ratio of binary phenotypes through the saddle point approximation.\nAgain, back to our example phenotype, colorectal cancer, and we see SAIGE successfully corrects the inflated type 1 error.\nWe have applied SAIGE to three other phenotypes in the UK Biobank on White British samples: coronary artery disease, which is relatively common with the case-control ratio 1 to 12, and the less prevalent disease thyroid cancer with a case-control ratio of 1 to 1000. As we can tell from the third column of the Manhattan plots, SAIGE has well corrected type 1 error rates.\nSAIGE contains two steps. In the first step, the null logistic mixed model was fit with phenotype covariates and genotypes that were used to construct the GRM on the fly as input. In step 2, score tests were conducted for each genetic marker with the Saddle Point approximation applied to account for the unbalanced case-control ratio of the binary phenotypes.\nSAIGE has been implemented as an open-source R package and has been used to run GWAS for UK Biobank data and for other biobanks and large consortia. PheWeb interface has been created for some of the projects for browsing the phenome-wide GWAS results.\nAs more and more sequencing data are being generated, more rare variants are being detected. However, single-variant association tests by GWAS are usually underpowered for rare variants, and set-based tests can be used to increase the power in which rare variants are grouped and tested together based on some functional units such as genes and regulatory regions.\nWe then extended SAIGE for region or gene-based tests for rare variants. We implemented different set-based association tasks including Burden, SKAT, and SKAT-O. And SAIGE-GENE also allows for incorporation of multiple minor allele frequency cutoffs and functional annotations.\nHere are some useful papers that you can read if you want to know more details about different methods: The first three are about SAIGE, SAIGE-GENE, and SAIGE-GENE+, which is the improved version of the SAIGE-GENE that we have recently implemented. All three methods have been implemented in the SAIGE R package. And the fourth one is the GMMAT, which is an R package implemented for the logistic mixed model. The BOLT-LMM is the first linear mixed model method that is scalable for biobank-scale data. “REGENIE” is a new method based on some machine learning approaches. It is a very computationally efficient method for biobank-scale GWAS.\nI would like to thank Dr. Zhangchen Zhao, who co-developed SAIGE-GENE with me, and my PhD advisors Dr. Shawn Lee and Dr. Cristy Willer. My postdoc advisors are Dr. Mark Daly and Dr. Benjamin Neale, and now my colleagues from the University of Michigan. Also, fantastic collaborators from the HUNT study in Norway.\nNext, we have prepared some SAIGE and SAIGE-GENE examples as Wiki pages on GitHub, and we will go through some of them together.\nIf you click on the first link, it will bring you to this SAIGE Hands-On practice.\nTo get ready, you can install the SAIGE R package following the instructions that you may find linked here on the SAIGE GitHub, and all the example data can be found in the EXT data.\nSo, SAIGE has two steps. The first one is to fit a logistic or linear mixed model, and the second one is to perform association tests for each genetic marker that you want to test, while applying the SPA (Saddle Point Approximation) to score tests to account for unbalanced case-control ratios.\nHere’s the First step: For step 1, you can use the trait type option to specify whether it is a binary or quantitative trait. For binary traits, SAIGE will automatically fit a null logistic mixed model, and for quantitative traits, it’ll fit a null linear mixed model. The first step takes two input files. The first input file is in the PLINK format, containing genotypes of genetic markers that we want to use to estimate the genetic relationship matrix, which which contains the relatedness coefficient of sample pairs. The second file is the called phenotype file, which contains non-genetic covariates, if there are any, and phenotype and sample IDs. Here’s a snapshot of the phenotype file, and you can see there are multiple columns corresponding to the phenotype you want to test, covariates included in the model, and sample IDs.\nHere’s an example command line to fit the null logistic mixed model for binary phenotypes. As we can see here, we have the PLINK file, and we have to specify the phenotype file and which column is for phenotype, and which columns are for covariates, and which column is for sample ID.\nSo, there are way more options available for step 1, and for more details, you can call this R script with “--help” to see all of them. Then, if you run the example step one command in the previous slide, you can expect to see the screen output ending with the following text: “If the job above has been run successfully”.\nStep 1 will generate three output files, as you can see: model file ending with the “.RDA”, and the variance file which contains a number for the variance ratio, and then association result file which is an intermediate file. So the first two will be used as input for step 2.\nIn Step 2, we perform single-variant association tests for each genetic marker. So, step 2 requires four input files. First of all, is the dosage or genotype file containing dosages or genotypes of markers we want to test. SAIGE supports four formats for dosages or genotypes: VCF, BCF, BGEN file, or the SAV file. We’ll use the BGEN file in the example today, but you can click on the links to see more details about those different file formats.\nThe second one is the sample file. So, this file contains one column for sample IDs corresponding to the sample order in the dosage file, and no header is included in the sample file. This file is only required for BGEN input, as for some BGEN files, there is no sample ID information included in a region. But for VCF, BCF, and SAV files, sample IDs are already included in those files, so this sample file is not required for those file formats.\nThe third and fourth input files are output from Step 1.\nHere’s the example code to run Step 2. As we can see, we specify the BGEN file, BGEN file index, and sample file. We also specify the chromosome number because only one chromosome can be tested in each job. We use a minimum allele frequency or a minimum allele count to specify some cutoffs for markers to be tested. The GMMAT model file and variance ratio file, both of them are outputs from Step 1. Then we use the “numLinesOutput” option, so we use 1,000 here to tell SAIGE to output the results of every 1,000 markers to the output file. We don’t want to use a very low number here because it will generate heavier overhead writing to the output file. The last option is the “IsOutputAFinCaseCtrl”. We specify this one to be true, and SAIGE will output the allele frequencies in cases and controls if we’re testing a binary trait to the output.\nHere’s some header information in the output file from Step 2, which contains the association results for the genetic markers we’re testing. Association results are all with regard to allele 2, as we see in the output file. I want to point out the column called “p.value.NA.” These p-values are obtained under the normal approximation. So, if we generate a QQ plot for “p.value.NA,” it’s very likely for us to see the inflation for binary phenotypes with unbalanced case-control ratios. The “p.value” column are the p-values that we want to use.\nNext, if you’re interested in conducting gene- or region-based tests for rare variants, you can click on the second link here, which will take you to the Wiki page we created for examples of SAIGE-GENE jobs.\nSo, here we can see this page contains similar formats as SAIGE that we have gone through for the hands-on practice. SAIGE-GENE also contain similar Step 1 and Step 2 as SAIGE does, and it contains the extra step called Step 0, in which we construct the sparse GRM based on the provided PLINK file. This step only needs to be done once for each dataset, and it does not change according to different phenotypes.\nSo, in Step 0, we’re creating a sparse GRM based on the genotypes stored in the PLINK file, as we use for Step 1 for the full GRM. The difference is that we create this sparse GRM and store it in a file, so it can be reused in Step One for different phenotypes.\nThe input file will be the PLINK file, same as the PLINK file for Step 1. The output file will be a file storing the sparse GRM and a file storing IDs of samples in the sparse GRM. If you run the example code and you expect the screen output to end with the following text if the job has been run successfully.\nThen we’re running Step 1 again. It is to fit a logistic or linear mixed model. So, Step 1 asks for four input files instead of two, as we see in SAIGE. The first two are the same as Step 1 for single-variant association tests in SAIGE: the PLINK file storing the genotypes and the phenotype file containing phenotype, covariates, and sample ID information. The last two, the third, and the fourth ones, are outputs by Step 0, which are files storing the sparse GRM and the file storing the sample IDs in the sparse GRM.\nHere’s the example code for the Step 1 job. As we can see, we can use the two options that are highlighted here to specify the sparse GRM and the sparse GRM sample IDs that we have obtained from Step 0.\nAgain, this “--help” flag can be used to see a more detailed parameter list. If the Step 0 job has been run successfully, the screen output will end with the following text. As we see, there are multiple values corresponding to multiple variance ratios, and there are actually four different minor allele count categories that we are using. So, for the specific cutoffs for different minor allele count categories, you can use “--help” to have a look.\nSo, this Step 1 will generate four output files, and the first three are the same as the Step 1 outputs by SAIGE and when we try to run single-variant association tests: the model file, the variance ratio file, and the intermediate file for the association tests of some randomly selected markers. The fourth file here is specific to SAIGE-GENE, which is a sparse Sigma file. This file will be used as input for Step 2 along with the first and the second output files, which are the model file and the variance ratio file.\nIn Step 2, we’re performing the set-based association tasks. It asks for five input files, and the first three are similar to SAIGE. There is a dosage or genotype file in different formats, a model file, and a variance ratio file generated by Step 1. Note that if you’re using BGEN for dosages or genotypes, you need to use the sample file to specify the sample IDs in the BGEN file.\nNow let’s look at the fourth and fifth files. They are specific to SAIGE-GENE. The fourth file is the group file. We can take a closer look at this one.\nSo, each line in a group file is for one gene or one set of variants that you want to test together as a set. The first element is for the gene or set name, and the rest of the line is for variant IDs included in this gene set. For VCF or SAV input, the genetic marker IDs should be in the format “Chromosome_Position_Ref_Alternate_Allele,” and for BGEN files, the genetic marker IDs should match the IDs in a BGEN file. Each element in the line is separated by a tab. The fifth file is called a sparse Sigma file. It is also an output by Step 1.\nWith all those input files, we can run the Step 2 job to perform set-based association tests. I want to point out this option called “maxMAFforGroupTest.” This can be used to specify the maximum minor allele frequency of genetic markers that you want to include in a group test. By default, this value is one percent, but if you want to test rare variants only, you can lower this number. Again, use “--help” to see a more detailed parameter list and information. If your Step 2 job runs successfully and you see the screen output ending with the text as we show here.\nThen we can take a look at the output files by Step 2. Step 2 will provide a file with region or gene-based association test results for each gene or region we’re testing. It will provide p-values from the SKAT-O test and also p-values from Burden and SKAT tests. There are a couple of columns showing the number of markers following each minor allele count category, and the category information can be found in the Step 1 script. If we specify that we want to output the single-variant association test result for genetic markers in the genes or regions we’re testing, the second file will be generated to provide that information.\nLastly, if we want to use multiple minor allele frequency cutoffs and multiple functional annotations to test each gene, for example, we can use different group files and run Step 2 for that gene, multiple times. To combine those different p-values for the same gene, we can use the Gaussian distribution to combine them. Here’s the code that you can use to combine multiple p-values for gene or testing region.\n\n\n\nCC-GWAS\nTitle: Tutorial to use CC-GWAS to identify loci with different allele frequency among cases of different disorders.\nPresenter(s): Wouter Peyrot\nWouter Peyrot:\nHi, thank you very much for your interest in applying CC-GWAS to identify loci with different allele frequencies among cases of different disorders. My name is Wouter Peyrot. In the next 15 minutes or so, I’ll show you how to run the CC-GWAS software. We also conduct this analysis in the cross-disorder working group of the PGC for the next wave of analysis. Alkes Price and I developed the software. If you’re interested in any further reading, please see our paper in Nature Genetics 2021 or visit my GitHub page. If you have any questions or suggestions, please do not hesitate to contact me. I’m most welcome to explain further or to receive any suggestions you may have.\nSo, and first, here I’ll show a couple of slides to give a big picture overview of the method, and then I’ll get hands-on with some real data to show you how real analyses are done with our package software. CC-GWAS is intended to compare cases of two different disorders. And when you’d like to extend, for example, to compare 10 disorders, you just need to repeat the CC-GWAS analysis accordingly, to compare two disorders at a time.\nSo, CC-GWAS compares the cases of two disorders based on the respective case-control GWAS results, and the case-control GWAS results are, of course, widely publicly available most of the time. CC-GWAS works by taking a weighted difference of these case-control GWAS results. And CC-GWAS combines two components, which have distinct functions: The first component is what we refer to as the CC-GWASOLS component. This component optimizes the power and controls for type 1 error at null-null SNPs. Null-null SNPs we refer to SNPs that have no impact on either disorder, so there is no case-case difference. If you’re interested in any more details about the CC-GWASOLS component, I refer you to our paper.\nThen there is a second component, which we refer to as the CC-GWASEXACT component. The CC-GWASEXACT component controls the type 1 error at stress test SNPs. Stress test SNPs are a very specific set of SNPs that impact both disorders but have no case-case difference. So, the impact on both disorders is exactly the same, so there’s no case-case difference. This set of stress test SNPs is a very tricky set of SNPs because they can get type 1 error quickly. That’s why we have these additional set of weights to control for type 1 error at stress test SNPs. So CC-GWAS says that a SNP is significantly associated with case-case status when the p-value of the OLS component is smaller than 5x10-8, and when the p-value of the exact component is smaller than 10-4.\nCC-GWAS also has an additional filter to protect against false positives, and that is a very specific set of false positives that may result from differential tagging of a causal stress test SNP. Again, the stress test SNP is a SNP that impacts both disorders, but there’s no difference in allele frequency between the cases. Here, in the right column, you can see the causal stress test SNP. It has an impact on disorder A, it has an impact on disorder B, but there is no case-case difference, so there’s no case-case effect. But now, suppose you have a tagging SNP which tags in population A, in which you study disorder A, you study it with an r of 0.6, and for disorder B, it takes a SNP with an r of 0.3. Then, when you look at the difference of the effects of disorder A and disorder B, you do find the case-case difference. So, this is a very specific set of SNPs for which you can find false positives. To protect against this, CC-GWAS has an additional built-in step to filter SNPs that show evidence of differential tagging. Once again, when you’re interested in any more details of this step, I refer you to our paper and its supplements.\nNow we get to how to run CC-GWAS. So, I first suggest visiting the GitHub page, CC-GWAS GitHub. At this GitHub page, you can download all the results, and in particular, I’m now going to already download the files and in the test folder there are the two dummy input files for the case-control input GWAS results. Returning to the previous page, you can see an explanation of how to run CC-GWAS, how to get started, there’s a detailed description of all the input parameters where you can look up for reference, and there’s a description of the output files. Here, there is this example which I’ll go through with you.\nSo, let’s first get to running CC-GWAS. First, here I have a terminal opened, and I can show you I’m in a folder where I have already downloaded these two files, so the test case-control GWAS for Bipolar 10 SNPs, and the test case-control GWAS for Schizophrenia 10 SNPs. So, we’ll look at the example of comparing Bipolar to Schizophrenia.\nBack to the GitHub page. First, we open R. I’m working on my MacBook, but I know this is exactly the same as working on a Linux-based server, and it also works on Windows. Here, you first need to load a couple of R libraries and install the CC-GWAS package. I’ll just copy-paste it from here, paste it here, and wait for R to run it. Now CC-GWAS is loaded, and you can see that CC-GWAS is now a functioning R package.\nYeah, you can see that CC-GWAS is now loaded into R.\nOkay, and I’ve already loaded these two files, so now I go to this example here, at the bottom of the GitHub page, I’ve loaded this CC-GWAS example, and you can see if it all works. I’ll copy-paste it, and you can see that all is loaded.\nBut before looking at the results, I’ll first get you to see the input parameters. Probably best to do it on this GitHub page. So here, in CC-GWAS, first, you set the name of the output file, and then you set the name of the first disorder. You set the name of the second disorder, which is Bipolar Disorder. You say where the summary statistics file of the first disorder is and the summary statistics file of the second disorder. Case-control subsets results. And then note, the six columns of these results are very specific, so they need to be exactly as I’ve described it here.\nSo, let me show it’s at the GitHub page. Here it says…Yeah, so the columns should be the SNP name, the chromosome number, the base pair position, the effective allele, the non-effective allele, the allele frequency of the effective allele, the odds ratio for the risk of the disorder for the effect per effective allele, the standard error of the log odds of the alternate allele, the p-value for this SNP, and N effective (Neff). And when you don’t have N effective, you can impute it as I also described here on the GitHub page. So, 4 divided by 1 over n-case plus 1 over n-control. And I note that the results for the case-control disorder A and disorder B will be based merged on the SNP name. So before you run CC-GWAS, make sure that the SNP names between those two sets are well aligned.\nNow, I return to the example that we just ran. So that’s where we left off. I just showed you what the columns should look like of these two gzipped files. And then there are more input parameters. “K_A1A0” means it’s the population prevalence of disorder A. So, for schizophrenia, that’s approximately 0.4 percent. And because, of course, it’s very hard to know exactly the population prevalence of a disorder, you can also put boundaries to it. So for schizophrenia, we set it at one percent and the low at 0.4 percent. It’s important to add these ranges because it also, again, protects against type 1 error at stress test SNPs. And then, for disorder B, you set the exact same three values. The population prevalence of disorder B and the high estimate and the low estimate.\nThen, you specify results that typically come from LD Score regression, but of course, you can use different methods for it. That’s the liability scale heritability for disorder A, the liability scale heritability for disorder B. And how to get these estimates, I refer to the software package that provides these estimates. Then, you want to know the genetic correlation between disorder A and disorder B, and also the intercept that you get from bivariate LD Score regression. And this is very important to set this value because it helps you increase the power of CC-GWAS. And you also set an estimate of the approximated number of causal SNPs. And so, this “m” value, you need to specify it. And I also show here in the input files, I give some extra explanations about it and also some papers where you can see if this “m” is already estimated or how you can estimate it, or if you want to make an approximation, how to do it. Details on setting “m” are here on the GitHub page under the input parameters.\nSo, we were left here with specifying the number of SNPs. And then, at last, you need to give the number of cases for disorder A, the number of cases for disorder B, the number of controls for disorder A, and the number of controls for disorder B. And then you also need to specify the overlap of controls. Once again, when you set this overlap of controls, it really helps you increase the power of CC-GWAS. So it has kind of the same function as the intercept. These are two ways to increase the power of CC-GWAS and also to double-check that you don’t risk any type one error result. So when you know these values, it’s important to set them to increase the power of CC-GWAS.\nI hope this clarifies the input parameters of CC-GWAS. Now, I’ll return to the analysis that we just did by running this command line.\nYeah, so we just ran it, and you can see that the R function gives kind of an output file, which is also saved in the log file. So it says when the analysis was started and that you run CC-GWAS and not CC-GWAS+. And for CC-GWAS+, I refer to my GitHub page. And this is when you also have a direct case-case comparison available.\nThen, it shows some summary about the data that were read from the gz file and it does some very rough QC on the odds ratios. And then how it merges the data. So this kind of the just the log file that’s plotted, and you also get provided in an FST plot for this. And let me see if I can quickly, yeah, so this is what the FST plot looks like. It’s an output of CC-GWAS. Here you can see the FST. Again, for the details, I refer to our paper, but it gives intuition of the genetic distance between cases and controls. So here you can see the controls for bipolar disorder, the cases for bipolar disorder, the controls for schizophrenia, and the cases for schizophrenia. And I don’t know what just happened…And these numbers give the relative difference between them. So you can see that even though schizophrenia and bipolar disorder have a very large genetic correlation of 0.7, the genetic distance is still considerable with a relative value of 0.49 compared to 0.66 or 0.6 for the case-control difference.\nOkay, and then when we look at the output files, I always like this system where from R, you can also use the command line in the terminal. Yeah, you can see that by running CC-GWAS, these are the input summary statistics for these ten SNPs. This is the output file. This is the FST plot that I just showed. This is the log file, which is also printed here on the screen, so I’m not going to open it, but it’s good for later reference if you’re interested. And I note this log file also gives you the OLS and the exact weights that I just discussed. So here in the log file, you can see for this example, the OLS weights were 0.54 for schizophrenia, -0.3 for bipolar, and the exact weights are also listed in the log file and may be good to report in the description of your CC-GWAS analysis. And here there’s also the results that are displayed here.\nSo now let’s have a look at these results. I’ll just copy-paste this also from the GitHub page, and for now, I’ll remove the “all,” and I’ll explain to you later what “all” means. So I need to give it the correct name, “test.out,” and then let’s have a look at “d”. Once again, I only have 10 SNPs. Now normally, of course, these are maybe 10 million SNPs or something, and here you can see the columns. So there’s the SNP name, the chromosome number, the base pair position, the effective allele, the non-effective allele, and then you have three columns: OLS beta, OLS standard error, and OLS p-value, and these represent the effects from the OLS component of CC-GWAS. And then there are three more columns: exact beta, exact standard error, and exact p-value, and these three columns give the CC-GWAS results of the exact component.\nAnd now, as we said, and here there’s also a column which labels CC-GWAS significant. So when CC-GWAS is significant, this column is labeled as “1”, right? So, and you can see for this specific SNP in the ninth row, so when we look here, you can see that OLS p-value is 9x10-12, so it’s smaller than 5x10-8. And you can see that the exact p-value is 4.7x10-11, so it’s smaller than 10-4. So this SNP is CC-GWAS significant.\nAnd so if you’re interested, I think it’s good to note that you can also have a more detailed output file. To do this, you need to add to this R script. Yeah, you need to add here an additional input parameter, which was named, if I remember correctly, yeah. So then you add, “save.all=TRUE”, here, “save.all=TRUE”. And when we look again at the files that are now in the folder, here you can see that an additional output file popped up. And when we have a look at this file, so I open this file, changing the file name a little bit with the “ALL” here. So now this file takes much more space. Of course, for 10 SNPs, it’s not really a problem, but when you have many SNPs, it can be tedious.\nSo, and there are many, many columns here. The first columns of “d” represent again the columns that I just discussed. So the first columns are the SNP, chromosome, base pair position, effective allele or non-effective allele, and then you have the case-control effects. And these are scaled to the standardized observed scale based on a 50-50 case-control ascertainment. Again, when you’re interested in the details, please see our paper and its supplements. And then you have the case-control results for this disorder B. Then you have the OLS results as we just discussed, and you have the exact results. But then there are additional columns. So there is the potential tagging for the stress test SNP, potential differential tagging of the stress test SNP. It’s not necessary to understand all the details. If you’re interested, please have a look at our paper or its supplements, but just know that behind the scenes, we test for it and it’s being excluded from the previous output file that I showed.\nAnd then here we also have this exact approach, which again, the exact component of CC-GWAS protects against type 1 error of stress test SNPs. And this is also based on the fact that you don’t know exactly the population prevalence of both these disorders. So here you can see the exact results when you take the low-low values of both population prevalence disorders, low-high, high-low, and the high-high population prevalence bound. And here’s the CC-GWAS significant folder. So as you can see, this “d”, this output file, “ALL”, if you’re interested to look into more details of your results, you can have a look at it. And if you have any questions, please don’t hesitate to contact me and ask me for the details. But in general, I advise not to look at these results because also in the trimmed results, which are the default output, and now I’m going to load it again, we already removed the SNPs that may have a problem with differential tagging or some other issues, right? So if you use this file, you’re safe in that aspect.\nAnd then finally, and I see I extended the 15 minutes, but I hope you don’t mind. So now we showed how to run CC-GWAS in practice, and for, yeah, for follow-up analysis, as I just said, use CC-GWAS results based on the “save.all=FALSE” results. And I know this is a default, so when you don’t set “save_all,” this is what’s being done, and it’s a trimmed set of results. And then we advise to use the CC-GWASOLS component. So these are the columns that are labeled “ols_beta,” “ols_standard_error,” and “ols_p_val” for clumping and, for example, polygenic risk analysis. So, to look at which loci have the different allele frequency. And when you’re interested in genetic correlation analysis or any other sort of LD Score regression-based analysis or methods that are alike, we advise to use the CC-GWASEXACT component. So the “exact_beta,” “exact_standard_error,” and the “exact_p_value.” So that brings me to the end of this tutorial. Once again, please don’t hesitate to contact me if you have any questions, and I hope this tutorial was helpful."
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "Chapter 7: Ancestry-Specific Analyses and Considerations",
    "section": "",
    "text": "Chapter goals:\n\nUnderstand that ancestry is an important consideration for performing genetic analyses\nUnderstand how population stratification can affect GWAS results\nUnderstand considerations for PRS across different ancestries and how to calculate cross-ancestry PRS\nUnderstand cross-ancestry methods for GWAS and post-GWAS analyses\n\n\n\nChapter 7.1: Cross-ancestry analysis\nTitle: Cross-ancestry PTSD and Polygenic Findings, and the Cross-Population SIG of the PGC\nPresenter(s): Laramie Duncan\nLevel: Beginner friendly\nLength: Video starts at 00:00 and goes until 13:39 (13:39 total)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n[Note: This paper is now available as Genome-wide Association Studies in Ancestrally Diverse Populations: Opportunities, Methods, Pitfalls, and Recommendations: https://doi.org/10.1016/j.cell.2019.08.051]\n\n\n\nChapter 7.2: Ancestry-specific PRS\nTitle: Clinical use of current polygenic risk scores may exacerbate health disparities\nPresenter(s): Alicia Martin\nLevel: Beginner friendly\nLength: Video starts at 32:39 and goes to 47:06 (15:45 total)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nChapter 7.3: Local Ancestry and Admixed Populations\nTitle: Tractor: A framework enabling the well-calibrated genomic analysis of psychiatric traits across admixed populations\nPresenter(s): Elizabeth Atkinson\nLevel: Beginner/Intermediate\nLength: Video starts at 14:50 and ends at 32:38 (total 19:28)\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to Tractor GitHub and Tutorial."
  },
  {
    "objectID": "chapter1.4_transcript.html#alzheimers-disease",
    "href": "chapter1.4_transcript.html#alzheimers-disease",
    "title": "Chapter 1.4 Psychiatric genomics: State-of-the-science (Video Transcript)",
    "section": "Alzheimer’s Disease",
    "text": "Alzheimer’s Disease\nLet’s move on. Let’s talk about the genetics of Alzheimer’s disease. Alzheimer’s disease, when you look at it as a whole, um, about 75 percent is going to be late-onset, non-familial. So symptoms begin after the age of 60-65, depending on your definition, and these individuals do not have a strong family history of the disease. About 20 percent will be late-onset familial, onset after 60-65, and there is a strong history of the disease in the family. Depending on what study you look at or your definitions, about five to ten percent of Alzheimer’s diseases are early-onset, and about one percent is genetic onset. And by genetic, I mean full one of those causative genes for Alzheimer’s disease.\nThose genes are PSEN1, PSEN2, and APP. They are all involved in the production of amyloid, and amyloid is one of the hallmarks of Alzheimer’s disease. In this cartoon here, the vertical structure you can see here is the cell membrane. Cytosol is the inside of the cell, and here’s the outside of the cell. The amyloid precursor protein, which is the protein coded for by APP, falls across the cell membrane, and it is cleaved or cut by two different enzymes: beta-secretase and gamma-secretase. PSEN1 and PSEN2 interact with gamma-secretase, and if we have a mutation in one of these genes, that can affect where gamma-secretase cleaves this amyloid precursor protein. And that results in different forms of amyloid, and some forms of amyloid are more likely to go and form these amyloid plaques, which are the hallmark of Alzheimer’s disease.\nUm, they tend to have different ages of onset, uh, PSEN1 being the earliest. Most sources will say 30s at the earliest. I have to say I knew somebody who had a PSEN1 pathogenic variant and had onset in his 20s, so we can see that, thankfully, very rare. And you can see onset into the 70s with PSEN2 or sometimes, um, I know one case of somebody who had a pathogenic variant in PSEN2 and did not develop the clinical signs of Alzheimer’s disease. That is very rare, unfortunately.\nThis is a typical family tree that we would see in one of these families. Let’s say this is our patient, a man 50 years old, onset of cognitive decline at 48. His father had onset at 42, um, his paternal aunt had onset at 51, and his grandmother on his father’s side had onset at 45. So, when we talk about these purely genetic early-onset Alzheimer’s families, this is the sort of thing that we see.\nNow, what about the other 99% of Alzheimer’s disease that is not purely genetic? Here we enter the realm of risk factor genes, and the best studied and most common of these is APOE. It’s the major lipid and cholesterol carrier in the central nervous system. There are three polymorphisms, or forms, flavors if you will, of APOE. E2 is associated with a lower risk of Alzheimer’s disease, E3 we consider average risk of Alzheimer’s disease, and that’s the most common form, and E4 is associated with a higher risk of Alzheimer’s disease. E4 has been linked to all different aspects of Alzheimer’s pathology, so it has been linked to neuroinflammation, synaptic dysfunction, mitochondrial or metabolism dysfunction, Tau aggregation (which forms those tangles, also a hallmark of Alzheimer’s), and amyloid aggregation and reduced clearance. So, this is a fairly well-studied gene and has been linked to a number of different things.\nHow does APOE affect our risk of dementia? Well, the specific risk for Alzheimer’s related to APOE varies between studies, and you can see different numbers quoted in different sources, but the numbers I’m showing here are compiled by one of the most careful epidemiologists I know, uh, Deborah Blacker at Harvard. And it shows the lifetime risk, or age through age 85, for people who might carry two copies of the E4 APOE genotype have a lifetime risk of between 30 and 55 percent. So it’s definitely higher than the general population, but not everybody who carries any four-four will develop Alzheimer’s disease. If you have just one copy of E4, you also have an increased risk, about 20 to 25 percent. And three-three, which we considered normal, is about 10 to 15 percent.\nNow, there are other genes related to Alzheimer’s disease, and I apologize, this is a very busy and complex graph, but I think it captures the complexity of what we know so far about the genetics of Alzheimer’s really well. So here, on the vertical y-axis, we start at the bottom with genes that are associated with a relatively low risk of Alzheimer’s disease and moving up to a near-certain chance of developing Alzheimer’s. And then here, population frequency, we go from very, very rare at zero percent or close to zero percent to relatively more common. And up here, we can see our old friends APP, PSEN1, PSEN2, being very rare in the population but having a near-certain chance of developing Alzheimer’s disease. APOE4 is relatively rare and has a higher chance of getting Alzheimer’s. And then we move into some of the more common genes that have a smaller effect on our chance of getting Alzheimer’s disease. There are many, many more genes than this, actually, and here the colors relate to some of the cell functions that are related to these genes, whether it be amyloid metabolism, APP metabolism, cholesterol, immune response, endocytosis, cytoskeleton. There are researchers working to develop what they call polygenic risk scores, where we can combine all the data from testing these multiple and interacting genes, and give somebody more of a accurate, hopefully closer to accurate, assessments of their chance of getting Alzheimer’s. So not there yet, but we may get there. Um, and before I move on to the genetics of frontotemporal dementia, Barb, any questions at this point?\nBarbara: No questions have come across. Do any of you have questions at this point?\nMalia: And it’s fine if you don’t. We can go ahead.\nBarbara: Everyone, just put your questions as they occur to you for this lecture in the chat box, and I’ll track them to the next time um we pause for questions.\nMalia: And I really don’t mean to put you on the spot."
  },
  {
    "objectID": "chapter1.4_transcript.html#genetics-of-frontal-temporal-dementia",
    "href": "chapter1.4_transcript.html#genetics-of-frontal-temporal-dementia",
    "title": "Chapter 1.4 Psychiatric genomics: State-of-the-science (Video Transcript)",
    "section": "Genetics of Frontal Temporal Dementia",
    "text": "Genetics of Frontal Temporal Dementia\nNow, this disorder is less likely to be sporadic than Alzheimer’s disease. We saw about 75 percent of people with Alzheimer’s didn’t have a strong family history of the disease. Frontotemporal dementia, we’re looking at about 40 percent. Also, about 40 percent have a positive family history of dementia, so that includes also psychiatric disease and motor symptoms. And it’s thought, perhaps in this 40 percent, we’re looking at a similar combination of risk factor genes as we saw in Alzheimer’s disease. These risk factors, risk factor genes, for frontal temporal dementia are not as well understood as Alzheimer’s disease, and then 20 percent of people who have frontal temporal dementia have a pathogenic variant in a single gene.\nAnd those genes are C9orf72, granulin or GRN, and MAPT. Now, each of these are more common in the behavioral variant of FTD versus the semantic variant of FTD. They each have their own flavor. C9orf72 is the most common cause of genetic frontotemporal dementia. It’s also the most common cause of ALS or Lou Gehrig’s Disease, and we do see sometimes in families that have a pathogenic mutation or variance in C9orf72. You can see people who have both frontotemporal dementia and ALS. You can see people in the same family who have one or the other. Um, C9orf72 can also look like Alzheimer’s disease, and so we do see that as well, where someone will have a family history of Alzheimer’s disease, we do the testing and find out it was likely also C9orf72. Granulin and MAPT can cause parkinsonian features, um, also Progressive Supranuclear Palsy or corticobasal degeneration, so we also see that in these families. There are also rare genes for frontotemporal dementia listed here. I will say a lot of these are extremely rare, but they are also fully penetrant for frontal temporal dementia.\nAnd you might be wondering, um, with all of these genes, how do you pick? How do you know which gene to test? And the, uh, quick answer is we don’t. Um, with current technology, it’s often the same price to test multiple genes. And so if you know it’s frontotemporal dementia versus Alzheimer’s disease, maybe you will just test for the frontotemporal genes. But, um, often if a patient hasn’t had an amyloid scan or any sort of positive verification that it is one disorder versus another, the clinician might just order a dementia panel and include all of the causative genes for dementia."
  },
  {
    "objectID": "chapter1.4_transcript.html#lewy-body-dementiaparkinsons-disease",
    "href": "chapter1.4_transcript.html#lewy-body-dementiaparkinsons-disease",
    "title": "Chapter 1.4 Psychiatric genomics: State-of-the-science (Video Transcript)",
    "section": "Lewy Body Dementia/Parkinson’s Disease",
    "text": "Lewy Body Dementia/Parkinson’s Disease\nSo, I will move on to the genetics of Lewy Body dementia. Say, this one, under this one, I’m including both Parkinson’s disease dementia, which, as you might know, is the onset of dementia more than one year after the start of motor symptoms or parkinsonian symptoms, and also dementia with Lewy bodies, which is the onset of dementia before or within one year of motor symptoms.\nThe genetics of Lewy Body dementia are not well understood, I have to say, but it seems to be midway between Alzheimer’s and Parkinson’s disease. So, genes recently identified for Lewy Body dementia include, here, um, some of the, uh, Parkinson’s um genes that you see in black, especially GBA, which is one of our more common risk factors for Parkinson’s Disease, has also a higher chance of dementia associated with it. And so sometimes you can see somebody with Lewy Body dementia who has a pathogenic variant in GBA.\nSNCA, which codes for synuclein, which is the protein that is characteristic for Parkinson’s Disease, that’s the protein that builds up in Lewy bodies. Pathogenic mutations in SNCA are more rare, but they are more likely to also involve cognitive issues and dementia. We also see some of the dementia genes in Lewy Body dementia, APOE we’ve already talked about, granulin and MAPT are actually both tied to frontal temporal dementia, but they also come up in Lewy Body dementia.\nIt’s a complex system, and I think the best quote I saw in the papers that I’ve been reading about Lewy Body dementia is that the genetic basis of it is not well understood. So I’m not able to give you a nice schematic on how many people have familial versus non-familial and how likely it is to be inherited in the family. I don’t think we’re quite there yet with Lewy Body dementia.\nUm, I will say that if you know a patient who has Parkinson’s Dementia or a family history or has Parkinson’s disease, I do happen to know a study that’s doing um genetic testing for Parkinson’s Disease. So that’s PD Gene from the Parkinson’s Foundation. You can just Google PD Gene Parkinson’s Foundation. It’s no-cost genetic testing for Parkinson’s, which includes Parkinson’s dementia, in case that’s helpful.\nUm, and Barbara, any other questions have come in before we move on?\nBarbara: Yeah, there was a question about whether or not insurance pays for any of these tests you just mentioned. The study that covers the cost, what about insurance covering the cost of any of these?\nMalia: It varies. Um, for early-onset dementia, where doing genetic testing can be diagnostic for a person’s symptoms, I’ve seen insurance cover the testing. For APOE, because we have these new anti-amyloid therapies, and in a person’s (and here I’m talking about people who have been diagnosed with dementia and are considering tests or treatment for one of the anti-amyloid therapies), having an APOE4 allele or variant increases the chance of side effects from those medications. So, I would expect insurance to cover those costs. In other cases, they don’t, and it is highly variable, so, yeah, I’ve seen it go both ways. Any other questions, Barbara? There was a question about typing out.\nBarbara: There was a question about typing out the PDG contact info, and I’m trying to find it right now. Um, I’m seeing it under Michael J. Fox organization, would that be right? Oh, somebody else got it before I did, never mind.\nMalia: Yep, sorry to put you on the spot with that. Um, it’s a full disclosure, I work on that study, and we have had a fantastic response to it, which means there might be some waitlists now. Um, but it is, you know, no-cost genetic testing for the seven most common genes for Parkinson’s, so it is a great service that the Parkinson’s Foundation is doing. And people can do it remotely; they can sign up through the website and be sent a kit, or they can sign up through a site at a local University Medical Center.\nBarbara: Yeah, the link, that’s great, thank you, Lisa.\nMalia: Yeah, it’s a great program, thank you. Anything else, Barb? We’ll say that’s a no,\nBarbara: not yet, sorry, no, not at all.\nMalia: Let’s move on to common questions. Um, so one of the most common questions that I get, maybe you get, is “Alzheimer’s genetic?” and this is what I say: there are rare cases of Alzheimer’s disease that are purely genetic. People in those families get Alzheimer’s early, like in their 30s or 50s, and there are several people with early-onset Alzheimer’s disease in the same family. So yes, it does happen, it is rare. Most cases of Alzheimer’s disease are caused by a combination of genetic and environmental factors, and I should say, speaking as someone who works in genetics, I consider everything that’s not genetic, environmental. So here we’re talking about, right, hypertension, hyperlipidemia, diabetes, sedentary lifestyle, what have you.\nOne way to think about this is that everyone has an Alzheimer’s jar, and this same analogy could be used for just about any common disease, cardiovascular disease, what have you. Everyone has an Alzheimer’s jar, and this jar can be filled with environmental risk factors that I just described; it can also be filled with genetic risk factors, and some of these, you know, might be APOE, might be TREM2, might be BIN1, any of those genes that we already looked at.\nWe start with a certain number of genetic risk factors, and over time we accumulate environmental risk factors, and if our jar fills to the top, we develop Alzheimer’s disease. Different people have different combinations of genetic and environmental risk factors, so some people are going to start with a relatively large number of genetic risk factors, and they will accumulate environmental risk factors just like the rest of us, but they don’t need to accumulate quite so many before they develop Alzheimer’s disease. Other people start with a relatively low number of genetic risk factors for Alzheimer’s, but if they accumulate enough environmental risk factors, they will still develop Alzheimer’s disease.\nThere are protective factors that can make the jar taller, and those can be social activities, mental activities, physical activity, healthy diets, good sleep habits, all of the things we know that decrease our chance of getting Alzheimer’s Disease. By doing these, we can sort of add rings to the top of the jar, make the jar taller, and give us more time before we develop Alzheimer’s disease. And I find, when talking to people, it’s helpful to steer the conversation and end on things that they can do, steps that they can take to lower their chance of getting Alzheimer’s disease.\nHere’s another question that I get: “My parent had Alzheimer’s, does that mean I’ll get it too?” And I think it’s helpful to put that in context. We all have a chance of developing Alzheimer’s; it’s unfortunately not a rare disease. But it depends on what study you look at. People who have a parent with Alzheimer’s have about a 20% chance of getting it themselves. That’s compared to about 10% in the general population. Another way to look at it: 20% chance of getting Alzheimer’s is an 80% chance of not getting Alzheimer’s. There’s not a surefire way to prevent Alzheimer’s disease, but you can improve these odds through lifestyle changes.\nMaybe I will pause there. Thank you, Barb, for monitoring the chat because I am awful at reading and talking at the same time. But it looks like maybe a couple of questions have come in.\nBarbara: Yeah, well, Joanne just mentioned that these visuals are really great for helping to translate some of this information to people. Lorraine is wondering if by environmental factors, are you referring to lifestyles or perhaps pollution and work-related exposures, or maybe all of the above?\nMalia: All of the above. I have a complete bias in this area, working in genetics. In my world view, there’s genetics and there’s everything else. So we should probably find another name for it, but yes, it could be medical conditions like diabetes, cardiovascular disease, could be air pollution, which has been linked to a slightly higher chance of getting Alzheimer’s. What have you - Anything that’s not genetic, I would call environmental. Anything else?\nBarbara: That’s it for now.\nMalia: Cool. Here’s a great one. Um, should I get tested for APOE? So there are clinical guidelines recommending against APOE testing for people without memory complaints due to its low predictive value. Many people who carry the bad copy of APOE, which is E4, do not develop Alzheimer’s disease, and about half of those with Alzheimer’s disease don’t have an E4 allele. So the catchphrase is, “APOE is neither necessary nor sufficient to develop Alzheimer’s disease.” You can get Alzheimer’s without having an APOE4, the bad copy, and if you have an A4, that doesn’t mean you’re going to get Alzheimer’s disease. A bunch of us got together and recommended against testing for APOE in people who do not have memory complaints.\nThat said, in the real world, things happen differently, and you might have seen this in the news last November. The actor Chris Hemsworth learned he carries two copies of APOE4 as a part of a National Geographic docu-series, and it’s not clear that he was fully informed before he was tested or that the results were adequately explained. Probably goes without saying, but don’t get tested as part of a television show or without carefully considering what the tests can and cannot tell you, and whether you really want that information.\nFor people who are thinking about getting tested and do not have memory concerns, there is a really helpful website, GeneTestOrNot.org, and I’ll leave this up for a few minutes so you can write it down if you want. It’s an online decision tool to help people decide whether or not they want to get tested, and it asks people to consider: Do you have a family history of Alzheimer’s disease? Will this genetic testing give you useful information? Is this the right time to get tested, or maybe you should wait? Whether the advantages outweigh the disadvantages. Among the important issues discussed on this website is the Genetic Information Nondiscrimination Act (GINA), which protects against discrimination based on genetic information for health insurance or employment. Important to note that this protection does not cover long-term care insurance or life insurance. And so, once you know something, you can’t unknow it, and it’s better to think things through before getting tested.\nAnother good question: somebody walks into your clinic and hands you their 23andMe APOE test results, or maybe they got tested through Cardiology or Nephrology, which also do APOE testing, and now they want to know, “Okay, what tell me what does this mean?” It’s a really complex question. It depends on a person’s family history, it depends on their age, depends on whether they’re currently having memory concerns. In order to address or begin to address all of these questions, we wrote a paper, and it’s a series of vignettes of different situations of when patients come in asking about their APOE test results. And here are some ideas and some guidance on what to tell them. I should also say that the table that I included earlier that had the various APOE genotypes (three three, four four, what have you) and the risk of Alzheimer’s disease is included in this paper. So this is a publicly available article through the general family practice. If you have any problems finding it, you’re welcome to contact me, and I can just shoot you a copy of the paper.\nAnd that is actually all I have unless there are other questions or comments.\nBarbara: What um questions might you have or um issues that have come up for you, if you, as you’ve connected with um your patient populations or people in the community?\nMalia: One of the main questions or sort of concerns I get is when we hear the word “genetic,” we think predestination, right? We think absolutely causative. If it’s in your genes, it’s in your genes, and you are going to get whatever it is. And the idea of this complex interplay between our genes and our environments and our lifestyle is a little harder for people to get their heads around. Um, that’s one of the big issues. Um, among the people I see for genetic testing, one of the big concerns is, especially if they have been diagnosed with Alzheimer’s disease, frontotemporal dementia, or Parkinson’s, what are the chances that their kids are going to get it? And almost all of these genes are autosomal dominant, which means that if you carry the gene, the pathogenic variant in this gene, there’s a 50-50 chance that each of your children will also carry this same pathogenic variant. Now, that doesn’t necessarily mean they’re going to get Alzheimer’s or Parkinson’s or frontotemporal dementia, depending on whether this is a causative gene. If it is a causative gene, then that means if that individual lives long enough, they will have a very high likelihood of getting the disease. Risk variants, it just means if they have inherited pathogenic variants in that gene, they have a higher chance of also developing the disease. Um, so those are the most common issues that I hear about from patients and research participants. Um, what does it mean if I carry a mutation, pathogenic variant, in one of these genes and what does it mean for my kids?\nBarbara: So, a question: What lifestyle changes or environmental changes do you see that are most effective at prolonging onset of symptoms of Alzheimer’s or dementia?\nMalia: The Alzheimer’s Association is a fantastic source for that, and I would definitely go to that website. Um, the main thing I talk to people about is the link between heart health and head health, or heart health and brain health. So, all of the things we know we’re supposed to be doing to protect our heart, we should also have implications for our brain health. Managing any high blood pressure, managing your blood cholesterol levels, managing your blood glucose levels, um, exercise is very important, eating a balanced diet—you know, the standard Mediterranean diet, is a very good idea. In addition to all of those things, um, staying socially active and staying mentally active are great things to do for your brain health. So, yeah, what I tell people is it’s all the boring stuff that we know we’re supposed to be doing anyway, and it’s hard to do, but it really does have an impact on our health.\nBarbara: Malia, you have an earlier slide, something about mild cognitive impairment. I’m wondering if you can talk a little bit about the relationship between mild cognitive impairment and genetics and the development of Alzheimer’s. Do you deal with that at all where you’re seeing people come with mild cognitive impairment and then wanting genetic testing done, for example?\nMalia: Yeah, I don’t often see people with mild cognitive impairment because they are not eligible for our Alzheimer’s studies. I do sometimes see people with Parkinson’s disease who are beginning to have cognitive issues. In terms of genetic testing for, let’s say, somebody is concerned about their memory and they want to get APOE testing to see if it’s Alzheimer’s or not. That is an option, but it gives you limited information since, again, APOE testing is not absolutely predictive, and half of people who develop Alzheimer’s disease don’t have an APOE4 allele. What I’ll sometimes talk to them about is what might be just as helpful or more helpful is doing baseline cognitive testing and then repeating that a year later to see if there has been decline, and also doing some of those lifestyle interventions in the meantime.\nSo, it’s a different story, I’ll have to say, if it’s a family with early-onset Alzheimer’s disease or early-onset dementia. In that case, you may want to think about doing testing for those genes, and in one of those families, it’s most helpful to do genetic testing on an individual who has been diagnosed with early-onset dementia, to know that you’re testing the right person in the family. Because if you say have, you know, somebody whose parents and uncle and aunts and grandparents died with early-onset dementia, and you do genetic testing on that individual in the next generation who does not currently have cognitive complaints. Let’s say that testing comes back negative, which is great. But you don’t know if you’ve tested for the right thing. I mean, maybe it’s come back negative because there’s a gene we haven’t found yet in the family, and so we are in our testing for the currently known genes, we’re not going to pick that up, and we may be giving false reassurance to that individual. So general concept in genetics, start with somebody in the family who has the disorder, make sure it’s a gene that you can identify in the family, and then you can offer predictive testing to other individuals as appropriate if they want the information.\nBarbara: I also hope you all don’t, it seems like a lot of the genes are focusing on the amyloid hypothesis, and yet there’s still some controversy about amyloid and Alzheimer’s, for example, and I’m wondering if you have some information about how that’s playing into the research that’s going on right now and what we might see in the future.\nMalia: Yeah, let’s, uh, flip back. Sorry for all the changes here. Let’s flip back to that table, um, that graph of all of these. Um, so there was, I mean, there’s been a huge debate for years, right? Um, is it Tau or is it amyloid? It used to be called the Baptist versus the taoists in Alzheimer’s research, and a lot of the genetics is pointing toward amyloid. Um, but there are genes like we can see here, BIN1, which has the red circle around it, is linked with Tau, and it’s entirely possible there are other genes linked with Tau as well. That, um, in just concentrating on amyloid and not Tau at all, we are missing part of the picture. We don’t know yet, but I think as we have seen many of these anti-amyloid therapies in the early stages fail and fail and fail, and now we have some anti-amyloid therapies which are somewhat effective, um, they are not a cure, they hopefully slow down the disease. Yeah, you do wonder if it’s an oversimplification to only go after amyloid. Um, so say we don’t know at this point, um, but it’s certainly seems that amyloid is very important in Alzheimer’s disease. Is it the only pathogenic mechanism? No, and is it the only thing we should be targeting in our therapies? Potentially not, but we will see.\nBarbara: So, Lorraine mentioned that, um, she thinks that since we don’t have treatment or specific improvement, it would be a big conundrum to decide to test or not. She said, “I can see this if you’re interested in research and helping for the future, you might test.” So that makes me think about other studies I’ve been involved with where you really puzzle about what kind of recruitment message you want to convey. And I’m wondering how you do that for the studies that you’re involved with.\nMalia: Yeah, it is really difficult. Um, I would say in our early onset families, our early onset genetic families where our testing is predictive, more than half of people decide they don’t want to be tested because it’s not information they can act on, um, and it’s not something they think will make their life better. And one of the exercises we go through in genetic counseling often is having somebody imagine getting their test results and finding out they do have the family variants, that they do have a strong genetic predisposition to Alzheimer’s or frontotemporal dementia. And, um, how that sits with them and whether this is something that they are going to regret learning or not. Some people, for some people, the uncertainty is worse than knowing, and they just want to know that if they forget their car keys, if they lose their car keys, you know, does that mean it’s the start of something, um, or not? And, of course, individuals in these families have, even if they don’t carry pathogenic mutation in the gene, they have the same risk for late-onset Alzheimer’s disease as the rest of us. Many use the information. Those who decide to get tested, many use the information to join studies, and there’s a study called DIAN (Dominantly Inherited Alzheimer’s Network), um, which has done great work in understanding the pathology of Alzheimer’s disease, understanding the very early signs of Alzheimer’s disease because in these individuals who carry one of these variants, um, you can look at the family and make a fairly good prediction of the age at which they will develop symptoms. And so those individuals are carefully followed in the five to ten years before average symptom onset to see what are the changes in, you know, on brain scans, on lumbar puncture, on blood tests, what are the very early signs we can pick up, um, as Alzheimer’s starts to develop in the brain but the person isn’t showing symptoms yet? And once we do have very effective therapies, it’s going to be really helpful, I think, to be able to identify people in those very early pre-clinical stages and start the therapies at that point.\nSo that’s all the causative genes, APOE. Um, some people, again, they just want to know whether or not they have it. Um, frankly, a lot of people I think get, you know, 23andMe testing. People I’ve talked to may not think through it that thoroughly, and then they find out that they do have an APOE4, and then they’re really wondering what it means.\nBarbara: So this is my stupidity Malia, I’ve never done 23andMe. What preparation is involved in that?\nMalia: So 23andMe tests for Parkinson’s, and that’s part of their standard um, panel for your health, uh, health screening. For APOE, they do ask you to opt in, they do ask you to check a box and say, “Yes, I want this information,” and they tell you a bit about what the information involves. So they do ask people to read through a description about what the test can tell you and what it can’t.\nUm, and I don’t think I’ve seen any numbers from 23andMe on how many they’ve tested, but it’s at least in the thousands. So that is a common way that people find out their APOE results.\nBarbara: Can you please repeat what DIAN stands for?\nMalia: Dominantly Inherited Alzheimer’s Network.\nBarbara: Thank you. And if a patient joins a study, in case, say, they want life insurance or long-term care insurance, so if they join a study and there’s testing, is that protected information?\nMalia: It is, so, well, it depends on the site, depends on the hospital. Um, the Mayo Clinic requires that any test done in a research study be added to the clinical record, and so that is something that, for our sites at Mayo, both, you know, Rochester and Jacksonville and they have many sites, um, that those individuals need to understand because their research results are not protected from their clinical record. That’s the only site I know of that does that. For everybody else, it is not included in their clinical record.\nOften, clinicians really want it in the clinical record because that’s useful information for a person’s medical care, and so participants are given the choice whether or not they want it in there. Um, now, if somebody, you know, decides that they don’t want it in their clinical record and they are asked by long-term care insurance, you know, “Have you ever gotten genetic testing for this or that for Alzheimer’s disease?” Um, they could say no. I don’t think there would be a way for that insurance company to find out, but it is insurance fraud, so it is something not to be taken lightly.\nUm, when we are especially talking about some of these more predictive testing, one causative genes, um, for especially Alzheimer’s disease, frontotemporal dementia, for someone who has not developed symptoms of that disorder, we say, you know, “Are you comfortable with your long-term care insurance? Um, are you comfortable with your life insurance?” That might be something you want to review before getting this testing because it is not protected by federal regulations.\nBarbara: And other than 23andMe, is it likely that anyone getting genetic testing would see a genetic counselor before the testing occurs?\nMalia: Um, it depends. So if you, uh, are seen in a genetics clinic, you’re certainly likely to see somebody in a genetic counselor for something like Huntington’s disease or some of the genetic forms of other neurological diseases, Charcot-Marie-Tooth or what have you. Um, especially for predictive testing, again, if someone doesn’t currently have symptoms, they’ll probably talk with a genetic counselor. For other, like, for our Parkinson’s studies, uh, people don’t meet with a genetic counselor prior to testing. We have a video that goes over the most important issues in genetic testing. It’s an issue in healthcare because, frankly, there aren’t that many genetic counselors and, um, genetic testing is getting more and more common. And so we’re trying to find ways to make sure that people are fully informed about the ramifications of this testing, um, while taking into consideration the availability of people to really sit down one-on-one and go over those issues. So, it depends, sometimes yes, sometimes no. It’s helpful.\nBarbara: Any other questions, folks? Well, I, for one, have learned an incredible amount from this. I really appreciate your expertise, Malia, and all the information that you’ve shared. Um, we’re getting comments about it being a wonderful presentation and just the information, the way you’ve presented it, makes it easier for us to convey to people out in the community. So, I appreciate that.\nMalia: Absolutely. And again, if you have any questions in the future, if you’d like a copy of that um, uh, paper, that’s my email address. Shoot me a note. And all right, oh goodness, all right.\nBarbara: thank you so much, and I everyone, again, this is the last day of the last session for this series. The next one will start up on March 28th. You should be getting an email with all of the information for registering. But what a great session to finish up with because, um, it’s there’s not a lot of people with your expertise, Malia, and so I really appreciate it.\nMalia: Yeah, absolutely.\nBarbara: See everyone in the spring, hopefully we’ll be done with the rain and the snow by then. Bye, everyone. Bye, Barb. Bye-bye."
  },
  {
    "objectID": "software_datasets_transcript.html",
    "href": "software_datasets_transcript.html",
    "title": "Software Tutorials: Datasets (Video Transcript)",
    "section": "",
    "text": "FinnGen\nTitle: Intro to FinnGen\nPresenter(s): Aarno Palotie\nAarno Palotie:\nGood morning, good evening, good afternoon, wherever you are, you happen to be in your time zone, and thank you for attending this PGC meeting today. I’m going to tell you about three major collections in Finland that utilize some of the Finnish characteristics and special characters in the Finnish population.\nSo there they are: FinnGen, the Super Finland project, and the Northern Finnish Intellectual Disability Court. So, the success of genetic studies in disease genetics, especially specifically, are relying on basically four cornerstones: the population isolation, the National Health Registers, the long-standing epidemiological studies which have further developed biobanks, and then, of course, knowledge of the genome.\nFirst, diving into what the National Registries in the Scandinavian countries are. They are ways to record the usage of the healthcare services, and these are very similar in all Scandinavian countries, not just in Finland. It means that whenever and wherever in the country you visit a hospital, your diagnosis is recorded in a central register. The same thing when you, when you purchase a prescription drug. These registers were originally developed for administrative purposes, but since they accumulate a lot of data of healthcare service usage, they become very interesting resources for research. And although they are silos as such, the typical individual identification number or social security number, whichever way you would like to look at it, then combines this data together once you get the appropriate permits.\nThe second aspect, the population, the history of Finland or the current population history of Finland, stems a few thousand years back where a small number of settlers were mainly living on the coastal regions of Finland, but then in the 6th Century, the Swedish King Gustav Vasa demanded Finns to move also to the East and the Northern parts of the country, which resulted in a second very strong internal migration and multiple bottlenecks. What happened then in all Nordic countries, after the 18th century, is a rapid population growth, which obviously then resulted in all the all the typical characteristics of a population isolate.\nThe Nationwide registers that I was describing. What is great about them is that every single individual – every citizen, every resident – is recorded, and they stem back decades back in a digital format. Some of them go back all the way to the 50s, but I think the most important time point is the late 1960s, when the hospital discharge data, the cause of death data, and the reimbursement data came into usage.\nAnd this obviously provides you an opportunity to look at longitudinal data because this Health Data is available from birth to death, and this is one of the very key characteristics of this data. Even if it doesn’t include some symptom-level data – it’s ICD codes, ATC codes – but the great thing is, it’s longitudinal.\nAnd this is what the FinnGen research project is based on. It’s a project that was initiated in 2017, and with the idea that it utilizes genetic strategies to understand disease mechanisms. It’s aiming to collect 500,000 individuals, which is roughly 10% of the population, genotype them, and impute them against a national or population-specific deep whole genome sequence backbone and then integrating the National Health register data, which results in a dataset of 500,000 individuals with both genome and Health Data for our association analysis.\nThis project is a public-private collaboration – a research project where all Finnish biobanks, which exist in all University hospitals around the country, all universities with a medical school are involved, as well as the Institute of Health and Welfare, and blood transfusion Center. Also, the Finnish Biobank Cooperative is a partner there, and currently, 13 Pharma partners are together with us working on our scientific goals.\nThis is a 10 year project and we have just completed year four. The sample collection will go on for still a little bit less than a couple of years, at which point we are full with our collection and then really have an opportunity to further focus on the analysis. Where are we in respect to the collection? We have currently collected 472,000 participants, which means that we are slightly ahead of schedule and seem well set to reach our goal. This includes not quite yet 200,000, but it will include some 200,000 Legacy Collections and 300,000 prospective collections. The prospective collection, what is crucial here most of them come from hospital biobanks, typically University Hospitals, which means that they are specifically enriched for diseases treated in these places. So, if you compare to the UK Biobank, this is a big difference in the content of the sample.\nWe produce data sets every six months. The current data freeze will consist of 350,000 individuals and more than 4,000 endpoints. All of these pheWAS analyses done from these data freezes are made public after 12 months once they become ready.\nIf you look at the type of phenotypes that we have, then the special thing is, again, that the age is relatively high here, which means that there are quite a lot of healthcare events already. The mean number of health events per individual is 340, including some 186 drug purchases. This means that there’s quite a lot of data. And how we build these phenotype endpoints is that we combine data from different registers, whether drug usage or hospital visits, and create a strength and endpoint by this in this way. This needs quite a lot of understanding of how the data works, or the National register data, how it functions, what are the strengths, what are the drawbacks, and so forth.\nAnd how do we then access the data? We have two levels; both of them work in the Google Cloud space. One is where we have the summary-level data, the results, in other words, from GWAS and pheWAS analyses. And then there is the secure environment, and this is something to stress in the way that those familiar with the European data regulations, which have become quite stringent. So this is coping with all those requirements and still being able – we are able to - access the data from outside the European Union. The idea is that it’s a secure environment where you cannot take data out; you can analyze it, you can look at it, but you cannot copy it out from there or download it to your own computer.\nWhat has been characteristic in analyzing this data is that there are a large number of Finnish-specific variants that even in very extensively studied, genetically very extensively studied, diseases, like type 2 diabetes. All the ones with stars here marked are Finnish-enriched alleles and Finnish-specific hits. And this goes through, this is the characteristics through all traits that we actually have studied. There are typically Finnish-enriched variants.\nThe other way you can look at it, since we have the medication data from all individuals and the entire country, so you can ask questions the other way around: What about those who use medicines? Do we see variants enriched that actually give additional information for us, not just from the basic diagnosis? And this seems to be the case, at least in some of the cardiovascular study medications and those related medication GWASs.\nSo, when we look at the numbers of mental health diagnoses in FinnGen, out of the 321,000 individuals that were in the data freeze, any ICD codes for mental health were observed in 76,000 individuals, but even more individuals had depression medication, almost 8,000 more. Depression diagnosis was then seen in 33,000 individuals, schizophrenia in 11,000, sorry, schizophrenia, schizotypal and delusional disorders in 11,600, schizophrenia in 6,000, and bipolar disorder in almost 6,000 individuals as well.\nA place where you can look into the distribution of diagnoses and how they have been constructed from the register data is called Risteys. If you go to the FinnGen website, you can be guided to this research website, which is open for everyone. You can see various age distributions of the individuals, years of first diagnosis, and comorbidities. It’s quite a helpful site to get an understanding of how our endpoints are constructed.\nThen we move to the second collection, which is the Super Finland collection. This has been critically supported by the Stanley Center, a collection that was done over the years 2016 to 2018. It consists of a little bit more than 10,000 patients with psychotic disorders. All samples have been GWASed and exome sequenced, a little bit less than 900 also have whole genome sequencing. Almost 90 percent of the individuals have consented for IPS research and can be re-contacted, and as of now, a little bit more than 5,200 PBMC lines have been collected for IPS cell generation. And this map is showing the distribution from where the patients are, which means that they pretty well represent the population density also in the country.\nAs you can imagine, there’s a huge number of hard-working people behind it. They don’t all fit here in the slide, but they worked very hard over the years to first collect them, and now later to analyze them.\nThe diagnostic groups in SUPER are such that a little bit more than half of them, a little bit less than 6,000 have a schizophrenia diagnosis. 2,600 have bipolar disorder, and the next largest group being schizoaffective disorders, psychotic depression, and other psychoses.\nFrom the recruited individuals, we can clearly see that, as expected, the education completion is lower than in the general population. They are less married than non-psychotic individuals in the population and so forth. So, we have an understanding that quite a lot of these people are chronically ill and represent that type of psychosis spectrum.\nWe also have collected questionnaire and cognitive test data at the time of collection, so we don’t have longitudinal cognitive data, just that the recruitment. We have the medical care register data and also the blood plasma stored.\nThen, the third collection and the last one is the Northern Finnish Intellectual Disability Cohort. We currently have a little bit more than 3,000 participants. They are all collected from the northern regions, and if you remember when I was describing the late settlement movement, which happened after the 16th century, so these were areas that were very sparsely populated in small villages. We can see from the frequency, that the prevalence of both intellectual disability and schizophrenia are higher in these areas, and that was one of the reasons that stimulated us to collect individuals that don’t have a clear ID diagnosis from this area currently. Currently, most of these have already been exome sequenced and GWAS analyzed. For instance, just for interest, 56 SCHEMA gene variants are observed in 80 carriers. It’s kind of interesting that since we are dealing with a majority of mild ID cases, which is slightly unusual for this type of cohort. It’s interesting to compare now the variant carriers specifically in this group and then in the SUPER Psychosis Study. Indeed, the same variants exist as well as the same genes having different variants in these two collections.\nAnd I think everyone for listening and hope that these cohorts provide an opportunity for good harvest. Thank you.\n\n\n\nPsychENCODE\nTitle: Introduction to PsychENCODE\nPresenter(s): Chunyu Liu\nChunyu Liu:\nHello. I’m Chunyu Liu at SUNY Upstate Medical University at Syracuse. I’m glad to be here to introduce the PsychENCODE project and explain how PsychENCODE data might be useful for your study of psychiatric genetics.\nSo, the PsychENCODE Consortium was established in 2014, funded by NIMH. Now, it has grown into a big Consortium with 47 grants supporting 12 institutes and more than 100 scientists. The Consortium focuses on human brain, and we do have individual projects covering non-human primates, and even mice, but the majority of the studies are on humans. We cover several major psychiatric disorders, including schizophrenia, bipolar disorder, autism, PTSD, and major depression. But a big chunk of the brain samples actually come from psychiatrically normal people. We also cover a wide range of life stages from prenatal fetal tissue, developmental early life, and adult brain.\nThe major features of PsychENCODE which make it different from other Consortia like ENCODE and GTEx are the following four points:\n1. Genetic variation: We use population samples and have hundreds, even thousands, of brain assays sequenced. With that, we can study genetic variation for its contribution or regulatory role in different omics.\n2. Brain focus: We focus on the brain over other organ tissues. The major brain region we study is frontal cortex. We do have a few studies that cover other brain regions, but frontal cortex is a major brain region we focus on.\n3. Major diseases: As I already said, schizophrenia, bipolar disorder, autism, major depression, and PTSD are the major diseases we study.\n4. Different omics: We have several different omics covered by different projects. For example, we have chromatin data, RNA-seq data, and Hi-C data.\nIn 2019, the Consortium had the first wave of publications. Eleven papers were published in the Science family journals. Since the papers have been published, I will not spend time on the major results of those studies.\nI just want to describe how you can access the data. All the PsychENCODE produced data have been deposited into Synapse, which is managed by Mette Peters and Kelsey Montegomery. So, this is the website they created as a public portal where you can browse the data and request access to all the data produced so far. It’s released to the public domain.\nThere are already more than 200 terabytes of data produced by the early phases of the PsychENCODE project. As you can see here, the biggest datasets are actually RNAseq data, followed by ChIP-seq data and ATAC-seq data. The other data types are relatively smaller, like Hi-C, NoMeSeq bisulfite sequencing, and so on.\nA web interface or service that the Consortium is creating is this PsychSCREEN. This is a project led by Dr. Zhiping Wong group. They are creating a website for you to query PsychENCODE’s analytical results. You can query the information by genes, regulatory elements, or genetic variants. The genetic environment is a simple path. We will talk a little more about how they relate to PGC.\nThe major thing I want to deliver today is how PsychENCODE actually can be connected to PGC. So, this blue box shows you that SNP associations with different disorders are actually the major products from PGC – those GWAS results. The green oval actually covers the major product from psychENCODE. It contains all the brain omics for gene expression, for epigenetic measures, and protein abundance. With that, you can calculate QTLs, including expression QTLs (eQTLs), which basically associate SNPs with gene expression. You can also do QTLs for the other omics. Slowly, you can do differential analysis – differential expression, for example. Other than that, you can also perform Mendelian randomization analysis, for example, to study the causal relationship between SNP and disease.\nI want to spend a few minutes talking about the project in my own lab that is called BrainGVEX. It’s one of the dozens of projects funded by PsychENCODE. My first collaborators on this project are Kevin White at Chicago and Junmin Peng from Children’s Hospital. My most recent project is collaborating with Stella Dracheva from Mount Sinai and Eran Mukamel from UCSD. We also have peripheral collaborations from Wang lab at the University of Texas, and Central South University from China, working on data mining and analyzing the data we have generated so far. I want to especially thank the program officers Gita and Alexander, who have provided us with continued support.\nFrom this BrainGVEX study, we have generated genotype data – either by the array or low-pass whole genome sequencing – on more than 400 brains. The majority or most of them have been sequenced for transcriptome using RNA-seq. We also have a good proportion of samples with ATAC-seq, riboseq, and proteomics by mass spectrometry.\nSo, through that, you can see that this really covers the major components of the central dogma, from genetic variants to gene expression, transcription, translation – translating the mRNA binding to the ribosome, then translating into protein.\nWe use this multiomics data, along with ChIP-seq data, and we can perform QTL mapping. We can also build co-expression networks. With this information, we can further extend to connect to eQTLs from PGC. The goal is that we want to use all the information to understand and explain the GWAS signals, also trying to capture some causal relationships which are not obvious by just looking at the GWAS signals alone. On top of that, we can also perform something like TWAS analysis so we can identify normal risk changes in the pathway and further build a prediction model.\nI want to share a few unpublished results. The first one is the QTL mapping of multiomics data. This is a study done by a former PhD student, Jiang Yi. So, he was analyzing our BrainGVEX data, which contains RNAseq, Riboseq, mass spec, and ATAC-seq from 200 to 400 brains.\nThe result shows different QTLs. Actually, can produce different, I mean, different omics can give you a different amount of QTLs. Like this panel A shows you eQTLs, splicing QTLs, has many more QTLs than riboQTLs, and protein QTLs. And if you look at the effect size of different QTLs, you can see that eQTLs have a stronger effect size than ribosome QTLs and protein QTLs. I think that’s easy to appreciate because of gene expression, ribosome binding is closer to genetic environments than protein, as it involves more steps in the central dogma. They do explain a different level of heritability from the GWAS results as well.\nWhen you compare the different types of QTLs obtained from different omics, you will see that for those SNPs associated with the same gene pair at different omics, they have some good consistency. This is reflected in the positive correlation diagonal. But at the same time, you do see some interesting pairs going in the opposite direction. For example, in this panel comparing eQTL with ribosome QTL, we do have a small number of SNP and gene pairs that show a negative correlation. The same scenario can be observed when comparing ribosome QTL to protein or gene expression to protein.\nAt the same time realize that many gaps remain to be filled by the PsychENCODE Consortium or the data analysis, particularly from the eQTL perspective. So, we still did not cover well for early development. We do not have good coverage for diverse populations, and we still need to cover in detail cell types.\nWe have several ongoing projects, and I’m going to describe one right now. Again, this is unpublished data, a collaboration between my lab and Michael Gandal at UCLA. This slide, prepared by Michael’s student Cindy Wen, shows the scene we are looking at. We are generating eQTL isoforms, splicing QTLs from GTEx, on fetal brain tissue from nearly 700 samples covering three different trimesters of three population sources. This data actually shows you that the fetal brain does capture some unique early developmental QTLs. You can use this information to study and analyze the GWAS signal, and you see, they do enrich in the GWAS signal as well, as you observe in adult brain QTLs.\nThat’s a study on fetal brain. We already have some cross-population studies, but in adult brain, the situation is also there. So, we don’t have good coverage for diverse populations. The current data we generate from bulk tissue RNAseq predominantly originates from European population. We only have 18 African American brains, and that has been corrected to a degree in the single-cell RNAseq data. We have more representation from Hispanic and African American populations relative to the Caucasian sample. But so far, none of the non-European samples have been formally analyzed here.\nWe have a project ongoing to study East Asian, Han Chinese, brain eQTLs. This is a project with my collaborators and Chen Chao in China. These slides were prepared by our students, Chen Yu. Using 150 brain samples from China, we can identify a large amount of eQTLs and splicing QTLs, referring to thousands of genes. So, 80% of the eGenes and splicing QTLs are consistent across two populations, at the same time, it tells us that we still have a substantial amount of eQTLs that only appear in one population and not the other. And the important thing is, when we use the QTLs to explain GWAS signals, we see an interesting phenomenon. So, if you use East Asian QTLs to explain East Asian GWAS signals, you have a significantly higher proportion that can be explained, and then using the European eQTL data.\nSo, I said we have a gap to fill about the cell types eQTLs. That is a major task for this current phase of PsychENCODE. Actually, right now, hundreds of brains are under investigation, meaning they’re being sequenced for single-cell RNAseq or ATAC-seq. We expect many major cell types will be covered.\nSo, stay tuned. In 2022, there should be publications coming out from our Consortium regarding cell types of eQTLs, early developmental stage QTLs, and some population-specific QTLs.\nBecause of the pandemic, this is our latest group picture. Several labs joined the Consortium later during the pandemic, so we don’t have a group picture. As you can see, we have many senior investigators and young fellows in this picture. I believe everyone will have their own vision or interpretation of the PsychENCODE data. You definitely can approach them and listen to their explanation or presentation about PsychENCODE. Hopefully, that will be a very useful resource for you to study psychiatric genetics.\nThank you, bye."
  },
  {
    "objectID": "chapter10.2_transcript.html",
    "href": "chapter10.2_transcript.html",
    "title": "Chapter 10.2: Caution in Genetic Prediction (Video Transcript)",
    "section": "",
    "text": "Title: Predicting the likelihood of future psychiatric disorders: a closer look, and some cautions.\nPresenter(s): Howard Edenberg\nPredicting the likelihood of future psychiatric disorders sounds very appealing, particularly if there’s the possibility of an effective early intervention.\nBut let’s take a closer look and look and look at some cautions before jumping ahead too quickly.\nHI. I'm Howard Edenberg from Indiana University. [These are my personal views.]\nPsychiatric and substance use disorders are complex genetic disorders. The risk is affected by both the genes and the environment. Neither works alone and they interact, sometimes in complex ways. For many disorders, including the substance use disorders, the total genetic contribution to risk is about equal to the environmental contribution, and for substance use disorders there’s a required environmental component: consumption or use of the substance. Thus genetics will never fully predict future psychiatric disorders, particularly the substance use disorders.\nEven the best powered genetic studies to date are far from being able to make confident predictions. We’re only beginning to identify the genetic variations that affect risk. For the substance use disorders, for example alcohol use disorders, at this moment, the common variants measured across the entire genome predict only a small fraction of the risk, about 10 percent, so current attempts at prediction are at best premature. What makes such attempts even more problematic is the risk that people may experience stigma associated with being labeled as having a substance use disorder or psychiatric disorder.\nThere are, however, some claims of very high accuracy in predicting risk for a complex disorder such as opioid dependence. We learned of a company claiming that they have a test to advise physicians whether someone is likely to become opioid-dependent before that physician prescribes opioids for pain relief. The company’s prediction is based on training a machine learning model on a small set of cases and controls using a handful of genetic variants, only 15, that are purportedly from brain reward pathways, and they claim an extraordinarily high accuracy. We examined their claims and found serious problems. The chosen brain reward system SNPs differ in allele frequencies in different populations, and machine learning models based on those chosen SNPs turned out to predict genetic ancestry much better than they predicted risk. In fact, we found that random SNPs matched to the same minor allele frequencies performed the same way as these brain reward system SNPs. Thus there are serious problems with the purported accuracy of this test. The first problem, of course, is that prediction is not accurate and can therefore lead to poor medical decisions. In particular, this may lead to the undertreatment of pain in those individuals who are flagged as being at increased risk for opioid dependence. Because the test picks up differences in ancestry, it may lead to increased medical discrimination against those of some ancestries and it may stigmatize those predicted to be at risk. Having that marked in one’s medical records could affect treatment down the line, and of course stigmatization can go beyond just medical treatment into the wider society. We’ve posted our analyses of these particular tests on medRxiv for those who’d like to know how we came to some of the conclusions that I just relayed to you. [Now published: PMID: 34710714; PMCID: PMC9358969]\nSo this has been a cautionary tale. Although our study focused on one example of a predictive test for opioid use disorder, similar issues are likely to arise in tests for other substance use disorders and psychiatric disorders in general. Particularly because these kinds of disorders are often accompanied by stigma, we have to be especially careful that our tests are accurate, that our tests don’t discriminate against people from different ancestries, and that the good that they do outweighs the potential for the stigmatization.\nThanks for listening to my comments."
  },
  {
    "objectID": "software_geneset.html",
    "href": "software_geneset.html",
    "title": "Gene Set Identification",
    "section": "",
    "text": "MAGMA\nTitle: Gene- and gene-set analysis in MAGMA\nPresenter(s): Christiaan de Leeuw\nLevel: Intermediate\nLength: 12:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\nH-MAGMA\nTitle: Annotating Genetic Variants to Target Genes Using H-MAGMA\nPresenter(s): Nancy Y.A. Sey, University of North Carolina at Chapel Hill\nLevel: Intermediate\nLength: 10:09\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to GitHub tutorial and datasets.\n\n\n\nE-MAGMA\nTitle: E-MAGMA: an eQTL-informed method to identify risk genes using genome-wide association study summary statistics\nPresenter(s): Zac Gerring, Eske Derks\nLevel: Intermediate\nLength: 7:16\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to GitHub tutorial and datasets.\n\n\n\nPRSet\nTitle: How to run pathway specific Polygenic Risk Scores\nPresenter(s): Judit García-González\nLevel: Intermediate\nLength: 15:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_genomicSEM_transcript.html",
    "href": "software_genomicSEM_transcript.html",
    "title": "Software Tutorials: Genomic SEM (Video Transcript)",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nAndrew Grotzinger:\nI’m Andrew Grotzinger, and in this video, we’re going over how to run genomic SEM using psychiatric traits as examples. Genomic SEM is a general framework for modeling genetic covariance matrices produced by methods like LD score regression to then estimate any number of structural equation models that can be used to test hypotheses about the processes that gave rise to the data that we observe. It only requires your summary statistics, and those summary statistics can come from samples with unknown and varying degrees of sample overlap. What that means is that you can now estimate models for really rare traits that you would not otherwise observe in the same sample.\nIt’s going to be split into two parts for the practical. The first is how to estimate a user-specified model using genome-wide estimates. The second part is how to incorporate the effects of individual SNPs to do things like estimating multivariate GWAS. All the practical materials are available at this Box link here, and this includes an R script and all the files needed to run that.\nOn the screen, at the top of that R script, there is going to be some code to actually download GenomicSEM if you haven’t done so already. Throughout the presentation, we’re going to be using GWAS summary statistics for schizophrenia, bipolar disorder, and major depressive disorder. Moving on to how to actually estimate a user-specified model, this takes three primary steps. The first is munging the summary statistics, the second is running LD score regression, and the third is using the output from LD score regression to then run the model that you specify.\nMunge is a general procedure to format the summary stats in the way LD score regression is expecting. For the sake of this practical, we’re using a subset of 10,000 SNPs for schizophrenia, bipolar disorder, and major depression. In general, you can download and use a full set of summary stats on a personal laptop. For the purpose of the practical and restricting file size, we’re using a subset here. It takes four arguments: the first is the name of the files, the second is a reference file that you need to allot to the same reference allele across traits, the third is the name of the traits, and the fourth is the total sample size.\nPutting it all together in this last line here, where I’m specifying all these arguments and running munge again, to really highlight that we are only using the restricted subset for the purpose of the practical. When munge runs, it’s going to produce a DOT log file that you should inspect to make sure things like column headers are interpreted correctly. In particular, I’ve highlighted this section here where it prints how the effect column was interpreted. For schizophrenia, it’s an odds ratio, and it’s interpreted correctly. But you’ll want to make sure to read the corresponding readme files for the summary statistics and then look at this log file to cross-reference whether or not that effect column is being interpreted right.\nIn this second step, we run LD score regression within the context of GenomicSEM. The first argument is ‘traits,’ and that’s the name of these now-munged summary stats, which will all enter that .sumstats.gc ending. Then, for binary traits, we do the liability threshold correction, which requires inputting the sample prevalence, reflecting the cases over the total sample size, and then the population prevalence, which you can get from either epidemiological papers or from looking at the paper from the corresponding univariate GWAS. The fourth and fifth arguments are the holder of LD scores and the LD score weights. In almost all cases, this is going to be the same folder. Here we’re using the European LD scores because we’re using the European-only summary stats. Finally, we specify the names of the traits, and this is how the traits are going to be named in your actual model. On this last line of code, we are at LD score regression. In the next step, we’re actually going to specify the model. But before we do that, I want to switch over to R and actually run through the code so you can see it.\nFirst, I’m going to load in this package, and I’m going to set the working directory to where I downloaded those workshop materials. Then, for munge, I’m going to set those files, the HapMap 3 list, that reference file, the trait names, the sample size, and then run Munge. In the second step, I’m going to take those munged summary stats and put the sample and population prevalence for the liability correction, the folder of LD scores, and all the score weights (which are the same), and then the trait names. Finally, actually run LD score regression. This is going to produce results that are actually interpretable because we use that subset of 10,000 SNPs. So when we now go on to step three of running the model, I’ve created an LD score regression object that uses the full set of summary statistics that we’re now going to load in, so you can actually produce integral results in the context of the model. We’re going to load that in and now switch back over to the PowerPoint to talk about how you specify a model in GenomicSEM.\nWe use the Little’s Bond syntax for running the model. In this case, we would specify a regression relationship of A using A tilde B, or for those of you who think of it in the format of Y tilde X, or outcome tilde predictor. For covariances, you would specify two. And of course, for a variance of a variable, it would be the covariance with itself. So, A tilde A for a factor, you specify the factor’s name here, followed by an equal sign utility, and then the factor indicators. To fix a parameter, you would put a number followed by an asterisk on the right-hand side of the parameter that you’re estimating. So, this would fix the covariance between A and B to 1. To name a parameter, you would write the parameter label using some set of letters. Here, I’m naming the covariance between A and B ‘Cov_AB.’ What this does is it allows you to use model fit statistics for this parameter. Let’s say the covariance is estimated as negative, but you have some real sense that it should be positive. So, you put this as a parameter constraint to keep it above zero.\nWe loaded in that pre-made LDSC data, which again is using the full set of summary statistics. This is not simulated data because we’re using summary stats. This is often something you can readily download online. Then, to actually run the model, this takes two necessary arguments and two optional arguments. The first is CovStruck, which is the output from LD score regression. The second is the model. So, we’re running a common factor model here, and we’re telling Laavan using this n_a_star that we want to freely estimate the first loading, and then we want to fix the variance of the factor to one. So, we’re using what’s known as unit variance identification for this model. An optional third argument is what estimation method you want to use. We offer DWLS and maximum likelihood, but the default is DWLS. And then, another optional argument is std.lb, and that’s whether or not you want to automatically specify the variances of variables to be one.\nYou would run the model. This is going to produce this set of results, and I’ll show that in R here in a second. But to walk you through what those results mean, the first three columns are the parameters being estimated. The fourth and fifth columns here are the unstandardized estimate and standard error for those parameters. The sixth and seventh columns are the standardized estimate and standard error for those parameters. The model fit here is all going to print as an ‘a,’ because when you’re estimating a common factor divided by three indicators, it uses up all of your degrees of freedom, so it perfectly fits the model. But I want to walk through how you would interpret these model fit statistics more generally.\nSo, chi-square is the model chi-square that reflects the index of fit, and the degrees of freedom and p-value for the model chi-square. In the next two columns, note that this will almost always be highly significant for your model, because chi-square is sensitive to sample size, which by definition is massive in GWAS space. The next piece is AIC, which can be used to compare models regardless of whether they are nested, with lower values indicating better fit. CFI is the comparative fit index, which has these general heuristic cutoffs, with higher being better. And then finally, SRMR is the standardized root mean square residual, with lower being better.\nGoing back over to R, we’re going to specify that argument, specify the model, the estimation method, and std.lb. We’re going to run the model, and you’ll see that we produce the same results that we produce over here, not in the slides, but in the code. I’ve included some alternative ways that you could specify this model. For example, if you wrote std.lb equals true, you don’t have to write that n_a_star F1_star F1. Or you could use what’s called the common factor function to estimate the same model and produce the same results.\nMoving on to part two, I want to talk about how you would run multivariate GWAS in GenomicSEM. And to be clear, you don’t have to do both of these parts. You can certainly run a genome-wide model in part one and publish that alone. You don’t have to bring in these individual SNP effects. But I did want to show that because oftentimes for people, the first two of which mirror what we already did, which is to run a munge and LD score regression. You don’t need to redo that. We’re only going to go over the last two steps: running the subsets function and the multivariate GWAS functions.\n`common_factor_gwas` using your GWAS summary stats. It takes a number of different arguments, which can be a little bit confusing, and I’ll note that we have a flowchart on our GitHub to help you figure out how the arguments should be specified. Again, we’re using the drastically subset SNPs for the purpose of the presentation, and we’re also using a subset of the reference file, which is used to align the alleles and to pull out the SNP minor allele frequency. The third argument is the trait names, and then the fourth argument here, we’re letting the `subsets` function know that the standard errors for these traits are on the logistic scale. You’ll want to be really careful with this because oftentimes, for binary traits, the effect column might be on an odds ratio scale, but then the standard error will be on a logistic scale. That’s something that you can determine from the readme file. If you have continuous traits or binary traits analyzed using a continuous model, it’s going to be a different set of arguments. For the sake of time, I’m not going to go over all of that here, but we do have that flowchart on GitHub, and of course, you can reach out to us if you’re confused or getting strange results.\nPutting that all together, you’d run the `subsets` function here, and much like the `munge` function, this will produce a log file that you want to make sure you go over to ensure everything’s interpreted correctly. Once that’s done, you can use that output in combination with the LD score regression output to run models that include the effects of individual SNPs. I’m going to go over both the `common_factor_gwas` and `user_gwas` functions, starting with the `common_factor_gwas` function.\nThis takes two necessary arguments. The first is the output from LD score regression, and the second is the output from the `subsets` function. Then, finally, once again, you can specify `dwls` or `ml` for the estimation method, and you can also specify whether you want to run in parallel. In practice, if you’re running a GWAS, you cannot run it on a laptop, unlike the models we were showing in part one. So for this, you really will want to be running in parallel on a computing cluster. But for the purposes of this presentation, because we’re using a small subset of SNPs, we’re going to run it not in parallel so you get a sense of how these functions work.\nI’m going to switch now over to R to go through the `subsets` function, where again, we read the files, the reference file, the trait names, whether the standard errors are on a logistic scale. We’re writing `TRUE` for all three, running `some_stats`, and now taking that output and specifying the LD score regression output, the output from `subsets` that we created, and actually running the `common_factor_gwas` function.\nSo, this is going to produce output that looks like this. There are a lot of columns: chromosome, base pair, minor allele frequency, A1, and A2. The parameter being estimated, which is the effect of the SNP on the factor, and the estimate, sandwich-corrected standard error, Z estimate, p-value, and then this other metric that we call QSNIP, which indexes whether or not that SNP really fits the model. This is the chi-square distributed test with degrees of freedom that will depend on the number of indicators in your model. So that indexes the extent to which a common factor model is insufficient for accounting for the pathways from an independent pathways model. If this common pathway of the SNP on the factor is really a poor representation of the independent effects of the SNPs on these individual indicators, then Q is going to be significant. That’s going to happen when you have things like a SNP that has directionally opposing effects on the indicators or let’s say the SNP has a really strong effect on one of the indicators but not the other.\nIf we now go over to these results, you’ll see the estimates here. One thing you want to inspect is these last two columns of fail and warning. Zero means it’s good to go. But if you look at the warning messages, you’ll see that a handful of the variances are estimated as negative. I want to use this as an opportunity to talk about how you might troubleshoot a warning like this and then also go over how you would use user GWAS in this context.\nFor `user_gwas`, it takes those same first two arguments of the output from the multi-SNP score regression and the subsets output. But then you’re also going to specify the actual model that you want to run that now includes the effect of this individual SNP. So we’re running that same model of the common factor model. The common factor GWAS is automatically specifying behind the scenes where the SNP predicts that common factor, but then we’re additionally adding in these model constraints. We’re renaming these parameter labels as the residual variances for schizophrenia, bipolar, and MDD, and we’re constraining them to be above zero because of that warning that we got when running common factor GWAS.\nIf we now go over and run that, I want to mention that another optional argument for user GWAS is this `sub` argument. This is whether or not you want to save a particular piece of the model output. The common factor GWAS is automatically only saving the effect of the SNP on the common factor. What the `sub` argument does is it allows you to tell user GWAS that, ‘Look, for each of these SNPs, I don’t want to save all of the model output, including the factor loadings and the residual variances, for the sake of memory. I want you to save the effect of this SNP on the common factor.’ So, `sub` does not change at all how the model is estimated; it changes how large the output file is. I would highly recommend setting this argument.\nNow we’re going to run this, and while that’s running, I’d like to make some final notes. Parallel processing is available for both user GWAS and common factor GWAS. Parallel processing executes the exact same code in serial processing, except that it takes advantage of additional cores. In an ideal runtime scenario, you would split your jobs across computing nodes on a cluster and run in parallel. There is also MPI functionality available. So again, all runs are completely independent of one another. I’ve listed a number of resources here, including the GitHub. I’m not going to go through this, but I’ve included some slides about some things to keep in mind.\nSo, the final thing I’d want to do is go back over here and show you that this produces the same set of results, common factor GWAS are very nearly, so now that we’ve added those residual variance constraints. But now, if we look at the warnings, we see that those warnings are now gone. So, with that, I’ll end it. You can reach out to us with questions, and I hope this was helpful to you."
  },
  {
    "objectID": "software_crossdisorder.html",
    "href": "software_crossdisorder.html",
    "title": "Cross-disorder Analysis",
    "section": "",
    "text": "Multi-trait Analysis of GWAS (MTAG)\nTitle: MTAG: Multi-trait Analysis of GWAS\nPresenter(s): Patrick Turley\nLevel:\nLength: 1:32:04"
  },
  {
    "objectID": "chapter4.3_transcript.html",
    "href": "chapter4.3_transcript.html",
    "title": "Chapter 4.3: Genetic Study Designs (Video Transcript)",
    "section": "",
    "text": "Twin studies {sec-video1}\nTitle: What are “Twin studies”?\nPresenter(s): OpenLearn from the Open University\nTwin studies allow us to estimate how much a trait comes from our genes and how much is influenced by environment. These identical twins share the same genes. They are the product of one sperm and one egg that divided into two embryos in an early stage of the pregnancy. They are therefore genetically identical.\nNon-identical twins derive from two different sperms and two different eggs. They are genetically as similar as ordinary brothers and sisters and share, on average, 50% of their genes. But they all share the same environment as they grow up in the same family.\nIn twin studies, it is possible to separate out these different influences by comparing the resemblance between identical twins with the resemblance between non-identical twins. Data is gathered from numerous pairs of twins in order to gain an accurate estimate of the relative importance of genes and the environment.\nSo, identical twins share all their genes and their share environmental influences. Differences between these twins can thus be only due to non-shared environmental influences. If both identical and non-identical twins resemble each other closely, this must be due to shared environmental influences. If identical twins resemble each other more closely than non-identical twins, then it gives us a clue that genetic factors must play a role.\nSo, twin studies can provide clues on the relative importance of genes and environment, and thus provide information on where the balance lies between nature and nurture.\n\n\n\nChoosing the Right Design {sec-video2}"
  },
  {
    "objectID": "software_datasets.html",
    "href": "software_datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Large genetic and genomic datasets are becoming more publicly available for research use. In the following videos, Drs. Palotie and Liu give introductions to large datasets available from the FinnGen and PsychENCODE consortia respectively.\nFinnGen is a large consortia of both public and private institutions (universities, hospitals, biobanks, pharmaceutical companies, etc.) to collect and analyse data from over 500,000 Finnish biobank participants, with the ultimate goal of finding novel insight into prevention, treatment, and diagnosis of disease.\nThe PsychENCODE consortium is composed of many academic institutions whose goal is the generation of large-scale genomic data (gene expression and regulation) from individuals with psychiatric disorders across different brain regions. This includes generation of both bulk and single-cell RNA sequencing, ChIPseq, Hi-C, ATACseq, and other measures of chromatin activity for multiple brain regions across different developmental stages.\n\n\nFinnGen\nTitle: Intro to FinnGen\nPresenter(s): Aarno Palotie\nLevel: Beginner\nLength: 19:24\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to FinnGen website.\n\n\n\nPsychENCODE\nTitle: Introduction to PsychENCODE\nPresenter(s): Chunyu Liu\nLevel: Beginner\nLength: 19:34\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to PsychENCODE website and PsychSCREEN website."
  },
  {
    "objectID": "chapter1.1_transcript.html",
    "href": "chapter1.1_transcript.html",
    "title": "Chapter 1.1: What are psychiatric disorders? (Video Transcript)",
    "section": "",
    "text": "Psychiatric disorders can be thought of as the severe chronic disabling disorders of the young. When you add it up they cause a disproportionate amount of suffering for individuals, for families. Unfortunately, we haven’t made as much progress as we would have imagined in the last 60 years. We need a revolution in both efficacy, but also in the breadth and number of symptoms that we’re able to treat medically. It’s easy to study cancer. It’s easy to study inflammatory bowel disease because you just go in and you take some of it out and you look at it under the microscope. You just can’t do that with the brain. The most important clue that we have though about human traits in particular and about human psychiatric disorders, comes from genetics.\nFor example, we know the adolescent brain is a period of vulnerability in a lot of neuropsychiatric disorders including schizophrenia, but we don’t know why. So there were all kinds of theories of schizophrenia which may or may not be true. The largest single risk factor genetically, at least in European populations, turns out to be a gene that encodes complement factor four or C4. In the brain, the role of complement proteins is to mark individual synapses that are weak or inefficient. One possible thought about what causes schizophrenia: Something having to do with these complement proteins inappropriately signals to these brain immune cells to remove synapses that actually you needed for your cognition, for thinking, for your processing of information from the world. The actual risk gene itself is very rarely a good target, but it gives you insight into the pathway or the mechanism by which that disease is manifest. So if you march up the pathway, or down the pathway, or sideways into other pathways that kind of impinge on this particular pathway, you can usually identify more tractable targets for drugging.\nIt’s still early days but the whole point of doing the genetics and the difficult follow on biology ultimately is therapeutics, to make a diagnosis ideally very early, to intervene ideally to prevent the disease from getting going. Certainly that is my hope and actually that’s my mission, to improve the lives of people with serious psychiatric illness. After all my own family is affected with that and I have seen firsthand the suffering from it. So, I’m fully committed to it and actually very optimistic."
  },
  {
    "objectID": "chapter5.1_transcript.html",
    "href": "chapter5.1_transcript.html",
    "title": "5.1 Quality Control (Video Transcript)",
    "section": "",
    "text": "Quality control: Introduction\nTitle: Quality control\nPresenter(s): Katrina Grasby (katrina.grasby@qimrberghofer.edu.au) and Lucía Colodro Conde (Lucia.ColodroConde@qimrberghofer.edu.au), from the 2021 International Statistical Genetics Workshop hosted by the Institute for Behavioral Genetics at the University of Colorado, Boulder.\nThanks for joining me for this session on Quality Control. In this recording I’m going to be talking about the quality control or QC steps that we apply to genetic data. So this is in the very early stages of a study. We’ve collected our DNA, it’s being transformed into data. We’re going to clean that data up and then we will impute and then we can do our statistical analyses. So there’s many points in a study that will be applying QC, but these steps that we’ll be discussing here and in the tutorial, are the quality control steps that we apply to our genetic data.\nWhy do quality control?\nSo why do quality control? Essentially, poor quality data is going to contribute to false positives and false negatives in our results. So we want robust results. We’re going to need to clean our data up. So we’ll be removing essentially genotyping errors. These can be errors in the calling of genotypes, or the translation of DNA into data. They can be due to lots of different factors. One of the pictures that I like to bring to my own mind was a story given to me by a woman that I work with who was involved in a project where they posted out two spit kits to a couple who were participating in a project, and somewhere in that delivery one of the kits went missing or was damaged. And the couple thought or were trying to be helpful and both of them spat into the same kit and posted that back to us --to her. In doing so they also included a letter to say what they had done, but it was a classic example of DNA contamination. It’s an example of human error. After all, we ended up with no usable data from two people instead of having usable data from one person. There is no way that we can disentangle that DNA in that spit kit and say this belongs to that person and this belongs to that person. It’s also an example of contaminated DNA, and even if they had not included a letter to say what they had done, the steps that we will go through in the tutorial would be able to identify a problem like this. So we can actually go OK, this isn’t a clear indication of data from a person, a specific person. We can remove that it doesn’t interfere with our analyses.\nSo one of the other things that will be doing in the tutorial is, after we’ve cleaned up our data, we’re going to have a look at the relationship structure within our data, and whilst that’s not necessarily a quality control step, it is a necessary aspect of coming to understand our data so that we can apply appropriate analyses and that is going to be important for minimizing our false positives and false negatives. So how do we go from DNA to data?\nDNA to data\nI’m a behavior geneticist. I use statistics to analyze data. I have no experience working in a laboratory, actually processing the DNA into data. But it is still useful for me to have an idea of these many different steps that are involved and an appreciation of what are the possible sources of error and what exactly does my data represent. So we are able to post out spit kits to participants who can spit into that kit at home and post it back. The sample is then processed so that the DNA is fragmented, it’s chopped up into little pieces. And then it’s amplified, so we’ve got more of it. And then DNA is extracted. We can store some and then we can plate some on to SNP chips or genotyping arrays. For it to be then further analyzed. So this down the bottom here. These images come from the Illumina website. This is an example here at A of a SNP chip or a genotyping array. So there are many different forms of SNP chips. The technology has improved overtime and I’m sure it will continue to improve. This here is an example of the bead technology. On this particular chip, there is space for information, DNA, from 12 different individuals. These horizontal bars here are each [for] a different individual. Now, if you’re thinking then, looking at this SNP chip, then if you got information from multiple individuals, and you’ll have many chips and they might be sent off to a DNA a genotyping company for processing in different batches. If you are thinking from an experimental point of view, and when you’ve got cases and controls, you want to have your cases and controls randomly allotted to both the chips and the batch runs that they’re being processed under. In a similar way, if you have males and females you want to randomized them across your chips and also your batch runs. That way we can ensure that we can actually pick up any particular batch effects in our data once we’ve got our data at the end.\nSo back to the chip. For each of these individuals, there will be hundreds of thousands of probes in order to test the alleles at hundreds of thousands of points in the genome. Many many many loci. So each of these wells have has a bead. This here is a schematic of a bead. So this bead is targeting an allele at a locus. So it has a particular sequence here, an address, so this is the order of bases. And then this here is the locus of interest. So once you’ve got your fragmented DNA, it’s going to come along, if it’s the right location in the genome, it will bind to this bead and then depending on if this allele here bonds to this C, so G will bond to C, this bead will fluoresce green. A different bead it might bond to, if there’s an A at that location and a T here, then it will bond this way and it will fluoresce red. So this is how we’re establishing at that locus. You might have a G or you might have a T and if it’s a G it’s going to bond to the C and fluoresce green. If it’s a T it will bond to the A and fluoresce red. So this is translating the DNA into a color, it is called an intensity. So if you’ve got you’ve got DNA coming from your biological father from your biological mother. You’ve got two alleles at that locus. If your two alleles are the same, you have two G alleles. They’re both going to be fluorescing green. Nice solid green color. If they both, if you’ve got two T alleles, they’re both going to be bonding to these A beads. They’re both going to be a nice solid red color. If you’ve got a G coming from one parent and a T coming from the other, then some of the beads that are C you’re going to fluoresce green. Some of the beads that are A are going to fluoresce red and that person is going to be heterozygous and they’ll have this yellow color. So these colors are then representing the three possible genotypes at that locus in the genome. And then these here are for hundreds of thousands of different loci in the genome. What I’ve got in this particular slide are examples of genotyping intensities. genotyping intensities So this is how we’re going to look at the color clusters representing the different genotypes. And see whether or not there are any problems. Now, this will likely, this is typically done by a genotyping company, you will probably not be doing this. But they will give you information about these first steps of quality control at this stage so you know what’s going on with your data. It’ll be there in a report from the company.\nThis top left-hand corner is a really good example of what we’re looking for. We have three nice, separated clusters. This is a homozygous A allele, This is a heterozygous group of individuals, and this is homozygous for the other allele. And in these two examples, with their little black Xs they are representing missing data. So missing data may not be terribly problematic if there’s just a little bit of missingness and it’s across all the different genotypes. However, if it is biased to one allele or one genotype, then that’s going to interfere with our allele frequencies in our sample, and that is going to mean that it’s not going to be representative of the population, it’s not representative in terms of how we can actually test for this genotype against this phenotype. We don’t want to have biased information about allele calling or genotype calling. Down here in the bottom left hand corner of it we have an example of a very rare allele. Sorry, a rare genotype, or it is a rare allele as well as a rare genotype. So there’s only one individual here who’s homozygous for the A allele. Very few heterozygous. In the middle down the bottom, this would be an example of a monomorphic group at this locus, so it really isn’t a useful locus for us to have genotyped. Or it could be that just this population is, there’s no variation in this population at this locus. And in the right hand bottom corner we have an example where there’s really been a failure to call the genotypes correctly. There is no indication of any red color, which is representing the heterozygous group of people. We’ve got these two kind of green clusters and the missingness is all off on this cluster it’s a complete fail.\nChecking the data\nSo the steps that we’re going to be going through with our quality control tutorial is we’re going to start off by checking the data. We’re going to have a look at the file format. How is data coded? How is missingness coded? We’re going to look at the build, so that we know what assembly our data is on. The genotyping company would have provided us with that, but you might not always have access to that information, so there are ways that we can check that out ourselves. This is a very useful resource, which we will use in the tutorial to do that. And... Knowing what build your data is on is very important, particularly for meta-analysis, but also if you’re going to do any follow-up analyses with, or follow-up work with, your results. We’ll be doing a sex check, which is to check that the sex that we can infer from the genetic sex check information is matching the sex reported by the individuals. So this check is looking at the heterozygosity of the X chromosome. And we have different expectations depending on whether an individual has one or two X chromosomes. So if the individual is reporting their sex and the genetic information comes back and it doesn’t match, and that happens for a lot of your sample, then you might have a problem with the information that is, matching your genetic information that has been returned after genotyping to your participant IDs. Bear in mind this is about biological sex and not about gender.\nGenotyping call rate\nWe will be checking for missingness. So there’s two types of missingness that will check for. One is this one, the genotyping call rate. This is where SNPs are missing information on individuals. So for each SNP we want to have information coming from most of our individuals. If there is too much missing data for that SNP, so too many individuals did not have information that was called correctly for that SNP, then that SNP might not be a good SNP for us to be using in our analyses.\nHardy Weinberg equilibrium\nWe will have a look at the Hardy-Weinberg equilibrium, to see whether or not our allele frequencies are matching what we expect. So this can highlight whether we’ve got some bias in terms of the frequency of alleles, or perhaps in our terms of calling genotypes appropriately, thinking back to those genotype Gwise intensities. Will be checking the minor allele frequency. So this is to make sure that we have enough information to do statistical analyses. If it’s too rare, then our GWAS is not the appropriate tool to use perhaps for this particular locus.\nSample Call Rate\nWe’ll be having a look at sample call rate. So this is another form of missingness. This is to say, do all of our individuals have information across almost all of their SNPs. So we don’t want individuals to be missing too much information across many SNPs.\nHeterozygosity\nWe’ll be looking at the proportion of heterozygosity. So this is a way of checking-- Think back to that sample where we had two people spitting into the same kit. That’s going to give us too much variation. There will be way too much variation in that DNA sample. So heterozygosity would be excessive. Inversely, reduced heterozygosity could be an example of inbreeding, but it could also just be that we had lots of missing data.\nReduced Heterozygosity\nSo that’s one of the reasons we’re going to check out our missingness first before we do our heterozygosity check. Because we don’t want to be making, or we don’t want to be setting ourselves up, to potentially making inferences that have social consequences that are negative. So if you’ve got missing data and that’s the reason you have reduced heterozygosity, you don’t want to end up looking at your sample going “oh, there’s lots of inbreeding here”.\nRelationship structure\nTowards the end of the tutorial, after we cleaned it will then have a look at the relationship structure in our data. So we might have lots of families or we might have extended families. We want to know whether or not our individuals are related so that we can apply the right type of statistical analyses.\nPopulation structure\nAnd finally, we’ll be having look at population structure or stratification. So that will be talked about more in another one of the sessions, but this is when we have a look at a little frequencies. There is differences in allele frequencies across different groups or different populations and that is an important thing for us to be aware of and to be including appropriately in our analysis. Elsewise, we’re going to get false positives and false negatives. If your population structure is also correlated in some way with your outcome of interest, that’s where we’re going to get a problem. And that’s when we’re going to talk about it in terms of population stratification.\nSo these are going to be out checklist for our key steps in QC that will be running through the tutorial.\n\n\n\nRunning Quality Control on Genotype Data\nTitle: How to run Quality Control on Genome-Wide Genotyping Data\nPresenter(s): Jonathan Coleman\nHello, I’m Joni Coleman and in this brief presentation I’m going to discuss some key points concerned with running quality control on genome-wide genotype data which is a common first step in running a GWAS.\nOverview\nI’m going to provide a theoretical overview, addressing the overarching reasons why we need to do QC. Highlighting some common steps, and discussing a few pitfalls the data might throw up.\nI'm not going to talk about conducting imputation, or GWAS analyses, or secondary analyses.  Nor am I going to talk at great length about the process of genotyping and ensuring the quality of genotyping calls. I'll similarly not go into any deep code or maths, however, if you are starting to run your own qc and analyses I recommend the PGC’s RICOPILI automated pipeline as a starting point. There are also some simple scripts on my group’s github that may be useful as well. They follow a step-by-step process with codes and explanations. We’re currently updating this repository, so look out for some video tutorials there as well.\nThe beginning: genome-wide genotypes\nSo here is our starting point. I’ll be using this graph on the top right several times through this talk, and this is a genotype calling graph with common homozygotes in blue, heterozygotes in green, and rare homozygotes in red. Hopefully your data will already have been put through an automated genotype calling pipeline, and if you're really lucky, an overworked and under-appreciated bioinformatician might have done some manual recalling to ensure the quality of the data is as high as possible.\nBut in point of fact the data you will be using won’t be in this visual form but rather as a numeric matrix like the one below, with SNPs, and individuals. This might be in the form of a PLINK genotype file or it’s binary equivalent, or it’s in some similar form that can be converted to the PLINK format.\nThe desired endpoint: clean, complete data\nWhere we want to go is clean data with variants that are called in the majority of participants in your study, and won’t cause biases in downstream analyses.\nThat should give a nice clean Manhattan plot from GWAS was like the one below rather than the starry night effect of this poorly QC’d Manhattan plot above.  However, something I’d like to emphasize across this talk is that QC is a data informed process, and what works for one cohort won’t necessarily be exactly right for another. Good QC requires the analyst to investigate and understand the data.\n\"Rare\" variants\nOften the first step is to remove rare variants, and this is because we cannot be certain of variant calls. Consider the variance in the circle on the right. Are these outlying common homozygotes or are they heterozygotes? We cannot really tell because there aren’t enough of them to form a recognizable cluster. Typically, we might want to exclude variants with a low minor allele count for example five. There are many excellent automated calling methods to increase the amount of certainty you have in these variants but it’s also worth noting that many analytical methods don’t deal well with rare variants anyway.\nAgain, the demands of your data determine your QC choices. It may be more useful for you to call rare variants even if you’re uncertain of them. Or you may wish to remove them and be absolutely certain of the variants that you retain.\nData missingness\nNext we need to think about missing data. genotyping is a biochemical process and like all such processes it goes wrong in some cases, and a call cannot be made. this can be a failure of the genotyping probe or poor quality of DNA or a host of other reasons but such calls are unreliable and they need to be removed.\nMissingness\nMissingness is best dealt with iteratively. To convince you of that, let’s examine this example data. We want to keep only the participants (which are the rows in this example) with complete or near-complete data on the eight variants we’re examining (which here are shown in the columns). So, we could remove everyone with fewer than seven SNPs, but when we do that - oh dear, we’ve obliterated our sample size.\nIterative Missingness\nSo instead let’s do things iteratively. So, we’ll remove the worst SNP again, variant seven goes, and then we remove the worst participant, bye bye Dave, then we remove the next first SNP, so that’s SNP two, and now everyone has near complete data and we’ve retained nearly all of our cohort. So this was obviously a simple example, how does this look with real data?\nReal data missingness\nSo here we have some real data, and it’s it’s pretty good data most variants are only missing in a small percentage of the cohort, but there are some that are missing in as much as 10 of the cohort. So let’s do that initiative thing removing variants missing in 10% of the individuals and then individuals who have more than 10% missing variants and then 9% and so on down to one percent. when we do this the data looks good. Nearly all of the variants are zero percent missingness and those that aren’t are present in at least 578 of the 582 possible participants, and we’ve lost around 25 participants for about 22 and a half thousand SNPS. but what if we didn’t do the iterative thing and we just went straight for 99 complete data.\nSo when we do that the distribution of variance looks good again, arguably it looks even better, and we’ve retained an additional 16 000 variants, but we’ve lost another 40 participants which is about six percent more of the original total than we lost with the iterative method. Typically, participants are more valuable than variants which can be regained through imputation anyway, but this again is a data-driven decision. If coverage is more important than cohort size in your case, you might want to prioritize well-genotyped variants over individuals.\nHardy-Weinberg equilibrium\nSo we’ve addressed rare variants where genotyping is uncertain, and missingness where the data is unreliable. but sometimes calling is simply wrong and, again there are many reasons that could be. we can identify some of these implausible genotype calls by using some simple population genetic theory. so from our observed genotypes we can calculate the allele frequency at any bioluelic snip we’ve called. so here the frequency of the a allele is twice the frequency of the AA calls (those are our common homozygotes in blue) plus the frequency of AB calls (our heterozygotes in green) and we can do the equivalent as you see on the slide for the frequency of the B allele.\nKnowing the frequency of the A and the B allele we can use Hardy and Weinberg’s calculation for how we expect alleles at a given frequency to be distributed into genotypes, to generate an expectation for the genotypes we expect to observe at any given allele frequency. We can then compare how our observed genotypes i.e the blue, green, and red clusters fit to that expectation, and we can test that using a chi-squared test.\nNow Hardy-Weinberg equilibrium is an idealized mathematical abstraction, so there are lots of plausible ways it can be broken, most notably by evolutionary pressure. As a result, in case control data it’s typically best to assess it just in controls, or to be less strict with defining violations of Hardy-Weinberg in cases. That said, in my experience genotyping errors can produce very large violations of Hardy-Weinberg, so if you exclude the strongest violations you tend to be removing the biggest genotyping errors.\nSex mislabelling\nThe previous steps are mostly focused on problematic variants, but samples can also be erroneous. One example is the potential for sample swaps, either through sample mislabeling in the lab, or correctly entered data in phenotypic data.\nThese are often quite hard to detect, but one way to detect at least some of these is to compare self-reported sex with X chromosome homozygosity, which is expected to differ between males and females. In particular males have one X chromosome, they’re what’s known as hemizygous so when you genotype them they appear to be homozygous on all SNPs on the X chromosome. Females on the other hand have two X chromosomes, they are holozygous, and they have a normal X distribution centered around zero which is the sample mean in this case. you could also look at chromosome Y SNPs for the same reason, however Y chromosome genotyping tends to be a bit sparse and is often not of fantastic quality, so there are benefits to using both of these methods. it’s also worth noting that potential errors here are just that - potential. Where possible it’s useful to confirm these with further information. For example if there isn’t a distinction between self-reported sex and self-reported gender in your phenotype data then known transgender individuals may be being removed unnecessarily. The aim here is to determine places where the phenotypic and genotypic data is discordant, as these may indicate a sample swap, and this might indicate the genotype to phenotype relationship has been broken and that data is no longer useful to you.\nHomozygosity and the inbreeding coefficient\nAverage variant homozygosity can also be applied across the genome, where this metric is sometimes referred to as the breeding coefficient. it’s called that because high values of it can be caused by consanguinity. related individuals having children together, which increases the average homozygosity of the genome. there can also be other violations of expected homozygosity, so it’s worth examining the distribution of values and investigating or excluding any outliers that you see.\nRelatedness\nExamining genetic data also gives us the opportunity to assess the degree of relatedness between samples. For example, identical sets of variants implied duplicates or identical twins. 50% sharing implies a parent offspring relationship or siblings, and those two things can be separated by examining how often both alleles of a variant are shared. Specifically, we would expect parents and offspring to always share one allele at each variant, whereas siblings may share no alleles, they may share one allele, or they may share two alleles. lower amounts of sharing imply uncles and aunts, and their cousins, and grandparents, and so on down to more and more distant relationships. in some approaches to analysis, individuals are assumed to be unrelated, so the advice used to be to remove one member of each pair of related individuals.\nHowever, as mixed linear models have become more popular in GWAS, and mixed linear models are able to retain and include related individuals in analyses, related individuals, therefore, should be retained if the exact analysis method isn’t known. Again, it’s worth having some phenotypic knowledge here. Unexpected relatives are a potential sign of sample switches and need to be examined, confirmed, and potentially removed if they are truly unexpected. and once again it’s important to know your sample, the data shown in this graph does not, despite what the graph appears to suggest, come from a sample with a vast amount of cousins, instead it comes from one in which a minority of individuals were from a different ancestry and that biases this metric. I’ll talk a little more about that in just a moment.\nAverage relatedness\nRelatedness can also be useful for detecting sample contamination. Contamination will result in a mixture of different DNAs being treated as a single sample, and this results in an overabundance of heterozygote calls. This in turn creates a signature pattern of low-level relatedness between the contaminated sample and many other members of the cohort. These samples should be queried with the genotyping lab to confirm whether or not a contamination event has occurred, and potentially be removed if an alternative explanation for this odd pattern of inter-sample relatedness can’t be found.\nPopulation structure\nFinally, a word on genetic ancestry. Because of the way in which we have migrated across our history, there is a correlation between the geography of human populations and their genetics. This can be detected by running principal component analyses on genotype data pruned for linkage disequilibrium. For example this is the UK biobank data, you can see subsets of individuals who cluster together and who share European ethnicities, other subsets who share African ethnicities, and subsets who share different Asian ethnicities, and in a more diverse cohort you will be able to see other groupings as well. this kind of 2D plot isn’t the best way of visualizing this, for example here it isn’t really possible to distinguish these South Asian and admixed American groupings, and you don’t get the full sense of the dominance of European ancestry data in this cohort. The Europeans in this case account for around 95% of the full cohort but because of over plotting i.e. the same values being plotted on top of each other in this 2D plot, you don’t really appreciate that. Looking across multiple principal components helps for that.\nAncestry is important to QC. Many of the processes I've talked about rely on the groups being assessed fairly of being fairly homogeneous. As such, if your data is multi-ancestry it’s best to separate those ancestries out and re-run QC in each group separately.\nTake-aways\nSo that was a brief run-through of some of the key things to think about when running QC.\nI hope I've got across the need to treat this as a data informed process, and to be willing to re-run steps, and adjust approaches to fit cohorts. Although we’ve got something resembling standard practice in genotype QC, I think there are still some unresolved questions. So get hold of some data, look online for guides and automated pipelines, and enjoy your QC.\nThank you very much for listening, I’m doing a Q & A at 9 30 EST, otherwise please feel free to throw questions at me on twitter where I live, or at the email address on screen which I occasionally check. Thank you very much.\n\n\n\nConsiderations for Genotyping QC\nTitle: Considerations for genotyping, quality control, and imputation in GWAS\nAuthor: Ayşe Demirkan (a.demirkan@surrey.ac.uk)\nhello everyone my name is aisha demerka i’m affiliated at the university of\nroningam from the netherlands and university of surrey from the uk this is a pre-recorded lecture\nin the second lecture of on-demand sessions introduction to the statistical analysis of genome-wide association\nstudies i will be talking about considerations for genotyping quality control and\nimputation in genomic association studies jivas\nso here you see an overview of the lecture we will shortly go over genotyping platforms\nLecture outline\nand options quality control then i will talk about definition and purpose of imputation and how it is done\nand this is going to include reference data tools analysis of imputed data\nimputation accuracy and accusing\nGenotyping and platforms Genotyping is the process of determining differences in the genetic make- up (genotype) of an individual by examining the individual’s DNA Sequence\nwhat we call is genotyping is the process of determining differences in the genetic makeup\nhence the genotype of an individual by examining the individual’s dna sequence\nof course the technology used for genotyping depends on the structural properties of the genetic variation\nwhether it is a single nucleotide polymorphism or a copy number variation or other structural variations\nit also depends on the project rationale or scientific question and your budget\nmainly and related to that of course how many snips\nyou want to genotype if it is a genomic association study and number of individuals you would like to include\ndepending on your study design you will also be limited with your dna\nsample quality and quantity\nso here on this slide you see the most common approach used for genotyping\nCommon approaches\nsynips and depending on your study you will be most likely using one of these what are those illuminati matrix arrays\nso on the y-axis you see the number of snips that are easily captured by the arrays and on the x-axis you see the\nnumber of individuals and then what do we have we have pcr rflp sequence\npyrosequencing and fluidicum platforms and tacman\num for instance one of the best examples are the illuminae arrays for whole\ngenome scans um whole genome genotyping by these arrays provide an overview of the entire\ngenome and enable you know white discoveries and associations so you using a high throughput\nnext generation sequencing and microarray technologies you can obtain a deeper understanding of the genome\nbecause you are covering a very wide proportion of the genome so you can use\none of their selection of this illumina or f metrics arrays which you think may be suitable for your study\nthere are many options and so for s4 illumina there are genome-wide genotyping is for 18 species\nat the moment so number of markers on each array it changed by products for human up to four minion markers per\nsample are possible now and then there is an infinium low cost screening\narray so for this one for example includes 600 000 markers on it\nyou can use start from 200 nanogram genomic dna and what you can also do you\ncan add some custom marker panels there is an add-on capacity up to\n50 50k markers\nand then there is this omni family of illumina arrays\nOmni family of Illumina arrays\nhere you see a simple description of their coverage and the inclusion of genetic markers in relation to their\nminor alleged frequencies so these expressed chips on the left include only common variation with minor\nlife frequencies higher than five percent some include cmes and some include snips with lower minor allied\nfrequencies so which one to choose among those will depend on your question research\nquestion and population you want to screen for instance are you looking for a rare or common variation in terms of\nsnips are you looking for cmes are you looking are you working with a rare or common disease and what is your sample\nsize and your budget now\ni listed some websites here please take 10 20 minutes to check on\nthe technologies mentioned in the first section using these websites\nQuality control (QC) of genotyping From machine to dataset: genotype calling\nnow let’s talk about genotyping quality control qc you designed your study you chose a\nproper array platform service you used or you used a service from your institute\nso one critical initial step from chemically induced intensity signals and data analysis is a transfer and\nqc of genotypes determined towards your computer this critical step is called genotype\ncalling so genotype calling algorithms are always implemented in the probability\nsoftware accompanying the genotyping platform you choose so you don’t need to invent them yourself\nso it’s typical calling software uses a sort of mathematical clustering\nalgorithm to inter to analyze the row intensity data and it estimates the probability that\ntheir genotype is one of the a a a b or bb for a given individual for a given b\nallelic marker locus so one method of checking initial synopquality is visually inspecting the\nintensity clustering of a particular snip in the overall population and depending on this one can decide\nwhether a snip is characterized by a clear signal or not so\nhere on the left of these figures you see a clear intensity clustering of a\nscene in the population so you see that the common variant is\ndepicted by red on the left there is some hetero heterozygous in the middle\ndepicted by purple and the homozygous um people for the less common allele are\ndepicted as blue and the table on the right it shows the row values that this plot\nis figured from so here the plot shows a tight\nclustering of genotypes and there is not moist much noise in the measurement of\nthis cinema following that there are imputed the\nimportant data qc steps one of them is\nto work on uh replicates so for inspecting plating issues and by\nlooking at you know type concordance this would be a a good thing to do to include the same\ndna sample on different batches of experiments and then there are mendelian areas to\ncontrol for for instance transmission and the inconsistencies for example snips with more than 10 percent manual\narea rate can be excluded this would be based on the number of trios that you would include in your\nexperiment unfortunately this option obviously is only available for family\nbased and trio designs only another thing another qc measure we use\nis snip call rate this is basically the missing genotype rate\n1 minus the missing genotype rate per snip so this can depend on the quality of tsa\nand this is generally between 95 percent and 99 is this is a very standard thing\nto include in your qc another thing is the hardy weinberg\nequilibrium uh deviance of your snip so this is another method for checking the quality and\nexclusion of this of your snips this will be explained in the next slide another one is the sample call right so\nthis is a sample based uh qc method this is a good indication of sample\nsuccess so different platforms have different thresholds but this will this is will be mainly\ndetermined by your initial dna quality and uh it will somehow will be in\nrelation to with a snip color so once you do snip call rate you could do sample call rate and you may want to\nrepeat snip call rate depending on that and another thing to do is sample gender\ncheck for this quality measure you need x chromosome information to calculate this\nand you may want to add this as an additional sanity check in your data to make sure that there is\na perfect overlap with your phenotype files in terms of sex and\nanother important one is sample heterozygosity this is to check for example outliers\nfor example samples with more heterozygosity than expected um can be an indication of contamination\nin your samples and you also want to do something in on top of all of that you\nyou need to check samples cryptic relatedness and unexpected uh twinning\nand whether there is actually a relatedness and structure in the data\nbut this will be more covered in the lecture of redik magi\nso let’s talk about hard wineback equilibrium shortly so as occurrence of uh two allies of a\nHardy-Weinberg equilibrium\nsnip in the same individual are two independent events the distribution of the genotypes across\nindividuals should be more or less in equilibrium with the frequencies of the alas of a b allelic snip\nso this is only possible in ideal conditions of course which would be random mating\nno selection equal survival no migration no mutation and selection based on\nmutation no inbreeding and large population size\nso under these conditions above deviations from high divine back equilibrium is an indication of\ngenotyping calling problems and a commonly used threshold for\ngenotypes variance is a p value of hardy weinberg equilibrium uh that is less than ten to the minus five\nis an indication of a deviation from hardy weinberg equilibrium and you may\nwant to take a look at these snips or you you may want to exclude them from your\ndata set another important thing to always\nGenome builds and alignments\nconsider is genome bills and alignments so the characterization of the human\ngenome is an ongoing effort and a genome build tells us the positions of the snips in the genome on\nthe genome so the latest build is called build 38 but the most commonly used one at the\nmoment is still built 37 for instance the head map was released on build 35\nand bill 36 so you need to be aware of issues relating to merging and meta-analyzing\ndata from different genome builds also for when preparing your data for\nimputation this is very important because you need to make sure that your data is coded according to the same\ngenome build between the target set and the reference data set\nso there are tools uh for that one they are called liftover tools for instance there is one from oxford that we use for\npurpose and i provide the link to that here\nCommonly used software for QC plink...\nso all of these qc steps i shortly went over here are pretty standard and there\nare a couple of widely used tools one very commonly used tool that we also\nuse for data storage analysis and qc is called plink\nhere on this slide i made snapshot of some of the blink\noptions that i also covered during the lecture and these functions are implemented in\nthe plink software and you can use it for the qc of\nyour genetic data so first thing to be able to use bling\nto obviously install plink and you will need to read your genotype call data in plink in the form of a map or pad files\nand then you can perform qc at the snip level remove or extract snips and you\ncan perform qc at the sample level you can remove or extract individuals and under the summary statistics option\nhere there are functions listed to check for call rate missingness hardy-weinberg\nequilibrium highlight frequencies and mandel errors you can also perform\nsex checks what billing can also do is to extract genetic principle components and\nidentify cryptically related individuals or twinnings in the data\nand and the genetic structure of the data and uh can be which you can then use to determine ethnic outliers in your\ndata sets i will not talk about this because this is a part of the lecture of redick muggy\nof the next session now\nIntermezzo\nhere i put two websites here one of them is for\nblink and how to use blink for qc and the other one is\nfor a beat studio which i mentioned is one of the algorithms that you could\nuse for a genotype calling so now take 10-20 minutes to have a look at\nthis website and try to grasp what you can do with them\nGenetic data missingness\nnow let’s talk about imputation why do we need imputation we need imputation to\naddress missingness in the genetic data this is all about missing values in the genetic data where do the missing values\ncome from so during the qc we already set some values to missing right and also during\ngenotype calling you could set some data points to missing but actually most of\nthe missing values come from the initial targeted coverage of the genotyping\nchips and platforms we used so remember that there are many types of arrays some of more dense less dense\nthere are arrays made specifically for oncological studies like onco arrays there is a metabolic\nchip that is designed for metabolic disease especially and there are areas focused on focusing\non mainly snips with higher minor area frequency or their whereas focusing on cnvs\nbut even the dense snip areas do not cover all of the genetic variation they\ncover much less than you would imagine and in addition to that snips included in one array may not be included in the\nother one and for many variable positions on the genome we do not have\nmatching information across genotype set of individuals for instance\nlook what i try to depict here i think of three individuals\nfirst two are typed on the array x and the third one is typed on array y\nso hence they have different missing data points and when you try to\npull their data for a pooled analysis or to be using meta-analysis you’re going\nto have even more missingness in this data because of the non-overlapping positions\nand you will not be able to replicate findings from one of data set and in the\nother one so additionally we will be analyzing only half of the\ngenetic variation and we may miss causal variance in the analysis this is this is\nbecause of all these reasons we use a genetic data imputation\nImputation principle\nso what do we do in principle in principle it means estimating the most likely genotypes in\nan individual at the missing positions by looking at the correlated snip values\nfrom a more complete data set and based on that writing the\nwriting over the missing values in the target data set so how does it work\nfirst of all we need a data set where dense genotypes are directly measured this can be a density array or it could\nbe a set of sequence individuals this we call a reference panel\nthen we use an imputation software or service and by looking at the correlation structure of the density\ntypes or sequence snips we estimate them in the target data set\nso at the end these are probabilities and we end up with dosage information for alleles or\ngenotypes rather than hard genotypes calls and this dosage information\nwhich accounts for the immunity in the estimation is then included in the\ngenomic association study analysis so to sum up the purpose of imputation\nPurpose of imputation\nis to increase power because obviously the reference panel is more likely to contain the causal\nvariance than a less dense geos array to improve fine mapping because\nimputation provides a higher resolution overview of an association signal across\na locus and then to enable meta-analysis because imputation is going to allow viewers\ntyped with different arrays to be combined up to variance in the reference panel\nHistorical milestones 2010-2018\ngoing over the historical milestones in terms of imputation also summarizes the\ntheoretical and technological advancements in human data immune genetic data imputation\nso one important advance in all of these was the generation of reference panels so the first reference panel was hepmap\nand the headmap2 was the most commonly used release of hapmap it consists of a limited sample of individuals from\ndiverse genetic backgrounds 60 yoruba indians 90 hein chinese and japanese and six\nindividuals that were utah residents descending of european ethnic origin\nnow it sounds funny to think that we imputed thousands of people based on the genetic material of 60 euro residents\nonly talking about the europeans but actually this yielded a lot of success and actually this is what we had um only\nfor a long time so we could only impute up to 3 million\nsnips with headmap at the time and then came the 1000 genome reference panel which included at the\nend 2500 individuals from multiple ethnic groups\nand later on and currently the most widely used reference panel is the panel\nof haplotype reference consortium hrc recall shortly this is a combined set of\nwhole genome and exome sequence data for more than 30 000 individuals and use 39 million\nsnips after imputation of course this this is going to depend uh this on the\nscaffold that you use for imputation as well many of these snips will not be include imputed with the good quality\nbut in the ideal conditions you can go up to 39 million and finally we now have a reference\npanel from the transomics for precision medicine topmed program and this consists of almost 100k deeply sequenced\nhuman genomes and it can yield up to 308 genetic variants\nto be identified one technical milestone is mentioning\nwas prefacing of haplotypes so genetic imputation is a highly\ncomputationally intensive process because of the probabilistic framework and high rate of missing data that we\nare trying to deal with one of the major milestones is to reduce the computational\nburden was introduction of prephasing so this idea involves a two-step\nimputation process so there is one initial step of previousing which is actually haplotype\nestimation of the geos genotypes and a subsequent step of imputation into the estimated steady haplotypes\nso this reduces complexity of the imputation process and speeds it up the current version of all imputation\nsoftware can deal with the prefacing approach\nand what is very important is a choice of reference panel\nso it is shown that making use of the all ancestry’s reference panels rather than\nethnic specific reference panel improves imputation accuracy for rare variants in\nany population and formatted reference panels for impude and minimax can be\ndownloaded from the software websites and it’s very important to make sure\nthat genotype scaffold and reference panels are aligned to the same build of the human genome i will get back to that\nlater as well\nso another and very important and current technological advancement that makes our\nlives easier is the imputation services these are freely available services such\nas the michigan and sanger imputation services you can simply format and upload your data in a secure way to this\nserver and get the data imputed and face genotypes back in a few days\nand this depends on the speed and how busy the server is and depending on the\nsample size you are trying to impute of course\nHistorical milestones -Sanger\nso in parallel to michigan imputation server uh there is also a sanger institute uh has\na similar service in this service also you can upload your data in a vca format and optionally\nperform pre-phasing using beagle or shaping software and current reference panels\nin the sanger imputation server includes hrc uk 10k and 1000 genomes\nas i said there is also a server dedicated to the\ntopmat this is all very self\nexplanatory uh this is how uh the sanger imputation server would like\nyou to prepare the data so there is a whole a bunch of instructions there that you would like to use\num so the use of these services comes with instruction and manuals so feel free to make an account there and run\nsome test data says in there you will need to qcn format the data as required\nin the instructions you will need to match the coordinates and reference level of the genome bills and prepare one file for each chromosome this is for\nsanger imputation server and another important thing in terms of imputation is of course the speed\nSpeed-Impute 5 PLOS GENETICS\nso increasing reference panel size improves accuracy of markers with low minor allele frequencies but\nthis positive every increase in computational challenges for imputation methods so recently a new imputation software\ninput 5 was introduced from the same group so it does memory efficient imputation by selecting haplotypes using\nthe positional borrows wheeler transform so using hrc reference panel\nthe developers of the software uh showed that input 5 is up to 30 times faster\nthan minimax 4 and up to 3 times faster than a beagle\n5.1 and uses less memory than both of these methods\nExample framework\nso using all the mentioned considerations up until now you can\nbuild an insico framework similar to this one so you can use for instance blink functions for the first two steps\nof genetic data qc then you can check cheap information and\nstrength issues using rhino tools and if needed you can update your genome\nbuild by using leftover tool and you can then preface by using shape it and\nfinally imputed in-house or using one of the servers are mentioned so two links\nto this software are given here\nnow take time\nprobably hours to explore these three imputation services uh of\nsanger michigan and topmatz uh you will be asked to make an account\nand perhaps it will be need to be improved so take your time to do so\nImputation QC\nand the next topic is imputation related qc so there are two qc steps um\naround imputation one is pre-imputation qc um so we have already discussed standard\nqc after genotyping and on top of that you you may want to exclude snips with less than one percent minor allyl\nfrequency and a post imputation quality is assessed\nby information measures which is in some value in the range of 0\nto 1 and it is typical to filter snips by this\nvalue less than 0.8 for a strict filtering or less than 0.4\nand in the impute software this is called infoscore and in the minimax\nimputation software this is called r square per snip so it’s important to check quality of\ntype snips in the scaffold in the region also by visual inspection of cluster\nplots and you may also want to produce quality plots per chromosome\nvarying by minor allele frequency strata and a position\non the chromosomes for instance is an example figure this\nImputation quality vs MAF\nshows a typical relationship between minor allyl frequency and imputation quality\nso on the y-axis you see the imputation accuracy as a determined by imputation\nquality as by r square on infoscore from different softwares and on the\nx-axis you see the minor allele frequency you see that the accuracy is top when\nthe minor alarm frequency is high when the allyl is common and more common and then the accuracy\ngoes lower where the minority frequency goes lower as well you still have some\nsnips which you still have some well-imputed snips\namong the rare ones as well but most of the low quality snips are going to come from a low minor added frequency\nsnips so keep in mind that when you filter by imputation quality you will be filtering out a lot of rare snips as\nwell so what are the factors affecting imputation quality\nFactors affecting imputation\nso at the genome-wide level the number of individuals imputed has something to\ndo with it for this reason we merge scaffold data sets before imputation if we are going to impute\nmore than one so the the more the merrier\nand the second factor is the reference panel\nthe choice of the reference panel as well as the whole idea is to use the correlation between snips\nacross different populations and this may be different from population to population\nyou want to go for a large multi-ethnic panel if you’re not able to go for a\nlarge ethnic specific panel and uh finally at the snip level uh the\nlower the minor life frequency the lower the quality of the imputation\nis going to be and how to analyze the imputed data\nAnalysis of imputed genotypes\nso for each individual imputation provides probability distribution of\npossible genotypes for each untyped variant these properties can be converted into\nbest guest genotypes but this is not something really generally recommended as it increases false\npositives and it reduces power but also you want to filter your best\nbest guess genotypes you want to put a strict filtering on the best uh guest genotypes and this would result\nmore nas in your data set so it’s better to convert the uh\nprobabilities to expected allele counts and analyze uh by taking the uncertainty\nin the imputation into account that’s really important and to do that you need to match the\ndata formats to the software not all software uses all types of data and you may need to do\ndata conversions um and um software called epex snip test to\nand link to supports the knowledge information and you need to check the lecture from\nmedic magi for analysis of genome-wide data\nMessages\num so this is the last slide of this lecture so the take-home message is that\nis we are dealing with hypothesis free approaches here unfortunately it all comes down to the bittersweet money and\nuh resources we have so you need to think what is the best and most cost effective way of getting\ngenetics done in large sample size and the answer is combining a dense genome scan array with imputation as the reference panels are free at the moment and cost of arrays are going lower as well but you really need to think as the in silico part of doing so also will cause staff and a computational resources to some level and what else should you consider so in comparison you want to know depending on your research question of course whether there is a better array for you or perhaps an array or a metabold chip if you are going to conduct your research in a in a very restricted field and the most importantly what are the future uses of this data because obviously you don’t want to build something that you’re going to use only a couple of for only a couple of years and finalize the research on that you want to you ideally want to invest in big data uh so are you gonna invest in population based cohort or disease-based cohort is it going to be a short-term project or is it going to be a follow-up study that’s going to be likely build up and extended extended throughout the years and by inclusion of new phenotypes so and finally who do you want to collaborate with which consortia which disease\num yeah so i hope this lecture will be useful for your research and future studies and for the people who are interested and to have a better and in-depth understanding of imputation every year two times we have a jivas course organized by university of surrey in collaboration with imperial college and university of tartu from estonia this is a hands-on this includes a hands-on workshop as well as theoretical uh lectures where we teach these concepts and matters in more detail so the last one was in five to ten july 2021 and for information there is an email address you can connect to thank you very much and have a nice conference of the remaining time"
  },
  {
    "objectID": "chapter4.2_transcript.html",
    "href": "chapter4.2_transcript.html",
    "title": "Chapter 4.2: Confounding, Chance, and Bias (Video Transcript)",
    "section": "",
    "text": "Title: Confounding, chance, and bias\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nThe goal of clinical studies is to investigate the efficacy and safety of treatments. They leave room for three important sources of error that could lead to false study results: confounding, chance, and bias. Confounders confuse us in interpreting study results and may lead us to infer incorrect conclusions about cause and effect relationships. For example, in one study, we observed that those who drink a lot of coffee are at an increased risk for coronary heart disease. Is coffee dangerous for heart health, or could another factor be responsible for this relationship? In our example, smoking is responsible for the observed relationship. Smoking is a risk factor for coronary heart disease, and people who smoke tend to be people who drink a lot of coffee. So, taking the confounder, smoking, not into account, one would draw the wrong conclusion that drinking coffee is a risk factor for heart disease. We are dealing with the confounder.\nProperties of confounders (= confounding factors)\nIf the following properties are present, the confounder must be related to the outcome regardless of the exposure. In our coffee example, this means that smoking is an independent risk factor for coronary heart disease, regardless of whether someone drinks coffee or not. The confounder must be related to the exposure. In our example, this means that people who smoke often often drink coffee. The confounder should not be on the causal pathway between the exposure and outcome. This condition would also be fulfilled in our example: drinking coffee does not automatically lead to smoking. How do you deal with confounders?\nHow do you deal with confounders?\nYou can get a grip on confounders that you know in advance in the planning phase. For example, the inclusion criteria of a study are defined so narrowly that the influence of confounders is eliminated. This procedure is called restriction. One can also consider confounding in the analysis. For example, with stratification, in our coffee study, we would stratify all individuals into groups; that is, divide them into smokers and non-smokers and examine in both groups if there is a connection between coffee drinking and heart disease. If only people who smoke are at an increased risk for heart disease, then we would see that drinking coffee is not the cause of heart disease. We are mostly dealing with several confounders here. Multivariate analyses can be calculated, which adjust for several confounders. But what do you do with confounders that are unknown to us?\nWhat do you do when a confounder is unknown?\nThe only way to deal with unknown confounders is randomization. The study participants are randomly divided into study arms. The randomization leads to an equal distribution of known and unknown confounders in the study groups. If one observes a difference between the groups after an intervention, we can assume that this was caused by the intervention and not because influencing factors were already distributed differently between the groups at the beginning. Randomization is the only effective remedy against unknown confounders.\nChance\nAnother source of error in studies is chance, also called random error. The result of a study may coincidentally differ from the true effect in the population just by chance. This random deviation has no definite direction. If you were to do many of the same studies, some would overestimate the effect, others underestimate it, and others would estimate it correctly. One can minimize the influence of chance by having a large sample size.\nImagine you have a bowl of 500 green and 500 red gummy bears. You draw two samples, one with ten gummy bears and one with a hundred. In the small sample, it can be easy for you to draw eight green and two red gummy bears. With 100 gummy bears, you are probably already much closer to the 50-50 distribution. It’s similar with studies. In very small studies, it may happen, despite randomization, that the study groups are not similar. A non-evidence-based rule of thumb states that studies with fewer than 300 study participants are susceptible to random variability. Additionally, a low event rate is prone to random error. A large study size can minimize random errors.\nBias is another source of error in studies. Bias is a systematic deviation from the true effect that can be caused through the design, conduction, or analysis of a study. In contrast to random error, bias would always distort the results in the same direction if the study were performed multiple times. There is a variety of bias subcategories, and here are four types of bias that play an important role in interventional studies: selection bias, performance bias, measurement bias, and attrition bias.\nSelection bias\nSelection bias describes the problem of systematic differences in the allocation of study participants. For example, imagine a study that examines the effects of a physical exercise program versus no intervention, and study participants are allowed to choose which group they want to be in. It is very likely that more sports enthusiasts and health-conscious individuals would opt for the sports group. As a result, two groups would be formed that are not comparable from the start in some aspects. If, at the end of the study, it emerges that the sports group had better results, this cannot be automatically attributed to the intervention itself, as the group already had better starting conditions. Selection bias can be avoided by employing randomization and ensuring secrecy of the randomization sequence, also known as allocation concealment.\nPerformance bias\nPerformance bias occurs when, apart from the intervention under investigation, there are systematic differences in the treatment and care of patients. For example, the type of care and attention changes with the intravenous administration of a medication. So, if you compare two drugs that are administered differently, it remains unclear whether a different effect is due only to the drug or to the other care and attention given. In order to prevent performance bias, it helps to have a standardized treatment concept for all study groups and to blind all people involved in a study. Blinding means that the participants in a study are not aware of which people receive which treatment.\nMeasurement bias\nMeasurement bias exists when there are systematic differences in measuring outcomes. For example, if a person evaluates their knee pain, knowing if they were in the intervention or comparison group can influence the perception of the pain. To prevent measurement bias, those who measure the outcomes should not know which individuals receive the intervention and which receive the control intervention. This allows them to assess the outcomes unaffected and objectively.\nAttrition bias\nPremature exit from the study results in systematic differences between study groups. In general, it is normal for some people to leave studies early. However, it becomes problematic if this happens systematically and not accidentally. Suppose those who feel particularly unpleasant or have many side effects leave the study. If, at the end of the study, we only look at those in the analysis that remained in the study until the very end, we would ironically believe that there were very few side effects. One can minimize the influence of attrition bias in the data evaluation by providing an intention-to-treat analysis that includes all individuals originally randomized to a study.\nRisk of bias\nWhen we critically evaluate studies, we try to estimate the risk of bias. Bias cannot be measured directly. The risk of bias can only be assessed indirectly through evaluation of the study design and the execution of studies. In addition, the risk of bias between outcomes may vary. For example, while subjective outcomes such as pain can be strongly influenced by lack of blinding, this has no effect on hard outcomes such as mortality.\nComponents of a study result\nThe aim of studies is to map the true effect of an intervention as well as possible. That is, the distorting influence of confounding, chance, and bias should be contained as much as possible. A large sample can minimize the influence of random error. The study design, the good execution of the study, and the analysis can minimize the influence of confounding and bias."
  },
  {
    "objectID": "chapter4.2_transcript.html#confounding-chance-and-bias",
    "href": "chapter4.2_transcript.html#confounding-chance-and-bias",
    "title": "Chapter 4.2: Confounding, Chance, and Bias (Video Transcript)",
    "section": "",
    "text": "Title: Confounding, chance, and bias\nPresenter(s): Cochrane Austria, Department for Evidence-based Medicine and Evaluation, Danube University Krems\nThe goal of clinical studies is to investigate the efficacy and safety of treatments. They leave room for three important sources of error that could lead to false study results: confounding, chance, and bias. Confounders confuse us in interpreting study results and may lead us to infer incorrect conclusions about cause and effect relationships. For example, in one study, we observed that those who drink a lot of coffee are at an increased risk for coronary heart disease. Is coffee dangerous for heart health, or could another factor be responsible for this relationship? In our example, smoking is responsible for the observed relationship. Smoking is a risk factor for coronary heart disease, and people who smoke tend to be people who drink a lot of coffee. So, taking the confounder, smoking, not into account, one would draw the wrong conclusion that drinking coffee is a risk factor for heart disease. We are dealing with the confounder.\nProperties of confounders (= confounding factors)\nIf the following properties are present, the confounder must be related to the outcome regardless of the exposure. In our coffee example, this means that smoking is an independent risk factor for coronary heart disease, regardless of whether someone drinks coffee or not. The confounder must be related to the exposure. In our example, this means that people who smoke often often drink coffee. The confounder should not be on the causal pathway between the exposure and outcome. This condition would also be fulfilled in our example: drinking coffee does not automatically lead to smoking. How do you deal with confounders?\nHow do you deal with confounders?\nYou can get a grip on confounders that you know in advance in the planning phase. For example, the inclusion criteria of a study are defined so narrowly that the influence of confounders is eliminated. This procedure is called restriction. One can also consider confounding in the analysis. For example, with stratification, in our coffee study, we would stratify all individuals into groups; that is, divide them into smokers and non-smokers and examine in both groups if there is a connection between coffee drinking and heart disease. If only people who smoke are at an increased risk for heart disease, then we would see that drinking coffee is not the cause of heart disease. We are mostly dealing with several confounders here. Multivariate analyses can be calculated, which adjust for several confounders. But what do you do with confounders that are unknown to us?\nWhat do you do when a confounder is unknown?\nThe only way to deal with unknown confounders is randomization. The study participants are randomly divided into study arms. The randomization leads to an equal distribution of known and unknown confounders in the study groups. If one observes a difference between the groups after an intervention, we can assume that this was caused by the intervention and not because influencing factors were already distributed differently between the groups at the beginning. Randomization is the only effective remedy against unknown confounders.\nChance\nAnother source of error in studies is chance, also called random error. The result of a study may coincidentally differ from the true effect in the population just by chance. This random deviation has no definite direction. If you were to do many of the same studies, some would overestimate the effect, others underestimate it, and others would estimate it correctly. One can minimize the influence of chance by having a large sample size.\nImagine you have a bowl of 500 green and 500 red gummy bears. You draw two samples, one with ten gummy bears and one with a hundred. In the small sample, it can be easy for you to draw eight green and two red gummy bears. With 100 gummy bears, you are probably already much closer to the 50-50 distribution. It’s similar with studies. In very small studies, it may happen, despite randomization, that the study groups are not similar. A non-evidence-based rule of thumb states that studies with fewer than 300 study participants are susceptible to random variability. Additionally, a low event rate is prone to random error. A large study size can minimize random errors.\nBias is another source of error in studies. Bias is a systematic deviation from the true effect that can be caused through the design, conduction, or analysis of a study. In contrast to random error, bias would always distort the results in the same direction if the study were performed multiple times. There is a variety of bias subcategories, and here are four types of bias that play an important role in interventional studies: selection bias, performance bias, measurement bias, and attrition bias.\nSelection bias\nSelection bias describes the problem of systematic differences in the allocation of study participants. For example, imagine a study that examines the effects of a physical exercise program versus no intervention, and study participants are allowed to choose which group they want to be in. It is very likely that more sports enthusiasts and health-conscious individuals would opt for the sports group. As a result, two groups would be formed that are not comparable from the start in some aspects. If, at the end of the study, it emerges that the sports group had better results, this cannot be automatically attributed to the intervention itself, as the group already had better starting conditions. Selection bias can be avoided by employing randomization and ensuring secrecy of the randomization sequence, also known as allocation concealment.\nPerformance bias\nPerformance bias occurs when, apart from the intervention under investigation, there are systematic differences in the treatment and care of patients. For example, the type of care and attention changes with the intravenous administration of a medication. So, if you compare two drugs that are administered differently, it remains unclear whether a different effect is due only to the drug or to the other care and attention given. In order to prevent performance bias, it helps to have a standardized treatment concept for all study groups and to blind all people involved in a study. Blinding means that the participants in a study are not aware of which people receive which treatment.\nMeasurement bias\nMeasurement bias exists when there are systematic differences in measuring outcomes. For example, if a person evaluates their knee pain, knowing if they were in the intervention or comparison group can influence the perception of the pain. To prevent measurement bias, those who measure the outcomes should not know which individuals receive the intervention and which receive the control intervention. This allows them to assess the outcomes unaffected and objectively.\nAttrition bias\nPremature exit from the study results in systematic differences between study groups. In general, it is normal for some people to leave studies early. However, it becomes problematic if this happens systematically and not accidentally. Suppose those who feel particularly unpleasant or have many side effects leave the study. If, at the end of the study, we only look at those in the analysis that remained in the study until the very end, we would ironically believe that there were very few side effects. One can minimize the influence of attrition bias in the data evaluation by providing an intention-to-treat analysis that includes all individuals originally randomized to a study.\nRisk of bias\nWhen we critically evaluate studies, we try to estimate the risk of bias. Bias cannot be measured directly. The risk of bias can only be assessed indirectly through evaluation of the study design and the execution of studies. In addition, the risk of bias between outcomes may vary. For example, while subjective outcomes such as pain can be strongly influenced by lack of blinding, this has no effect on hard outcomes such as mortality.\nComponents of a study result\nThe aim of studies is to map the true effect of an intervention as well as possible. That is, the distorting influence of confounding, chance, and bias should be contained as much as possible. A large sample can minimize the influence of random error. The study design, the good execution of the study, and the analysis can minimize the influence of confounding and bias."
  },
  {
    "objectID": "software_genomicSEM.html",
    "href": "software_genomicSEM.html",
    "title": "Genomic SEM",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 14:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial scripts and datasets.\nFor more information and tutorials on Genomic SEM see Chapter 9.3."
  },
  {
    "objectID": "software_genomicSEM.html#genomic-sem-tutorial",
    "href": "software_genomicSEM.html#genomic-sem-tutorial",
    "title": "Genomic SEM",
    "section": "",
    "text": "Title: Genomic SEM Tutorial\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 14:59\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial scripts and datasets.\nFor more information and tutorials on Genomic SEM see Chapter 9.3."
  },
  {
    "objectID": "software_conditional.html",
    "href": "software_conditional.html",
    "title": "Conditional Analysis",
    "section": "",
    "text": "mtCOJO\nTitle: How to perform mtCOJO\nPresenter(s): Zhihong Zhu\nLevel: Intermediate\nLength: 14:51\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to mtCOJO website."
  },
  {
    "objectID": "software_ewas_transcript.html",
    "href": "software_ewas_transcript.html",
    "title": "Software Tutorials: EWAS (Video Transcript)",
    "section": "",
    "text": "Title: How to Perform Epigenome Wide Association Studies\nPresenter(s): Agaz Wani, Seyma Katrinli, PhD\nHello, I am Agaz Wani. I am a postdoc in Dr. Uddin’s lab at the University of South Florida. Today, I’ll be talking about quality control steps that we need to do in epigenome-wide association studies. So, I’ll be talking about checking for failed assays, mislabeled and contaminated samples. Then, I’ll be talking about dye bias correction and normalization, followed by a removal of low-quality probes and samples. Finally, I’ll be talking about adjustment for batch effects.\nSo, we need to install and load some packages. We will be using “ewastools” to perform quality control checks. So, the first thing we need to do is to load the idat files. That is, the R data. Each sample has two idat files, that is green and red channels.\nWe also have a sample sheet that comes with the raw data, which looks something like this, where we have Sentrix ID and Sentrix positions. We have the sample name. So, we need to use this sample sheet to read the idat files. We are reading the idat files, it will take a few seconds. We are almost done reading the idat files. Then, we will check if we have all the samples on the sample sheet and the methylation data.\nThe next step we need to do is to evaluate 17 quality metrics that are recommended by Illumina. We will remove the samples that fail this check. It looks something like this, where we have the sample ID, which is the combination of Sentrix ID and Sentrix position. It will be unique for each sample, and we will have the information here whether the samples failed or passed this step of quality control. If the sample passes, the value will be “TRUE” here.\nNow, we need to do the genotype check that will help us to find the mislabeled or contaminated samples. It will give an outlier score. If the outlier score is greater than -4, it is recommended to remove that sample. So, we have the information here. We have the written to the file where we have the outlier information. If it’s greater than -4, it’s recommended to remove the sample or flag the sample so that you can track it in the downstream analysis. We have the donor ID and the number of samples with the donor ID.\nSo, in the case of a longitudinal study where we have more than one time point of data for a participant ID, it will have the same ID for that participant. As we have unique samples, all these IDs are unique. In cross-sectional analysis, we need to remove the duplicate samples of IDs. Then, finally, we will write this data so that we can use it in the next quality control steps and downstream analysis.\nIn this part, we will be performing normalization, dye bias collection, sex check, and ComBAT adjustment. So, we will install and load the packages. We will use some main packages like “minfi”, “sva”, and “CpGassoc”. R packages and some helper R packages to make the process easy. First, we will load the idat files and the sample sheet that we saved in the previous file. We will need some more information like age and sex and PTSD that we will use in the ComBAT adjustment.\nYeah, so we will load the data first. As you can see, it is loading the data, the green channel data, and it will perform dye bias correction because we have data from two different channels, that is red and green. It will perform a SS noob normalization. There are various approaches, but in this part, we will use SS noob. Followed by that, we will do the sex check.\nWe are almost done. Now, in the next step, we will perform a sex check. The data in the phenotype file and methylation data should match. So, if there’s any sex mismatch between those two files, we need to remove the sample. It means there’s some issue with that sample, so we don’t want that sample in the analysis.\nWe also need to have P value, beta value, Signal A (that is unmethylated information), and Signal B (that is methylated information) to perform some other steps. So, we will also save this information so that we can use it in the future steps. We will also save this updated phenotype information to use it in the next steps.\nSo, once we have saved this data, then we will clean the environment and load the data again to perform some other quality control steps. We are almost done with this step now.\nWhen we have saved the data, we will load the data here and do the check, that is to remove the low-quality probes and samples. So, if there is any sample that has greater than 10 percent of the missing values or any group with greater than 10 percent of the missing values, we will remove that. As you can see, zero samples have been removed, but 1,585 CpGs have been removed due to high missing values.\nNext, we need to remove the cross-hybridized groups - those groups that match more than one location in the genome. So, those are recommended to be removed. You can refer to this paper where you know it has known probes that are known to be as cross-hybridized. We will remove those probes and then save the data again for future use.\nThe next step, very important step, will have to do with ComBAT adjustment. It will remove the batch effects that are in the data. But the main thing we have to keep in mind is that we have saved the variation or preserved the variation for the phenotype of interest, for example, in this case, it’s PTSD. We also have to save factors, sex and age. So we will build the metrics for that and use that matrix to preserve the variation in ComBAT adjustment.\nSo, for ComBAT adjustment, we have loaded the saved data and we have loaded the saved phenotype data. We have removed the samples that were QCed. Now, we will order the samples or sort the samples so that they should be in the same order in the phenotype file and the methylation data file. We will have sex, age, PTSD, chip ID, and chip position. We will be using ComBAT adjustment to adjust for the batch effects for chip ID and chip position and save the variation for sex, age, and PTSD.\nWe also have to remove the missing values if there are any missing values in the phenotype information. So, we will remove those samples. The other way is to include the samples, the information in those samples, but here in this part, we will remove those samples.\nNow, convert the columns, such as Chip and Chip position as factors, and PTSD and sex as factors, so that they can be considered as categorical variables. Here is the matrix we have designed, where we have PTSD, age, and sex. The variation for all these variables will be saved when the ComBAT adjustment will be performed.\nWe are performing log transformation so that we have a uniform distribution of the data. We will include the missing values in the methylation data because ComBAT will not work for data that has missing values. And it’s imputing right now the data.\nHere, the first adjustment will be done based on chip ID and the data. The outcome data will be used for the next adjustment, that is based on the chip position. So, you can see, as we have three chips, we have three batches that are being adjusted for the batch effects. The data is being adjusted for chip position, as we have eight positions on the chip, so we have eight batches. After ComBAT adjustment, we will be converting the data back to a beta transformation, where it will have the data in the range of zero and one.\nI’m converting the data back and inserting any values back because we imputed the data for ComBAT adjustment, so we need to put the missing values back. And we will finally receive the data that is ComBAT adjustment normalized, you know, after sex check, we will see the final data so that we can use it in the downstream analysis. Thank you.\nSeyma:\nHello, so for the second part of our pipeline journey, I will talk about how to run the analysis, the epigenome-wide analysis and what we need to run epigenome-wide association studies, and how we can get what we need using our pipeline.\nThis is the general structure of the pipeline. Agaz has kindly talked about the initial quality control steps, which are described in script 1 and script 2, and I will talk about scripts 3, 3.1, 3.2, 4, and 5. Briefly, the EWAS is done by using script 4. But, in order to run the EWAS, we first need some preliminary scripts to estimate cell proportions, principal components, and smoking scores.\nCell-proportion estimation will be done using script 2. This script needs normalized and QCed beta values and phenotypes, which were prepared by Script two, to calculate cell proportions (CD8, CD4, natural killer cells, B cells, monocytes, and neutrophils), using “Epidish” R package and an IDOL reference panel. For the output, we will get cell proportions.\nScript 3.1 is actually not a mandatory script. We will use it if PCs from GWAS are not available. So here, in script 3.1, we still use normalized and QCed beta values from Script 2 to calculate methylation principal components using CpG sites that harbor SNPs. This procedure was described in the paper of Barfield et al. As an output, we will get methylation principal components (mPCs).\nThen, we move to script 3.2, which uses 39 probes to estimate a smoking score. So, this EWAS, our main analysis, uses normalized QCed and ComBAT-adjusted beta values and phenotypes from Script 2, as well as cell proportions from Script 3, principal components from Script 3.1, and also smoking scores from Script 3.2 for running smoking sensitivity analysis.\nHere, after running this script 4, we will get our output summary statistics that each group can share for us to run the meta-analysis.\nAnd finally, we have script five, which we use to calculate cell-specific differential methylation. This script identifies cell-specific differential methylation using PTSD as the main effect and age and sex as covariates. So, at the end, we will get summary statistics for each calculated cell type.\nSo, how we calculate cell proportion estimation with principal components, and smoking scores. Here’s how we do it: For cell proportions, we use the “Epidish” R package using the Epidish function with the robust partial correlations mode.\nHere, we use an EPIC-specific reference panel, as described in the paper Salas et al. This reference panel is more advanced than the previous ones because it includes more samples – 37 samples -from diverse backgrounds. It includes 6 African-American, 2 Asian, 21 European, and 8 mixed ancestry. It also has female and male smokers and non-smokers, and has a wide age range.\nThen, we calculate methylation principal components. If principal components from GWAS are not available, we use CpGs within zero base pair of SNPs, and we use methylation PC2 and PC3, which are shown to correlate the most with ancestry. Principal component 1 has been shown to correlate with cell type, so we don’t include it because we already included cell proportions that we calculated with the Epidish package. Finally, we calculate smoking scores using the 39 smoking-associated CpGs. That’s much better than using self-reported smoking scores because it helps us distinguish, let’s say, Jack, who smoked one cigarette per day, and Amy, who smoked a pack of cigarettes per day.\nFinally, we run our EWAS, using the normalized and ComBAT-adjusted data, we test the association between PTSD and CpG sites, adjusting for cell types. Here, the important thing is we only include five cell types and leave out neutrophils, which have the highest proportion, because, at the end, the sum of all six equals one, so it doesn’t make sense to include all six, so we exclude neutrophils. We include principal components either from GWAS or methylation data, age, and sex if available.\nAnd then, we run a smoking sensitivity analysis, including smoking scores as a covariate. Then, we can run sex-stratified analysis if applicable, those include sex-stratified and ancestry-stratified analysis. Finally, we will have summary statistics that will be used in the meta-analysis.\nFor cell-specific differential methylation, we use R package, TOAST, to identify cell-specific differentially methylated CpGs that associate with PTSD. The idea with that is each individual may have different cell type proportions, which may mask the association. So, in using this method, we will have CpGs that also associate with PTSD in each of the six cell types.\nSo, to sum up, using this pipeline, we will get EWAS summary statistics for our main model, smoking-adjusted model, and sex-stratified model, and race-stratified model if applicable. Additionally, we will have summary statistics for each cell type.\nI would like to once again mention our PI’s Alicia, Caroline, Monica, and Mark, and our analysts Agaz, Andrew, and Adam, who helped in making this pipeline. Also, if you want to join our calls, our regular calls are on the third Tuesday of the month. Oh, thank you."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PGC Video Textbook",
    "section": "",
    "text": "Welcome to the PGC Video Textbook!\nHere, we’ve pulled videos from our PGC experts to give a comprehensive overview of Psychiatric Genetics: From its history, current state of research, and resources for conducting analyses in Psychiatric genetics.\nEach video is embedded into the textbook webpage, and a link to the video transcript has been provided below the video.\n\nTable of Contents:\n\nWelcome and Introduction\n\n\nChapter 1: Introduction\n\n1.1: What are psychiatric disorders?\n1.2: Epidemiology\n1.3 History\n1.4 Psychiatric Genomics: State-of-the-Science\n\n\n\nChapter 2: The Genome\n\n2.1: Organization of the genome\n2.2: Types of genetic variation\n2.3: Evolutionary signatures\n2.4: Linkage disequilibrium\n\n\n\nChapter 3: Technologies\n\n3.1: SNP array genotyping\n3.2: Next Generation Sequencing\n\n\n\nChapter 4: Study designs\n\n4.1: Epidemiological study design\n4.2: Confounding, Chance, and Bias\n4.3: Genetic study designs\n\n\n\nChapter 5: GWAS analysis\n\n5.1: Genotyping Quality Control\n5.2: Imputation\n5.3: Association testing\n5.4: Meta-analysis\n\n\n\nChapter 6: Polygenic Scores\n\n6.1: Polygenic Risk Scores\n\n\n\nChapter 7: Ancestry-specific analysis\n\n7.1: Cross-ancestry analysis\n7.2: Ancestry-specific PRS\n7.3: Local ancestry and Admixed populations\n\n\n\nChapter 8: Post-GWAS Bioinformatics\n\n8.1: SNP Heritability\n8.2: Genetic correlations and partitioned LDSC\n8.3: Gene-association analysis\n8.4: Gene-set analysis\n8.5: Fine-mapping\n8.6: Quantitative Trait Loci (QTLs)\n8.7: TWAS\n8.8: pheWAS\n\n\n\nChapter 9: Advanced Topics\n\n9.1: Copy Number Variation\n9.2: Mendelian Randomization\n9.3: Genomic Structural Equation Modeling\n9.4: Interactions with Environmental Factors\n9.5: Family-based analysis\n9.6: Therapeutic Implications\n\n\n\nChapter 10: Other considerations\n\n10.1: A Career in Psychiatric Genetics\n10.2: Caution in Genetic Prediction\n10.3 Small Effect Sizes\n10.6 GDPR for Dummies\n\n\n\nSoftware tutorials\n\nCNVs\nConditional Analysis\nCross-Disorder Analysis\nDatasets\nEpigenome-Wide Association Studies\nGene Set Identification\nGenome-Wide Association Studies\nGenomic SEM\nMendelian Randomization\nMulti-Trait Analysis of GWAS\nPolygenic Risk Scores\nSNP Heritability and Genetic Correlation\n\n\n\nGlossary\n\n\nSoftware Resources\n\n\nAdditional Reading"
  },
  {
    "objectID": "chapter10.3_transcript.html",
    "href": "chapter10.3_transcript.html",
    "title": "Chapter 10.3: Small Effect Sizes (Video Transcript)",
    "section": "",
    "text": "Title: Small effect sizes\nPresenter(s): Howard Edenberg\nA brief note about small effect sizes.\nHi. I’m Howard Edenberg from Indiana University. [These are my personal views.]\nPsychiatric and substance use disorders are complex genetic disorders. The risk is affected by both genes and the environment. Neither works alone, and they interact, sometimes in very complex ways. No single genetic variant causes a complex trait such as a psychiatric or substance use disorder. Variations affecting many genes contribute to variations in physiology and these in turn contribute to variations in risk for the disorder, the course of the disorder, and the response to treatments.\nGenome wide association studies (GWAS) are currently the best approach to identifying individual genetic variations that contribute to the risk of these and many other disorders. GWAS are unbiased analyses testing common variations across the genome to determine if they affect a trait. But because only about half of the total risk is contributed by genetic variation and that part is distributed among hundreds to thousands of genes, the effects of any one common variant are typically very small. So very large studies are needed to find these variants.\nSince the effect of any one variant is very small, why bother to find them? Some have argued that discovering variants of such small effects is not helpful.  I'll argue here that they’re wrong. What we want to do is to reliably identify the hundreds of variants that contribute to risk even though each individual variant contributes only a tiny amount. There’s much we can learn from this. There are some immediate uses of GWAS findings. The overall patterns of association can help predict part of the risk for the disorder through polygenic risk scores. They can show if traits are genetically related. They can examine potential causality by Mendelian Randomization. And they can help explain heterogeneity in many cases, if there are different patterns of variants among different people with the same overall disorder.\nWhat else can we do once we’ve discovered many variants of small effect? The genetic variants and the pattern of them can lead us into biology. They can show us which genes are involved and which pathways are involved. These lead us into the physiology of the disorders and that in turn can lead us to better diagnosis, to rational drug discovery, and to personalized medicine, all of which I think will be very valuable.\nThe effect of a drug on a pathway is not limited by the effect size of the variant that led us to discover the pathway that’s important in the disorder. Even if the variant is rare or the effect size of the variant is very small, if it leads us to a biological pathway, that pathway can be targeted by drugs that have a very large effect. The impact of a drug upon a pathway is in no way limited by the effect size of the variants that led us to understand the role of the pathway.\nSo I, for one, continue to think that using large GWAS to identify variants and genes that contribute to the risk for these and other disorders will prove of great value. Perhaps not immediately, but in the long run. Without such knowledge we’re really flying blind when we look for new treatments.\nSome thoughts about translating genome-wide association findings into new therapeutics for psychiatry have been discussed in a review article published a few years ago that’s worth looking into. [PMID: 27786187. PMCID: PMC5676453]\nThanks for paying attention. Goodbye."
  },
  {
    "objectID": "chapter7_transcript.html",
    "href": "chapter7_transcript.html",
    "title": "Chapter 7: Trans-ancestry analysis (Video Transcript)",
    "section": "",
    "text": "7.1: Cross-ancestry analysis\nTitle: Cross-ancestry PTSD and Polygenic Findings, and the Cross-Population SIG of the PGC\nPresenter(s): Laramie Duncan\nCaroline Nievergelt:\nWelcome to the PGC Worldwide Lab meeting, I see that people are still calling in, so let’s give them a minute. Okay, looks like it’s stable now. So today’s topic is analysis across ancestries. We have three speakers today: Laramie Duncan, Elizabeth Atkinson, and Alicia Martin. All three talks are centered around GWAS analysis across ancestries, which is a really timely topic.\nToday, we’re gonna do something a little bit different than usual. We are actually gonna wait until after all three talks are over to take questions. Then, we will allow you to unmute yourself. So if you have a question for any of the presenters, you can just unmute yourself and ask a question directly. And if that’s not gonna work, then we can still take questions via the chat function.\nOkay, so let’s get started with Laramie Duncan, who is currently an assistant professor of psychiatry at Stanford University. Laramie received a joint PhD in neuroscience and clinical psychology from the University of Colorado, and then did a postdoc in statistical genetics in the labs of Jordan Smoller and Mark Daly at MGH [Massachusetts General Hospital] and Harvard Med School. Today, she will be talking about GWAS analysis in PTSD across diverse populations, a special interest group in the PGC. So take it away, Laramie.\nLaramie Duncan:\nOkay, great. Thanks so much, Caroline. I really appreciate this opportunity to present. So thank you to you and Pat. Like you mentioned, I’m going to talk about three topics today related to cross-ancestry analyses. First, the PGC PTSD analyses. Second, polygenic risk scoring analyses. And third, the cross-population special interest group within the PGC. So, starting with PGC PTSD, the original wave of the PGC PTSD analysis was led by Dr Karestan Koenen and my postdoctoral mentor, as Caroline mentioned, was Dr Mark Daly. They provided incredible leadership and mentorship for this so I wanted to mention them upfront.\nAnd the reason that we’re talking about PGC PTSD is that, actually, compared to most large-scale GWAS, our samples were relatively diverse. So, as you can see here, we actually had a minority of European ancestry participants, and we had a rather large sample of African Americans, as well as some Latino/Hispanic individuals. This is again for the first wave of PTSD analyses within the PGC.\nAt the time when I made this figure, I looked at the other large psychiatric GWAS. And like the rest of medical genetics, we can see that most samples are of European ancestry. The samples that aren't of European ancestry across medical genetics tend to be from East Asian populations. So, despite the fact that PGC PTSD does have greater representation of more populations, it’s important to note that it’s still not reflective of world ancestry.\nBut nevertheless, in conducting these analyses, the fact that we had these more diverse samples necessitated modifications to our analysis. And so, when we approached this problem, we did what anyone does: we thought about what might be appropriate, talked to experts, and tried a number of different analysis approaches. And there were a lot of modifications that were necessary.\nJust showing one here. We decided ultimately to conduct a trans-ancestry meta-analysis. So, in order to do that, we needed to assign ancestry for each of the participants in our study, such that within each cohort, we could individually conduct ancestry-specific GWAS prior to meta-analysis within each ancestry, and then trans-ancestry meta-analysis. So, actually, though this work has continued in terms of making further analytical improvements to the pipeline, I just want to point everyone here to Caroline’s work on this, and Adam Maihofer and her group. They’ve continued to improve the pipeline with some really nice additions and changes to quality control, imputation steps, and the analysis. This second round of the PGC PTSD analysis is now available on bioRxiv. The link is here. So, for questions about the most up-to-date improvements, I would suggest that people check out this paper or talk to Caroline.\nGoing back to PGC PTSD, the first wave of the PTSD analyses, though, I’d like to point out something that was interesting and informative from these analyses. So, purely by chance alone, it happened to be the case that we had about 10,000 individuals of African American ancestry and European ancestry, with about 25% cases each. Despite these comparable sample sizes, it turned out that we had no significant results in the African American samples, whereas, as expected, we had significant SNP heritability estimates in the European ancestry individuals, as well as polygenic predictions. For example, using schizophrenia external GWAS results to predict PTSD in the European ancestry samples. And based on power calculations, we thought that we would have enough power in these European ancestry samples. But due to expectations of lower transferability in African American samples, we didn’t know if we would have enough power. And in these samples, it turned out that we did not.\nThe reasons for this are at least somewhat well understood. Number one, we had worse coverage of African ancestry variants in our samples. This is both because African ancestry individuals have greater genetic diversity than other populations and so, for any given number of variants, you can’t cover as much of the genome. But also, as many people know, there’s a bias towards European ancestry variants on genotyping chips. So, both of these problems probably contributed. Also, the lack of external data resources from non-European populations is really a problem, particularly for these African ancestry samples. But this is true for other populations as well. And then, finally, there are also methodological inadequacies currently. For example, differences in linkage disequilibrium and allele frequencies are still not handled as well as they could be in many different analyses.\nSo, the good news is that in the second wave of the PTSD analyses - this paper that I mentioned before from Caroline - we had larger sample sizes for all populations. And with these larger sample sizes, we could actually estimate heritability and have polygenic predictions in the African American samples. So, it does look like this was an issue of power. And, certainly as expected, it helps to have larger sample sizes.\nI’m going to turn now to a couple of quick points about polygenic risk scores. I don’t know if it’s partly just being in Silicon Valley and being near 23andMe, but everyone is talking about polygenic risk scores here. Not within my psychiatry building, but just people are very interested in these. A couple of years ago, when, for example, 23andMe was releasing results without any polygenic results, without any mention that these scores might, or certainly would, have differential performance across ancestry groups. So, we thought that this was a really important topic to spend some time exploring.\nSo, to give a bit of background, as early as 2009, the ISC paper, Purcell et al., demonstrated that polygenic risk scores that are derived from European ancestry populations have poor performance in African American samples. And this poorer performance in non-European ancestry samples has been shown many times since in lots of different polygenic risk scoring publications. So, the new question that we wanted to look at was, we wanted to quantify the decrement of performance between European ancestry populations and other ancestries.\nWe looked at a couple of ways of doing this. I’ll just show one bit of information here. We looked at 10 years’ worth of polygenic scoring studies. First, to just see who was included in these studies. As expected, these studies also have primarily European ancestry. But then, getting to the performance of polygenic scores across different ancestry groups, we made comparisons. For example, one of the results is that we found that polygenic scores performed about three times better in European ancestry individuals than in African American samples. I don’t know if I think Alicia may or may not mention additional data that she has here that has, I think, even better analyses addressing this question.\nSo, in summary regarding polygenic scores, the performance is worse for non-European ancestry populations. This is not surprising; this was known for quite some time. But efforts are underway to quantify how much worse the performances are in different populations. I think it’s important to know what to expect both for scientific research, where we want to have accurate power calculations, and to the extent that anyone is using these scores in clinical practice ultimately, or just in their own lives getting 23andMe reports, it’s important to know how well they might work. Efforts are also underway to improve prediction across populations.\nBut getting to a bigger picture question, I think that the question a lot of us have on our minds is, what do we do about the substantial underrepresentation of most populations in genetic studies? So, this gets me to my last topic. And at first, or as a point of background information, there are many potential solutions to this problem, and I know that many researchers within the PGC have been working in this area for a considerable time. And so we are just mentioning one additional approach. But, you now, if I had more time, I would mention other efforts that have been ongoing. Something that we did, with Hailiang Huang, who is now junior faculty in Mark Daly’s group at the Broad, is that we started a cross-population special interest group within the PGC, with the goals of improving the applicability of genetic results, conducting analyses, and supporting the involvement of researchers from different parts of the world.\nIn one of our very first meetings, Roseann Peterson had a really great idea to write a best practices paper describing the ways in which genetic analyses ought to be modified for samples of diverse ancestry, in particular admixed samples and mixtures of different samples. Some of the topics addressed in this paper include specific recommendations for how to modify quality control steps, which parameters should be modified and how and why. As well, there are recommendations for imputation and how to analyse samples that are GWAS or a mixed model context. And this is absolutely a huge team effort, and I want to just say thank you to this group. I think that the reason such a best practices paper doesn't already exist is that there isn’t really any one person in the world who could have written a best practices paper. It’s thanks to the analysts in this group especially, that have expertise in particular areas, that we were able to cover these different components of analysis and really write about the various approaches, and what are the pros and cons are for taking one approach over another. So that's what's in this best practices paper.\nHailiang spoke to an editor at Cell, and they were very interested in this paper. They suggested that we present it as a primer, so that’s the current format of the paper. We’ll be submitting it on Monday, so that’s our first work product. The special interest group is open to anyone; it’s technically within the stats group. If you’re interested, we have meetings on the first Wednesday of the month at 1:00 p.m. currently. You can get in touch with us. Thanks to all these groups for their incredible collaborative efforts. So, thank you.\n\n\n\n7.2: Ancestry-specific PRS\nCaroline Nievergelt:\nOur third speaker is Alicia Martin. Alicia is a instructor at MTH and an affiliate researcher at the Broad in Harvard. She received her PhD in genetics from Stanford University, and her current research focus is on developing novel statistical methods to improve the generalizability of genetic risk prediction from your Eurocentric genetic studies. So, her talk today is entitled “Clinical Use of Current Polygenic Risk Scores may exacerbate Health Disparities.” Go ahead, Alicia.\nAlicia Martin:\nThanks, Caroline. Um, sorry, try to move this other way. Well, hopefully this goes away. So I’m excited to talk to you guys today, and thanks for having me here. Can you guys all see this part of the zoom screen, on the screen?\nCaroline Nievergelt:\nWe can see your slides.\nAlicia Martin:\nYou can see my slides, okay? Perfect.\nSo, I think an important point to start at is in thinking about human population history. So all of our genetic differences across populations are sort of shaped by how we originated and how we migrated and mixed as we moved out of Africa. So of course, humans originated in Africa. This has been shown through genetic evidence as well as archaeological and linguistic evidence. So many sources showing this, and then humans migrated Out of Africa, and as they did so, they took a subset of genetic diversity with them as they populated Europe, Asia, Australia, and into the Americas.\nAnother point that I think is worth talking about—sorry, here is the fact that GOS are becoming super increasingly powerful, and so that’s very, very exciting because we’re making so many more biomedical discoveries these days. Some drugs are even being developed out of this, and this has been growing at such an exponential rate that it’s been really, you know, impressive and somewhat challenging even to keep track of all of this progress. So that’s been really, really awesome to watch.\nUnfortunately, however, as Laramie mentioned earlier, and Elizabeth did as well, genetics has this diversity problem. So now, shading the same growth and progress in genetics by the populations that are represented in these genetic studies, at the individual level, you can see, of course, that the vast majority of participants in genetic studies, about 80% these days, are of European descent. And this is far out of step with the global population, where about 16% of the world is of European ancestry. And perhaps even more troublingly, if we look at the fraction of individuals that have been in these GWAS studies as an overall proportion, the progress in diversifying genomics has somewhat stalled or perhaps slid a little bit since about 2014. And so this is a really big issue if we’re trying to generalize studies to everybody.\nAnd so one of the nuances I want to mention here is that it’s not that the studies have been getting smaller; studies have generally been either staying the same or have been growing in different populations. But one thing that we’ve seen is that studies have been growing much, much, much more rapidly in European descent populations than elsewhere. And so one question that I really focus on is in understanding how to do ancestry study biases in genetics impact the generalizability of the knowledge that we can learn from these studies. And so I break that down in a few ways in different aspects of our work. But in general, I want to highlight a few key points that I think are worth keeping in mind throughout all of these questions.\nSo one of these is that the fundamental biology is really shared across different populations. So one person from one population will probably have, if they’re gonna have a heart attack, it’s probably for the same underlying reasons, for example, as an individual with another ancestry. And that’s true in genetics, too; in genetics, this is true as well. So when we’ve looked across many different biomedical domains and tried to understand what causal variant effects are in different populations, these really tend to be mostly shared. So it’s not that there’s anything special about genetics that’s different from other biology in general. The causal genetic variants seem to be the same and shared across populations.\nBut there are some complications in interpreting genetics across populations for several different reasons. So, of course, there’s—it’s also worth keeping in mind that there’s more genetic variation within than between populations. So populations are not substantially genetically differentiated to the point where we’re finding completely distinct genetic populations. In fact, most of the genetic variation is shared; that’s common across populations, and there’s more genetic variation within than between.\nAnother point that I think is really worth keeping in mind is the LD structure, this correlation structure of the genome, is one driving factor that’s created a lot of challenges because this really differs across population as a function of human history going back to the first map that I showed you.\nSo, to try to address this question of how generalizable our genetic studies are, I started with computing polygenic risk scores. So Laramie talked about this earlier, this great enthusiasm in polygenic risk score space, and how it’s been really impressive to see the growth in this area in the past few years. Just so we’re all on the same page, I think most of you are familiar with a polygenic score. But in general, this is just predicting an individual’s phenotype from genotype. So we’re basically taking genotypes from some target individual, some effect size estimates from GWAS that exist, multiplying these together, summing them up across the genome, and that’s basically our phenotypic prediction. That’s a very simplistic method; there are other methods. But some considerations that sort of cut across all of these different methods for computing polygenic scores are which SNPs we should include, what weights we should use, and one thing that we always need to address in terms of the utility of our polygenic scores is how accurate is the score. And this is really going to vary a lot with sample size, the heritability, the genetic architecture of the trait, and a number of different factors. So definitely worth keeping in mind all of these complexities and interpreting polygenic scores across the literature.\nSo a study that we had done previously showed that population history really impacts genetic risk prediction across diverse populations. And so a few points here are the genetic prediction accuracy decays with increasing genetic divergence between the discovery and target populations. So, in this figure on the right, I’m showing you a polygenic score distribution computed in the thousand genomes project. So you can see that we predicted European populations, for example, to be taller than American and South Asian populations, and we predicted East Asian and African populations to be the shortest globally. But these differences really don’t line up with anthropometric studies and they’re rather misleading. These distributional shifts are really, really massive, and so this is clearly not necessarily reflecting reality.\nWe also set up some coalescence simulations alongside some statistical genetics simulations and showed that these polygenic scores can differ across populations arbitrarily, and these are not necessarily meaningful. And we also showed that neutral human evolution alone can be sufficient to explain these differences. We don’t necessarily rule out selection, but we do say that neutral evolution and drift, in particular, may be driving some of these differences. There’s a really interesting couple of papers that I want to point out by Michelle Sohail, Jeremy Berg, and colleagues that were published in Elife yesterday on this topic.\nSo we wanted to look at this in a large-scale setting, and so to do that, we looked at the UK Biobank data. The UK Biobank is, of course, mostly consisting of European descent individuals, and so we used those individuals to conduct GWAS for several different traits, 17 traits that were all quantitative, so things like height and BMI, and then a number of blood panel traits. And then we used the diversity in the UK Biobank in this non-European subset to try to understand how generalizable prediction accuracy is.\nIn general, what we saw is that if we normalize prediction accuracy across all of these 17 traits to how accurately we predicted in Europeans, we saw a pretty substantial drop-off in prediction accuracy across these different populations. So, for example, on the rightmost part of this graph, we saw a four and a half-fold improvement in prediction accuracy in Europeans with respect to how well we’re doing in African descent populations. We’re also doing about twice as well predicting in European populations as in East Asians, and you can see how well we’re doing in these other populations. So these are really, really quite large disparities here.\nSo why is this happening? A lot of people have written about this before, and it’s been well-covered in the literature, but in general, there’s a pretty predictable basis of polygenic risk for disparities, and it obviously relates to who we’re studying. So why, though? Well, as you know, GWAS are best powered to discover variants that are common in the population. So if we’re studying European populations over and over again, in general, we’re detecting variants that are most common in European populations, which are then able to explain more of the phenotypic variation in European populations than those variants that are less common in other populations. Additionally, the LD differences across populations mean that we are probably getting better tagged SNPs in the GWAS in European populations than in other populations. And then there are other really much more complex topics that are influencing this generalizability—differences in environment, selection, and other more complicated differences.\nBut I want to emphasize that there’s a lot of consistent promise from diversifying efforts so far. So, for example, the PGC schizophrenia working group, led by Hi-Liang Wong and with a first author Max Lam, have been working to really increase the sample size of East Asian individuals in schizophrenia studies. So right now, the European case-control studies in schizophrenia are still about threefold larger than the East Asian Studies, but the progress in the East Asians has been really, really rapid and massive lately. And so when we looked at how well we were able to predict East Asian schizophrenia risk in these case-control cohorts. In general, what we saw was that we were much better able to predict East Asian schizophrenia risk using the ancestry-matched East Asian data, despite the fact that the European training data was about threefold larger. So that indicates that there’s really a lot of promise and value in doing ancestry-specific studies here.\nSo lastly, I want to turn to this effort that we did to try to compare some biobank-scale analyses. So generally, what we were really interested in was doing equal-size GWAS in the UK Biobank and in the Biobank Japan, where we have a lot of overlapping traits that have been deeply studied. Then, we’re interested in predicting in the European populations and in the Japanese population to try to understand what prediction accuracy looks like, to see if this was symmetric and comparable across populations. We also tested prediction accuracy in the UK Biobank African descent populations.\nSo in general, what we saw was that ancestry-matched disease prediction was most accurate. So on the Left, I’m showing you along the x-axis the disease, and on the y-axis, I’m showing you the prediction accuracy, and the ancestry-matched results are indicated by matching colors. So on the y-axis is the target of prediction, and the summary statistics that we’ve used to generate the predictors are shown in blue for the Biobank Japan individuals and in red for the UK Biobank individuals. And so in general, we did better in both scenarios with the ancestry-matched disease prediction. One thing that’s sort of interesting that we learned was the overall disease prediction was more accurate in Japan, and that was sort of a consequence of how these cohorts were constructed. So the Biobank Japan cohort is a more hospital-based disease-ascertained cohort, whereas the UK Biobank is a healthier and wealthier than average population-based cohort.\nA similar finding emerges from the quantitative traits. So for these general health measures—anthropometric and blood panel traits—in general, the ancestry-matched quantitative trait prediction is also most accurate. Taking a look at these y-axes, you can again see notable differences in how accurate prediction is in each of these populations, and the quantitative traits are predicted most accurately overall in general in the UK European samples, and that’s again because of this ascertainment in the UK Biobank.\nOkay, so I want to stop here and wrap up and think about next steps. So in general, I think polygenic scores are really exciting and interesting and may have some power to improve clinical models, but right now I’m a little concerned or kind of a lot concerned that they’re likely to increase health disparities due to these vast Eurocentric GWAS cohorts. So I see this as a call for a few pushes that we really need to make concerted efforts. One is we need much more diverse GWAS studies, and another is that we need new statistical methods to address these major issues. And on this topic, since it can be a bit sensitive, I just want to urge everyone that’s working on these cross-cultural, you know, widespread topics to communicate their research responsibly and widely and anticipate the implications of your research, thinking about the potential negative consequences that will happen.\nI also wanted to point out that there are large efforts, for example, going on in Africa to scale up GWAS there, especially in psychiatric space, for, for example, developmental disorders and for schizophrenia, and I think this is really exciting and it’s taught us a lot about how to do cross-cultural research and an ethically responsible way that accompanies some capacity-building efforts.\nSo I want to acknowledge that this is a huge effort involving a ton of really awesome people, including my advisor and mentor, Marc Daly.\n\n\n\n7.3: Local Ancestry and Admixed Populations\nCaroline Nievergelt:\nElizabeth Atkinson - Postdoc at MGH in that Broad with Mark Daly and Peniel. Elizabeth did her PhD at Washington University in St. Louis and is currently developing resources that allow for improved genetic analysis in admixed populations. She’s a member of the PGC PTSD group, and she has developed local ancestry analysis framework that enables well-calibrated genomic analysis of psychiatric traits across admixed populations.\nElizabeth Atkinson:\nAll right, thank you so much again for inviting me to talk about this project. I’m really excited to hear the feedback. So, yeah, this Caroline said, “Today, I’m going to tell you about a project I’ve been working on, hoping to improve the ability to do sophisticated genomic analyses on admixed populations.”\nSo, to start off, there we go. A whole reiterate a point that you know, we’re hoping is now becoming common knowledge in the GWAS community, and that Laramie just spoke deeply about. The vast majority of our association studies are conducted on European cohorts. And if we look more closely at this sort of wedge of the pie that is non-European, we’ll notice that only a few handful of percent are populations that are admixed. Just to make sure that we’re all on the same page, starting off when I say “admixed,” I’m talking about an individual whose ancestry is not homogeneous but rather is comprised of several ancestral populations.\nSo importantly, there are actually many more samples out there who have been genotyped or sequenced alongside phenotypes, but they’re not actually making it into this figure. They’re being excluded from analysis due to being too admixed. And there are also several significant large-scale efforts to collect psychiatric phenotypes alongside genomic data in diverse populations, some of them led right here at the Stanley Center. So there really is a pressing need for the development of novel methods that will allow the easy inclusion of admixed people into association studies. We really can’t afford to kind of leave large swaths of our data on the table anymore, and you know, let alone the other problematic aspects of leaving entire ethnic groups understudied.\nSo, admixed individuals are generally removed due to the challenges of accounting for their complex ancestry. So there are kind of concerns about population structure infiltrating analysis and biasing results, which can lead to false positive associations. Studies that do include admixed individuals tend to generally only correct for PCs. So PCs, actually, I think I can make a laser pointer here. Let me do that. All right, so PCs generally only correct for kind of average ancestry. So if somebody is, say, you know, 75% African, 25% European, or vice versa, and they actually, you know, can’t really consider a lot of the more fine-scale population structure that can still be present on the data, and this is important which, you know, this sort of fine-scale pattern might be different, for example, in one case in control cohorts and still leave the door open for false positives.\nSo here, I’m going to present a novel analytical framework to hopefully rectify this issue and allow for the easy incorporation of admixed people into association studies. We specifically do this by accounting for local ancestry, which takes into account this finer scale of population structure. So to get a little bit more into the actual kind of method, we’re calling this method “tractor” for the moment. The core feature of the proposed framework relies on inferring population structure, as I said, informed by local ancestry. So the first step is this automated pipeline to call local ancestry tracts in your sample. Just in case you know you haven’t seen any painted karyogram figure before, here’s an example of a Latino individual. The autosomes are along the x-axis, position along the chromosomes on the ,Y and then the two strands of each chromosome are painted according to the ancestral origin of that tract.\nSo we use this information after we collect it to kind of improve long-range phasing and haplotype tract recovery, which I’ll go into in a lot more detail in coming slides. And then the end goal is basically to be able to extract out the component ancestries of interest. For example, if you have a large European cohort and you have this admixed population, you could use tractor to scoop out the European bits of your admixed cohort to include alongside your European individuals. And, you know, the same goes for the African and Native American components in this example. So, in this way, you can kind of leverage the information in admix people; you don’t have to exclude them from analysis anymore.\nSince the first step in this pipeline involves calling local ancestry, I wanted to validate that it was performing well and kind of the target populations that we had in mind. So the use case I’ll be talking about for the rest of this talk is the African-American demographic context, and we modeled this after PGC PTSD African-American cohorts. So to test local ancestry inference using one of the very worthy existing methods to call local ancestry, RFmix, I simulated a truth dataset that resembles these realistic PGC PTSD populations using real haplotype data. This will retain the LD patterns and, you know, other genomic features present in real data that are sometimes hard to simulate. And using this truth dataset with known phase and local ancestry, I then ran tractor step one, calling, you know, local ancestry, and quantified how often we got it correct. And, hardeningly, about 98% of the time, no matter how you slice it, we were getting correct slope lines, which recall, so this seems to perform very well.\nNext, I wanted to see how the pipeline would work in kind of a more real-looking dataset. So usually, we don’t have perfect phase; we have statistically phased data. So I took our truth dataset and ran it through a standard phasing pipeline, SHAPEIT2, using a thousand genomes as a reference panel. And I noticed something that we hadn’t initially expected to see, which is that a lot of the originally long tracks were really broken up by these switch errors in phasing. So even though local ancestry inference is performing really well, you know, if it’s calling well if we have zero, one, or two African tracks at a given position, these long-range haplotypes are being disrupted due to switch errors. So we ended up building in an additional step in tractor to find and unkink, as we’re calling it, this garden hose, to recover these long-range haplotypes.\nSo here’s our karyogram again. Now, after this unkinking step has been implemented, we noticed that there were still sort of stretches where due to the local ancestry, you know, we painted the chromosomes on the statistically phased data. So there are some overhang areas that were still not able to be recovered. So we decided to implement one more iteration of RFmix on this sort of ironed-out dataset to see if this could help recover these full haplotypes. And indeed, this did dramatically improve the situation.\nSo just to convince you that not only did this, you know, make our karyograms look prettier, but this actually made the data look more realistic. I modeled the track distributions using a poisson waiting time centered at nine, which was the number of generations ago that the pulse of admixture occurred. So given this, this is what we would expect the European tracts in our data set, in this, you know, the European tracks of these African American individuals to look like, you know, given all things being correct. And we can use this to put a p-value on how likely it is to get the distributions in the various treatments of the data. So in our truth dataset, again focusing on these red tracks, which are the European segments, we do see pretty close to expectations distribution here, almost around nine. After statistical phasing, however, the tracks get extremely short; they’re actually P times 10 to the negative 28 likely to observe a distribution of this many tiny tracks, so very, very unlikely. After unkinking, it’s about twice as improved, and with one iteration of the pipeline, we have gotten things almost back up to where they should be. And just to sort of zoom out and put this all into perspective another way you can see, you know, how we really dramatically improved things from this original purple phased situation back down to this yellow line, which is as close to the truth dataset in black here as we’ve gotten it so far. So we are indeed making the situation look statistically significantly more realistic by recovering these long-range haplotypes, which can be important too if you’re concerned with things like LD, for example, in your dataset.\nWe’ve also tested this in a bunch of other demographic scenarios, and it performs, you know, similarly well. So with different admixture fractions, putting a different, you know, demographic models in, so pulse times at different points in the past and also between different ancestries with varying kind of divergence times.\nSo in the last couple of minutes here, I’d like to kind of take a step back and look at one of the main applications of this framework, which is really to show that it improves performance in GWAS context. So not only should we better correct for population structure by using this local ancestry information, but we can actually identify a new locus through an increase in power from local ancestry as well. The statistical model that’s built into tractor basically tests each SNP for an association with the phenotype using the logistic regression model that I’ve shown here, where X is the number of copies of the risk allele from the, you know, the first of your ancestries, X2 is the number of copies from your second ancestry, and then you can put in whatever other covariates, importantly including global PCs, you know, as for the rest of your parameters. You also notice that this is currently written as a two-way admix context, but it could be scaled up to an arbitrary number of ancestries, and you can further test if the risk allele is ancestry-specific by evaluating the difference between beta1 and beta2 using AZ test.\nSo we wanted to see if there were any kind of power gains using this model, and I want to acknowledge Adam here, who heroically ran a whole lot of these simulations with me. So to do this, we simulated again a realistic African-American demographic scenario. We drew a biallelic disease variant with the probability of each genotype copy drawn from the minor allele frequency. We then simulated a disease phenotype assuming a 10% disease prevalence, and an individual’s risk of developing the phenotype was modified basically by their percent admixture and the presence of the minor allele on the African genetic background. So this is basically assuming no effect in a European genetic context but an effect on an African genetic context, which would be, you know, equivalent to modifying effect sizes due to a tagged SNP being present in African background and not in Europeans for a shared causative mutation.\nSo we ran a whole lot of simulations varying all of these kind of parameters to sort of characterize the landscape of how tractor would improve your power. This is the simplest context, just showing the allele frequency held constant at a 20 percent minor allele frequency. In all of these plots, the dotted line or the dash line is tractor, and the solid line is sort of the traditional GWAS method which does incorporate PCs. And you’ll notice that both the same sample size is shown in blue and black, we do see a significant power gain using local ancestry, and this can get very large, and, you know, this gain can get really big in different genomic contexts. So, for example, when you introduce a minor allele frequency difference between the ancestries, now shown in this red line, this sort of gap between the methods grows very dramatic. And I kind of want to dwell in this for a moment just to show how much, you know, how significant this power improvement is. So if you pick your favorite power, let’s say 80%, you’d be able to detect variants that are of an odds ratio of about 0.1 lower using our local ancestry incorporating method compared to your traditional GWAS. Or if you pick your favorite odds ratio, let’s say 1.2, you would basically have very little power to find this variant in a normal context, but we’d be able to detect it at very high power from using our method.\nSo we’ve also done a lot of sort of tests characterizing other aspects of the genomic landscape. Since I’m running a little low on time, I’ll just sort of briefly mention the most significant one, which is the effect size difference across ancestries. So now we’ve introduced an effect in both backgrounds, not only in the African genetic context, and the sort of major take-home point is that, you know, if you have the effects, so on the right-hand side here, we have the effect now swapped only in the European background rather than the African background, and you’ll notice that we would have effectively zero power to detect it using a traditional model but can detect it at reasonable power with Tractor. This is because European ancestry only makes up about 20% of the sample, so the signal would really have gotten swamped out if we hadn’t deconvolved the local ancestry tracts. And importantly, in a context where we would actually expect no improvement from local ancestry, the effect is the same, everything is the same in both ancestries, we don’t see much of a power loss, a very minimal power loss from using Tractor. So there doesn’t seem to be, you know, a major hit from including this in your analyses. So, you know, if you’re uncertain of the effect of ancestry, it seems like it’s not a massive problem to incorporate this. And importantly, we’re talking about, you know, the perceived effect of this tag SNP, not the real causative mutation. So, you know, even if the shared causative mutation is the same across ancestries, what you actually would detect in your GWAS could differ depending on the minor allele frequency or LD patterns or environmental interactions or many other factors that could be different across these ancestries.\nAlright, to sum up, today I’ve talked about this readily implemented pipeline that should allow you to be able to include admixed individuals in your GWAS studies in a well-calibrated manner. I am currently optimizing it with members of the Hale team from the Broad Institute, so it should be very quick and work in all systems once it’s finalized. But it’s already written up in Python, and I can, you know, distribute it to whoever’s interested.\nWe have also built in these extra features to improve long-range phasing, and I’ve shown you today that we not only boost your sample size by allowing inclusion of admixed individuals, but we can actually further boost your power by leveraging this information from admixed populations to find novel variants. We’re also hoping to take this in a few kind of directions. I’m starting testing on an empirical dataset that has a very well-established, you know, ancestry-specific hit, so blood lipids in African-American individuals, to make sure this really does work not only in simulation but in real empirical data as well.\nWe’re hoping that, since recently admixed individuals have disrupted LD blocks, we should be able to leverage this to more kind of fine-scale pinpoint causal variants, so we’re sort of diving into this now too and developing a heterogeneity test to try to automatically suggest and eliminate candidate sites based on your admixed populations. And I’m also working with Alicia Martin, who’s speaking immediately next, and some other collaborators here at the Broad and MGH to try to build a polygenic risk score framework that would actually produce reliable estimates for admixed populations, which remains another kind of notable gap in current methods.\nAnd I’ll just close with saying, not only is this procedure useful in the GWAS context, but it could really be applied to any situation where you need to control for population structure at a very fine-scale manner. So, for example, even things such as, you know, evolutionary studies doing genome-wide scans of selection.\nSo with that, I’ll acknowledge my collaborators and advisers, and I guess I’ll take questions at the very end. But thank you for your attention."
  },
  {
    "objectID": "software_mtag_transcript.html",
    "href": "software_mtag_transcript.html",
    "title": "Software Tutorials: MTAG (Video Transcript)",
    "section": "",
    "text": "Title: Multi-trait Analysis of GWAS (MTAG)\nPresenter(s): Patrick Turley\nPatrick Turley:\nWe’ve been toying with what we were going to do with this afternoon lecture. We had talked about maybe doing something that replication, and we also talked about doing MTAG. We talked about doing both – I don’t have time for both, so we’re just going to talk about this multi-trait analysis of GWAS paper. It’s one part self-indulgent because this is kind of the thing that I’ve been working on for the past year, but also I think it’s a nice illustration of a lot of the topics that we’ve been talking about for the past several days. So, I figured we’d go through, talk about this paper, and focus on some of the things that we’ve been learning about for the past few days.\nAudience comment: It’s also something that is going to become very common in the next few years.\nPatrick: It’s going to become so common, yeah.\nAudience comment: It’s good to know about. And also, Patrick is going to talk about things to look out for, so you know if it’s being applied well or not being applied well. Response: Yep, okay, great.\nAudience question:Where are the slides?\nPatrick: Oh, sorry. Oh yeah, so this is going to be a really informal lecture, which means that there’s actually very little information on these slides. I mean, like, I’m going to show a bunch of pictures, but I’m actually going to do a lot of work on the whiteboard. If you want, I mean, like, you know, but the work that I’m going to do on the whiteboard is in the paper but not the paper that you guys have access to. We’re working on it. The reason why you don’t have access to it is because it’s not written yet. The version that we gave you was a previous iteration of this method that was based on maximum likelihood. So if you read it, then that’s what you read. We realized that it could be generalized to be a GMM framework with much weaker assumptions, and so we’re going to talk about that framework today. Once that paper’s finished being updated and written, we’ll post it to bioRxiv, and then you’ll have that one. But, oh yeah, so that’s why. The plan was to have done this before the Institute, but you guys only give me like 10 minutes a day to do any of my own stuff.\nThe idea here is; we’ve been talking about. For our people doing genome-wide association or any study, what’s the first thing we ask them about? Sample size and power. And so, this is the problem that we have. So, a lot of times, we want to look at outcomes, and we just don’t have big enough samples, you know, and so if you have a really small sample, you don’t get very many hits, you have weak polygenic scores, and so this is a problem. However, in a lot of cases, there are people who are studying different but related outcomes. So, for example, what I’m going to show you today is, you know, there’s a big or like medium-sized GWAS of depression with maybe 100 to 300 thousand individuals, and we also have a GWAS of subjective well-being, so how people respond to the question “how happy are you?” Now, these two things aren’t exactly the same thing, and so you might think that it’s probably incorrect just to meta-analyze them together. But they seem like they’re probably really closely related. We’ve seen, Raymond showed us the LD score regression results, and we see that knowing the βs for one trait, the effect sizes for one trait, is related to the βs for another trait, and so it would be nice if we could figure out a way to get the information out of the GWAS for subjective well-being and import that into a GWAS of depressive symptoms or the other way – just so that we can increase our power and get all these nice benefits that we get from running well-powered studies. So, what’s the right way to do this?\nSo, MTAG, let me just talk about some of the hurdles that we’re going to face in trying to design a method to do this. First off is, in a lot of cases, we don’t have the individual-level data. So, you know, we just get people to release their GWAS summary statistics because that’s something that they’re allowed to post without any privacy issues. But, you know, we don’t have the actual genotypes of the individuals. The method that we want to come up with is one that can use just these publicly available summary statistics. Another problem is that the summary statistics often include the same individuals, and so with the UK Biobank coming out, if the UK Biobank asks the question about the phenotype that you’re interested in, then, you know, it’s going to be in this GWAS and that GWAS, and so we’re going to have a lot of overlap in what’s used. And the reason that this is a problem is if we took the summary statistics for one trait that was estimated in the UK Biobank and for another trait that was estimated in the UK Biobank, they may co-vary partially because they’re genetically correlated, but they’re also going to co-vary because they came from the same sample, so the estimation error is going to co-vary. And so, we’re going to need to be able to address that, especially because a lot of times, we don’t even know how much overlap there is. We suspect that there might be some, but we’re just not sure. Another thing that would be useful. There are a bunch of methods that allow you to jointly analyze different traits, but it would be great if, when we were done, we could get estimates that we said, “Yeah, these estimates correspond to the trait that we’re interested in.” So, we don’t want to get results and say, “Well, because we jointly analyzed this with depression and subjective well-being and whatever else we threw into it, it corresponds to some weird composite phenotype that we don’t understand.” It would be nice if the results that we have actually correspond to the trait that we’re interested in. And just, a computation issue, it should be nice if we could actually get them in finite time. So, MTAG, the reason that I’m excited about it is because it deals with all of these really practical issues that we deal with in running these analyses.\nSo, if you read the paper, which, of course, you did. You saw that we originally made this assumption, based on this random effects model. So I don’t think that we talked about this super carefully, but in the LD score regression, as you recall, we were treating these βs as if they were random variables and that may or may not be just a little bit weird, but it seems to get us a lot of mileage and so we just run with it. And so, we’re going to do that again.\nIn the original version that you’ve read, we made this assumption that the βs were normally distributed with mean 0 and variance-covariance (and I called it Ω). Okay, so these [annotates β and 0] are all vectors, and this [points to Ω] is a matrix. But so what these βs here – so previously when we talked about a vector of βs, we meant it was the effect size for one trait across a bunch of different SNPs, so if we had 10 million SNPs, β had 10 million elements, one for each SNP. In this case, I should actually put a j; that would make it more clear. In this case, we’re going to assume that the β corresponds to a single SNP but for a set of T traits. So, if we’re looking at three traits, then β has three elements. And so we’re going to say that they have mean 0, and I’m going to say that they have a variance-covariance, which I just defined to be Ω. So this is going to be related to the genetic correlation, because if the βs are correlated, then these traits will be correlated genetically.\nSo, I said this is the one that you read, but we’re going to relax this and instead just say that the expected value of βj is equal to zero – these are vectors. And I’m going to say that the variance of βj is equal to Ω. So, I’m not assuming anything else about the distribution, as long as we just know what the expected value is, and then we assume that there’s this Ω here. So I’ve not put a subscript on this node, and so this is actually going to be one of the key assumptions that allows us to get the mileage out of MTAG: that there’s this Ω that exists that corresponds to every single SNP that determines the relationship between these effect sizes. So that’s big – we’re going to go back and talk about how this might be violated and then how that’s going to affect all of the results that I’ll show you.\nSo given this, MTAG is a method of moments estimator, and so for those of you who haven’t seen method of moments, I thought we would just take a minute to talk about what that is. And so, we’ve actually seen a bunch of moment methods since we’ve been here; so for example, David, when he was talking about the twin lectures, that’s a moment method. And the idea is we need to have some function of the data that has some expectation that we know about. And so, for David, our moment method was that the phenotype, y. One second, let me think for a minute, I’m going to write this differently. So we would say that the correlation between monozygotic twins for the outcome is – I should have written this down before because I’m going to mess this up – we have some additive factor. I’m really bad at twin models, I shouldn’t have even gone here. We have some correlation, yes, A squared plus C squared, right, good enough, right? Okay, that’s right. So, this is going to be one moment condition, so we know something about our data. We know, and we know something, and we want to estimate these parameters. And then for the R squared for dizygotic twins was just one-half A squared plus C squared, right? So now you know, we can estimate this from the data. We can estimate this from the data. And so, now we have two equations and two unknowns, and we can just solve it all, right? And so that’s why it’s called method of moments, because we have these two things which we call moment conditions and we solve for them.\nSo this is fine, it’s perfectly identified, we have exactly the number of equations and unknowns. Let’s say that we also had, like, non-twin siblings. We could add another moment condition like this, and under some assumptions about how we’re treating siblings versus twins, like even though they’re different ages, these relationships hold, we could add a moment condition like this if we wanted. And note that this is adding more assumptions, so these moment conditions are defining the assumptions of our model, but now we have three equations but still only two unknowns. So, we could take, like, these 2 [gestures to first and second equations] and solve for A and C or we could take these two [gestures to first and third equations] and solve for A and C. And it might give us different answers, right?\nSo, the question is then, how do we deal with that? Well, generalized method of moments is dealing with the types of questions that are like this. So I could rewrite each of these so that they’re equal to zero, all right? So I can do this – I’m just subtracting these terms onto this side, right? So, what we could do is we could instead of saying that these are going to be exactly equal to 0, I can make each of them as close to 0 as possible. But then, we still have this decision about which one are we going to put the most weight on. And so, we could put more weight on this one [gestures to second equation] and less weight on this one [gestures to third equation]  if we think that this one’s [gestures to second equation] maybe more reliable, but we’re making this kind of weight decision. But then you could minimize, so we’d actually estimate these things. So for certain values of A and C, each of these are not going to quite be 0, and so there’s some sort of error. And we could then maybe square that error and then minimize the square of the error, or we could do a bit of weighting across the moment conditions. And so that’s the general principle behind generalized method of moments is we have more conditions than we have covariances, and so we need to have some weight function, but we’re still okay and the estimate that we get is going to depend on the weight function.\nOkay, so we’re going to take this now into the MTAG model. So let’s say that I have a GWAS summary statistic for some trait, so this is β-hat for SNP j, trait s – just one of my traits. And I want to find what the best linear predictor of this GWAS coefficient, conditional on the effect of a different GWAS coefficient for a different trait. And so, just using – if we know our OLS properties – expectation, this thing is going to be equal to the covariance of β-hat (I’m going to leave GWAS off everywhere just because all of these are going to be GWAS coefficients, so if it’s a hat, I mean the GWAS estimate, just to make notation easier) so “J, s” and “J, t” – so this is the effect of the SNP on trait t [gestures to βj,t] and this is the estimated effect on trait s [gestures to β-hatj,s] – over the variance of β­j,t.\nAudience question: Don’t you need a hat?\nPatrick: On this one [gestures to βj,t], yeah, no, no, because I just told you I didn’t need it because what I’m trying to predict the GWAS estimate as a function of the true value. And so, in this case, I’m saying I want to regress the GWAS estimate on the true value.\nThe coefficient of such a regression is going to be this thing here [gestures to β-hatj,s]. So because this is the estimate, it’s equal to the true value plus some error, and that error is going to be independent of the actual effect size. So, the covariance of the estimate with this true value here is just going to be the covariance – I mean, I could write this as β plus E and then say that E is independent of βj,t and then erase it, but I’m just going to erase the hat to skip those steps.\nAnd so now, we have the covariance of the effect on trait s, the covariance of s and t over the variance of t, and so we know what those are because of this assumption that we made here. And so, I can just write. I’m going to call the t,s-th element of Ω, ωt,s, and I’m going to say the t,t-th element is ωt,t. βj,t.\nOkay, so now, we have a moment condition because I’m saying the expected value of our GWAS estimate is equal to this thing. So, I could subtract this over to the side, and then we have one of these moment conditions for every trait. If we were to predict the GWAS estimate for trait 1 using trait t, that’s one element. If we did it for trait 2 using trait t, that’s a second element. We’re going to have a vector of moment conditions, and so I’ve rewritten this in vector form here [note that slide has not changed for video – go to minute 25 for reference]. And so, in this case, β-hatis our vector of GWAS estimates. So, instead of writing a whole vector of ωt,s’s, I’ve turned this into just a vector here. I call it ωt, and that’s just a vector of the covariances between the effect size for trait t on everything else. This is just the same, and this is just the same [points to slides]. So, now, now we have some moment conditions. You have T of them, okay? We have moment conditions, and so now we can solve them.\nAudience question: [Unable to hear on video].\nPatrick: So, I guess I should have said this up front. So, what we really want to know, we’re trying to get an estimate we want, is βj,t, which is the effect of SNP j on whatever trait t, whatever trait t we’re focused on. So, if we want to know the effect of SNP j on depression, then t is depression. And so, you know, even though I’ve used a different subscript here, s could be equal to t, and so it’d be like, “What’s the expected value of our estimate for the beta for the estimate for trait t, given the true value of trait t?” And you see how this reduces here. The covariance of the effect size with itself is just equal to the variance. And so, we have these two things cancel, and so the expected value of the GWAS coefficient given the truth is just the true GWAS coefficient.\nOkay, I kind of just glossed over how we might get at these weights. So, if we have a vector of moment conditions, which I’m just something call m – so this is my principles of GMM, again – just to give you some notation, so in this case, we might be interested in some parameter θ. In this case, θ is βj,t. We have some parameter θ, and this is a vector-valued set of moment conditions. And so, so m of θ, in this case, is this thing [points to upper half of slide] of βj,t. And then the weight function, we have some big matrix here, and that’s just a matrix of θ. So, when we do generalized method of moments, this is –when I was saying, we do kind of a weighted sum that we’re minimizing, this is actually the thing that we’re minimizing [points to equation on board]. And so, if you’ve had a little bit of background with linear algebra, this sort of function is called a quadratic form. And even though there’s a vector on each side of these… let me try to think of the right way to explain this. So, let’s assume that W was equal to I – so this is the identity matrix; it’s just ones down the diagonal. When you multiply this out, what you’re going to get out is just the sum of the squared moment conditions, which is how I described it at first. But if you wanted to give more weight to a certain moment condition, you would maybe just change the diagonal element corresponding to that moment condition to be a little larger or a little smaller. You also have all these off-diagonal moment conditions that can account for potential covariance between your moment conditions.\nAnd so, we have to pick this W, and the question is, how do we get at it? Well, by the properties of GMM, any W, in principle, will get you to the right answer. It’ll be a consistent estimator if you have a big enough sample; you’re going to converge on the truth no matter what you choose. But maybe we want something that’s more efficient, something that’s going to get us to the right answer more quickly, and lots of theory about GMM, it turns out that the right weight matrix, the efficient weight matrix, is just equal to the variance of the moment condition, inverse. So, if we want to figure out what the efficient GMM estimator is, we need to take the variance of this thing [points to top half of slide].\nAnd so, I’m not going to work through that carefully, but I’m going just kind of point out where the different pieces come from. These are missing inverses as well. This should say inverse; this should say inverse; I just noticed that [points to bottom half of slide]. But this matrix in the middle is actually the weight function that you get, and you can kind of see that.\nSo, the variance of our estimate... oh, I never showed this last thing. I need to find some notation. The variance of β-hat [sub j] is equal to is equal to the variance of βj plus the error εj. All right, and then assuming these things are independent [points to βj and εj], this is going to be equal to the variance of βj plus the variance of εj. Okay, we’ve defined this thing [points to βj] to be Ω from here, and then we’re going to define the variance-covariance of the error to be Σj. Now, I put a subscript on this one [points to Σj], and the reason I’ve done that is because this is supposed to be the sampling variance and so it’s supposed to capture… you can think of this like at a baseline as just the standard error, so the standard error is measuring how much variance there is due to the error in your estimate and that’s what this is supposed to do. And if, for one SNP, you have a really large sample size, but then for another SNP, maybe a few cohorts got dropped and so you have a smaller sample size. We’re going to allow for that to happen by putting the subscript here, and so when you have a bigger sample, Σ is going to be smaller to account for the more precise estimate there.\nAudience question: [Unable to hear on video].\nPatrick: Yeah, so it depends on why there’s a difference. So, if the variance is different just because there’s a difference in an estimation error because of sample size differences, then it will capture that. If the difference is because we actually think that Ω is different across SNPs, then it won’t be able to capture that. We’ve had to assume that Ω is the same for all SNPs, and we need that in order to get our estimates.\nLet’s just think for a minute. So, we want to know what the true β is, the true genetic β, absent any confounds. So, I’m going to treat β throughout as true β, not including stratification biases or anything like that, and I’m going to actually throw anything that causes the GWAS estimate to deviate from the truth, and I’m going to store that in here. So, now, Σ not only includes a sampling variance, but it also will include stratification bias and things like that. And so now, when we’re estimating, when we want this βj – I mean true βj, like not βj with error in it or like biases in it. So we can take our moment condition, so I told you, it’s the variance of the moment condition. So, the variance of βj, we worked it out here, is Ω plus Σj. And the variance of this term, with a little bit of work, you can get this term here. So that’s where these come from.\nAnd so then, we plug this into our objective formula here, we do some calc. We take the derivative with respect to β, solve for 0, and what tumbles out is this guy [points to bottom formula on slide]. And so this is great because we now have a closed form solution for the efficient GMM estimator. The only assumption that we’ve made is this one here [points to initial assumptions on slide]. And so, so, yeah, so it’s clear kind of where identification is coming from, and so we can now use this to apply it to different traits.\nAudience question: [Unable to hear on video].\nPatrick: So you’re saying you could add additional moment, so we would have not only just one for each trait but we would have T squared of them. We could choose to not only vary s, but we could also be varying t. Is that kind of what you mean?\nAudience response: [Unable to hear on video].\nPatrick: Okay, well, so the problem is going to be that we need to come up with this Ω, we’re not going to just kind of make it up. We’re going to estimate it, and so we’re going to estimate it using all SNPs. And so if the Ω only holds for some subset of SNPs and we don’t know which subset that is, then I don’t know how you estimate this parameter. Like, in principle, you could say, “Let’s run MTAG and let’s restrict ourselves just to, like, coding SNPs,” and maybe the set of coding SNPs have a different Ω. You could estimate the Ω among the particular type of SNP that you’re interested in, but you’d have to make a choice before you get started that there is some class of SNPs that all have the same Ω matrix.\nAudience question: [Unable to hear on video].\nPatrick: Okay, I should have explained that, and I didn’t. Okay, so the question is, we spend a bunch of time just talking about LD regression. Okay, I don’t want to talk about that. I’ll talk about that after we show results. Well, so Ω, I mentioned, is what’s capturing the genetic correlation, right? The correlation of the effect sizes is going to live in here, just because that’s what we’ve defined it to be. Now, when I was talking about one of the reasons that you can’t just take the correlation of your GWAS coefficients, it’s because the error is correlated due to overlap. And so, by including this Σ, if there is overlap across your traits, what that’s going to mean is that this Σ matrix that you have has nonzero entries on the off-diagonal. So, if they were all estimated independent samples, then the error is going to be uncorrelated across traits, and so this thing will be diagonal, because the error for each of your estimates is uncorrelated because they’re drawn from independent samples. But let’s say that it’s all the same people, then that’s going to mean that your error covaries. By allowing the Σ matrix to be non-diagonal, then that’s how we deal with the overlap problem.\nI want to show you just some quick results of what happens by applying this, and then after we get through that, and I want to talk about some of the limitations and how strong the assumptions that we’re making and how far we can take these results.\nAudience question: [Unable to hear on video].\nPatrick: It feels like it’s going to be similar, but I’d have to think about it. Yeah, because seemingly unrelated regressions allows you to deal with correlation and the error, right? Okay, so it feels like it’s going to be related concepts, but I’m not sure.\nOkay, so, for our application, we took this as an expansion of the sample from the Okbay et al. subjective well-being paper that I think you guys have already seen. And so, we were looking at depressive symptoms, neuroticism, and subjective well-being. And so, our samples all come from different places, and so one of the biggest additions relative to the Okbay paper you saw before is the inclusion of a large 23andMe sample. But you can see that we have a big sample for subjective well-being, a big sample for neuroticism. In the UK Biobank, we have measures for all three of these phenotypes. And what MTAG is going to be really helpful for is this 23andMe sample. We have this really big sample. They overlap, we presume. We have no idea how many people are in this overlapping segment, but we want to be able to combine the information across these things. And so, MTAG is going to allow us to analyze all these together despite the large amount of overlap across these summary statistics.\nSo, to give you a sense of how much you gain by applying MTAG, so this is a Manhattan plot of a GWAS for depressive symptoms. So, we’ve expanded our sample size a lot, and so we’ve already jumped up to 30 SNPs from – [asks audience] how many did we have before?\nAudience response: Three.\nPatrick: Okay, we’ve gone from 3 to about 30.\nAudience comment: Two?\nPatrick: Just because of our expansion of the GWAS. Once we apply MTAG, we jump up to 64, so we’re gaining twice as many hits because of this. For neuroticism, we start with 9 and jump to 37 after MTAG. For subjective well-being, we started at 15 – okay, I was, like, does that say 19? It says 49. I’m, like, I thought that it was much bigger than 15 to 19, and it doesn’t look like 19 X’s here either.\nOkay, so victory – are we good? Like do you believe all of this? You shouldn’t, at least not yet. David can’t because he’s seen everything else that I’m about to show you. So someone sees doesn’t say, “No way, like, these aren’t reliable.” This is your crazy new voodoo method, like, “I don’t believe you.” So what can we do to help show that maybe we’re okay?\nAudience response: [Unable to hear on video].\nPatrick: I’m talking about something even easier than that. Whenever we want to talk about credibility, what’s the first thing we should do? Replication! So, let’s replicate, okay?\nSo, let’s say that what I want to do is I take the subjective well-being, all of the top hits from subjective well-being, and I have this replication sample and I’m going to compare my hits from MTAG to my effect sizes in my replication sample. Okay, I’m just going to do a direct – I’m going to regress one on the other and see how it looks. How do you feel about that?\nAudience response: [Unable to hear on video].\nPatrick: Yeah, yeah, they’re non-overlapping, so I’ve done a really good job. Why don’t we want to just do a direct comparison of the results from here to just results in an independent sample?\nAudience response: [Unable to hear on video].\nPatrick: Yeah, winner’s curse! I taught you useful stuff. So, if we wanted to do this comparison, if we didn’t deal with winners’ curse, what’s going to happen is we’re going to look at our top hits here and then we’re going to look in our replication sample, and these hits are all going to be a lot bigger or bigger – I don’t know how a lot, it depends on how noisy this is. And so, we might show this to someone, and they say, “Oh, well, you know, MTAG’s just cheating. Look, your replication sample, your effect sizes are a lot smaller,” because they know they’re obviously a lot smaller. And even better than showing them that, you can just fix it and so they don’t worry in the first place. And so what we did is we took these hits, and we did a winner’s curse correction, exactly like the one that you guys are doing in your problem set. Almost exactly like the one that you’re doing in your problem set. There’s a little bit of nuance we just discussed in the problem set. You guys can read through that. Well, yeah, so we’re going to shrink these according to how much winner’s curse we have, and then we’re going to regress these estimates or we’re going to regress the replication estimates onto the estimates, the genome-wide significant hits that we have here. And if we replicate, then expect a slope of about 1.\nSo what I’ve plotted here is the estimates of the slope for the genome-wide significant hits for the 3 traits, and it looks pretty good. I mean, these are kind of noisy because in our replicate our replication sample here is the Health and Retirement Study and Add Health. And so, it’s not a really big sample and so the estimates are kind of noisy, and as a result, this regression is a little bit noisy. But in every case, you know, we can reject 0, which means that these new hits that we’re finding are likely to be something real, and we can’t reject 1, which is probably about as good as we could expect to see for something like this.\nGreat, so that’s replication. Another kind of replication you think about is what if we did prediction?\nAudience question: [Unable to hear on video].\nPatrick: So in this case, I just used the genome-wide significant hits.\nAudience response: [Unable to hear on video].\nPatrick: The number of hits you’re going to get, so do you mean hits, SNP hits? What kind of hits are you asking about?\nAudience response: SNP hits.\nPatrick: So what we did is we took this – so the HRS was in one of the discs, it was in the subjective well-being discovery sample, and so the first thing that we did is we removed it from discovery. Then we applied MTAG, and it gave us a set of between 40 to 60 hits and those are the ones that we used. I don’t think that probably any of those hits are actually genome-wide significant in the Health and Retirement Study. I don’t even know if any of them are Bonferroni significant. But on average, they should be the same. And so when we do this regression… I’m not sure if I’m answering your question because I wasn’t totally certain what you meant about the replication hits.\nAudience response: [Unable to hear on video].\nPatrick: Yeah, so we have 60 hits that are significant in our discovery sample. We only do the regression on the 60. You’re asking if we did it with all.\nAudience response: [Unable to hear on video].\nPatrick: So in the replication regression, it’s just kind of one number. We’re taking the effect sizes, the effect size estimates. Maybe I didn’t explain this very well. So, what we did is we did a GWAS just in the HRS, right? And so just for the 12,000, however many that are in there, and none of those are significant because the HRS is way too small.\nAudience response: [Unable to hear on video].\nPatrick: Yeah, yeah, so in expectation, these two estimates should be the same, assuming that MTAG is working. But our estimates in the replication sample are super noisy, which is why we’re going to test all of the genome-wide significant hits at once, because even each of them are very noisy. But if we do them all in a regression, we’re hoping to say, on average, do our hits replicate?\nAudience response: [Unable to hear on video].\nPatrick: Yeah, so if we had a bigger replication sample, we could even do a SNP by SNP.\nAudience response: [Unable to hear on video].\nPatrick: Yes, that’s true. And in fact, we kind of want to do this. So, UK Biobank is going to be available in, like, a couple of weeks or however soon, and with the additional 300,000 from the UK Biobank, in principle, we could do a SNP by SNP replication of all of our hits. So, and we intend to – I think it would be interesting.\nSo, I don’t know if that’s actually you meant, but what I thought you meant at one point was, so this is just to check to see how confident we are in the set of hits. We might be interested in how confident are we in general across all sets. We’ve done the GWAS in all of the hits. So, so do we have maybe some – are we going to get an increase in performance throughout the whole genome? So, we could do this with prediction. So, what we’re going to do in this analysis is I’m going to take the GWAS summary statistics and I’m going to use and create a polygenic score like you’re going to learn all about next week, just based on the estimates from the GWAS summary statistics. And then I’m going to also do the same thing. I’m going to take the MTAG summary statistics and make a polygenic score just based on those, and I’m going to then go into our replication sample and I’m going to see how well the GWAS predictor does relative to the MTAG predictor.\nAnd so, that’s what we see here. So, none of these traits are incredibly heritable, and so kind of you notice that, in levels, each of these things are not incredibly predictive. They, you know, they all predict in between 1% and 2%, but what we can see is that in all three cases, when we start with the GWAS and then go to the MTAG predictor, we’re seeing pretty substantial – these are about 25% increases in the predictive power of these scores. Now, it’s a little deceiving when you look at these standard errors because you say, “Oh, but those aren’t significant differences,” right? The problem is that these estimators are highly correlated. So, if we actually just tested the difference between the levels of those bars, we actually have statistically significant increases in all three cases. And so, this is helping us be even more confident. We are getting additional information. Our predictor is getting even better because we’ve applied MTAG. So, you know, it helps us think that it’s even more credible.\nAnother claim that I made was that it could just be that these summary stats we’re giving you are just this weird Frankenstein GWAS that’s contaminated by all of these different traits. And so, you know, but I’m going to claim that actually the GWAS summary stats from MTAG are giving you trait-specific estimates, and so the GWAS summary stats is for GWAS and not for neuroticism. And so, in order to test that, what I’m going to do is I’m going to take the score that we made for depression and the score that we made for neuroticism and the score we made for subjective well-being, and I’m going to try to predict depression with each of those scores and I’m going to do the same thing for each of the other traits. In all three cases, the score for the own trait predicts better than the score for the other two. So, you can see, depression’s the best here: the depression score. In the middle one, neuroticism, the best score. And for subjective well-being, we see that the subjective well-being score is the best one. And again, if we want to look at how significant these differences are in five out of six cases, the own trait score is more predictive than the other trait score. So, you know, even this claim about that we’re getting phenotype-specific information by combining these traits, it seems to be valid.\nOkay, so then we say, “Well, what can we learn? Is there anything that we can...?” Oh, go ahead.\nAudience response: [Unable to hear on video].\nPatrick: Yeah, sorry. So, this is how much better the depression score is than the neuroticism score, and this is how much better the depression score is than the subjective well-being score. So, a positive number means that the own trait score does better than the other trait score. So, yeah. So, depression does this much worse than the neuroticism score when you want to predict neuroticism.\nSo we might want to be interested in what can we learn about pathways and biology and what’s involved, and looks like we get a lot of information as well. So, these are results from DEPICT analysis, James did all of these for us, and so we’re going to use DEPICT and say, “What are the tissues that seem to be implicated in the GWAS summary statistics?” And you see there’s, like, maybe a little bit of enrichment here in the nervous system, so that’s seems reasonable. Depression, nervous system. When we apply MTAG, we see enormous enrichment in the nervous system, plus a little bit in neuro stem cells in the retina, but the retina is just a neuron, so this is totally reasonable. We’ve added additional information, and we’re seeing inflation kind of where we expect to and then we could then dig into what those tissues are if we want to try to better understand what depression is.\nYou can do the same thing looking at gene sets, and so these are the lists of the gene sets identified by DEPICT, and you can’t really read this, so the SNPs that we’re finding for the depression results are things related to fear response or things related to synapse. I think that James was telling me that one of these gene sets is a mouse phenotype that measures how quickly they give up when you put them in mazes or something like that. James can correct me if I’m wrong, but I like that story.\nAudience response: [Unable to hear on video].\nPatrick: Because they’re so sad. Yep, but, again, it seems like we’re trying to study depression and it seems consistent with kind of the bio stuff that we already know, and so this is kind of exciting, interesting things.\nAnd so, you know, I’ve shown you these results here. I can also give you a preview. So, cognitive performance, I think, is something that a lot of us are very interested in. It’s really hard to study because it’s only available in very small sample sizes, right? So, if we took the largest GWAS of cognitive performance and we use the summary statistics to make a score for that, the predictive power of that score is 1% or something along those lines. Robbie King knows this better than I do, so you can tell me if I’m wrong. If you, instead of taking the cognitive performance score, actually use our education analysis and you created an education polygenic score and tried to predict cognitive performance, now that might be reasonable because these two things are related. But we have a much larger sample for education and so we might think we can do better – and we do. We get about 6% predictive power if we try to predict cognitive performance using a score that’s designed to predict educational attainment, alright? But if we MTAG the education results with the cognitive performance results and then create a cognitive performance MTAG score, the predictive power jumps to 8.5%, and so this is like – I’m really excited because before these cognitive performance scores were just not big enough to be useful, and suddenly using methods like this, we’re getting enormous… I mean, 8.5% is enough to start doing some really interesting polygenic score work.\nThis is a QQ plot, also, of what happens if you look at the number of hits that you get. So, cognitive performance, I think, in the largest, most recent study had, like, 30 hits or so, something on that order. I think that when we MTAG it, we get 192 genome-wide significant hits for cognitive performance based on MTAG. So, this is exciting, exciting times. This is why Dan is like, “Oh, maybe we’ll start seeing it.” I’m excited. I hope that everyone in the world gets excited about it.\nThere’s a couple problems, though. So, let’s go back to some of the assumptions that we’ve had to make along the way. So, the first thing is not really an assumption. It’s just a practical problem. Now, this whole time, I’ve just been using this Ω and this Σ, and I just said, “Yeah, we know what those are. It’s great, right?” We don’t actually know what they are. We have to estimate them. And because we have to estimate them, it’s going to add additional noise into our estimator.\nAs an aside, let’s chat a little bit about, just briefly, about how we’re going to estimate them. So, first, let’s talk about this Σ. I claimed before that this is capturing both sampling variation and biases due to things like stratification. So, we’ve seen something today that measures the variance due to sampling variation and biases. Who can remember what that thing is? We’ve only talked about one thing today before this. So, in LD score regression, we had the slope and we were focusing on these heritability estimates, but the intercept, when Raymond puts this on the board, the intercept was… I’m running out of space, I’m going to erase this because who cares? So, we saw that the expected chi-squared statistic was equal to a thing that was proportional to the LD score plus 1 plus Na. Alright, so this term [gestures to “1 + Na”] here is what we’re getting when we get the intercept. This 1 is variation due to the sampling – this is what we expect if there was just sampling, and Raymond tried to claim that this a here was variation due to biases and I believe him – Raymond’s a smart guy, you know. So, in order to get these estimates along the diagonal, we’re going to use the intercept from these univariate LD score regressions, okay? And then I claimed earlier that the off-diagonal portions here captured things related to sample overlap or correlation of biases, and that also sounds like something that Raymond was talking about earlier today, all right? When he was... earlier today, we had the expected value of z1, z2 is equal to a thing proportional to the LD score plus this thing related to… but there’s this term here that was related to the sampling overlap and it turns out that this is exactly the term that we want in our off-diagonal terms for Σ as well. And so we’re able to use LD score regression to get that as the estimates here of this Σ. And remember that the Σ is what allows us to use summary statistics with the overlap, and so it’s a really important term. We’re really building hard off of LD score regression and taking advantage of some of the results that we talked about this morning.\nAnd then to get Ω, what we do is we just look at the actual variance of the... we can take all of our β-hats and estimate the variance-covariance matrix of the estimates and we know that the variance-covariance is equal to Ω plus Σ. We have an estimate of Σ, so we just subtract it out. And so when we want to get an estimate of Ω, we’re just going to do this directly by subtracting out the Σ that we estimated from the variance-covariance matrix of the Σ here. And there’s a method of moments interpretation for that too, but I think it’s pretty easy to see why this works, okay? So I said we estimated this, they’re noisy estimates. But when we plug them into MTAG we didn’t do anything to account for that noise, which might make us worry that our standard errors from MTAG are going to be too small because those standard errors are only capturing variation due to sampling in the beta hats and the GWAS estimates and not sampling in these variance-covariance matrices.\nThis is going to become especially a problem if you want to apply MTAG to lots of traits, right? Because the more traits you add, then the number of elements in these matrices is going up by the square. And so, as you add more traits, there’s more noise and there’s more noise that we’re not accounting for, and so we might think that MTAG is going to do worse and worse and worse as you add more traits. And that’s actually true.\nAnd so, what we’ve done here, we’ve done a set of simulations where we generate summary statistics, but because we’ve generated them, we would generate just pretend summary statistics that we know what the Ω and the Σ are. And so, we run MTAG using the true values of Ω and Σ and then we run MTAG using the estimated values of Ω and Σ, and we look to see how much inflation there is in the test statistics in that case. And so, you know, for just one trait, there’s not really much inflation. In fact, even when you have, like, 2 or 3 traits, it looks like the inflation is not that large. But so, the mean chi-squared is a measure of the power of the GWAS. So, if you take the mean chi-squared statistics from all the chi-squared statistics from your GWAS and just take the average of them all, the mean of them all, then we use that as how powerful the GWAS is to give you a sense of what 1.4 is what we see in our three traits. And so, depression, which has a heritability of 4% or 5% in our data and a sample size about 300,000, that’s about 1.4. Or neuroticism is more heritable. I think it has a heritability of 0.8, but it’s available in only 150,000, so that’s also 1.4. This number here scales with sample size – and so, a GWAS that has a mean chi-squared of 1.1 is 4x as small as a GWAS that has a sample size of 1.4. And going from 1.4 to 2 is like 2.5x larger. And so you can see that if you have a low-powered GWAS and you combine a bunch of low-powered GWASs together in this way, as you get up to like 20 traits, the inflation goes up about 3%. So, I mean that’s maybe not a ton, but maybe it’s enough to worry about. However, when you have kind of medium to high-powered GWAS summary statistics, the error, the problem that I was telling you may exist, doesn’t seem to be a problem.\nAudience question: [Unable to hear on video].\nPatrick: Okay, yes, so when I say “traits,” I mean GWAS, and that’s an interesting question because there’s nothing about a trait that means that it’s a different thing. So, we could, for example, use MTAG and use it as a form of meta-analysis, right? So, let’s say we have six cohorts, and those cohorts maybe come from overlapping samples and so we can’t just meta-analyze them because there’s overlap across the cohorts. However, MTAG knows how to deal with the potential overlap between your cohorts, and so you could maybe instead of just doing a standard inverse variance weighted meta-analysis or sample or something like that, you could just pass it in to MTAG and MTAG should fix those problems – except for this thing here.\nRight, so in educational attainment, EA2, how many cohorts did we have? I see, like, 70?\nAudience response: [Unable to hear on video].\nPatrick: Yeah, so 64. So we’re way out here – I’m leaving the camera and I’m back. We’re three times further over there for something like EA2. So, if we had meta-analyzed EA2 with MTAG, each of these cohorts has a really low mean Chi-squared, probably like 1.01 or something on that order, and so the bias, it goes up through the ceiling for... for something like that. So you need to be a little bit careful in applying those settings, but if you have moderately powered cohorts but you’re looking at the same trait, then MTAG in principal could work.\nAudience question: [Unable to hear on video].\nPatrick: So actually, this is only just for one trait at a time. Everything I’ve shown you is under a model where we only care about the single trait, but we have information on these other traits. I mean, the MTAG estimator has these whole matrices in them, and so I don’t think you can get it out. I mean, maybe you can. I don’t see how one would.\nAudience question: You have two proxies for some underlying trait, and you have different samples where you have estimated the effect of some variable on those proxies. You basically do this to get a better estimate. Seems like, say you need that sample of a 100,000 to have power, so couldn’t one adjust these methods for the same thing to a sample of 10,000 with 10 different traits that proxy for the same thing and basically get kind of the same result? It seems like if you do this correctly, you can basically stop to worry about all this and look at proxies within the same sample. You can basically just use one of these samples.\nPatrick: So the question is, so let’s say you have thirty traits and they’re in three groups, right? And so, you could just first meta-analyze each of these three groups and then do MTAG on each of these three groups. Is that kind of what you’re saying or no?\nAudience response: No. Here you are basically working on the summary stats level, because you don’t have access. Let’s say you have Add Health, which only has 10,000 people, but let’s say you have 50 proxies for educational attainment.  It seems like each proxy is the same underlying plus noise, right? You could rerun everything for each individual and then basically cluster at the individual level to take care of standard errors, but you should be getting 10x the number of observations. I’m sure you could play with the math – that’s why I was asking about the Ω. Here, the Ω you are using the βjs, all the betas in each cohort, and correlate to find the Ω. But let’s say you have only one sample.\nPatrick: Yeah, you could. So, if you just did a GWAS, so let’s say we just had UK Biobank data, right? And so, we do a GWAS of… so the UK Biobank has data on depressive symptoms, neuroticism, and subjective well-being, so we could do a GWAS on all three of those traits and then pass those straight into MTAG. And the overlap is going to get addressed by this thing. So given that we have the individual-level data, there may be more efficient things that could be done and methods like that exist.\nAudience response: The simple thing is you can calculate what Ω should be.\nPatrick: How? If you just have the individual-level data, why can you get Ω any better? Because you’re still just estimating the SNP effects.\nAudience response: [Unable to hear on video].\nPatrick: Oh, Σ. So Σ here, we wouldn’t have to estimate. Well, we still kind of need to estimate a little bit. It’s not quite as bad because if you have perfect overlap, then Σ turns out to just be the covariance variance matrix of the phenotype itself, approximately.\nAudience response: [Unable to hear on video].\nPatrick: This was designed to do kind of to deal with very specific data constraints that we face, which are the ones I showed you before. We don’t have the individual level data in the first place.\nAudience question: So suppose you really had like 20 phenotypes. It seems like you maybe could do better and get less inflation if you either make your Z-scores and then take an equally weighted sum or maybe you do the simple regression predictor from the other 19 things. And then you just have 2 things and you kind of impose that. It would be interesting to see those simulations.\nPatrick: Yeah, so things that you could do if you’re willing to make assumptions about... So, for example, if you wanted to assume that your traits are perfectly correlated or that we know stuff about the traits, so if we’re doing meta-analysis and we really believe that we have the exact same trait in every case, then we can impose that assumption. So, if it is the same trait and the only reason that they’re different is because of noise, then that means that the Ω should correspond to perfectly correlated traits, and so we can impose this assumption directly on here and that should give us a little bit more precision.\nAudience response: I’m saying that might be a robust way to do things when the assumption is wrong.\nPatrick: Yeah, so, we actually explored that a little bit in the paper – what happens when you just make these assumptions and fix these parameters? In general, we do a little bit better when we allow them to be free, but I mean, it really depends on how close. So, if the genetic correlation is just 0.95 and you fix it to 1, you’re not going to gain a lot by allowing it to a free parameter.\nAudience response: I was talking about the case where you have a huge number of phenotypes.\nPatrick: But even then. So, if the truth is close to whatever assumption that you’re making, then you probably are fine just fixing it to that assumption and you probably gain a little bit, in fact, since there’s less noise. But if it’s not quite there, then you know how much you gain depends on how good your assumptions are.\nAudience question: [Unable to hear on video].\nPatrick: But then like that thing that you’re describing, it also can be described exactly in an equivalent way of just putting structure on the... So you know, in practice, you could just meta-analyze things you think are similar and then put it in. It’s equivalent to just doing MTAG with some structure on these parameters.\nSo, that’s just one problem. The other problem is actually a pretty major theoretical problem, and it’s this constant Ω assumption. Okay, so by this constant Ω assumption, it means that there’s kind of just one class of SNPs, and the relationship between the SNPs are the same no matter where you are in the genome. So now I’m going to imagine that I’m doing a regression, and I have two phenotypes that are pretty highly correlated just in general, but I have a SNP where it’s highly correlated… so, for the trait I’m interested in, it’s actually no effect size for that, this β is equal to zero for that one. But then for another trait that I’m including, it’s actually really important; it’s super strongly significant; it has a big, large effect size. So what’s going to happen in MTAG with something like that, if we assume that there’s this kind of strong relationship between the βs throughout, but it just so happens that there are SNPs that meet that criterion that I described there, where it’s actually no for the trait you want, but the other trait is strongly significant? Well, what MTAG is doing is it’s kind of using information from the other traits to get a better guess of what’s happening for this one that we want, and so it’s, like, “Wow this is... this is really strong, you are so confident that this is real. And then you have this one here, like I don’t really know, it’s kind of low. But this one’s for sure, it’s great, right?” And so MTAG is going to look at that and say,“Well, then this one’s probably great too, because you’re doing at correlation is like 0.8.” And so, it’s going to pull that away from 0; we’re going to be biased away from 0 in a case like that. And so, it may even pull it away enough that that it becomes genome-wide significant when it really has no business being genome-wide significant. And so, that would be an inflation of the false discovery rate because we’re going to find more of these SNPs that are actually null for the trait we want, but because they’re causal for a different one, those SNPs are getting pulled into the nominal category.\nAudience question: [Unable to hear on video].\nPatrick: So, this inflation I’m talking about here is because when we calculate our standard errors, we assume that β and Ω and Σ are known, but because they’re unknown, we probably should add a little bit more to the standard errors to account for that uncertainty, and so that’s why that’s why we’re getting inflation here. Whereas in this case, it might be even if we knew exactly what Ω was, we estimated there’s a genetic correlation of 0.9, right? Even if we knew that exactly, we would still have some of these SNPs that are being pulled away from 0 or we may still have some of these SNPs that are pulled away from 0, just because there are SNPs that are of the type that I described: where it’s actually null for the trait you want to know about, but nominal for the one that you’re just using to boost power.\nI’m trying to think of what a good example of such a SNP would be. So like, let’s say, okay, who can think of an example? Raymond?\nAudience response: And I mean, for both smoking and alcohol dependence, there are loci of large effects that correspond specifically to the metabolism of the substance. So, in alcohol, there’s a couple of mutations that make consuming alcohol extraordinarily unpleasant because of how you metabolize the alcohol and, thus, people with those mutations just basically never drink. In smoking, it’s a mutation in the cholinergic receptor that makes the nicotine very enjoyable. So you can imagine that while there is general overlap between ADHD and substance use phenotypes, that the effects of these substance-specific variants are indeed specific to the substance use and have nothing to do with ADHD. And so, we have exactly this kind of problem of a very large effect in one of those substance use phenotypes, but it is generally correlated with ADHD but we’re fairly confident that it’s basically null.\nPatrick: So that’d be a problem, that’d be problematic. So, we would see, it’s like, “Oh, we’ve done MTAG, and we see that the nicotine receptor that’s really strong in smoking all of a sudden is associated with ADHD,” and that’s silly. I mean maybe it’s real, but it seems a little odd, right? So, given the story I told you, it should be pretty clear that this is going to be a bigger problem when the trait that we’re interested in is not very well-powered and the trait that we’re adding to it is super highly powered, because remember, this story, it’s like: “I’m, you know, when I say I’m really confident about this SNP, if we had a really large sample, we become really confident about that SNP, right? We’re just totally sure we’ve estimated with perfect precision and this one here is really noisy, and so MTAG’s, like, ‘Oh, well, there’s a lot of noise here, but you’re so confident.’” It’s going to have a lot of pull when the trait that we want is weak and the trait that we’re adding is very strong.”\nSo, I just showed you some results that were exactly like that, right? So I showed you what happens when you MTAG cognitive performance, which is a really weakly powered GWAS, with educational attainment, which is really strongly powered. And so, even though I just showed, “Oh, there’s 200 hits,” I think that you should be very, very cautious in thinking that those hits might be real, because this is exactly the type of circumstance I’m going to show you.\nSo, let me show you a picture to kind of highlight this. So, the big issue is that we don’t know what SNPs these are, right? So, there may be SNPs of the type that are going to break MTAG, but we just don’t know what SNPs those are. And we don’t even know what the joint architecture is: if there are any SNPs like that. And so, what I’ve done here is I’ve calculated the maximum false discovery rate under some assumptions about a class of genetic architectures. On the x-axis here, I have the genetic correlation between the two traits, and the y-axis, I have the maximum false discovery rate you can get given the correlation between the two traits, and so each one of these lines, for the trait that we’re interested in, I fix the mean chi-squared to be 1.1, so this is kind of the low power setting, and then I’m going to allow the mean chi-squared of trait 2 to get larger and larger and larger. Let me actually highlight something at this point here, so if the genetic correlation is exactly 1, there can be no SNPs like the ones I told you about. Why not? Why will there be no SNPs like the ones that I was explaining that break us? Well, it’s not the p-values that are relevant; it’s the βs. Well, if they’re estimated in different samples, the chi-squares might not even be similar. What’s relevant is the βs, right?\nAudience response: [Unable to hear on video].\nPatrick: If they’re perfectly correlated, rβ of 1, that means that one is just a constant multiple of the other. And so, if it’s causal for one, it has to be causal for the other. If β is 0 for one, it has to be zero for the other. And so, the false discovery rate tends to be limited when the genetic correlation is pretty high, but it’s in kind of this medium range where it can really get really bad. As one example, you can see false discovery rates up to like nearly 20%, so 20% of the things that you find are likely to just not be real or up to 20% could be – in the worst-case scenario. So, this is just a word of caution if you’re combining.\nOh, go ahead.\nAudience question: Patrick, would this be that you take some SNPs across 5x10-8 and understand which one of those are false discoveries?\nPatrick: Yeah, that’s what I meant. So, I say of the genome-wide significant hits, how many of them actually have an effect of exactly 0?\nAudience question: Could this be a problem in the following scenario? Let’s take IQ and educational attainment. So, there’s something that was previously not significant for IQ, but you throw together in MTAG and it’s significant for educational attainment. Maybe it’s false, maybe it’s not. We’re trying to figure out how big a problem that is. It seems that MTAG has the potential to identify completely novel SNPs, like you’re combining the power from both of these things – could this be a problem for that, too? You get what I am saying? It could be that you put IQ and educational attainment together, there could be some set of SNPs for both of those things, but if you put them together, MTAG might discover completely new SNPs.\nPatrick: We had 30 hits for depression in the GWAS, and then we have 60 and MTAG, so 30 of those, in principle, are new. So, is that not what you mean, or what do you mean exactly?\nAudience response: Gotcha. But were any of those 30 hits previous hits in neuroticism or subjective well-being?\nPatrick: Oh, no, not really, because I mean, those are all really low powered. I don’t think we’ve actually looked at that, but I just don’t think so. There were only nine neuroticism hits, for example.\nAudience response: My question is: are we worried about false discovery in this novel set, like the 30 you were talking about? Or are we really only worried about previously significant educational attainment SNPs? Or previously significant depression SNPs dragging up educational attainment or neuroticism?\nPatrick: So, in this calculation, we’re talking about the whole set, so we’re not talking about which, like, additional ones have been added. We’re talking about false discovery in the complete set that MTAG finds. That’s an interesting point. Well, I’d have to think about that some more if there is kind of nuance in talking about which SNPs were added because it could be even that we lose a couple after applying MTAG as well.\nAudience question: [Unable to hear on video].\nPatrick: I saw I’m a little confused by what you mean.\nAudience response: [Unable to hear on video].\nPatrick: Well, I mean, no, I think it’s kind of related. But if you think about it, in if there are any SNPs that are these “MTAG killers” where it’s zero effect for this guy and a non-zero effect here, putting any weight on this guy will drag it in the wrong direction.\nAudience response: [Unable to hear on video].\nPatrick: Is this is this one, null SNP, or there’s real hit?\nAudience response: [Unable to hear on video].\nPatrick: So you have a big sample for the true one.\nAudience response: [Unable to hear on video].\nPatrick: And I think that’s what’s reflected here, right? So, this corresponds to the big sample, and so as you’re giving more information, you’re saying, “Oh, I have more information in the nominal hit, and so I’m going to put shove that information into the trait that I’m interested in.” And so, that’s why the false discovery goes up as you add more information here, because you’re saying, “I’m more and more confident about this guy, and because I’m making assumptions about this relationship, I’m allowed to take some of this information that I keep adding and push that information into this estimate.”\nAudience response: It is feeling a little bit like this method of moments procedure is good for creating compelling genes to explore, but if you really want to carry over hits from educational attainment to cognitive, maybe you want to do the Bayesian imputation of the spike-and-slab kind.\nPatrick: So, MTAG originally was a spike-and-slab model with three phenotypes. I think it took like three weeks to run.\nAudience response: [Unable to hear on video].\nPatrick: It’s on the docket. And so yes, this is going to be really good in cases where you don’t think that there are a lot of these really awful SNPs. But just to give you also a sense, so, this is again I was trying to do this, this is worst-case scenario, alright? If you have low-powered SNP, a low-powered trait, and a high-powered trait, and you want to know what the effect sizes are for the low-powered trait, then this is a measure of how bad things can get, right? And again, this is worst-case scenario. Might be that actually, given the true joint architecture, the true false discovery rate is down here, but because we don’t know what the joint architecture is, I plot this to say, “This is a scenario that maybe could kill you.” You could, in principle, if you think you have a guess of how many of these killer SNPs there are, you could impose that and actually calculate what the false discovery rate is in that setting, and maybe you’d find that actually the false discovery rate is down here. And so, in that, you just need to take that information, so say like, I have a hundred hits, my false discovery rate is 9%, here’s some hits, I think about 9% of them are probably not real, but 91% of them are fine.\nAudience question: [Unable to hear on video].\nPatrick: Yes, so I’m going to get there on the next slide.\nSo, I was trying to show this, because it’s worst-case scenario. So let’s say that instead, we’re combining comparably powered GWASs for those traits. So, in this case, I plotted max FDR again, and you’re combining two traits that both have a 1.1, and so this is actually the same line, but if both of them have a power of about 1.4 or both of them have a power of 2.0, then the maximum false discovery rate is actually quite low. In the results, I showed you for depression, all of our traits are about 1.4, and they have a genetic correlation of about 0.7, and so, you know, it looks like we’re kind of living right there.\nAnd so, the intuition about why, even if there are really bad sets of SNPs, why we’re okay. So, before I told you the story, it’s like we have this, you know, this estimate, which is actually 0, but it’s a very noisy 0, and we have this one here, and it’s a really strongly powered, you know, non-zero because there’s so much uncertainty here, a lot of information is getting pushed in here. It’s like, well, this guy’s like, I don’t know, so I’m just going to use this, right? But if we actually had a really powerful estimate here, then even if we had an enormous super-powerful estimate here and so this guy says, “Hey, hey, like, I think that we’re probably good, right?”, this guy’s like,“No, because I’ve been estimated in a large sample size, and so even though, even though I know that you think that I’m really big because of Ω, I don’t think I’m very big,” and so this one doesn’t budge as much. And so, as a result, it leads to false discovery rates that tend to be a lot lower.\nAudience question: [Unable to hear on video].\nPatrick: It has to do with some of the assumptions that we’re making. And so, we’ve assumed that at least 10% of SNPs are causal in calculating this max false discovery rate. You can get some really funny things happening if you assume that, like, 99.5% of SNPs are null for all traits, and then there’s a really high correlation in a small set of them and things just get really wonky. And so, but we think it’s likely that at least 10% of SNPs are nominal for any trait that we’re interested in. You can actually estimate this. In depression, we think that about 60% are non-null, but yeah, so the peak here has to do with the boundary being really binding but not quite as binding in this area.\nBut yeah, if you have kind of high-powered things and you’re combining them, then maybe you’re okay. But the thing to really, really be nervous about is settings where you have something that’s low-powered and you’re combining with something that’s very high-powered. And something to be kind of worried about is if you have a whole lot of low-powered things that you’re trying to combine. I think that’s my last slide, so like I said, we’re excited about this. It seems to work really well in practice. I think like Miles was saying that we have some theoretical results which suggest that even if you have these death SNPs, predictive power, if you do MTAG, should always get better. And so, if you just want to make a polygenic score, then I think MTAG is pretty much always a good choice. If you want to find novel GWAS hits, then you have to worry about this depending on the setting. So, it just really depends on what question you want to ask and what your data look like, and so you should think about kind of these assumptions that we’re making, and this is true about kind of all the things. You know, we’ve been focusing so hard this week on what are the assumptions of this model, because depending on where you are, the assumption may or may not be more or less reasonable. And depending on what you want to do with the results, violations of those assumptions may be better or worse.\nAudience question: [Unable to hear on video].\nPatrick: I mean, except for this problem, right? So that would be the only reason why maybe you wouldn’t do just every single trait you can get your paws on. But if you have a bunch of things that you think are moderately powered and you just want to make a polygenic score, then I don’t know why you wouldn’t do that.\nAudience question: [Unable to hear on video].\nPatrick: As long as you live more here – if each of your each of your GWASs have a chi-squared of 1.4-ish, then combining all of these things together is fine. I mean, even if you’re at 1.1, you’d expect to see a 1% inflation with 10 questions. Maybe you’re okay with that. This is the case of the mean chi-squared, so each SNP has its own chi-squared statistic and we take the mean of all of them.\nAudience question: [Unable to hear on video].\nPatrick: Yeah, I guess you could. So yeah, you could run a simulation like this and with whatever the mean chi-squared is and your number of traits. That’s an interesting idea. So yes, I guess you could actually figure out how much inflation there is and then account for that.\nAudience question: [Unable to hear on video].\nPatrick: Yeah, we don’t know that, but it’s something we could look into. It’d be interesting.\nAudience question: [Unable to hear on video].\nPatrick: I wrote it and then I erased it to say, “I’m going to” – wait, what did I erase? Did I actually write the original and then erase it?\nAudience response: [Unable to hear on video].\nPatrick: Okay, so the question is what exactly then?\nAudience response: [Unable to hear on video].\nPatrick: So this was the thing. So, before we assumed that the βs were normally distributed, right? Yeah, so the βs maybe aren’t normally distributed, right? In fact, they probably aren’t normally distributed, and so then people like worry about what’s going to happen if they’re under violations of that assumption, and so we could go through and we could try to like – and this is what we did in the original paper, we did it with like a t-distribution and with the spike-and-slab, we went crazy and we tried to look at what violations of this distributional assumption, what the consequences were for MTAG. But we found in general that it didn’t seem to matter, and we realized that the reason it didn’t matter is because our assumption was too strong. In the original paper, our estimating equation was this thing, and again, it’s this exact same thing, the equation didn’t change. And so, the reason we weakened the assumption is we can say, so now we can say it doesn’t matter what the distribution of β is, as long as this holds, everything about MTAG holds. And so then, people don’t panic as much about, “Oh, whoa, my effects have thick tails? It’s okay – is Ω constant?\nAudience question: [Unable to hear on video].\nPatrick: Yeah, that’s true, and so the standard errors that we use hold exactly if they are normally distributed. It turns out they hold asymptotically in general, but we had we’re a little funny about the asymptotics here, because the unit of observation here is number of traits. And so, it holds asymptotically with number of traits that, we don’t go to infinity there, but you can also show that actually even if the distribution assumptions violated, it turns out that the MTAG standard errors are conservative on average. And so, the standard errors should be okay for the most part. Except for the SNPs that are null for one trait and nominal for the other, you can show that it is anti-conservative for those.\nAudience question: [Unable to hear on video].\nPatrick: That is a great question. So, the MTAG standard errors are... well, I’ll just point to it here, it’s easier. So, MTAG standard errors are 1 over this thing, assuming that we fix to the inverse, right?\nAudience response: [Unable to hear on video].\nPatrick: And so, and what, oh yeah, that’s another... I wrote these last night at like midnight. This should be minus, and so, yeah, so but if you actually use the standard errors that are in the paper, then you know what Ω is and what your Σ is, maybe, or you can get a guess at it for your power calculation, then you could calculate the standard error should be. You can then say, “Okay, what do I think my effect sizes are?” And so, once you know what your standard errors are, power calculations are the same as in a GWAS. In GWAS, you would say that the standard errors are 1 over n; in this case, you would say it’s square root of this. Yeah, that’s a good question. I like it when people ask about power. Okay, I think that’s it."
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Below are common terms and acronyms used within this book and across the field of Psychiatric Genetics.\n\n\nA\n\n\n\n\n\n\n\n\nTerm\nDefinition\nReference chapter\n\n\n\n\nAdmixture\n\n\n\n\nAllele\n\n\n\n\nAntisense\n\n\n\n\nArea under the ROC curve (AUC)\n\n\n\n\nAutosome\n\n\n\n\n\n\n\n\nB\n\n\n\nBase pair\n\n\n\n\nBiallelic (SNP)\n\n\n\n\nBonferroni adjustment\n\n\n\n\n\n\n\n\n\n\n\n\n\nC\n\n\n\n\n\n\n\n\nCandidate Gene study\n\n\n\n\nChromatin\n\n\n\n\nChromatin Immunoprecipitation Sequencing (ChIPseq)\n\n\n\n\nChromosome\n\n\n\n\nCodon\n\n\n\n\nCommon variant\n\n\n\n\nCongenital\n\n\n\n\nCopy DNA (cDNA)\n\n\n\n\nCopy Number Variant (CNV)\n\n\n\n\nCrossover\n\n\n\n\n\n\n\n\nD\n\n\n\nDeletion\n\n\n\n\n\n\n\n\n\nDuplication\n\n\n\n\n\n\n\n\n\n\n\n\n\nE\n\n\n\nEffect estimate\n\n\n\n\nElastic Net Regression\n\n\n\n\nEnhancer\n\n\n\n\nEpidemiology\n\n\n\n\nEpigenetics\n\n\n\n\nEpigenome\n\n\n\n\nEpigenome-Wide Association Study (EWAS)\n\n\n\n\nEpistasis\n\n\n\n\nExome\n\n\n\n\nExon\n\n\n\n\n\n\n\n\nF\n\n\n\nFalse Discovery Rate (FDR)\n\n\n\n\nFASTA file\n\n\n\n\nFrameshift mutation\n\n\n\n\n\n\n\n\nG\n\n\n\n\n\n\n\n\nGene-Environment Interaction (GxE)\n\n\n\n\nGenetic Correlation\n\n\n\n\nGenome\n\n\n\n\nGenomic Inflation factor (lamda)\n\n\n\n\nGenotype\n\n\n\n\nGenome-Wide Association Study (GWAS)\n\nChapter 1.2; Chapter 5\n\n\n\n\n\n\nH\n\n\n\nHaplotype\n\n\n\n\nHardy-Weinberg Equilibrium (HWE)\n\n\n\n\nHeterogeneity\n\n\n\n\nHeterozygosity\n\n\n\n\nHistone modification\n\n\n\n\nHomozygosity\n\n\n\n\nHorizontal pleiotropy\n\n\n\n\nHyperparameters\n\n\n\n\n\n\n\n\nI\n\n\n\nIdentity-by-decent (IBD)\n\n\n\n\nImputation\n\n\n\n\nINFO score\n\n\n\n\nInsertion\n\n\n\n\nInsertion/Deletion (Indel)\n\n\n\n\nIntergenic\n\n\n\n\nIntron\n\n\n\n\n\n\n\n\nJ\n\n\n\nK\n\n\n\nKilobase (Kb)\n\n\n\n\nKinship\n\n\n\n\n\n\n\n\n\n\n\n\n\nL\n\n\n\nLasso regression\n\n\n\n\nLD Score Regression (LDSC)\n\n\n\n\nLinkage\n\n\n\n\nLinkage disequilibrium (LD)\n\n\n\n\nLocus\n\n\n\n\n\n\n\n\n\n\n\n\n\nM\n\n\n\nManhattan plot\n\n\n\n\nMendelian Randomization\n\n\n\n\nMessenger RNA (mRNA)\n\n\n\n\nMethylation\n\n\n\n\nMicrobiome\n\n\n\n\nMinor allele frequency\n\n\n\n\nMissense mutation\n\n\n\n\nMitochondrial DNA (mDNA)\n\n\n\n\nMosaicism\n\n\n\n\n\n\n\n\nN\n\n\n\n\n\n\n\n\nNagelkerke P\n\n\n\n\nNonsense mutation\n\n\n\n\nNucleosome\n\n\n\n\nNull hypothesis\n\n\n\n\n\n\n\n\nO\n\n\n\nObservational study\n\n\n\n\nOdds ratio\n\n\n\n\nOpen Reading Frame\n\n\n\n\n\n\n\n\nP\n\n\n\n\n\n\n\n\nPedigree\n\n\n\n\nPhasing\n\n\n\n\nPhenome\n\n\n\n\nPhenome-wide Association Study (pheWAS)\n\n\n\n\nPhenotype\n\n\n\n\nPleiotropy\n\n\n\n\nPoint mutation\n\n\n\n\nPolygenic Risk Score (PRS)\n\n\n\n\nPolygenic trait\n\n\n\n\nPolymorphism\n\n\n\n\nPopulation stratification\n\n\n\n\nPower (Genomic)\n\n\n\n\nPower analysis\n\n\n\n\nPrecision Medicine\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\n\n\nProband\n\n\n\n\nPromoter\n\n\n\n\n\n\n\n\nQ\n\n\n\nqqplot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n\nR2\n\n\n\n\nRare variant\n\n\n\n\nREF/ALT\n\n\n\n\n\n\n\n\nS\n\n\n\nSensitivity\n\n\n\n\nSingle Nucleotide Polymorphism (SNP)\n\n\n\n\nSingle Nucleotide Variant (SNV)\n\n\n\n\nSNP Heritability (h2SNP)\n\n\n\n\nSpecificity\n\n\n\n\nStructural variant\n\n\n\n\n\n\n\n\nT\n\n\n\nTranscriptome\n\n\n\n\nTranscriptomic Imputation\n\n\n\n\nTranscriptomic Risk Score (TRS)\n\n\n\n\nType I error\n\n\n\n\nType II error\n\n\n\n\n\n\n\n\nU\n\n\n\nV\n\n\n\nVariant\n\n\n\n\nVariance explained\n\n\n\n\nVariant Call Format (VCF)\n\n\n\n\nVertical pleiotropy\n\n\n\n\n\n\n\n\nW\n\n\n\nWhole Exome Sequencing (WES)\n\n\n\n\nWhole Genome Sequencing (WGS)\n\n\n\n\n\n\n\n\n\n\n\n\n\nXYZ\n\n\n\nX-linked"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Welcome to the PGC Video Training Textbook!",
    "section": "",
    "text": "Welcome to the PGC Video Training Textbook!\n\n\n\n\n\n\n\n\n\nHello! I’m Cathryn Lewis from King’s College London, and I’m the Education and Training lead for the Psychiatric Genomics Consortium. I’m very pleased to welcome you to the PGC’s Video Training Textbook.\nThe aim of the textbook is to provide comprehensive training materials in psychiatric genetics. We start with an introduction to mental health disorders, give a background to genetics and the technologies used to generate genetic data, and then we step through the methods that are used to analyze that genetic data, from quality control, to genome-wide association studies, to polygenic scores, and pathway analysis.\nThese videos have been collated from online resources, including those produced for the Psychiatric Genomics Consortium, and others from external groups. They range in length from a few minutes, to an hour, and cover both teaching lectures, and methods tutorials. Some videos are marked as “Basic”, “Intermediate”, or “Advanced”, so you can plan your learning program accordingly.\nYou can use the textbook in any way you choose, working systematically through each section, or picking specific topics that you want training in. If you identify areas that we don’t cover, or that you want to suggest additional videos for, please let us know! The textbook is a flexible resource that we hope to update regularly.\nFinally, a huge thank you to the team, who have given their time, their energy, and their enthusiasm to create the textbook. This is an international team from the US, Europe, and New Zealand, and it’s been a great pleasure to work together.\nWe hope that this Psychiatric Genomics Consortium Video Training Textbook fills a need in building capacity in the psychiatric genetics community, and so, enables us to train a new generation of researchers worldwide. With the rapid expansion of available genetic data, the need for skilled analysts has never been greater! And that is an essential step, if we are to realize the potential of using genetics to improve the prevention, diagnosis, and treatment of mental health disorders. And I hope the Video Training Textbook helps you achieve that aim.\n\n\n\nThe Video Textbook Team\nAnna Docherty\nHoward Edenberg\nLachlan Gilchrist\nLaura M. Huckins\nJessica Johnson MPH: Data Scientist, Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA\nMichelle Kamp\nHannah Kennedy PhD: PostDoctoral Researcher, Department of Psychological Medicine, University of Otago, Christchurch, New Zealand\nAda P. Kępińska PhD: PostDoctoral Researcher, Departments of Psychiatry and Genetics and Genomic Sciences, The Seaver Autism Center for Research and Treatment, Icahn School of Medicine at Mount Sinai, New York, NY, USA\nCathryn Lewis\nChris Lo\nGrace D. Lutter\nAdam Maihofer\nCaroline Nievergelt\nOliver Pain\nCarina Seah\nKayla Townsley\nTim Van Der Es\nJiayi Xu\n\n\n\nAbout the Psychiatric Genomics Consortium"
  },
  {
    "objectID": "chapter8.5_transcript.html",
    "href": "chapter8.5_transcript.html",
    "title": "Chapter 8.5: Fine-mapping (Video Transcript)",
    "section": "",
    "text": "Title: Introduction to fine-mapping methods\nPresenter(s): Hilary Finucane\nSarah [Host]:\nGood morning, everyone, and welcome to the MPG primer for today. It’s 8:30, so we’ll go ahead and get started with the introductions. So, this is our penultimate primer for the season, and we are very happy today to have Dr. Hilary Finucane today to speak to us about fine-mapping methods. Her background includes a Bachelor’s in math from Harvard. She then followed that up with a Master’s in theoretical computer science, and then went on to complete a Ph.D. in applied math at MIT. She was selected for a very prestigious NIH Director’s Early Independence Award and has been doing wonderful work here at the Broad. She’s now co-director of the program in Medical and Population Genetics and she’s also an assistant investigator at the Analytic and Translational Genetics Unit at MGH and is about to be an assistant professor at HMS. And we are so thankful for her today for sharing this presentation with us. She’s happy to take questions and has natural pauses built in her talk, but I will also keep an eye on any raised hands and Q&A, and so we welcome your participation. Thank you very much.\nHilary:\nThanks very much, Sarah, for that lovely introduction, and hi, everyone. I’m happy to be talking today about Bayesian fine-mapping methods. And this isn’t going to be a comprehensive review. I’m going to try to give an overview of some of the main ideas in the field, but as Sarah said, I’m very happy to take questions as I go and answer - I’ll be moderating those questions.\nSo let me start by talking about the context for fine-mapping. So in a genome-wide association study, we see often these days many genome-wide associated regions. So here’s an example of a Manhattan plot from the 2014 schizophrenia GWAS, where every green diamond is a genomic locus that has passed genome-wide significance. And that naturally invites the question, what’s actually going on in the locus? And there are a lot of questions that we can ask about a particular locus.\nThat can mean a lot of things, and what I’m going to focus on now is, what are the actual variants that are driving the association at the locus? And so typically, when we zoom in on a locus, we might see something like this. So here we’ve got genomic coordinates on the x-axis and then the level of significance on the y-axis. And this is an example from Hailiang Huang’s IBD analysis. And what we imagine is going on is that there’s actually a simple underlying causal structure, or maybe there are only two causal variants in the locus, and it’s only because of patterns of LD, and then the noise due to finite sample size, that we see all of these many variants coming up as associated in this way. So, the goal of statistical fine-mapping is to take the GWAS data that shows this complex association at the locus and to try to detangle it and figure out what’s the actual simple story that’s underlying it. What are the causal variants that are underlying this association?\nAnd so why might we want to do something like this? Well, one reason is if we’re interested in genes: If we can identify the causal variants, these variants sometimes implicate genes. For example, the variants may be coding variants that directly implicate a gene, or they may be regulatory variants that we can then tie to a gene. So, fine-mapping can often help us with this goal of finding causal genes. And another reason might be, even once we’ve got the name of the gene, we want the variant-to-gene mechanism. And that, for example, might enable us to do an experiment that more realistically recapitulates the disease-relevant biology than knocking out the gene altogether. Then there’s another set of reasons having to do with the genetic architecture. So, for example, by looking at many fine-mapping results across many loci or by building models that are based on fine-mapping models, you might be able to do enrichment analyses - which types of variants tend to be associated or causal for disease? Moving from association to fine mapping can also enable cross-population and cross-trait comparisons and has the potential to be particularly useful in prediction. And so there are a lot of things that we’re trying to do that become easier once we have some model that lets us get not just association, but rather to make some inference about causal structure.\nAnd so today, I’m gonna focus mostly on different aspects of statistical methods for fine-mapping. This is the outline: I’ll start by talking about posterior inclusion probabilities and credible sets, and then I’ll go through a few different methods points, and then I’ll close with some thoughts on evaluating fine-mapping methods. And, I’ll pause after each section here, and so maybe I’ll just start by pausing after that brief introduction if there are any questions so far.\nGreat, so then let me continue with PIPs (posterior inclusion probabilities) and credible sets. What are these kind of basic concepts?\nSo, our goal in fine-mapping is to recover the causal variants, but of course, we can’t always with precise accuracy and perfect confidence recover exactly what the causal variants are.\nAnd so, what does the output of a fine-mapping algorithm typically look like? Well, there are two aspects that I’ll focus on here. We’ll take each variant in the locus, and then we can plot it now, with the y-axis being the posterior inclusion probability. So, each variant gets a PIP, and then we can also identify sets of variants called credible sets. Here, one credible set is red, and one credible set is blue. So, what are PIPs and what are credible sets?\nThe posterior inclusion probability for a variant is the posterior probability that the variant is causal, and this, of course, is according to the model. So, once you’ve bought into all of the assumptions of your model, then the PIP reflects the probability that the variant is causal. And so, a PIP of 1 would be the most confident you can possibly get, and then as the PIP gets lower, that means you’re less and less confident that this is likely that a causal variant driving the signal. And this has a couple of different names; posterior inclusion probability is the most standard one that I’ve seen, but some people call this posterior probability of causality, or you may see other acronyms in the literature.\nThen, a credible set, typically we talk about 95% credible sets, is a set of variants that contains a causal variant with at least 95 percent probability. And this has also been defined in some alternative ways and in some places in the literature, but this is now, to my understanding, the most standard use.\nAnd so, if we go back and look at this particular locus, you can see that the blue credible set is a set of variants that contains exactly one variant, and that variant has a very high PIP. So that means that there’s one signal that’s been really resolved very well. The blue credible set says, “I think that one of the causal variants is here, and I’m pretty confident about it.” And then there’s a red credible set, so that means there’s a second causal variant. “I think it’s one of these five red variants. I’m not quite sure which of the five, and my posterior inclusion probability is going to quantify exactly what do I think is the probability that each one of these variants is the causal variant for this second signal”. And so you can think of each credible set as corresponding to one putative causal variant, and it’s reflecting the uncertainty around which variant is that actual putative causal variant.\nSo typically, when we think about fine-mapping methods, what we’re interested in is getting a PIP per variant in the locus, and then a credible set, each one of which reflects one causal variant and the uncertainty around where that causal variant might be.\nSo, let me again ask if there are questions so far on PIPs and credible sets. Okay, so then with that, I’ll dig into some of how do we actually try to compute these PIPs and credible sets, and I’ll start with the case of single causal variant fine-mapping. So you can imagine you’ve done a GWAS, you’ve got a particular locus you’re interested in, you’ve got the data on the locus, and I’ll discuss later on whether by that, I mean summary statistics and LD or genotypes and phenotypes. And now what you’d like to get are some PIPs and some credible sets, and you have a choice now: which is, are you going to figure there’s probably only one causal variant in the locus, or there may be multiple causal variants in the locus? And there’s increasingly good evidence in the field that many loci harbor multiple causal variants, and so that’s going to be an important point, but single causal variant fine-mapping is very robust and statistically straightforward, and it’s also a building block for a couple of the different multiple causal variant methods. So first, I’m going to talk about single causal variant fine-mapping.\nSo here we have our locus, and we’d like to know what’s the PIP for each variant, and then there’s only going to be one credible set here because we’re assuming one causal variant. And so, which variants should we put into our credible set? The PIPs now, these PIPs will sum to one. We’re saying there’s actually one causal variant; we just don’t know which one it is. And so now I’m going to talk in a little bit of technical detail for a few slides on how we actually go about doing this.\nSo, what is the PIP at SNP J? Let’s start by computing the PIP at SNP J, and we can write this as the probability under our model that SNP J is causal given the data that we have. And we’re being Bayesian, so let’s say that we have a flat prior on which variant is causal. Then Bayes’ rule allows us to rewrite this probability as the probability of the data given SNP J is causal, divided by the sum over all variants in the locus of this probability of the data given that the variant is causal. This is a pretty straightforward application of Bayes’ rule.\nThen, a trick comes in, saying that in order to make the computation easier, let’s just divide everything into both the numerator and the denominator by this null probability - the likelihood of the data under a null model in which none of the variants is causal.\nAnd now we can call this new quantity that we’ve got a Bayes factor. So, the Bayes factor is the likelihood of the data given that the variant K is causal, divided by this null probability. And this just allows us to rewrite our PIPs as the Bayes factor for SNP J, divided by the sum over all variants of the Bayes factors. And the reason that this is a nice thing is because this Bayes factor turns out to be pretty simple to compute.\nSo, Maller et al. showed that the Bayes factor - you don’t actually need to model all of the data at the locus to compute the Bayes factor for a single variant. You only care about what the genotypes are at that particular variant. And then Wakefield and others showed that this Bayes factor can, in fact, be computed or approximated depending on the model that you’re fitting, from summary statistics. And so, in computing this Bayes factor, you can just go one variant at a time and compute a pretty straightforward transformation of the summary statistics that you’ve seen. So, in particular, this doesn’t depend on LD at all and is a linear-time computation. So, this is how for simple causal variant fine-mapping you might compute PIPs.\nAnd how about credible sets? Well, let’s first remember how we defined a credible set. S is a set of variants that we’ll call a 95% credible set if the probability that it harbors the causal variant (because we’re in a single causal variant land) is at least 95 percent. So, we now have a probability for each one of our variants that it’s the causal variant, and we want to know which causal variants should be put together so that we are covering at least 95% of the probability space.\nAnd because we only are assuming the single causal variant assumption, the probability that the causal variant is in S is just the sum of the PIPs of the variants in S. So, to construct the smallest 95% credible set, we can just add the variant that has the highest PIP and then add the variant that has the second-highest PIP, and just keep on going until our PIPs sum to 95%. And typically, I should note, there’s a lot of different ways to construct credible sets. You could always just throw all of the variants in, and that’ll sum to more than 95 percent. So usually, the goal is to construct the smallest possible credible set because what you’d like is to have as much resolution as possible and to be able to say, “We really narrowed down our signal to as few as possible variants.”\nHost: Then the question is asking what values are part of the flat prior and what assumptions are made in order to calculate that flat prior?\nHilary: Absolutely, absolutely. So, when I say flat prior, yeah, I should have clarified this better. What I mean is a flat prior over which variant is causal, which is also something that I’ll come back to. So, the flat priors here are saying a priori, before I’ve seen the GWAS data in my locus at all, I’m going to say that every variant is equally likely to be causal. There’s another prior that has to be defined that has to do with what’s the effect size of each variant in the locus, and there you do have to specify it, that there’s different ways that different folks do that, and it turns out that that might actually be pretty important, but for the sake of time, I’m leaving that out of this particular presentation. And so here for in order for what I said on this slide to hold, what you need is for the prior, on which variant is causal, to be uniform across the different variants. Does that answer the question?\nHost: Yes, there is a follow-up one, whether a single causal variant is a prior that there is only one or no causal variant, or a constraint?\nHilary: In this case, it’s a constraint. So, in this case, when I say single causal variant fine mapping, what I mean is the model that you write down says there is exactly one variant, and it’s going to be one of these. Under the prior, if you have M variants, your probability is 1 over M that your first variant is causal, and it’s 1 over M that your second variant is causal, and that sums to 1 across the whole locus. In subsequent work that I’ll talk about in the next section, we put priors on the number of causal variants, and those might up-weight or down-weight, well those seem to up-weight sparse solutions like single causal variant solutions. But in this case, there’s a hard constraint: there is only one causal variant at the locus.\nHost: And does LD structure affect the PIP? There’s a lot of questions coming in, great!\nHilary: Excellent, great! No, that’s kind of the magical thing about single causal variant fine mapping. This was first shown in 2012 in this Maller et al. paper. For one single causal variant, there are a couple of different ways to see it, and if I had a whiteboard, then I would show some of them. But the fact that these Bayes factors, that you can actually compute the probability of all of the data given that a SNP is causal divided by the probability of the data under the null model, that no longer depends on all of the other variants in the locus. You can see this, for example, if you’re looking at a linear model or a standard model for quantitative traits that you usually write down. You can actually write down the normal likelihoods and watch things cancel, and then a bunch of stuff disappears, and you wind up with something pretty simple. But there are also probabilistic arguments in both Maller et al. and Wang et al. that show that whether you’re conditioning on X or consider X to be part of your data, you actually get this canceling, and so your Bayes factors don’t depend on any variant except the variant that you’re computing the Bayes factor for. I think that’s part of why people like single causal variant fine-mapping so much; it means there’s no way to misspecify your LD, and it’s super simple and straightforward.\nHost: I think a related question, just to finish up, is whether there are any other methods that prefer proximity. So, if you have a clustering of variants instead of a single variant, if that adjacency is considered in any alternative models.\nHilary: Interesting! So the question there is, now you’re modeling multiple causal variants, and you want to put a prior that your causal variants are likely to be close together, but you don’t want to up-weight or down-weight any particular variant, is that right?\nHost: Well, so that was my interpretation of the question, but I’ll read the question which was, “Does your candidate selection require that variants are adjacent, or is there a method that prefers proximity?”\nHilary: Ah, so this is about credible sets now. With credible sets, there’s nothing explicit about adjacency. I think that typically, if you have a single causal variant, then the variants that have the highest PIPs are going to tend to be in LD with each other. So typically, credible sets tend to consist of variants that are in at least a medium amount of LD with each other. This can even be used as a diagnostic in some methods. If your credible set contains a bunch of variants that are in very loose LD with each other, then there’s a sense in which things didn’t work, and you should become suspicious. So I would say that if the model is well-specified, then you might expect a credible set to consist of variants that are in LD with each other, but there’s nothing explicit here that enforces that.\nHost: Thank you so much!\nHilary: So, to recap, how might you do the single causal variant fine mapping? Well, first, you can take your summary statistics and compute approximate Bayes factors, transform these into PIPs and then compute credible sets from your PIPs.\nOne nice thing about single causal variant fine-mapping is that it also allows us to build some intuition about some basic concepts in fine-mapping. So, one thing that we might be very interested in is “what factors affect our ability to fine map effectively?”. We’re happy if we get a few variants with high PIP and other variants with low PIP, and that means we’ve really been able to zoom in on the causal variants. Another way to think about “power” (in quotes because it’s a very frequent term used in statistics) is intuitively, we’re trying to say, with what confidence have we been able to identify these causal variants? And you can imagine that if there’s a lot of LD in your locus, then it’s going to be harder to identify the causal variant. If you know, in the extreme, if you have two variants in perfect LD, then it doesn’t matter what your sample size is or what your algorithm is. You’re never going to be able to tease apart which of those variants is causal without bringing in some extra information. The less LD there is in the locus, the easier it becomes to kind of tease apart which variant is causal and which variants are non-causal. Similarly, as with GWAS, both sample size and effect size are very important for being able to confidently zoom in on a small number of most likely causal variants.\nIn this work by Schaid et al., the authors wrote down an approximate expected PIP at a causal SNP under a simplified model. And so here’s an example: you can imagine you have a locus with ten SNPs. All SNPs have equal LD; they’re correlated to each other at level R. There’s a single causal SNP that explains 1% of the variance in your phenotype. The authors wrote down an analytic expression for roughly under this scenario what would you expect the PIP of the causal variant to be, and they created this figure.\nSo, here high values are good, that means we were able to narrow in with a lot of confidence on the causal variant. You can see that on the x-axis, as the amount of LD among variants in the locus changes, you’re less and less confident that the causal variant is actually causal. The colored lines show how, as you increase your sample size, you’re more and more confident. Being able to get this kind of quantitative sense of what’s the trade-off between LD and sample size as you’re trying to zoom in on particular causal variants can be a useful way to build an intuition. And one comment I want to make here is that when we think about cross-population fine-mapping, one reason that it can be particularly effective to combine information across multiple populations in fine-mapping is because it changes the LD structure. The relevant LD is related to the average LD between the two populations. So, if you compare, let’s say, the same sample size, but you can choose to have it either all in one population or all in another population, or half-and-half in two populations, then because there are differences in LD structure among the two populations, combining across populations can help you move to the left in this plot, which is, as you can see, a good way to also move up, which means you’re more confident in the causal variant.\nSo, that’s an overview of single causal variant fine-mapping. Now, I’ll give kind of a high-level introduction to multiple causal variant Bayesian fine-mapping, and maybe I’ll pause one more time for questions. We had some in the middle, but not just because I’m at the outline slide again. Are there other questions?\nGreat, so we know that there’s often not just a single causal variant in a locus, and so that’s usually not an assumption that we’d like to hard-code, and especially as our sample sizes increase, this becomes more and more relevant and is reflected more and more clearly in the GWAS data that we see. So now, if we think about multiple causal variant fine mapping, there are two main approaches. The first one is to say, “Okay, there are multiple causal variants. Let’s split our locus up in some way, and then apply single causal variant fine-mapping because that’s a really robust tool that we can use.” So, then how does this typically work? What does it mean to split the locus up? There are a lot of different ways to do this.\nOne standard way is conditional analysis, and so here’s a figure describing conditional analysis. Let’s say that this is your locus in the top left here, and in conditional analysis, you take the top signal, and then you include the genotypes at that variant as a covariate in your association, and if that variant is in high LD with a causal variant, and there’s only one causal variant, then that variant explains all of the other associations in the locus, and so by conditioning on that variant, you get this, you know, you kill all the signal, and you get this null pattern here. So, if there’s a single causal variant and if the top variant is in high LD with that causal variant, then conditioning on the top variant will kill all of your signal.\nOn the other hand, if you’ve got two causal variants, then conditioning on the top variant is unlikely to kill all of your signal. And in particular, if that top variant is in high LD with a causal variant, then after you’ve conditioned on it, there’s a sense in which you’ve, you know, accounted for the effect of that causal variant, and now you’ve got a locus that’s got one fewer causal variant than before. So, you can iterate this and then get these set of index SNPs. Conditional analysis is one commonly used way to break complex loci into multiple signals. Then, one way you might then use single causal variant fine-mapping would be to fine-map each of these signals conditioning on the other. So, once you’ve got all of your index variants that you got by a conditional analysis, and maybe you’ll include all but one as covariates and apply single causal variant fine-mapping, and then repeat that excluding each signal one at a time.\nSo, that’s a commonly used type of approach, conditional analysis, and it has some limitations. One limitation is that you might, there’s no guarantee that your top variant is in high LD with a causal variant. So here’s an example from the SuSiE paper where they did a simulation where SNP 1 and SNP 2 are the causal SNPs, but because the yellow SNP tags both of the two red SNPs, it comes out as most associated, even though it’s not in particularly high LD with either one of these causal SNPs. This would be a case where if you did conditional analysis, you start by conditioning on the yellow SNP, but that wouldn’t properly kill either of your signals because this idea that your top variant is in high LD with a causal variant is violated in this particular case. So, conditional analysis is one approach, but examples like this motivate instead writing down a Bayesian model to jointly model the effects of multiple variants at the same time.\nAnd that’s what I’m calling, you know, approach number two: “How might we jointly model multiple causal variants in one Bayesian model for the locus?”\nHost: Hilary, there are two questions about that last approach. What do you mean by top variants? Is that defined by the GWAS score?\nHilary: Yes, yes, sorry, by marginal significance.\nHost: And then when you iterate for variants conditionally, there’s an assumption that it’s not done manually. What’s the process like in sorting out hits?\nHilary: So there’s a software to do this, and it’s pretty automatic. Right, at each step, you want to take the most significant variant. So typically, the kind of manual part is, you have to decide when you’re gonna stop, and that’s often done by setting a threshold on significance. At what point are you going to say you’ve killed all of the signal? And so you take the most significant variant and you include it as a covariate. If any variant passes whatever your predetermined level of residual significance is, then you’ll do that again. You’ll take the most significant variant, condition on it, and then iterate. And then you consider yourself done when no variant passes your predetermined level of significance. Does that answer the question?\nHost: I think so.\nHilary: Great, so then I’ll move on to how we might jointly model multiple causal variants. So here, let’s start by analogy to single causal variant fine-mapping, but here, instead of one variant, we’re gonna look at sets of variants. So let’s let Sj be a set of variants, and we want to know what’s the probability that this set of variants is causal given the data, and we can again apply Bayes’ rule and start to try to compute some likelihoods, but we get stuck very quickly.\nAnd the reason is before, we were only summing over variants in the locus, and so we could say, like, what is the space of all things that could possibly happen? Well, variant one could be causal, variant two could be causal, variant three could be causal, and so on. There’s only a number of variants possible choices. But now, what’s the space of all things that could possibly happen? Well, variant one could be causal, or variant 1 and 2 could be causal, or variants 1, 3, and 10 could be causal. And so now, if you want to just naively apply Bayes’ rule, you’re summing over all possible configurations of causal variants, and that’s large, two to the size of the locus. So, that’s way too many terms to be tractable. There are a number of different methods to do joint modeling of multiple causal variants, and each one of them approaches this challenge differently. Caviar, which to my knowledge was the first work to write down this model in this way, limits the maximum number of causal variants and is typically applied to smaller loci. Once you limit the maximum number of causal variants, then that limits the total number of configurations as well in a pretty direct way. And then there are methods such as FINEMAP and DAP-G that sum over what their algorithm thinks are the most likely configurations. And then, more recently, the SuSiE method takes a different approach based on variational inference, for those of you who know what that is. It’s analogous to iterative conditional analysis, where instead of just doing conditional analysis once through the locus, they then go back and redo the conditional analysis multiple times until convergence. This has some nice theoretical properties as well. So, this isn’t a comprehensive overview of multiple causal variant fine-mapping, but just to give a sense that when you want to do joint modeling of multiple causal variants, there’s kind of a fundamental challenge to the first way we would think of doing it. There’s been a series of really nice work making that more and more efficient in these different and in other works.\nI’m not going to go into the details of exactly how these different methods work; though that’s something that I find very interesting. Instead, I’m going to touch on two other method topics, and one of them is functionally informed fine-mapping. So let me pause again for questions before I move on to functionally informed fine mapping.\nHost: There is one question: Do you need to take into account effect size when you do this? Either assume effect size of each causal variant is the same or weight causal variants by effect size?\nHilary: Yes, that’s a really subtle point that the different methods deal with differently. You have to put a prior on effect size is the usual way to do it, and then integrate out the prior. The question is, how do you figure out what the prior should be? Some methods do this by having the prior be a mixture of normals or learning the prior from the data. In some cases, it’s shared across all variants, and in some cases, it’s different for the different variants. So, that’s an important point that different methods deal with differently.\nHost: Thank you.\nHilary: So let me, sorry, is there another question? I might be looking at the wrong place.\nHost: Just popped up. Um, you mentioned that there’s evidence that there are multiple causal variants for GWAS loci, and just curious as to which studies have confirmed that?\nHilary: Yeah, there’s a couple of different ways to see that, I guess. I mean, one way to see that is if you look at the applications of multiple causal variant methods that then give you a posterior on how many variants there are, then that posterior is often concentrated away from one. Another way to see that is doing conditional analysis. If there’s a single causal variant, then conditional analysis should kill your signal pretty well, and it very often doesn’t. Another is, depending on how you define your locus, sometimes you can just look at the locus zoom plot, and it’s pretty clear that there’s more than one signal. For example, if you’ve got variants with a high marginal effect that are in low LD with your top variant, that’s not really consistent with more than one variant at the locus. There’s been some work on estimating amounts of allelic heterogeneticy from Farhad et. al. and [indistinguishable], where they try to, you know, model this specifically. But I’d say that there’s just, for the fact that it often happens that there are multiple causal variants, that seems to be something that you can see in a lot of different ways. And then the question of how often and how many causal variants, I think, is a much subtler and more difficult thing to get at.\nHost: Thank you! One more just popped up, yes. So, Caviar, SuSiE, and DAP-G each use different models. Is there a way to judge a priori which method best suits our user’s data?\nHilary: That’s something that I’ll get into towards the end: evaluating fine- mapping methods. In my opinion, one of the things that this field really needs more of is benchmarking in realistic settings, and so I’ll talk a little bit about that at the end. But you can also base it a bit on intuition based on just the assumptions that the methods make. But I think actually, rather than go into that, I think that empirical, like more empirical evaluation, is really needed. A common thing is also to apply more than one method and then when they agree, to have more confidence. That’s something that our group has done, where we apply both FINEMAP and SuSiE, and then one way of evaluating the methods is to look at functional enrichments of the variants that get prioritized by these two different methods. And if you look at the enrichment when they agree versus the enrichment when they disagree and you go with either method, then you can see much stronger functional enrichment at the loci where the two methods agreed, than when they disagree, but in our hands, at least, they mostly agree, which is, I think, a good sign.\nHost: Thank you.\nHilary: Okay, so I got a question earlier about flat priors, and what I was saying was the methods that I’ve described so far assume that before you look at the GWAS data in the locus, you think every variant is equally likely to be causal. But intuitively, of course, that’s not the case. If you haven’t looked at your GWAS data yet, you just know which variants are in the locus, but some of them are coding and some of them are non-coding. Then a coding variant is more likely to drive disease than a non-coding variant.\nAnd because we’re doing Bayesian analysis here, that can be incorporated into a prior. So, a functionally-informed prior is one where you take into account the functional annotations of a variant to up-weight and down-weight certain variants according to which ones are more or less likely to be causal a priori. And then the question is, how do you set that prior? Do you have to just kind of trust your own intuition that, I don’t know, enhancer variants are five times more likely than other non-coding variants to be causal? One way to get around this question is to learn the prior from the data. So, a lot of the methods that I described so far, if you want to just say waht the prior is, that can actually be done pretty simply. And what makes this difficult is learning from the data by looking across many loci what prior would make sense to set. And so now what you’d like to do is say, “Ok, I’ve got several different loci. I’m gonna fine-map them simultaneously, but I want to learn by looking at these loci, are they consistent with, like, what, how much enrichment are they consistent with?” And so different methods again have done this in different ways. fgwas is a functionally informed single causal variant fine-mapping method, and then PAINTOR allows for functionally informed fine-mapping at multiple causal variants, and then CAVIARBF allows for many annotations in a multiple causal variant framework. And most recently, PolyFun leverages polygenic enrichment by leveraging stratified LD score regression.\nAnd so, to give um just an example of how this works sometimes, I’ve pulled a pic or a figure from the PolyFun paper. And so here, if you first focus only on the squares, then you can see that the squares reflect, here, the PIPs that are not functionally informed. And if you look only at the squares, then what you can see is that none of the PIPs are bigger than 0.4, and these are RS288326, the red square, gets a PIP that’s, you know, somewhere below 0.4. But that particular variant turns out to be non-synonymous. And so, the functionally informed fine-mapping results, which are displayed in circles here, up-weight that in the prior. And so, then if you look at the posterior inclusion probability or the posterior causal probability here, then you can see that incorporating this functional information has bumped up that nonsynonymous variant to a posterior probability closer to one, which might match our intuition better from the combination of the data together with our understanding that this is a nonsynonymous variant.\nSo, this is, you know, an example of the kinds of ways that functional information can be incorporated into fine mapping, and this has pretty clear advantages. For example, if your prior reflects true biology, then you’ll get a more accurate posterior. One disadvantage would be if you want to use functional information downstream to, for example, evaluate your fine-mapping method, or if you sometimes it can be useful to say, “My fine-mapping results don’t actually have, like, I haven’t incorporated the functional information yet,” and so then I can do, for example, enrichment analyses. But I think that especially as these methods become more efficient and robust, as they have recently, then this is going to be an important direction as well. A very useful type of information to be incorporating into fine-mapping. So, are there any questions on functionally informed fine-mapping?\nI’m great, so then, um, sorry, was that… sure.\nHost: It was just a question about variants that might be in trans and how that complicates this analysis.\nHilary: Yeah, for sure, for sure. So, in order to do functionally informed fine-mapping, you need a set of annotations. So when you say, so what you’re taking advantage of is, you know, how to characterize variants. If you don’t know how to characterize the variants, then you can’t take advantage of that anymore. So typically, you first start by writing down a set of functional annotations. Here are my coding variants, here are my promoter variants, and one thing that’s different among the different methods is how many of those can you write down. But if something is regulatory and trans in a way that hasn’t been well characterized or that you can’t work into your model, then yeah, then that’s not something that you can take advantage of with these types of methods.\nHost: And how specific is PolyFun to a particular cell type, disease, or phenotype, and can that be customized?\nHilary: So actually, let Omar field this question, but in general, if you think about functionally informed fine-mapping, it again depends on which annotations get used. And so if you only incorporate annotations from a certain cell type, then it’ll be cell type-specific. My understanding is the default for PolyFun is not cell type-specific, and that it uses annotations that don’t correspond to a particular phenotype, which makes it pretty widely applicable to polygenic phenotypes where you can only pick enrichment estimates. I don’t know if Omar is on the call, but if he is, then he should feel free to chime in.\nHost: And do those annotations include features like promoters and enhancers?\nHilary: Yeah, coding is just one example, but there’s, depending on which method you’re looking at, typically a large number of annotations that can be incorporated.\nHost: And then this is testing the limits of my zoom abilities, but Layla has a hand up.\nLayla: That was an accident.\nHost: Thank you so much. Great!\nHilary: Alright, so then maybe I’ll say a few words about summary statistics.\nSo many of the methods that I’ve described, I haven’t been differentiating so far, but many of them, rather than requiring your full genotype matrix and phenotype vector, can actually be run given only your LD matrix and summary statistics. This is convenient because, depending on what your sample size is and how you’re defining your loci, the LD matrix can be a bit smaller. But it’s particularly convenient if you can estimate patterns of LD from a reference panel. And I’ll get into that in the next slide, but let me first point out that this isn’t actually a coincidence. If we call our genotype matrix X and our phenotype vector Y, our LD matrix is then, up to normalization, proportional to X transpose X. And our summary statistics allow us to recover X transpose Y. X transpose X and X transpose Y are actually sufficient for V in the linear model that most of these methods are based on. And so what that means is that X transpose X and X transpose Y statistically have all the information about V that you would want to get from X and Y. And so the fact that there continues to be summary statistics-based methods is based on this very nice fact. As long as we’re starting from this Y equals X beta plus epsilon model, then it’s gonna be possible to do it from summary statistics. Although here, the only guarantee is if you have the actual X transpose X from your entire genotype matrix, so this is full in-sample exact LD. And of course, it doesn’t apply to, you know, logistic regression, there are things like that. And so when do you actually need, so the statistical guarantees come from in-sample LD, and when is it okay to use a subset of your samples or LD that you’ve estimated from a different population?\nAnd so, Benner et al. have written about this particular question, and this is their schematic of what is the question that we’re asking here. So starting from the right, you can do fine-mapping from summary statistics and LD information. If your LD information comes from your GWAS data, traits, and genotypes, then that’s optimal. And then the question is, if you have a reference panel, then can it work to compute LD from the reference panel instead?\nTheir conclusion is that it depends on the size of the reference panel and the size of your GWAS. So as your GWAS gets bigger, you have to have a bigger and bigger reference panel, and of course, the population has to match as well. So for, I think what they say is, for a GWAS of over 10,000 individuals, you need a reference panel of at least 1,000 individuals or something like that. And then I think this question of the population must match as well. To my understanding I haven’t seen much work exploring exactly how well do you have to have chosen a perfectly random subset of the individuals you did your GWAS in, or is it okay to get the right continent, or is it something in-between there? And I think that the fact that a small perfectly matched subset doesn’t suffice means that as your GWAS gets bigger and bigger, you have to really be getting the LD very close to perfect. So I think that continuing to explore exactly in what situations reference panel LD is okay and gives accurate answers is something that it would be helpful to still have more work to understand. That set of kind of constraints because then, you know, if it did work, that would be very good.\nSo, that’s summary statistics versus full data. Now, move on to my last small number of minutes. Oops, and it looks like I don’t actually have time to talk about evaluating fine-mapping methods. So maybe I’ll actually conclude there and just say the high level of evaluating fine-mapping methods is that it’s important to try to break them in all of the ways that we think they’re broken. I’ll show you just this one slide.\nFine-mapping methods tend to assume that all the causal variants in the locus are modeled, there’s no imputation noise, you have exactly between one and five or one and ten causal variants, and that your phenotype is normally distributed and conditional on your genotype, you know, things like that. And then typically when fine-mapping methods are evaluated, all of these assumptions are satisfied in the evaluation. So one thing that my group has been working on that we think is very important is trying to find other ways to evaluate fine-mapping methods, both in simulations that might break some of these assumptions and also by real data analyses that can give us insight into what’s working and what’s not.\nSo, with that, I will conclude because we’re out of time. If there are any final questions, maybe I could take one.\nHost: First, was that Omar wrote and did completely agree with you that PolyFun can be customized, but isn’t by default. And then I’ll just take one question. I think this is an interesting one I’ve been actually wondering is: summary statistics preserve privacy, but is there a way to publish the true underlying LD matrices or approximations there - that it will also preserve adequate participant privacy?\nHilary: I think that’s a super interesting thing to look into, and I don’t know the answer to that. I’m pretty sure that you can release approximate LD while preserving privacy because approximate LD should be the same in different samples from the same population, but I’m not sure whether it’s possible, whether you can publish infinite precision exact LD while preserving privacy. That’s not something I’ve worked on myself, and I don’t know of any work on that in particular. If someone else on the call does, they should chime in.\nHost: Good, this was a wonderful session. Thank you so much, Hilary. This was our most interactive primer yet. Clearly a topic of great interest, very well presented. But thank you all, and we’ll see you in just a few minutes for the MPG session."
  },
  {
    "objectID": "chapter9.6_transcript.html",
    "href": "chapter9.6_transcript.html",
    "title": "Chapter 9.6: Therapeutic Implications",
    "section": "",
    "text": "Title: Pharmacogenomics knowledge for personalized medicine\nPresenter(s): Michelle Whirl-Carrillo, Cristina Rodriguez-Antona[Host]\nCristina Rodriguez-Antona:\nDirector of the Pharmacogenomics Knowledge Base from PharmGKB at Stanford University. She leads the PharmGKB team and is responsible for the development of new content, projects, and futures. For the last 20 years, she has led pharmacogenomics research and its application to personalized medicine and personal genomics. She has leadership roles in multiple NIH-funded pharmacogenomic projects and, in addition, formed PharmGKB, including the Clinical Pharmacogenetics Implementation Consortium and the Pharmacodynamics Clinical Annotation Tool. She has served on multiple national and international pharmacogenomic working groups and steering committees. So, her research interests include the translation of human genome sequencing data to clinical implementation using curated pharmacogenomics knowledge. Michelle, thank you.\nMichelle Whirl-Carrillo:\nThank you, thank you so much for the introduction, and thank you for the opportunity to speak with you all today. I really appreciate it, and I’m well aware that I’m standing between you and dinner for most people, so I’ll try to move it along.\nOh, just a disclosure really fast, I don’t really have anything to disclose other than my public funding.\nSo we had an excellent overview of pharmacogenomics from the previous speaker, Munir did a great job explaining it, so I’m just going to start with talking about how the amount of pharmacogenomic information that we’ve had over the past 20 years has really gone up. This is almost a very similar diagram to what Munir showed, where I just started in the year 2000, there were only about 387 publications for pharmacogenomics that were found in PubMed for that year, versus in 2021, in that year alone, we’ve got over 2,600 publications. So we are accumulating knowledge all the time. There are over 35,000 publications in total in PubMed right now about pharmacogenomics and genetics.\nSo it’s great to have all this information. More and more information means that we are more informed, but just the information alone can be tricky. What do we do with that? So, this raises some issues about how do we organize all this information and how can we standardize it across all these different publications? People use different terminologies or are measuring different things, and it’s not standardized currently. It also leads to questions about how can we use this information for clinical actionability. So, we really need centralized resources to help us be able to organize this information, standardize it, so we can easily search across it. So, I’m going to talk to you today about a few resources that are available that help centralize this information.\nBut first, I’m going to talk about a few sources of pharmacogenomic knowledge that we can accumulate together in order to get a better picture of the field. One is the peer-reviewed published studies. I already showed you what’s in PubMed right now – thousands and thousands of studies. We also know that there is some information in regulatory agency-approved drug labels. You heard a little bit about that from Munir as well, that some labels from the EMA, for example, from FDA, do have information on them about testing for particular genetic variants or having a metabolizer status and how you can choose a drug or change the dosage accordingly. But that’s very few drug labels that have that information on them currently, although it is getting more and more all the time.\nWe also have published guidelines, so there are different groups that write actual clinical guidelines for clinicians on how to go from the genotype to dosing the patient. Most of those guidelines are derived from the peer-reviewed published literature, though, so they’re kind of a derivative of that data source. We also know there’s a lot of unpublished data out there, right? There are clinical trials and other experiments that may be proprietary from pharmaceutical companies. That information is more difficult to use and to standardize and also to vet as a public resource. It’s hard to deal with that information, but we know that a lot of that information is used in those regulatory agency-approved drug labels as well. A lot of that is submitted to these agencies when the drugs are going for approval.\nSo, one of the centralized sources for dealing with organizing this type of information that I just discussed is PharmGKB, which is the Pharmacogenomics Knowledge Base. This is a website with a database backend. It’s publicly available. It’s probably the biggest resource that’s publicly available in the world right now. And we started this around the year 2000. It’s based at Stanford University, but our mission is basically to collect, encode, and disseminate pharmacogenomic knowledge for uses ranging from research and discovery all the way through clinical actionability and implementation.\nSo, what do we do at PharmGKB? We take the information that is out there that I kind of already went over: guidelines, drug labels, what’s published in the literature, and we think of this as our knowledge information stream. This is all publicly available right now for everyone. But at PharmGKB, we have a team of scientists that extract information from these sources, and using expert manual curation, we take the relevant parts of what drug, what genotype, what gene, etc., what variants, we standardize the terminologies, and we aggregate this information together. We’re always working towards clinical implementation and adoption, and in the end, we present some resources that I’m going to show you in a minute to actually go back out into this knowledge stream, and we hope enrich it and make it a better source of information for people who are trying to implement pharmacogenomics in their clinics.\nSo, one source of information at PharmGKB are these guidelines that I mentioned earlier. The two groups that probably publish the most information about clinical implementation of pharmacogenomics would be CPIC and the Royal Dutch Pharmacists Association’s Dutch Pharmacogenetics Working Group (KNMP) out of the Netherlands.\nSo, PharmGKB, this is just an example webpage. I don’t expect you to read it. That’s just an example. But, for example, the Dutch working group has some guidelines about the use of amitriptyline and how CYP2D6 variants can affect the implementation or affect dosage of this particular drug. The Dutch working group puts out a PDF that is text-based, that people can read and get this information themselves. But what we tried to do at PharmGKB is make this a little bit simpler presentation through tables and by using all the information available from the Dutch group itself.\nWe have on the web page a way for people to input what the particular variants are of a specific patient or an example. And pull up what the recommendation is for that specific genotype. So just a little bit of an aid for people to be able to take the information from the Dutch group and get right to the source without having to understand the mapping from the actual genotype in the gene to the metabolizer phenotype, such as poor metabolizer, etc., to the actual recommendation.\nSo, we have on our website just the broken-down information from multiple groups, including CPIC, the Dutch group, and there are a few other groups that have some one-off recommendations as well. You can compare across these, and due to resource issues and so forth, not all the same drugs have been have guidelines written about them from both organizations. And so, sometimes one organization may have something about a drug that the other one doesn’t. Most of the time, the recommendations agree or at least are very similar, but sometimes there’s a little bit of differences, and you can compare those at the PharmGKB to see what the differences or similarities are. We also highlight where guidelines tell you whether to give you testing guidance – what to test and when to test.\nSo, another source of information that we organize would be drug labels. We annotate drug labels from FDA but also from EMA, Canada, and we’ve had a couple of collaborations with groups in both Japan and Switzerland. So, we have some labels that have been annotated from those groups as well, and we’re always keen to collaborate with others. So, if anybody is interested in working with us on that, please let me know. But yes, our biggest group of drug labels would be from FDA, and then followed by EMA.\nSo, we saw an example of a drug label earlier from Munir, and so sometimes on drug labels, they do highlight that there is information, but much of the time, the language on drug labels can be not precise or somewhat vague. And so, we had feedback from many of our users that what they really wanted to know from these drug labels is, “Just tell me which labels say I need to test, which one is recommended testing, and which ones have any kind of information that I can use.” So, we came up with this labeling system where we can tag these labels that we curate with those categories. And also we get very specific if there is prescribing information based on pharmacogenetics on a drug label, such as changing the dose or changing the drug, we highlight that as well.\nThis is just an example screenshot (I don’t expect you to read it) of annotation of the EMA label for abacavir and HLA-B. You heard a little bit about this earlier. So yes, this is a very kind of famous example of pharmacogenomics, very well understood. And so yes, testing is required according to the label. The label gives you specifics about what variant it is, specifically in HLA-B, that you should be interested in, and gives you advice about what to do. This is an example of a nicely laid out drug label, and we were able to capture this information very easily. But not all drug labels are quite so simple.\nSo, again we have a page at PharmGKB. Please go ahead and check it out for yourself at some point if you are able and inclined. We have drug labels from FDA, EMA, and a few other groups as well, and you can use this table to compare across, and you can drill down to see that exact annotation like I just showed you for the EMA one in abacavir. But what’s interesting is that some regulatory agencies have different information on their labels, depending on either they may be lacking information altogether about pharmacogenomics on that label, whereas other countries or groups have a very high level of actionability for pharmacogenomics. So, it’s kind of interesting to look at this webpage to see overall, again, where the similarities and differences are, and it just has to do with what is submitted to these regulatory agencies and what is deemed actionable at the time.\nWe also heard from Munir a little bit about randomized controlled trials. Again, yes, this is considered the gold standard, and a lot of clinicians prefer to have information that comes from RCTs, and this is where they would like to see any kind of proof of clinical actionability. But as was pointed out, it’s really not feasible. A lot of times, we have small populations in these studies. Again, when you mentioned that we don’t have huge cohorts like in diabetes, etc. So, the small populations do often make statistical significance difficult. But we do know that statistical significance is not always just the same as clinical significance anyway. And there is a lack of standardization again across all of these studies and publications that come out. But we can use replication of data from multiple different sources to help us address some of these issues, right?\nSo, we can take an article like this (this is from the New England Journal of Medicine) and is talking about a particular association between a variant and SLCO1B1 and a reaction to Simvastatin. And if we take this and manually curate it at PharmGKB, again we’re standardizing across all the publications, the gene names, the gene variants. There are many different names for genetic variants, as I’m sure you’re all aware, right? People can use RSIDs, they can use cDNA changes, protein, amino acid changes, etc. So, we can standardize so that we can see across all publications which ones are talking about the same variant. We can standardize the drug terms, etc. And we take a bunch of information out about you know statistics and what type of study and the study size. We can collect the information from each article.\nAnd over time, there are more and more articles, right, about a particular association that can be published. Many times, these articles replicate the original findings, but sometimes they do not. So, sometimes an association is published, and the next paper that comes out might refute actually that association. But if you look at a large enough cohort of papers and like I said, you’re able to compare across because of the standardization, we can start seeing the bigger picture and write summaries about the association based on the corpus of evidence. And then we can also take the information from those drug labels and the guidelines that I referenced earlier that are published and annotated at PharmGKB as well, and we can get an even bigger and better picture of what’s going on with a particular association. Once we have all this evidence together, we can not only write a summary about what’s going on, but we can assign a level of evidence about how confident we are and the strength of this particular association.\nSo, this is just a screenshot again for a particular association with the same variant with simvastatin, and down here, we can see that there are two dosing guideline annotations available in PharmGKB – one from CPIC and one was from the Dutch group. And you know, in this case, we only have nine publications, but there can actually be many, many more publications depending on the phenotype that’s studied.\nSo, based on that grouping of information, we can come up with different levels of evidence. You can read more about the system that we have come up with for standardizing when we’re trying to collect evidence together or visit the webpage. I can’t go into a lot of detail here because it would take a while to explain. But basically, we score the information that we take from every single publication. We take that information and we add it together across all the different publications, taking into account if any regulatory labels are available and the information on those labels, and also guidelines if their information is available. We come up with a total score for that particular summary annotation. Then, based on that score for the summary annotation, we can figure out which cutoff for a level that it meets and assign accordingly. So, for all of these summaries, you can go, if you’re on a PharmGKB page, you can get a feeling for how much support there is for a particular association based on the little labels on the left-hand side that are color-coded and also have the level written in them. In most cases, as a level one or instances where we know of a guideline or a label that really confirms basically actionability or clinical implementation can be instituted for a particular association. Level two would be those that are very close but maybe don’t have a guideline or a label yet, but there’s a lot of information published and the association looks very strong. Some people, on the other hand, are very interested in low-level associations if they’re not looking to implement clinically but they’re looking at it from a research perspective.\nOkay, so I’m going to switch gears for just a couple of minutes to tell you a little bit more about CPIC. Again, this is one of the groups that writes guidelines for how to implement pharmacogenomics in the clinic. The Dutch group also writes guidelines as well. I happen to be involved in CPIC. I’m going to give you a little bit of the background of that particular group. The goal of CPIC is not to tell people whether or not to test or even what to test, per se, but if you are a clinician and you have genetic results in hand, we want you to be able to understand and quickly be able to know what you can do with that information. Preemptive genotyping is becoming more widespread. There are direct-to-consumer genotyping companies out there, and some patients are going to their doctors, at least in the states, with information about their genetic variants, and they want their doctors to be able to understand that information and act on it. But most clinicians, as we heard, they don’t have the time for patients to sit and really research for themselves what a particular genetic variation might mean or might imply for a particular drug prescription. So, they need a facile way to access information that can tell them upfront what it is that they should do.\nSo, these guidelines are written by expert groups put together. These groups include clinicians, gene experts, pharmacists, and, in some cases, just research scientists as well. And again, we start with a PubMed literature review. So, these guidelines are based on PubMed literature reviews. But the information is collated together and graded for every single outcome by the clinical authors, the authors of the guideline. And so, a consensus is reached from that particular group, and then a statement about what you could do with a particular genetic test result.\nSo, as part of this process, we have to understand for every genetic allele what the functional implication is. Somebody had mentioned that before. So yes, we do have to care about what the function is for all of these variants. And that’s part of the process for creating CPIC guidelines – to basically do a deep dive literature review of every allele for the particular gene in that guideline and see if we can come to an agreement about what the function of that allele is.\nThen, once we have a function defined for an allele, we can put the two alleles together and map to a phenotype, such as a poor metabolizer or an ultra-rapid metabolizer that you heard about in the previous talk. Once we have the metabolizer status for a given genotype for a given patient, then we can come up with what the therapeutic recommendation would be for that particular phenotype.\nCPIC also makes available clinical decision support flow charts and wording for CDS alerts to help aid with clinical implementation for groups that don’t have the resources to do that as well. So, these are just example charts and CDS language that people could use if they want to implement in their institution.\nThere’s a bunch of CPIC guidelines right now. The last count was 26, but that might have changed really recently because we’re always publishing. But please go and check out the website for yourself. We also have a database and API for people that are interested to access the information computationally and import it into their own databases.\nSo, another challenge with implementing pharmacogenomics would be our nomenclature. You heard a little bit about the star nomenclature already. I get people ask me all the time who aren’t in the pharmacogenomics field but more in clinical genomics, “What’s the deal with the star alleles? Why do you guys have them, and what do they mean?” So, just really briefly, right? We have the gene symbol and that star with the number after it is just an allele designation. And so, for example, this CYP2D6 star 8. This is HGVS representation for it. So, there are, you know, three different variants across the gene that define this allele. In pharmacogenomics, with most of the cytochrome P450s, we’re interested in what the combination of variants are across the entire allele. It’s not necessarily one particular variant or another, but what’s the combination of variants across the entire gene? It’s the haplotype that matters. And so, to define the haplotypes, the star allele shorthand was born. And these variants, these are SNPs, so that’s quite simple. But in many cases, the variation across the gene can include repeats or indels, there can be structural variants and copy number variations as well. The variation occurs not just in the exons but also introns or upstream or downstream of the genes, sometimes. And they all have functional effects and are very important to capture and understand what the variation is across. And so, this shorthand helps the community understand what the variation is.\nI’ve been asked many times, “Well, can the star alleles just go away?” from people who are clinicians but deal with Mendelian diseases many times or other kinds of genetics. Unfortunately, I don’t think that’s going to happen. It’s used throughout the literature, lab tests, and assays, test reports all refer to the star allele nomenclature right now. And it’s used by many of the prescribing guidelines. And this started in the 1990s, and it was organized to a great extent in the early 2000s by the Human Cytochrome P450 Allele Nomenclature Database in Sweden. And they, I think, were the first group to really start tracking these alleles and defining them as what variation is in each allele, and they’re the ones that started naming them with the star alleles. That transitioned in 2017 to a group called PharmVar.\nThe PharmVar group has taken over the reins there. So, PharmVar, please check out this group as well. So when we’re talking about standardization, PharmVar is critical to standardization for these pharmacogenetic alleles, and it is the central repository. People who find new allelic variation can submit that variation to the PharmVar group, and there’s a very rigorous process and rules that must be followed for a new star allele to be assigned. But if that’s the case, if the submission reaches the level of a new star allele, it will be assigned.\nJust to show you a little bit about this – this is not the most complicated example by any means, but just to show you a little bit what I’m talking about – so this is again a CYP2D6 illustration. We’re looking at the star alleles star 10, 36, 37, etc., and then this is a list of variants down the side. And so, what we see here is the 100C&gt;T variant is a part of all of these different alleles. So is this variation down here. So, if you, for example, were just going to test this 100C&gt;T, you might not really know which star allele you’re talking about. For simplicity’s sake, many times people just call it a star 10 because okay, that’s the defining variant in that particular star allele. But you can see that there are many other star alleles that also contain that variant and then other variants as well. So, it can get much more complicated than this as well, and some genes, such as CYP2D6, have structural variants, and it gets crazy. But these star alleles really are important for understanding what the variation is, and that all has functional implications. So, the functionality of these different star alleles can and do differ.\nThat leads to one of the other challenges of pharmacogenomics and implementation – the type of the test, what is tested, can affect your results, right? There have been studies done where they’ve sent the same samples out to different labs and gotten different results back, and a lot of times, that has to do with whether you’re talking about a panel or an exome or whole genome sequencing, as someone had brought up earlier. And it’s true that if you have a panel and not everything is on that panel that you can see in whole genome sequencing, you could potentially miss genetic variation that exists in that particular patient. And that’s why we have to be really careful with this. Often in the field, if you don’t see any variation, it is defaulted to what’s called a star 1 allele, which implies that you don’t have any genetic variation or wild type. But of course, depending on what’s tested, that may or may not be true. So, just something to keep in mind when people are trying to implement pharmacogenomics – that transparency about the tests and the test results can be very important. There can be situations where a patient is tested for pharmacogenomic alleles and none are found, and it goes into their EMR or their record that they have no genetic variation in a particular gene. But we know that that may not always be 100% true, right? For example, there are some known existing genetic variations where we know of the genetic change, but we don’t know what the function is. We don’t know what to do with that change. But over time, you know, research is going to eventually elucidate that for us. Also, there’s new variation out there that may not be covered, so, the current star allele nomenclature can’t cover everything that’s not known yet. So, as new discoveries happen, they’re submitted and we have new star alleles that may not have been tested for previously. So, at the very least,\n[audio cuts out]\nknowing what the results are can be very key to both clinicians and patients, especially going forward in the future. Another project that I’m involved in is called the Pharmacogenomics Clinical Annotation Tool. This is a software where the goal is to take the output from a genetic test result report – sorry, the output from a genetic test, such as a panel test or whole genome or exome sequencing – and come up with the star allele designations for those genes, which can be very complicated, as was mentioned earlier. So, this software aims to be able to take whole genome sequencing and determine that without using those tag alleles that are used on the SNP panel, and then connect the resulting genotypes with clinical guidance. And we’ve started with the clinical guidance from CPIC, but we’ll be expanding that to other groups as well, such as the Dutch guidelines and also what’s available on regulatory agency drug labels. That’s the goal.\nSo basically, the way this tool works is it takes a VCF file, which is just a way that genetic variation can be output from testing. And then we take those allele definitions from PharmVar, which has the definitions of what variants are in each star allele, and the information from the guidelines that can be accessed via the API in the database. And then we also have some messaging from the PharmCAT tool as well, with caveats and disclaimers, etc. We take all of that together. So, what the user supplies is the actual genetic variation that is detected, and we come up with a genotype summary, which tells you what the star alleles, what the diplotypes are or genotypes for that particular patient based on that VCF file, and give you a summary of the output. Then, the recommendations by drugs – so you may be a CYP2D6 star 4 star 8, for example. What does that mean? We have recommendations for each drug that we know can be affected or have guidelines showing that the drug response can be affected by CYP2D6. And then we would tell you exactly what the recommendations are for that particular genotype.\nAnd then there’s a section of the report that is very key but not necessarily, you know, first and foremost in any clinician’s mind or patient’s, for that matter. But very importantly, we highlight what variation was given to us in the VCF file versus what is known to date. So, we can easily see in this section of the report if not every known genetic variant that could be part of a star allele, etc., was tested. In this way, we can see with an assigned genotype at the end if information was essentially missed – if there were known star alleles that were not tested for. In this case, you would know then if, say, a patient is a star 1 star 1 as the result, whether or not there are potentially variations that that patient is carrying in that gene that was not covered by that test.\nIt’s great to have this as a report. You get a report out of PharmCAT, and that’s a report that a patient could have and take to their doctor, or a clinician could look at. But in many cases, as we know, clinicians don’t have the time to sit and read reports either. So having this information go automatically into an EMR or EHR system is extremely important, and that is a big challenge since they are not standardized across all hospitals and hospital systems. And so, that’s another area that we’re working on for the future.\nOne of the final challenges I’m going to talk about today would be allele frequency, and this again was mentioned a little bit earlier. But the issue is that in many cases, in Mendelian disease, a low frequency is a sign of maybe pathogenicity, right? So, frequency can be used as a way to figure out if something might be very important in a particular gene. But unfortunately, in pharmacogenomics, some of these variations can be quite common, up to, you know, 30 percent or even more in certain populations. So, you cannot use frequency the same way in pharmacogenomics as a sign for what could be effective function. And yet, you know, knowing the frequency of the alleles is very helpful. To know, for example, if we can only test a handful of alleles, which ones should you be testing? Unfortunately, we don’t know the frequency for many of the defined alleles. They may be undiscovered in a particular patient, submitted to PharmVar, for example, gotten a star allele, now we know we can test for them. But if people haven’t tested for them previously, we don’t really know what the frequency in any given population is. So many times we think in pharmacogenetics, in the field, we think certain star alleles are rare because it’s not published much in the literature, we can’t find statistics about it. But that’s not necessarily the case. So, we do have to be careful there. Also many populations around the globe are understudied. So again, we want to make pharmacogenomics available for all diverse populations, but we haven’t done a lot of studies yet in many of the populations around the globe - lot of them have been focused on European populations or perhaps Asian populations as well. So, research is needed there to understand what the frequency is for these variants. But we always must remember that when we’re trying to implement this, the population frequency is never a proxy for what a patient actually has. So, testing the individual patient is really important.\nSo, the last challenge I’m going to talk about is just the separation of pharmacogenomics from the rest of clinical genomics. So, I gave you a little bit of an overview of all these four resources: PharmGKB, PharmCat, PharmVar, and CPIC. These resources talk to each other all the time, they trade information back and forth. In the US, we have very big projects for clinical genomics. ClinVar and ClinGen are huge resources that are funded through the NIH, but they’re kind of isolated from pharmacogenomics. I should say pharmacogenomics is isolated from them because they are very large and capture most of the genetic clinical genomics that are documented today. But pharmacogenomics kind of exists as a silo. We are trying to address that by depositing information from both CPIC and PharmGKB into both ClinVar and ClinGen. There are some issues with trying to take pharmacogenomic information and wedge it into a clinical genome database, and so it’s a little bit tricky at times, and the deposition of this information is slow. But in addition to depositing it, we are looking forward to being able to exchange information with ClinVar and ClinGen as well. So what we really want to do is not have pharmacogenomics be isolated and siloed from clinical genomics overall and genomic medicine.\nAnd our vision, right, would be full integration of pharmacogenomics with genomic medicine. And this is just a diagram to try to illustrate that all those pharmacogenomic resources, and more, could, you know, eventually be combined together in some way to have a much easier interface, maybe one place that people can go, and that group could interact very closely with both ClinVar and ClinGen. And efforts are underway. We have a panel that we’re starting within the ClinGen group to address pharmacogenomics and how to better cross-talk between pharmacogenomics and genomic medicine in general.\nSo, in many cases, we don’t know the implications of pharmacogenomic discoveries for clinical implementation, but we definitely know that the clinical utility is there for a couple of dozen examples, at least. And then we know that integrating this together with the risk of genomic medicine is going to push implementation forward. Having to try to address this separately is slow going, as Munir described. It’s not being implemented across as many places as we’d like pharmacogenomics to be yet. It does differ from genomics disease models. The haplotypes and those star alleles, and the fact that you have to be aware of the diplotypes and map that to metabolizer phenotypes in many cases is very different, and we are aware of that. So, we do still need some specialized resources, but integrating would be the way to go in the future. And as of right now, we have a few centralized knowledge sources that people can use in the meantime.\nAnd with that, I’d like to just thank all my colleagues. There are many different projects that I was talking about today, and that takes many people in many different institutions to help make all these projects a reality. So, I’d like to thank them and thank you for your attention, your time.\n[Applause]\nCristina: We have questions. I will start from the chat. One question regards SLCO1B1 and statins, and basically what they ask is, are there studies regarding drug adherence and pharmacogenetic testing or genotyping? I mean, does it influence adherence of the patients?\nMichelle: Adherence, yes. I’m not aware of specific studies about adherence to drugs, but we know colloquially, just anecdotally, that this is true, and that patients who have adverse effects that are not as severe as what we saw in the previous talk, but just myopathy, maybe, you know, pain or some other kind of nausea, etc., can lead patients to stop taking their medication. And so yes, it has been shown that because of the myopathy that’s associated with a particular genetic variation, there have been cases, like I said anecdotally, where patients just stop taking their medicine because of it. Yeah.\nAudience member: Hi Michelle, over here. Thank you for a great presentation. Fabulous resources, I think incredibly useful for the community. Can I just ask, are there plans or thoughts about translating those resources into different languages to make them more accessible?\nMichelle: That is an excellent question. We would love to do that, unfortunately, resource-wise, being government-funded, we don’t have the resources to pay for that, but if there are people that would like to collaborate with us, we could see what we could do. Yes, that’s a question we get a lot. You know, can this be in multiple languages? And we’re happy to do it, it’s just a resource issue right now.\nCristina: Okay, another question from the chat is, despite the fact that pharmacogenomics data is growing rapidly and the importance of testing is clear, the integration in the clinics is very slow. So maybe you could comment on the barriers or the reasons for that.\nMichelle: Right, yes, I think a lot of the reasons for the slow uptake were just mentioned previously, just having the education, I think, of clinicians, for them to be familiar with this type of data, familiar with how to use it. Also, there are issues putting this kind of information in medical records, so getting this into the EHR system, I think, would be key. That’s one barrier we know to uptake. And then just also, you know, testing and reimbursement. What are you supposed to test for? When should you test? Who’s going to pay for it? These kinds of issues definitely affect implementation as well.\nCristina: Last question.\nAudience member: Yeah, thank you as well for a great presentation. We, in our company, started testing the PharmCat, and I’d like to ask how many drugs are, in this moment, supported? Because we have only a limited number, and also if it can be adjusted by the users, the parameters, and what are you going to expand it?\nMichelle: Yeah, absolutely. So yes, if you check out pharmcat.org, we’ve updated a lot of the documentation, maybe since you last checked it out. All of the drugs that are covered by CPIC guidelines are currently covered by PharmCat and they’re in the report. All the genes, and yes, you can alter—well, first of all, it’s freely accessible, so if you want to download the tool and branch off, you can change the code however you like. But it is also customizable in terms of if you have genes that you want to include and you have the definition files for those, you can add those as you want, and same with the recommendations as well. And we are going to be expanding those within the next year, is the goal, to include drugs and recommendations from the Dutch Pharmacogenetic Working Group and also what information we can find on the FDA labels. We’ll probably start with FDA, but then ultimately can expand from there as well.\nAudience member: Okay, those are bioinformatics that I work in our company on, so I’m not completely familiar, but is it possible to speak to your IT staff?\nMichelle: Yeah, sure, sure. You know, send—I didn’t have the email up here, but if you send an email to just feedback@pharmgkb.org, we will make sure it gets to PharmCat. I think there’s also pharmcat@pharmgkb.org, but I can’t swear to that. So, if you send it to feedback@farmgkb.org and you say, “This is a question about PharmCat,” we can deal with that. We’re a very small team, and we are the same people working on many projects at once, but yeah, but yes, definitely.\nAudience member: Thank you. Thanks."
  },
  {
    "objectID": "chapter2.1_transcript.html",
    "href": "chapter2.1_transcript.html",
    "title": "Chapter 2.1 Organization of the genome (Video Transcript)",
    "section": "",
    "text": "[Music]\nHumans around the world have much in common, but also enormous diversity. Some of the differences between each of us come from our environment and life experiences, but our DNA plays an important role in determining our appearance, our traits, and our health. There are thousands of genes in the human genome sequence. Changes in individual genes can determine if we have freckles, can digest lactose, have wet or dry earwax, are red-green color blind, or are likely to have blue eyes or think broccoli tastes better. Individual genes can also determine if we will develop sickle cell anemia, cystic fibrosis, or Huntington’s disease. Multiple genes act together with our environment to determine our hair and skin color, our height, our weight, our blood pressure, and our risk of developing type 2 diabetes, depression, cancer, some autoimmune disorders, and many other conditions.\nIn spite of all these potential differences humans are 99.9% genetically identical. How is it possible that we are all so similar and yet so different? Let’s zoom into the smallest genetic unit: a single nucleotide of deoxyribonucleic acid or DNA.\nA DNA nucleotide is composed of sugar and phosphate groups, and one of four nitrogenous bases: adenine, thymine, guanine, and cytosine, annotated in shorthand as A,T,G, and C. The sugar and phosphate groups form the DNA structural backbone, allowing nucleotides to concatenate into a long single strand of DNA, while the bases determine DNA sequence. The chemical properties of DNA allow bonds to form between the bases in order to create a double strand with two hydrogen bonds pairing A and T and three pairing C and G. Though different types of human cells can be very different in appearance and function, they contain the same genome, which consists of about three Giga bases or three billion base pairs of DNA. All the DNA in the cell would be about two meters in length if it were stretched out and must be condensed down to fit into cells as small as 10 micrometers across.\nThe DNA is first coiled into its canonical helix structure, and then wrapped around histone proteins to form a DNA protein structure called a nucleosome. These nucleosomes can be further wound and coiled together to create a compact structure that fits into the nucleus. During cell division, the DNA is organized into tightly wound chromosomes, 46 in total, with 23 coming from each parent. These chromosomes can be easily and accurately separated during cell division, guaranteeing that each new cell contains an exact copy of DNA. Outside of cell division, the DNA is decondensed in the nucleus, allowing greater accessibility.\nThe transcriptional machinery regulates expression of the approximately 20,000 genes, which, although they correspond to less than 2% of all genomic DNA, encode all the proteins necessary to build and run a human cell. So, to return to the original question, “how is it possible for all of human diversity to exist when we are 99.9% genetically similar?”, it is important to remember that the 0.1% of DNA that varies, on average, between each of us, actually corresponds to about 3 million differences across the genome, with 20,000 of them on average falling into protein coding genes. Although that equals approximately one difference per gene, in reality, these differences are not evenly distributed across the coding regions.\nDifferences in DNA sequence are called variants, and those affecting a single position are called single nucleotide variants or SNVs. Common SNVs that occur in more than 1% of a population are called single nucleotide polymorphisms or SNPs. SNPs, along with larger scale sequence changes like deletions, duplications, and rearrangements, create all the richness of human genetic diversity at the population level.\nThis raises many questions on an individual level as well: How does DNA determine our traits? How can we understand what a gene does, and how variants in that gene might affect our lives? What can our DNA tell us about our risk, and our loved ones risk, of disease? How can information from our genomes improve our medical care? Understanding genetics allows us to apply the concepts of heritability and genetic variation to questions of human health and disease in our world today."
  },
  {
    "objectID": "chapter3.2_transcript.html",
    "href": "chapter3.2_transcript.html",
    "title": "Chapter 3.2: Next Generation Sequencing (Video Transcript)",
    "section": "",
    "text": "How to sequence the human genome\nTitle: How to sequence the human genome\nPresenter(s): Mark J. Kiel\nYou’ve probably heard of the human genome, the huge collection of genes inside each and every one of your cells. You probably also know that we’ve sequenced the human genome, but what does that actually mean? How do you sequence someone’s genome?\nWhat is a genome\nLet’s back up a bit. What is a genome? Well, a genome is all the genes plus some extra that make up an organism. Genes are made up of DNA, and DNA is made up of long, paired strands of A’s, T’s, C’s, and G’s. Your genome is the code that your cells use to know how to behave. Cells interacting together make tissues. Tissues cooperating with each other make organs. Organs cooperating with each other make an organism, you!\nSo, you are who you are in large part because of your genome. The first human genome was sequenced ten years ago and was no easy task. It took two decades to complete, required the effort of hundreds of scientists across dozens of countries, and cost over three billion dollars. But some day very soon, it will be possible to know the sequence of letters that make up your own personal genome all in a matter of minutes and for less than the cost of a pretty nice birthday present. How is that possible? Let’s take a closer look. Knowing the sequence of the billions of letters that make up your genome is the goal of genome sequencing. A genome is both really, really big and very, very small. The individual letters of DNA, the A’s, T’s, G’s, and C’s, are only eight or ten atoms wide, and they’re all packed together into a clump, like a ball of yarn. So, to get all that information out of that tiny space, scientists first have to break the long string of DNA down into smaller pieces.\nDNA binds to DNA\nEach of these pieces is then separated in space and sequenced individually, but how? It’s helpful to remember that DNA binds to other DNA if the sequences are the exact opposite of each other. A’s bind to T’s, and T’s bind to A’s. G’s bind to C’s, and C’s to G’s. If the A-T-G-C sequence of two pieces of DNA are exact opposites, they stick together. Because the genome pieces are so very small, we need some way to increase the signal we can detect from each of the individual letters. In the most common method, scientists use enzymes to make thousands of copies of each genome piece. So, we now have thousands of replicas of each of the genome pieces, all with the same sequence of A’s, T’s, G’s, and C’s.\nReading the genome\nBut we have to read them all somehow. To do this, we need to make a batch of special letters, each with a distinct color. A mixture of these special colored letters and enzymes are then added to the genome we’re trying to read. At each spot on the genome, one of the special letters binds to its opposite letter, so we now have a double-stranded piece of DNA with a colorful spot at each letter. Scientists then take pictures of each snippet of genome. Seeing the order of the colors allows us to read the sequence. The sequences of each of these millions of pieces of DNA are stitched together using computer programs to create a complete sequence of the entire genome. This isn’t the only way to read the letter sequences of pieces of DNA, but it’s one of the most common. Of course, just reading the letters in the genome doesn’t tell us much. It’s kind of like looking through a book written in a language you don’t speak. You can recognize all the letters but still have no idea what’s going on.\nInterpreting the sequence\nSo, the next step is to decipher what the sequence means, how your genome and my genome are different. Interpreting the genes of the genome is the part scientists are still working on. While not every difference is consequential, the sum of these differences is responsible for differences in how we look, what we like, how we act, and even how likely we are to get sick or respond to specific medicines. Better understanding of how disparities between our genomes account for these differences is sure to change the way we think not only about how doctors treat their patients, but also how we treat each other.\n\n\n\nNext Generation Sequencing: A Step-by-Step Guide to DNA Sequencing {sec-video2}\nTitle: Next Generation Sequencing: A Step-by-Step Guide to DNA Sequencing\nPresenter(s): ClevaLab\nThe Human Genome Project uncovered all 3.2 billion bases of the human genome. This project started in 1990 and took until 2003  to complete 85 percent of the first genome. But, in 2022, the gaps got filled and the sequence  became complete. So in total, sequencing the human genome took 32 years. Now, with Next Generation sequencing or NGS, it takes only a day to sequence a person’s entire genome.\nNGS vs Sanger Sequencing\nOne day is a dramatic speed increase compared to 32 years! The difference is due to the number of DNA strands sequenced at once. Billions of DNA strands get sequenced simultaneously using NGS. However, only Sanger sequencing was available for the Human Genome Project. With Sanger Sequencing, only one strand can get sequenced at a time. However, NGS only works because the Human Genome Project created a human reference DNA sequence.\nThe Basic Principle of NGS\nThe basic principle behind NGS is that DNA can be cut into small pieces and sequenced. The sequences of these small pieces then get assembled based on the reference genome. NGS can be used to sequence both DNA and RNA. First, samples get collected, and the DNA or RNA gets purified.\nDNA and RNA Purification and QC\nNext, the DNA or RNA gets checked to ensure it’s pure and undergraded. RNA first needs to be reversed-transcribed into DNA before it can get sequenced. A library then gets prepared from the DNA.\nLibrary Preparation - The First Step of NGS\nA library is a collection of short DNA fragments from a long stretch of DNA. Libraries get made by cutting the DNA into short pieces of a specified size. This cutting gets done by using high frequency sound waves or enzymes. Then sequences of DNA called adapters get added to each end of a DNA fragment. These adapters contain the information needed for sequencing. They also include an index to identify the sample. Finally, any non-bound adapters get removed, and the library is complete. Depending on the application, there can be a PCR step to  increase the library amount. A successful library will be of the correct size. It will also be of a high enough concentration for sequencing. The main sequencing instruments used in NGS are from Illumina.\nSequencing by Synthesis and The Sequencing Reaction\nThese instruments use a method called sequencing by synthesis. The sequencing occurs on a glass surface of a flow cell. Short pieces of DNA, called oligonucleotides, are bound to the surface of the flow cell. These oligonucleotides match the adapter sequences of the library. First, the library gets denatured to form single DNA strands. Then this Library gets added to the flow cell, which attaches to one of the two aligos. The strand that attaches to the oligo is the forward strand. Next, the reverse strand gets made, and the forward strand gets washed away. The library is now bound to the flow cell. If sequencing started now the fluorescent signal would be too low for detection.\nCluster Generation From the Library Fragment\nSo each unique library fragment needs to get amplified to form clusters. This clonal amplification is by a PCR that happens at a single temperature. Annealing, extension and melting occur by changing the flow cell solution. First, the strands bind to the second oligo on the flow cell to form a bridge. The strands get copied. Then these double-stranded fragments get denatured. This copying and denaturing repeats over and over. Localized clusters get made, and finally, the reverse strands get cut. These strands get washed away, leaving the forward strand ready for sequencing.\nSequencing of the Forward Strand\nThe sequencing primer binds to the forward strands. Next, fluorescent nucleotides G, C, T and A get added to the flow cell along with DNA polymerase. Each nucleotide has a different color fluorescent tag and a terminator. So only one nucleotide can get sequenced at a time. First, the complementary base binds to the sequence. Then the camera reads and records the color of each cluster. Next, a new solution flows in and removes the terminators. The nucleotides and DNA polymerase flowing again, and another nucleotide gets sequenced. These read cycles continue for the number of reads set on the sequencer. Once complete, these read sequences get washed away.\nThe First Index is Read\nThen the first index gets sequenced, and washed away. If only a single read is needed, the sequencing ends here. But, for paired-end sequencing, the second index is sequenced, as well as the reverse strand of the library.  \nThe Second Index is Read\nThere is no primer for the second index read. Instead, a bridge gets created so that the second oligo acts as the primer. The second index is then sequenced. These two index reads use unique dual indices. These allow the use of up to 384 samples in the same flow cell.\nSequencing of the Reverse Strand\nNext, the reverse strand gets made, and the forward strands are cut and washed away. The reverse strands are then sequenced. Once the sequencing is complete, any bad reads get filtered out.\nFiltering and Mapping of the Reads\nThese include the clusters that overlap, lead or lag with sequencing or are of low intensity. The clusters cannot overlap on a patent flow cell, but there can be more than one library fragment per nanowell. These polyclonal wells will also get filtered out. Next, the reads passing the filter get demultiplexed.\nDemultiplexing and Mapping to the Reference\nDemultiplexing uses the attached indexes to identify and sort reads from each sample. Finally, the reads get mapped to the reference genome. The different reads align to the reference genome, overlapping each other. Paired-end sequencing creates two sequencing reads from the same library fragment. During sequence alignment, the alogarithm knows that these reads belong together. Longer stretches of DNA or RNA can get analyzed with greater confidence that the alignment is correct.\nWhat is Read Depth in NGS?\nRead depth is an essential metric in sequencing. Read depth is the number of reads for a nucleotide. Average read depth is the average depth across the region sequenced. For whole genome sequencing, a 30x average read depth is good. A 1500x average read depth is suitable for detecting rare mutation events in cancer. Another essential metric is coverage. The aim is to have no missing areas across the target DNA.\nHow is NGS being used?\nNGS gets used in a wide variety of applications. In diagnosing cancer and rare disease, treatment guidance for cancers, and many research areas from ecology to botany to medical science.\nWhat Types of NGS Applications Are There?\nBoth DNA and RNA can be sequenced. It could be the whole genome or transcriptome, just the coding regions (called exomes) of the DNA, or target genes in the DNA or RNA. All types of RNA can be sequenced including non-coding RNAs such as microRNAs and long non-coding RNA. In addition, cell-free DNA, single cells, as well as methylation or protein binding sites can also get sequenced."
  },
  {
    "objectID": "software_ewas.html",
    "href": "software_ewas.html",
    "title": "EWAS",
    "section": "",
    "text": "Epigenome-Wide Association Studies\nTitle: How to Perform Epigenome Wide Association Studies\nPresenter(s): Agaz Wani, Seyma Katrinli\nLevel: Intermediate\nLength: 21:17\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.1_transcript.html",
    "href": "chapter9.1_transcript.html",
    "title": "Chapter 9.1: Copy Number Variation (Video Transcript)",
    "section": "",
    "text": "Title: How to run Copy Number Variation (CNV) analysis\nPresenter(s): Daniel Howrigan, Broad Institute\nHello, my name is Daniel Howrigan, and today I’ll be talking about how to run copy number variation analysis. In today’s talk, I’ll address a couple of questions, namely: what is a copy number variant, and how do we detect it with genotype data? What does the CNV file format look like, and what does CNV analysis output look like? Finally, how do I use this to run CNV burden and Association tests?\nSo, what is a copy number variant? Well, I define it here as a subset of structural variation involving a gain or duplication or a loss/deletion of genomic sequence. Now, I call it a subset of structural variation because structural variation can encompass basically any change in the length of a genomic sequence, which can be as small as a single base pair insertion or deletion. Now, most SNPs that we think of are just substitutions, so the actual size of the genome isn’t changed with a single nucleotide polymorphism. But a structural variant can encompass anything from very small all the way to entire chromosomes.\nWhat we call CNVs, well, being a general name, falls into the category of roughly, at least defined here, at minimum one kilobase to submicroscopic. With array data, we generally say something around 10 to 100 kilobases, given our sensitivity to detect these CNVs, whereas larger CNVs usually are greater than 500 kilobases up to multiple megabases, and then we get into much larger events. Now, I’m not going to get into the details of the mechanisms that cause it. I’ve listed a few here; you can look them up. But there are different hot spots in the genome that are more prone to these copy number variants due to repeat regions or often ways in the machinery that can make mistakes and lead to these gains or losses of genomic sequence.\nHow do we detect it using genotype data? So when we run across the genome collecting a bunch of SNPs...\nI show here the basic mechanism, or at least a figure showing how we take light intensity from different experiments looking at capturing either you know allele A, I have a A as allele A, or the B allele being TT here, and then a heterozygous, you can see a mix of both red and green. Now we can leverage these allele frequencies in what we call the B allele frequency, or at least the frequency of the T in this case. Now, normally when there’s no copy number variant, these will be roughly equal and so be around 50 percent. When we see something like a large deletion or any sort of deletion, we should see a loss of heterozygosity and therefore we will see a gap in these B alleles, at least at any site that you would normally be heterozygote at. With the duplication, these would be a little bit off, so it would be more like two-thirds, one-third, and you would see maybe a movement kind of akin to where these pink lines may be going, even though they don’t quite define that you would see in a duplication that the B allele frequencies move off 50 but not to one and zero.\nNow that’s looking at the B alleles, the other thing we use to detect it is the log R ratio (LRR) and this is basically measuring the light intensity and when you see a drop here, where we would see maybe in all of these calls, you should see a drop of say 50 percent when we have a deletion, and you will see subsequently a rise of about 33 percent for a duplication. Now I’m not going to get into the details of the collars that are used to detect these copy number variants, suffice to say this is the foundation that it’s built upon. What I’m going to talk about next is given that you’ve run copy number variant callers, here are the kind of QC calls that you’re moving into the analysis stage. Now, I use plink to analyze the CNV data, so you can get a bunch of different file formats, copy number variants from different callers, but you would let you know if you want to use plink you would convert this into this particular file format, it’s definitely which is the .cnv file, and it’s basically individual identifiers, the chromosome, the start, and end position of this CNV, the type one being a deletion, three being a duplication, and then also there’s a few other fields here, the score and sites field. In this example. I use the score being, say, the number of copy number variant callers that agreed upon the skip and call, and it could range up to six - sites here being the number of SNPs that are used to call the CNV. Now, I note here that score and sites are not forced into a particular convention, you could say replace score with the number of genes overlapping the CNV, or the site being some other variable that you would be interested in measuring. Now along with this file, plink creates a cnv.map file, and this is basically breaking down the break points of each CNV into akin to a map file, similar to what we have for SNP data.\nAnd you can see here, I note that every different position is mapped, even the end position, but also maybe a single position after that end because you may want to do an additional test after the end of a CNV to see how things have changed.\nNote that the CNV file format commands are not available in plink 1.9, but the initial version granted the speed-ups that you get using the newer version of plink aren’t all that applicable here because we’re generally dealing with rare variation and so these file sizes generally not too large, and the sorts of computing that you use is not too heavy.\nSo what does CNV analysis output look like? Usually, whenever you run a command in plink looking at your CNV files, you get out cnv.indiv file, so this is a per-sample file where you say the number of CNV segments that this individual has, the number of kilobases that these segments cover, and the average kilobases covered per segment. You also get a cnv.summary file; this is akin to the map file that is summarizing the number of affected and unaffected individuals at any given breakpoint or start and end-plus-one of a CNV. And this is basically, you know, the output here, well, it seems quite simple. I’ll show how with these files you can do quite sophisticated analysis by using a lot of different filters.\nSo a lot of the magic in plink is all the flags that you can use to subset your list of CNVs.  What I have here is a verbose command just to show the optionality available in plink, and for each of these, I describe what that function is doing. So with plink, you have `--cfile`, this reads in the CNV and CNV map file. I want to select only deletions, I want to select CNVs that are at least 100 kilobases in length, I want CNVs with a score of four or higher and at least 50 sites. I want to exclude CNVs that overlap a particular region, so I can insert a different text file with a list of chromosome and start-end positions here, and I want to make sure in this exclusion that the CMVs must overlap by at least 50 percent to be excluded.\nI can also look at frequency where at least 10 CNVs overlap; I would like to exclude those because maybe I’m more interested in very rare CNVs. I can also write out the frequencies of these CNVs as well just so I can guarantee which CNVs are being dropped, which CNVs are being kept, and then I can run a basic burden test using a permutation model, and here I just set the number of permutations to be ten thousand. So you can see there’s a lot of different flags here, and it’s manipulating a lot of these flags that can give you just what you would like in terms of your analysis.\nNow granted, using the burden tests in plink doesn’t handle covariates, it basically just looks at something like case-control status, and so what I recommend is taking the output, particularly the CNV `cnd.indiv` file, and reading that into say Python or R. I prefer R to run more sophisticated models, and so you can see when you put more filters here, if I go back, the number of segments will change depending on what filters and obviously subsequently the number of kilobases covered by these segments will change as a function, and it’s kind of iteratively reading in these files at different filtering steps that can produce a wide range of tests.\nSo, I’ve shown a couple figures here that we published when looking at CNV burden in the PGC schizophrenia. So, I took in those .cnv files into R. I ran a logistic regression predicting schizophrenia status and adding a number of covariates such as principal components, genotype platforms, and then basically as I iteratively ran different commands in plink to look at Kb, CNV counts, lengths, frequencies, whether or not they’re in or not in a particular region, you can build up a number of tests of overall burden. So, in this example here, this would be all CNVs, deletions, and duplications stratified by different genotyping platforms, and then all together, and then in panel B here, I’m stratifying by different frequencies, and say previously implicated CMVs as a region. I want to say in the blue bars, we’re looking at enrichment here in the green and blue bars, but you can see the big deviation here. There’s a lot of enrichment when we talk about CNVs at this size or at least this frequency. I mean, and they go away because most of these have been implicated. So, I’ve excluded these regions, rerun the burden test, and you can see that we’ve captured much of the signal already with previously implicated CNVs.\nSo, how do we use this to run CNV association tests at individual CNV loci or at individual break points of CNVs? Basically, I would run a very similar command. I could use all the same filters, basically get rid of a number of commands, in particular, you’re getting rid of this CNV in indiv.perm step, but you still run a permutation test, and what you’ll get out is a .cmd.summary.mperm file, and at each base position, you can run an association test here using permutation. This would be the pointwise permutation value, but there’s also a family-wise permutation p-value that corrects for all the tests here. So, this association is run at all possible start and end plus one positions, and one of the things you can do if you like to include covariates in your data, at least what I’ve done in the past, is maybe run your logistic regression model with a lot of your covariates, pull out the residuals, and then use this as a quantitative trait, and you can run association mapping in plink to get p-values that way.\nSo, what does this look like? I think having a figure is instructive here. So, I plotted through a browser. I’ll break this down. This is our signal at the NRXN1 gene. So, in red, we have our deletions, light red deletions in our schizophrenia cases, dark red deletions in our schizophrenia controls. I also have duplications in blue, which don’t make up much of the signal here. But as you can see, I’ve also plotted the negative log 10 p-value, and if you look closely, you can see each little break point. You can see a different test being run, and you can see at you know a spot like this, where there are many different break points, a lot of granularity, you can run many different tests and get a shape of the association around this gene. Now, you can also collapse. Another test that we’ve done is collapsing across all the exons of this gene, and so this would be akin more to like a gene burden test where you collapse this region and then you test for overlap at that region, run a similar model, and then you can aggregate all the CNVs and cases and controls to report say a gene-based p-value.\nSo, that’s a very quick overview of how to run burden and association with CNV data, and a few considerations are that one of the things that you don’t have access to when looking at CNVs is imputation. And so, it is a consideration to think about, you know, there isn’t kind of a reference, you know, reference haplotypes or a larger dataset to do additional QC. So, there can be additional challenges, particularly with subpar data that they can’t be rescued in the way that imputation can rescue SNP genotypes. And on that note, you know, genotyping chip does matter very much because you can’t impute a bunch of different new sites. The variability in terms of the number of SNPs, particularly for smaller CNVs, is a very kind of important consideration. And so, you can think of, in a particular genotyping chip, if you don’t have a good case-control balance there, you may not have the sensitivity to properly detect CNVs. So, there’s a lot of work that goes into determining at what length of CNVs, or at least at what case control balances, given your genotyping chip, do you have the right amount of power and sensitivity to do a proper case-control association test?\nAnother thing too, ancestry PCs, most of the CNV hotspots and associated CNVs that we see in psychiatric disease aren’t very impacted, and mainly that’s because these are recurrent de novo CNV areas where there’s a higher mutation rate. But this is not really ancestrally, defined in terms of the fact that there’s not a large difference in the allele frequency across different ancestries. But it is still useful to include, particularly as you get to higher frequencies and you get more inherited CNVs. Finally, multiple testing correction permutation is more one of the more robust ways to account for the multiple tests because the nature of CNV data is such that, given the type of genotyping chip that you use, the size of your data set, no particular study is going to be very similar to, similar to another study, and that leveraging the correlation structure within your own data set, given you know your ability to detect CNVs at various, of various sizes and frequencies, using permutation is usually the best way to go about properly testing for Association.\nSo, if you have any questions, feel free to email me. I’ve also put down some sites you could search here for the PGC CNV paper that we did in 2017, and also I think that the write-up and plink on how to do this is really good and very descriptive of all its functionality. Thank you."
  },
  {
    "objectID": "chapter8.2_transcript.html",
    "href": "chapter8.2_transcript.html",
    "title": "Chapter 8.2: Genetic Correlations (Video Transcript)",
    "section": "",
    "text": "Title: Genetic Correlation and Partitioning\nPresenter(s): Patrick Turley, ATGU, Massachusetts General Hospital\nPatrick Turley:\nSo now, we’re going to talk about genetic correlation and partitioning. So, we’re going to, we’re going to start with genetic correlation.\nI’m going to talk more broadly about a term that we call genetic overlap. So, a lot of traits have really similar genetic architecture. That is, if we find a SNP that is causal for some trait, like educational attainment, it’s more likely, once we found it for education attainment, we might think that it’s more likely to also be associated with things like cognitive performance or a bunch of other traits, you know, schizophrenia. So, we maybe want to get an estimate of how strong that relationship is. You know, if we do find a SNP for one trait, how likely is it that it’s going to also be causal for the other? Just as a vocab that I don’t think has come up, we haven’t defined specifically. So, there’s the state where one SNP is associated with a variety of phenotypes, that’s called pleiotropy. So, someone says, “Oh, there’s pleiotropy,” it means that genes do more than one thing. And, because there’s pleiotropy, that’s why there’s going to be genetic overlap. So, why might we care about overlap? Well, you know, it might help us untangle complicated causal relationships. So, if we see that all of the SNPs that are important for education are also important for schizophrenia, then, you know, might say, “Oh, so those two things have a genetic relationship.” You know, if we see that depression is highly genetically correlated with neuroticism, I mean, like, “Okay, yeah, the genetic, you know, additive contribution to those two traits is similar,” and so that can help us as we’re thinking about what might be causing both of them. It also can help us prioritize causal pathways. And so, like, let’s, this is sort of related to maybe the proxy phenotype method. And so let’s say that we know that education and cognitive performance are highly genetically correlated, but we don’t have very large samples for cognitive performance. Well, if we know that they’re highly related, then we could, you know, limit the space over which we’re looking at the genome by just taking, you know, SNPs that are associated with educational attainment above a certain level and look for the association with cognitive performance there, and as a result, we don’t have to do as large of a multiple testing correction because we’re doing fewer tests. You know, also if we know that things are related and we’re trying to figure out how, you know, it’ll just point us in the right direction.\nAudience member: How does that differ from candidate gene?\nPatrick: So I think that when at least historically when people have said candidate gene, it was based on sort of a theoretical biological relationship. And so we said, “oh yeah, this is this gene we think it has to do with this,” and so we’re going to test for that. Whereas when I say proxy phenotype, it’s less of a theoretical relationship that we’re using to select about an empirical one. Yeah, great.\nSo when we say overlap, there’s sort of two ways to think about it. So one’s enrichment and one’s genetic correlation. And so enrichment is this idea of kind of a proxy phenotype method, you know, our SNPs that are important for a phenotype A, also important for a phenotype B. And so we could do this test, we’re just going to take the SNPs with a p-value of less than p0 and we’re gonna test them in a GWAS for phenotype B. And so that could be pretty successful in seeing like if, you know: do the SNPs in phenotype B in that subset are they, you know, more likely to be more significant? That’s sort of the very high-view question. But there are a few technical questions. First off, how would you pick the threshold? Are we just going to look at SNPs that are genome-wide significant or should we look maybe lower in the distribution? How do you deal with LD? So, let’s say you have two SNPs and they’re highly correlated with each other, but this SNP is really important for phenotype A but not at all important for phenotype B, and this SNP is really important for phenotype B, but not for phenotype A. If we just compare GWAS summary statistics, we would say, “Oh yeah, these are both important” and so these traits might be related. But when it comes down to the actual effect of those particular SNPs, they’re not related at all. And so, we should keep in mind that we have this sense when we have GWAS summary statistics, you shouldn’t think SNP, you should think sort of like region around the SNP.\nAudience member: So this is another generality. Is it usually the case that genes that are implicated in the same phenotype, are they on the same chromosome?\nPatrick: Oh yeah, that’s a question I don’t know the answer to actually. Does anyone, is there anyone with a bio background who knows this? I was wondering this just the other day. So the question is if you have a gene that you know is important for some phenotype, are other genes that are going to be important for that phenotype maybe going to be nearby, like on the same chromosome or nearby? Yeah, either close to each other on the chromosome or just on the chromosome.\nAudience member: We talk about the cis transcription regulation elements, local control of the gene expression.\nPatrick: Yeah, yeah. So I guess for expression, we know that there’s some localness.\nAudience member: Most of the genes that control expression are nearby but there can be some that are far away. My understanding that it depends on the phenotype. There are some things like immune function where there’s a huge density of immune-related genes in an area called the major histocompatibility complex on chromosome 6. And then, as you and I were talking about the other day, Patrick, acetylcholine nicotinic receptor genes that tend to be close to one another, so those are ones that are associated with smoking behaviors, but for other things, like educational attainment, there tend to be hits over the entire genome. I wonder if there’s local correlation.\nPatrick: I mean if it’s highly polygenic.\nAudience member: I guess my question is related more directly to one biological pathway.\nPatrick: Mm-hmm. Yeah, yeah, I wish I knew the answer as well, and someone maybe does, but I don’t know. And it’s relevant for a lot of the things that we do. Like remember how I told you one of the assumptions in LD score regression is that these betas are independent, and if genes that are important for specific things tend to be close to each other, that also means that they’re in high LD with each other, and so it might break, you know, might violate the assumptions about these score regressions. So, I don’t know the answer, but I wish I did.\nSo here’s another question, so like let’s, let’s say that we, you know, we have our education, our 74 education-associated SNPs, and then we looked them up in a height GWAS, and we find that all of them are associated with height. You know, do we think that that is a signal that the two traits are strongly related? I’m going to say no, who can give me a reason why we might not expect that? You, yeah, so, so you wouldn’t want to interpret the fact that every single education SNP is associated with height as well.\nAudience member: Direction effect?\nPatrick: Yes, I guess the direction could be one thing, what else might we think about because the direction if it’s associated could go in. But we’re just looking for general enrichment, we’re not thinking about direction.\nAudience member: I don’t know if this is maybe missing the point, but maybe if height and education attainment are correlated?\nPatrick: Mm-hmm, yeah, I mean, if that’s the case then we actually do think they’re related.\nAudience member: It’s just active through the channel that taller people get better education.\nPatrick: I guess, so, so yes, I guess maybe I should have picked something that are unrelated, let’s say there are traits that aren’t. I guess maybe I shouldn’t make you keep guessing, but maybe it’s good that you were thinking. But height’s really polygenic, right? So why is that relevant? Yeah, height is really polygenic.If we had a trait that wasn’t polygenic, let’s say there was only one gene, and it was the only gene, and we find that that gene also is important for height, because of random SNP drawn from the genome is likely to be associated with height. And so we should keep that in mind also when we’re doing these tests. So we’ve got to think, what is our expected significance? And so, because we’re talking about “are things more significant than we expect”? You have to ask the question: what do we expect? And saying that we expect null may be not right.\nThere’s also a question of one-sided or two-sided tests, which gets to this idea: do we want to think about the sign or not? If we see a positive effect in education, if we just want to see if that leads to positive effects in height, then that’s a one-sided test. But if we just want to say, “do we expect a large effect in height?”, we don’t care if it’s negative or positive, then that would be a two-sided test. And so that’s just a decision that we need to make.\nHere are some examples of kind of the thing I told you, so we take the 74 education SNPs. We’re about out of time, but I will stop at the end of the overlap stuff. And then I want to see if it is associated with the GWAS on the size of your thalamus in your brain. The 45-degree line is what we would expect to see if they were all null, and so we see that, you know, it does sort of peel away from null, so there might be a little bit of extra signal relative to the null. But we can test how much enrichment there is based on how polygenic we think the trait is. And we have a p-value of 0.08, we can’t reject that the amount of inflation that we’re seeing is just due to chance. However, if we look at the education SNPs and a GWAS of cognitive performance, you know, we start seeing it peel away pretty quickly, and our p-value there is 0.002, just based on, you know, how significant are these relative to what we’d expect. If you look at schizophrenia, it peels away even more. Schizophrenia, I think is a funny case. So if we look at cognitive performance, the sign concordance is 90%, so that means a SNP that increases education, you know, 90% of the time we’ll also estimate it to increase cognitive performance. So it is highly enriched. Schizophrenia, we see, you know, it’s highly, highly enriched – we can see just by looking at these points that there’s a lot of enrichment. However, the sign concordance is 51%, which means that a SNP that’s associated with education is likely to be important for schizophrenia, but we can’t really guess if it’s going to be protective of schizophrenia or what’s the opposite, more causal, it’s going to be more likely to lead to schizophrenia. And so that’s kind of a funny case. So I think that we should break for lunch and we can talk about genetic correlation briefly after lunch."
  },
  {
    "objectID": "software_geneset_transcript.html",
    "href": "software_geneset_transcript.html",
    "title": "Software Tutorials: Gene Set Identification (Video Transcript)",
    "section": "",
    "text": "MAGMA\nTitle: Gene- and gene-set analysis in MAGMA\nPresenter(s): Christiaan de Leeuw\nChristiaan de Leeuw:\nWelcome to this tutorial on the use of MAGMA, our tool for gene and gene-set analysis of GWAS data. During this tutorial, I will give you a brief overview of how to get started with running a basic MAGMA analysis.\nA full MAGMA analysis consists of three separate steps. First is the annotation step, which serves to map SNPs [single nucleotide polymorphism] onto genes. Second is the gene analysis step, in which, for every annotated gene, an association p-value is computed from the GWAS [genome-wide association study] input data, as well as a gene-gene correlation matrix. And third is the gene-set analysis step, which computes competitive gene-set p-values for each gene set in the input from the gene analysis output.\nThe GWAS input data can be provided to MAGMA in one of two ways. First, you can use raw genotype data in binary PLINK format, which can optionally include separate covariate or input files to be used for the analysis. The second option is to use previously computed SNP summary statistics in combination with genotype reference data. A number of additional files are needed for the analysis as well. For the annotation step, both SNP and gene location files must be provided. For SNP locations, in most cases, the most convenient option is to simply use the BIN files from the PLINK genome data that you will use in step 2. Gene location files with Entrez gene IDs can be found on the MAGMA site [https://ctg.cncr.nl/software/magma], but you can also construct and use your own gene location files if needed. For the gene analysis step, as noted, when using GWAS summary statistics for input, you need to provide reference genotype data as well. A common choice for this is to use a 1000 Genomes reference data, and ready-to-use PLINK format file sets for this data, for different ancestries, can be found on the MAGMA site. Finally, for the gene-set analysis step, a file defining the gene sets to be analysed is needed. Which gene sets are best to use for the bundle and research question you aim to answer? A good general-purpose source of gene sets is MSigDB. This contains a broad range of different gene collections, including Gene Ontology terms and collections of canonical pathways. These are suitable for general gene-set analysis.\nThe MAGMA program itself, alongside documentation and auxiliary files, can be found on the MAGMA site, shown on the screen. MAGMA is a command-line program and does not need to be installed. To run it, just download the archive for your specific operating system and extract the MAGMA executable to the folder you will be running your analysis from. If you are using a Linux system, it is possible that the standard Linux version of MAGMA is not compatible with your Linux distribution. In this case, it is recommended that you try the statically compiled version of MAGMA instead. If this also doesn’t work, you can also compile the program from source on your system. To do so, simply download and unzip the source code archive and run the `make` command shown on the screen. This will create a MAGMA executable binary for your system, which can be used in the same way as the pre-compiled versions.\nBefore you begin, make sure you have downloaded all the files you need, including the MAGMA binary issue. You are now ready to run the annotation step, which will create a file mapping SNPs to genes. To do so, we will use the following command:\nThe `--snp-loc` and `--gene-loc` flags specify SNP and gene location files to use, and the `--out` flag specifies the prefix of the output files. This will produce the annotation file as well as a corresponding log file you can use for later reference.\nBy default, the annotation will map SNPs to a gene if they fall inside the transcription region of that gene, as defined by the gene location file. However, you can use the `window` modifier for the `--annotate` flag shown here to additionally include SNPs within the specified distances in kilobases upstream and downstream of the transcription region.\nGenerally, using your PLINK BIM file for these applications will be most convenient, but you can use a different file as well. In this case, you need a plain text file with no header and with three columns: SNP ID, chromosome, and base pair position, in that order, as shown here.\nSimilarly, you can create and use your own gene location files as well, as needed. These should have the same format as the first four columns in the gene location files provided on the MAGMA site and as shown on screen here as well: gene ID, chromosome, and transcription start and stop site, in that order. The fifth column containing the strand alignment of the gene is only required when you are annotating using an asymmetric window around the gene.\nFor the annotation step, two things are crucial when selecting the input files. First, both the SNP and gene location files must be in reference to the same human genome build. As such, always use SNP locations from files for which you are certain of the build. Note, however, that the SNP locations are only used during this annotation step, so SNP locations in the GWAS input files for the later gene analysis step are not used.\nSecond, the SNP and gene IDs used in these location files need to be of the same type as those used in the input files in the later analysis steps. As such, for example, if you have gene definition files that use Ensembl gene IDs, you should use gene location files with Ensembl IDs in this step as well.\nTo run a gene analysis using raw GWAS data as input, we will use the command shown on screen now:\nThe `--bfile` flag is used to specify the file prefix of the PLINK dataset, while the option of `--covar` flag specifies a file with covariates we want to include. By default, all the covariates in the file will be used, but it is possible to have MAGMA include only a subset of those as well. With the `--gene-annot` flag, we specify the step-to-gene mapping file we created in the previous step. The runtime of gene analysis in MAGMA will depend on both the sample size and the number of SNPs included in the analysis. The dataset I’m using for this demonstration is very small, and so the analysis completes very quickly. But for larger modern datasets, it will take rather longer. However, a batch mode is available that can split the analysis into smaller pieces, which can then be run in parallel. Moreover, if you only need gene analysis output and are not planning to perform any gene-set analysis, you can use a `--genes-only` flag to omit computing the gene-gene correlation matrix. This will speed up the analysis as well.\nOnce the analysis is complete, MAGMA will produce two output files in addition to the log file. The first is the .gene.raw file, which is the output file that will be needed for running gene-set analysis in the next step. The second is the .genes.out file. This is the main results file for a gene analysis. The exact columns included in this file can vary somewhat depending on the analysis settings, but it will look very similar to the file shown here. Note that the gene p-values provided in the output here are raw p-values, and you will still need to apply any multiple testing correction yourself.\nRunning gene analysis with GWAS summary statistics is done in a very similar way, and it requires an input file of the kind I’m showing on screen now. The only two columns that are required to be in your summary statistics file are the SNP ID column and the SNP p-value column, the first two columns in the file here. However, ideally, you should have a sample size column for the sample size per SNP in the file as well. It is not necessary to remove other columns in the file, so output files from most SNP analysis software can directly be used as input for MAGMA.\nThe command to run this analysis looks like this:\nThe `--bfile` flag is now used to specify the reference genetic data, and the `--pval` flag is used for the summary statistics file. With the `use` modifier, you can tell MAGMA which columns contain SNP ID and p-value, in that order. And the `--ncol` modifier defines a sample size column.\nAs with the raw data analysis, the runtime for this type of analysis will vary depending on the data and settings you use. As before, the batch mode and genes-only setting can be used to reduce the total time needed for the analysis. Finally, if you have no sample size column available for your summary statistics, the global sample size for the data can be specified instead with the `--N` modifier for the `--pval` flag, as highlighted on screen now.\nIn addition to those demonstrated now, there are various other options available for the MAGMA gene analysis. These include the batch and genes-only modes already mentioned, as well as options for meta-analysis, rare variant burden scoring, and various others. Please consult the manual for a more detailed description of all available options and settings for the MAGMA gene analysis.\nWith the gene analysis complete, we are now ready to run the gene-set analysis. To do so, we also need our gene-set definition file, which should be formatted like this. In this file, each line defines a separate gene set. The first value on the line is the name of the gene set, this is followed by a whitespace-separated list of gene IDs mapped to that set. In the example here, we are using Entrez gene IDs, but as stated before, any type of gene ID is permissible.\nTo actually run the analysis, we will use this command:\nThe `--gene-results` flag is used to specify the gene results file that we obtained in the previous step, with the `--set-annot` flag specifying the gene-set definition file to use. The gene-set analysis will usually be very quick regardless of the input data, since it doesn’t depend on the sample size of the GWAS data.\nThe main output file produced by the analysis is the one with the .gsa.out suffix, which looks like this. Note that each gene set is analysed separately, and the p-value for each of the gene sets is therefore independent of which other sets were analysed in the same MAGMA run.\nAs in the gene analysis, the p-values here are raw p-values and thus still need to be corrected for multiple testing. However, if there are any gene sets that are significant after Bonferroni correction for the total number of gene sets in the input, an additional .gsa.set.genes.outfile will be created for each of the significant gene sets. This file shows the gene-level results for all the genes in that set, which can be useful for further interpretation of significant results.\nThe gene-set analysis we performed now is just a basic competitive gene-set analysis, but MAGMA allows for a number of more complex analyses as well. This includes the analysis of continuous gene-level information, such as, for example, tissue-specific gene expression levels; joint and conditional analysis models which allow for correction of gene-cell associations for the effects of overlapping gene sets; and gene-set interaction analysis to test whether genetic associations are due to specific combinations of gene sets rather than individual sets.\nThis concludes this MAGMA tutorial. More information on the different analyses offered by MAGMA can be found in the MAGMA manual and various published articles. See the website for more details. You can also always contact me by email should you have any questions or run into any issues when using MAGMA.\n\n\n\nH-MAGMA\nTitle: Annotating Genetic Variants to Target Genes Using H-MAGMA\nPresenter(s): Nancy Y.A. Sey, University of North Carolina at Chapel Hill\nNancy Sey:\nHi everyone, my name is Nancy, and today I’ll be talking to you about annotating genetic variants to target genes using Hi-C coupled MAGMA, or H-MAGMA for short.\nBackground\nSo, before I get started, I would just like to credit doctors Christian and Danielle for developing MAGMA Gene Set Analysis. MAGMA is widely used in our field, and this is for various reasons, with a few being that the tool is pretty revolutionary in that it’s aided in making sense of GWAS findings by identifying the target genes of genetic variants identified from GWAS. Additionally, MAGMA is very easy to use, in that you do not have to be computationally savvy to use the tool. Also, it is very efficient compared to other tools in the field, so it takes a very short time to run MAGMA. However, despite all of these benefits, there are a few limitations to the tool. Namely, MAGMA relies on positional mapping to link variants to target genes. We know from prior studies that the gene regulatory landscape is complex, and it’s therefore possible for variants, especially non-coding variants, to interact with and regulate distal genes. Additionally, the regulatory landscape can be tissue or cell type-specific, so we have to take into account the tissue or cell type that’s most relevant to the trait or disease that we’re interested in understanding. So, these two limitations served as the foundation for developing Hi-C coupled MAGMA, or H-MAGMA for short. So, H-MAGMA complements MAGMA by using Hi-C data.\nMaterials\nAs mentioned previously, H-MAGMA complements MAGMA, so the materials needed to run H-MAGMA are the same as the ones needed to run MAGMA, with the only difference being the gene-to-variant annotation file. In this tutorial, I will walk you through how to generate the H-MAGMA annotation file, which links non-coding variants to their target genes using Hi-C interactions. So again, I will just walk you through how to generate the annotation file for H-MAGMA.\nRequired Files\nThe materials needed to run H-MAGMA are similar to the ones needed to run MAGMA, with the only difference being the annotation file. To generate the H-MAGMA annotation file, you need a few other files. Namely, you need BED files for gene exon and promoter coordinates. Additionally, you need a Hi-C dataset in the BEDPE file format. You will also need an ancestry reference genome, and for the sake of this protocol, we’ll be using the European ancestry. Once you have the annotation file generated, you need GWAS summary statistics to run H-MAGMA.\nSo again, the focus of this tutorial will be on Steps A and B, which walks you through how to generate the H-MAGMA variant annotation file.\nStep 1: Load Libraries and Required Data\nSo, I have broken the annotation files into seven different steps, with code for each step provided as part of this recording. So, the first step is to load your libraries and all required data into a work directory. The next step is to read in your exon and promoter BED files and create genomic range objects for the exons and promoters using the exon and promoter coordinates from GENCODE version 26. Once you have this loaded and the genomic range object generated, you will save them as an R file. Similarly, you would generate a genomic range object for all SNPs using the reference genome from the European ancestry, and you will also save this as an R file in the work directory for later use.\nStep 2: Overlap SNPs to Genes\nIn the following step, we will overlap SNPs to genes, starting with the exonic SNPs. As a reminder, the exonic SNPs are assigned to your target genes using positional mapping, and this is because they are more likely to impact the genes in which they reside. To assign the exonic SNPs to your target genes, we will overlap the gene range objects for exons with the genomic range objects for SNPs that were created in Steps 2 and 3, respectively. Similarly, we will also overlap the genomic range object for promoters with the genomic range object that was created for the SNPs. Once you have both genomic range objects created for the exons and the promoters, these will be saved as an R file for later use.\nIntergenic SNPs\nSo, after we’ve assigned the exonic and promoter SNPs to their target genes, there will be a subset of SNPs that do not overlap with either. These are known as the intergenic or intronic SNPs. In the following steps, I will walk you through how to match those SNPs to your target genes using high C interaction, as shown in this diagram.\nTo achieve this, we will first identify those intergenic and intronic SNPs and save them as an R file. Then, we will load our Hi-C data. For the sake of this tutorial, we’ll be using Hi-C data from the adult brain as an example. Once you load the Hi-C data in the BED Plink format, you will generate a genomic range object for it.\nNext, we will find SNPs that physically interact with exons and SNPs that physically interact with promoters using these set of codes. Similarly, we will also find SNPs that physically interact with promoter regions.\nThen, we will combine those genomic range objects for the exons and the promoters to retrieve unique items. Lastly, we will overlap the intergenic and intronic SNPs with Hi-C data to identify their target genes.\nOnce we have all these SNPs - the exonic, the promoter, the intergenic, and the intronic SNPs - mapped to your target genes, the last few codes that I will show you in Step 7 will be used to generate the annotation file that is compatible with MAGMA. To achieve this, we will structure the annotation file so that it has the genes in the Ensembl format, the chromosomal location, and followed by the SNPs that are assigned to each particular gene. Following these seven steps, you will have your H-MAGMA variant-to-gene annotation file that can be used to run H-MAGMA for any trait or disease. With this, you can run H-MAGMA for any trait. Here, I will use an example using Parkinson’s disorder as an example. So, this is the portion of the MAGMA code where you place your H-MAGMA variant-to-gene annotation file.\nOutput Files\nHere, I am showing you one of the output files that you get from running HMAGMA. This is the ‘genes.out’ file. With this file, you can actually extract the significant genes at various false discovery rate (FDR) thresholds for downstream analysis.\nLimitations\nAs I mentioned previously in the beginning of this tutorial, MAGMA has transformed the field of psychiatric genetics, and with H-MAGMA, we can address some of its limitations. However, it is important to note that there are still some persistent limitations that aren’t addressed using H-MAGMA. Namely, the directionality of effect. Even though HMAGMA can assign SNPs to their target genes using high C data, we do not know whether the variants are upregulating or downregulating the gene. To address this, you can use various eQTL datasets to analyze the directionality of effect.\nAdditionally, we know that not all SNPs are actually implicated in a trait or disease, so you need to do further functional validation to prune down the list of genes. Also, we know that the sample size of a GWAS greatly impacts the number of target genes that is identifiable, so H-MAGMA also suffers from this. And lastly, the lack of diversity in genetic studies. Most of the analysis that we’ve done with HMAGMA has mostly been from European ancestry. However, it’s important to note that you can also run H-MAGMA for other ancestries.\nConclusion\nSo, with that, in the last slide here, I would just like to acknowledge where we currently are with H-MAGMA. Previously, we have generated H-MAGMA for brain tissues, including adult and fetal brain. We have also added various cell-type-specific annotation files, including cortical, iPSC-derived neurons, and iPSC-derived astrocytes, as well as midbrain dopaminergic neurons. However, H-MAGMA is more versatile than just brain-related cells. To expand its use, we have created additional annotation files for 28 different cell and tissue types to allow other researchers to use the tool. These cell types are listed here, and most of all these annotation files can be derived from our GitHub page.\nAnd with that, I would just like to thank my lab as well as my funding sources. Additionally, I’ll be hosting a Q&A section on October 15, 2021, at 3:45 PM Eastern Standard Time. So, if you have any questions or you would just like to say hi, please drop by to see and ask any questions. Thank you.\n\n\n\nE-MAGMA\nTitle: E-MAGMA: an eQTL-informed method to identify risk genes using genome-wide association study summary statistics\nPresenter(s): Zac Gerring, Eske Derks\nThank you for attending my talk entitled ‘eMAGMA: An eQTL-Informed Method to Identify Risk Genes Using Genome-Wide Association Studies Summary Statistics’. MAGMA was initially developed to extract biological insights from GWAS by linking risk variants to their nearby genes. The method assigns single nucleotide polymorphism (SNP) associations to gene-level associations while correcting for confounding factors such as gene length, minor allele frequency, and gene density. While MAGMA is a reliable and commonly used tool, there is room for alternative approaches for how SNPs are assigned to genes. For example, conventional MAGMA assigns SNPs to the nearest gene, which is not always the most accurate approach. Non-coding SNPs can affect the expression of distal genes known as Expression Quantitative Trait Loci (eQTL). eMAGMA, or H-MAGMA, is a modified version of conventional MAGMA that leverages tissue-specific eQTL information to assign SNPs to genes.\nThe Genotype-Tissue Expression Project (GTEx) is an ongoing effort to build a public resource to study tissue-specific gene expression and regulation. The current version of GTEx contains samples collected from 53 non-disease tissue sites across nearly 1,000 individuals, including 13 brain tissues from around 200 individuals. This diagram gives an example of how GTEx can be used for the functional interpretation of genome-wide association signals. The eQTL annotation from various tissues can be used to propose one or more potential causal genes whose regulation is either tissue-shared (shown in green) or tissue-specific (shown in yellow) for a trait-associated variant. These associations would not be identified by assigning SNPs to genes based on proximity alone.\neMAGMA largely adopts the MAGMA pipeline, which consists of three broad steps: gene annotation to assign SNPs to genes, gene analysis to compute gene-based p-values, and gene-level analysis to test the enrichment of gene-based results in curated gene sets. eMAGMA only modifies the annotation step. This is achieved by mapping significant eQTLs from GTEx to nearby genes in a tissue-specific manner. Rather than a single annotation file containing the mappings of SNPs to genes, eMAGMA uses 48 annotation files, one for each tissue in GTEx. After running gene annotation, eMAGMA generates all intermediary files required for the gene-level analysis.\nThis slide shows the basic code for each step of conventional MAGMA with alterations for eMAGMA outlined by red boxes. In the gene annotation step, the SNP location file is altered to include significant eQTLs for one of 48 tissues in GTEx. The tissue-specific eQTL annotation files are used in the gene analysis to generate gene-based p-values. In addition to generating gene-based p-values, we also generate intermediary or raw files for gene set and biological pathway analysis.\nWe tested the performance of eMAGMA against other gene-based approaches, including conventional MAGMA, AS-PAS, FUSION, and summary-based Mendelian randomization, using a simulation experiment. The simulations use SNP genotype information from the QIMR Adult Twin Study. We excluded non-founders, SNPs with more than one percent missingness, and SNPs with a minor allele frequency of less than 0.05. We only analyzed SNPs on chromosome 1. This resulted in 7,138 SNPs with around 60,000 SNPs. eQTL information was derived from GTEx whole blood, which included some 650,000 eQTL-gene combinations for around 8,200 genes. Phenotypes were simulated using GCTA using gender type and eQTL reference data from chromosome 1. Only genes with at least one significant eQTL were included in the analysis, giving 651 genes. We performed 10 simulations per gene, and gene-based association analyses on the 6,510 generated phenotypes were performed using PLINK. We corrected the results for the number of genes in the eQTL reference set.\nWe first evaluated the type 1 error rate across methods by calculating the proportion of genes that were significant in the absence of a true association between eQTLs and phenotypic values. The figure on the left shows that all methods had good control of the type 1 error rate. We subsequently evaluated the statistical power to detect associations at a gene level for varying levels of phenotypic variance explained by eQTLs. We assessed the proportion of significant associations relative to both the total number of causal genes (in Figure A) and when accounting for the total number of causal genes included in each method (shown in Figure B). eMAGMA performed well across different proportions of variance explained by gene expression. After correcting for the number of genes included in each gene-based method, eMAGMA still outperformed the other approaches.\nWe estimated statistical power as a function of the number of eQTLs per gene with one percent phenotypic variance explained by eQTLs. Power significantly increased with the number of eQTLs per gene, as shown in this figure. It should be noted that there was a significant association between the number of eQTLs per gene and statistical power for all methods. However, eMAGMA was less sensitive to the number of eQTLs compared to the other approaches.\nWe compared three gene-based methods: eMAGMA, conventional MAGMA, and the TWAS approach AS-PAS. Using GWAS for major depressive disorder, AS-PAS identified 137 genes (shown in the red circle), eMAGMA identified 99 genes (green circle), and AS-PAS identified 57 genes (blue). A total of 16 genes were identified across all three methods. The figure on the right shows AS-PAS minus log term p-values on the y-axis and eMAGMA p-values on the x-axis, with the color of the points indicating the tissue for which the gene-based association was found. As you can see, there is good overlap between eMAGMA and AS-PAS, and the overlap included established risk genes such as ANK3 and TMEM106B.\nIn conclusion, eMAGMA is an eQTL-informed extension to conventional MAGMA and can be applied to any trait with GWAS summary statistics. eMAGMA maintains appropriate controls with the type 1 error rate while outperforming other methods in detecting causal associations. A tutorial and input files can be found using the following GitHub repository. Thank you.\n\n\n\nPRSet\nTitle: How to run pathway specific Polygenic Risk Scores\nPresenter(s): Judit García-González\nJudit García-González:\nHello, my name is Judit García-González and I’m a postdoctoral fellow at the Icahn School of Medicine at Mount Sinai. Today, I will walk you through how to run pathway-specific polygenic risk scores, a tool that has been recently developed in the Polygenic Lab, either by Paul O’Reilly.\nSo, first of all, it’s important to know what are and why it’s interesting to use pathway polygenic risk scores. For that, I’m going to use one example that most of you might be familiar with, which is the PGC2 GWAS [Psychiatric Genomics Consortium genome-wide association study based on the second data freeze] for schizophrenia.\nSo, we know that psychiatric disorders like schizophrenia have complex aetiologies, where different environmental and genetic factors contribute to the liability of these traits. When we talk about genetic factors, we can imagine that GWAS is a composite of signals, where each signal might represent functional roots to disease. For example, there might be some genomic regions that harbour genetic variants associated with abnormal synaptic pruning. There might be other regions that are associated with biological pathways and processes related to immune activation, and other regions in the genome that pick up signals associated with cannabis consumption.\nThe idea behind pathway-specific polygenic scores is that, instead of aggregating effects across the entire genome, we will aggregate them across relevant pathways. And because we are separating the genetic contribution of a trait, accounting for the genomic structure, we hope that pathway polygenic scores might be more useful for patient stratification or to investigate the disease heterogeneity. Because now, one single individual, instead of having one single polygenic score, will have as many scores as pathways we investigate.\nSo, to calculate these pathway scores, I will walk you through the tool PRSet, which is an extension of the PRSice software, which uses a clumping and p-value thresholding method. It’s important to know that the pathways can be any type of gene set that reflects the encoding of different biological functions. So, this tool is quite flexible to define pathways. PRSice, as well as the newest version of PRSet, have been developed by Dr Sam Choi, and you can find the software and the manual on this website: precise.info.\n[This website link does not work, but the PRSice and PRSet guides are still available online: https://choishingwan.github.io/PRSice/]\nOn the website, you will find a quick start guide on how to download it, as well as some more detail about the available commands and how the method works.\nSo, how to use PRSet? PRSet is very similar to PRSice in terms of use, where we need as an input the GWAS summary statistics and individual-level data for genotype and phenotype. But for PRSet, we also require information about the pathways or gene sets that we want to use. As I said, the tool is quite flexible; pathways can be defined in a variety of ways, including pathways defined by existing canonical databases like Gene Ontology or Reactome, but also by experimental perturbations or some functional outputs, like gene co-expression or protein-protein interactions.\nSo, I’ll go into a bit more detail on the different input options that are available to define pathways. One is using GMT and GTF files using the commands --msigdb and --gtf. The commands that are for PRSet will be shown in green.\nSo, MSigDB [Molecular Signatures Database] will contain information related to pathways, so that is the genes that form each pathway. You will need a [GMT] file where each pathway is specified in a different row, with all the units that compose that pathway. Then, the GTF file will contain the position coordinates for each gene. So, in general, GTF files contain different features, and by default, PRSet uses the features exon, gene, protein_coding, or CDS. But this can be modified with the command --feature.\nThe second option that can be used is a BED file. In this case, you will need to include one BED file per pathway. So, if you want to include multiple pathways, then BED files can be included, separated by a comma. In the BED file, three columns are required, and those are: chromosome name, start position, and end position.\nOne important thing to know when using BED files is that they are indexed differently from PLINK files. So, whereas PLINK files are 1-based, BED files are 0-based.\nSo, you can see in this example that we have a sequence of nucleotides, and, whereas for 1-based indexing, one nucleotide is one position, the 0-based indexing will use a range around each nucleotide. So you will need to subtract -1 in the 0-based indexing for the BED files.\nThe input option three is a SNP [single-nucleotide polymorphism] list, so similar as the BED files, each pathway will be a different file, and they can be separated by a comma, as in this example here.\nWhen you download PRSice for PRSet, there is some toy data that you can use, and these are the commands that you can use: you define the GWAS, your target sample. In this case, I’m using the first option to define the pathways. Then, you can set the number of permutations that you want to run and also the name of the output.\nSo, after walking you through the input, we can see a step-by-step of how PRSet calculates the pathway polygenic scores. To do that, I will compare it with the genome-wide PRS.\nSo, whereas for the genome-wide PRS, you clump and p-value threshold for given r2 and linkage disequilibrium, for pathway PRS, this will be done for each pathway separately. Then, similarly, whereas with the genome-wide PRS, each individual will have only one polygenic score, for pathway PRS, each individual will have k scores, for k number of pathways.\nFor the results, whereas for genome-wide PRS, what is reported is the association measured, which is the phenotype variance explained by the genome-wide PRS. In the case of the pathway PRS, there will be the same, i.e., the association of phenotype variance explained by each pathway PRS. But you can also report enrichment of GWAS signal in the pathway, which is based on R2 of the pathways.\nSo, I’ll go a bit into more detail about these two different methods that PRSet can output. So, when we are talking about association, what PRSet will output is the self-contained p-value. It’s just the regression of the phenotype on pathway PRS and covariates. So, pathways that have many genes, containing SNPs that are associated with the phenotype, are highlighted here in pink, and those will be significant. But it’s important to note that for these results, the self-contained p-value does not account for pathway size. And for example, a large pathway will be more likely to be significant because it’s easier to have a larger number of SNPs associated with the phenotype.\nBut the second output that we can have is this enrichment, that will be output as competitive p-value. This is resulting from a permutation procedure, and this will test whether a pathway is more associated with the phenotype compared to null pathways that have the same size. So, in this case, with the competitive p-value, we’ll be accounting for the size of each pathway.\nIn this plot, I’ll illustrate how we calculate this competitive p-value that accounts for the pathway size. We can imagine a pathway A that has some SNPs in the genic region of the pathway, and for which we will calculate an observed p-value. Then, we will calculate as many null pathways as permutations we run, and these null pathways will have the same number of false clumped SNPs as pathway A, but these SNPs will be randomly allocated. So, we will obtain a distribution of null p-values. Because its power will give us a p-value of the association, the competitive p-value will be defined as the number of tests under the null that have a p-value smaller than our observed p-value + 1, divided by the number of tests performed + 1.\nSo, this is how the competitive p-values are calculated. This approach and this output is similar to the type of enrichment analysis that are performed by MAGMA and LD Score regression, but pathway PRS have the advantage that they provide individual-level data that can be used for other applications.\nSo, let’s see how the output of PRSet looks like. Similar to PRSice, for every PRSet run, we will have a log file, with extension .log, and this will contain the commands used for the analysis and all the information regarding the filtering, the fields that were selected, etc.\nThen, we will have another file, with extension .precise, and this will contain the polygenic model for each pathway across thresholds. So, in this case, I was running only the threshold 1, which contains all the SNPs. But you can see that there is information for each pathway: the R2, the self-contained p-value, coefficient for the regression, standard error, and the number of SNPs. You will always see this base pathway, which is the background pathway used for the competitive p-value calculation.\nThen, we have the .best file, which will give us the scores for each individual and each pathway. So here, each row will be one individual, and then the score for each pathway will be indicated in a separate column. So, there will be as many columns as pathways you are using. In this case, we see four examples from the KEGG [Kyoto Encyclopedia of Genes and Genomes] database. If you wanted all the scores at all p-value thresholds, because this .best file will report only the one with the best p-value threshold, you can use the command --all.\nFinally, we will have another file with extension .summary that will have the information of the best model fit of each phenotype pathway. And in this case, each pathway will be a row. And then, it will contain, again, information on the R2, null p-value, coefficient, and also this competitive p-value that I was talking about, that will indicate the enrichment of this pathway compared to other pathways of similar size.\nFinally, I would like to leave some heads up and useful commands. So, for example, some commands that might be useful for the user is the window size, which is like, for example, if you are defining SNPs that are within a gene, you can extend that window. And so, you can include some number of N bases to the 3’ region of each gene or to the 5’ region of each gene. You can also exclude SNPs that are within a certain range with a command --x-range. So, for example, the MHC [major histocompatibility] complex or the APOE region if you want to do some Alzheimer’s analysis and you want to exclude that one.\nIf you want to improve the computational efficiency of PRSet, you can parallelise the process using the --thread command. You can also speed up a little bit of PRSet at the expense of an increase in memory using the --ultra command. Additionally, you can use the --keep and the --extract commands to exclude individuals or SNPs from the analysis.\nIf you are doing permutations to calculate the competitive p-values, this can be computationally intensive. So, it might be useful to adjust for the covariates like sex or age beforehand on your phenotype and then use the residuals as the phenotype because this will speed up the process.\nSo, this was this quick introduction to PRSet. Thank you for listening. If there is anything that is not here or you have any questions, there is a Q&A session on the 15th of October at 11:30, and this is the Zoom meeting ID. So, I’ll be happy to clarify or answer any questions. Thank you.\n[Please note that this is an archival recording and the information about the meeting pertains to a past event.]"
  },
  {
    "objectID": "software_mtag.html",
    "href": "software_mtag.html",
    "title": "MTAG",
    "section": "",
    "text": "In this video Dr. Turley gives a comprehensive introduction to Multi-Trait Analysis of GWAS, or MTAG. The video discusses the theory behind combining different but correlated traits' GWAS and the benefits and considerations of doing this. This video goes fairly deep into the mathematical models underlying this method.\nYou can read more about this method here:\nTurley, P., Walters, R.K., Maghzian, O. et al. Multi-trait analysis of genome-wide association summary statistics using MTAG. Nat Genet 50, 229–237 (2018). https://doi.org/10.1038/s41588-017-0009-4\nBelow the video there is a link to the Python command line tools on GitHub for conducting MTAG.\n\nTitle: Multi-trait Analysis of GWAS (MTAG)\nDescription: Intermediate\nPresenter(s): Patrick Turley\nLength: 1:32:04\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to MTAG GitHub here."
  },
  {
    "objectID": "chapter8.3_transcript.html",
    "href": "chapter8.3_transcript.html",
    "title": "Chapter 8.3: Gene Association Analysis (Video Transcript)",
    "section": "",
    "text": "MAGMA\nTitle: How do we go from genetic discoveries from GWAS/WGS/WES to mechanistic disease insight?\nPresenter(s): Danielle Posthuma\nDanielle Posthuma:\nWell, welcome back, this is part three of the session on how do we go from genetic discoveries to mechanistic disease insight, and in this last part I will focus a little bit on the software tool MAGMA for conducting gene based and pathway analysis. So in the practical you will learn how to work with MAGMA, and that’s a tool that was created by Christiaan de Leeuw a couple of years ago and it can be downloaded from this website over here.\nIt is a tool for gene-set analysis and requires you to work with the Command Line interface, which should now be quite familiar to you and as an input you can either provide raw genotypic and phenotypic data or you can also provide summary statistics from already published results, but then you would also need some reference data. Because we need information on the LD structure of between the SNPs that are part of your analysis and then the other input is gene definitions. So that’s with SNPs belong to which genes and I will come back to that in the next couple of slides and also definitions of gene sets. But if you download MAGMA, there’s some files that have, some default files that you can use, that do this for you, but you’re also free to use your own files if you want to. So just as an aside, if you want to have access to public summary statistics, we created a database and on this slide I’ve just added one example when you’re interested in looking at one particular GWAS, then it gives you the Manhattan plot that gives you gene-based plots, it gives you the QQ plot. It also gives you gene sets outcome already. And it gives you some information about this GWAS with the link to the PubMed ID and where to download the data. So that’s, you can use this database if you want to play around with any software tool that requires you to input summary statistics, then yeah you can just download summary statistics from this database but there are also some other databases that have the same purpose. In MAGMA gene set analysis, there are three main steps, so step one is the annotation where we match SNPs to genes, and so MAGMA needs to know which SNPs do I have to analyze as part of which gene. So that’s step one.\nStep 2 is the gene analysis, so that’s where we compute the association of the gene with the phenotype. So here the unit of analysis is the gene and then step three Is the gene set analysis, where the association of gene sets is tested against your phenotype. And then, because it’s a very general linear regression framework which can easily be extended, it’s very easy to use continuous sets. So instead of having a dichotomous set where genes are either a member of the gene set or they’re not a member of the gene set that you can also have quantitative, quantitatively defined set of genes where every gene has a value that indicates how likely it is to be part of a gene set, or that indicates the expression level of a gene in a cell type and then the cell type is the gene set. And it also allows you to do conditional and joint analysis and interaction analysis as was explained in part two of today’s lectures.\nAnnotation\nNow going back to the three main steps, annotation. If you download MAGMA it comes with a general annotation file and there SNPs are mapped to genes based on the physical location, and but you can also change this annotation file so you can, if you would like to have eQTLs included in it, you can map SNPs that are physically located outside of a gene but have a known eQTL link to the gene, or chromatin interaction, that’s also possible to use. You can also add a window around the gene so you can say, well I would like to have maybe 1 megabases before and after the gene and those SNPs should also be analyzed as part of this gene. An one SNP can actually be linked, can be mapped to multiple genes.\nGene Analysis\nThen if you run the analysis, there are four models that are available in MAGMA. If you have the raw genotypic data, then it will conduct a principal component linear regression analysis and that that can only be done when you have access to the raw data. So if you have, if you input summary statistics which most of you will probably do, then there are three different models that you can use to evaluate statistical significance of your genes and of your gene sets.\nSo the first model is the SNP-wise mean and it performs the test on the mean SNP association, so that evaluates the evidence for association of all of the SNPs in, that are located in gene and then uses the average association to evaluate whether the gene is actually associated. Or you can do to SNP-wise Top Model where where the focus of the analysis is on the strongest SNP association, and you can also combine these two models and get this, the SNPwise multi model where the evidence from both of these previous models is combined into one p-value for your gene. And yeah, deciding which model is best for you, that depends on what your actual hypothesis is. So what kind of sensitivity would you want? So there’s no, we don’t think that there’s a best, best model. It really depends on the situation or your research question. That’s why we provide multiple models in the MAGMA tool. So what’s being done in the MAGMA tool, so when you do a gene set analysis, that’s basically an analysis of genes. So instead of individuals being your unit of analysis or your data points, the genes are the data points in the analysis. So in this table we have listed 10 different gene IDs and each of these genes have been tested for association in the gene-based step in MAGMA. So they all have some kind of measure for the strength of the association with your phenotype based on your GWAS summary statistics. And then there’s also an indication of whether or not they are part of the set of of your gene set that you would like to test.\nSo in this case the genes are the data points and the gene set is the grouping variable and the genetic association with the phenotype, that’s the outcome that you would like to get, so this is basically a simple T-test testing whether the average association of the genes that are inside your gene set is different from the average association of the genes that are outside of your gene-set. Yeah, so that’s basically just a one-sided test of genes because you have a very strong hypothesis of what association should be stronger. Now, there are two kinds of tests. So you could do a self-contained analysis where you ask whether the mean or the average genetic association of genes in a gene set is greater than zero. Yeah, so that’s your null hypothesis and your alternative hypothesis, whereas in competitive analysis you ask whether the mean genetic association of genes in the gene set is greater that of the genes outside of the gene set.\nYeah, so that’s your competitive analysis. And compare this with with a randomized controlled trial or any experimental setup, So in a self-contained analysis we would ask if the mean improvement of patients in the treatment group is greater than zero, whereas in a competitive analysis you would have a control group, so you would ask whether the mean improvement of patients in the treatment group is actually greater than that of patients in the control group. Now everybody would agree that we would want to do a competitive analysis. We would need a control group, otherwise we cannot really say that the treatment is causing the patients to improve. So that’s also the reason why we think competitive analysis is the way to go in gene set analysis and that self-contained analyses are not informative for asking the question whether your set of genes that you tested is actually causally associated with your trait of interest. That’s why we advise never to do a self-contained analysis, but always to to use a competitive gene set analysis.\nOK. This just is stressing that same point and also in the, in part two of these lectures of today I’ve indicated this or if this is not clear than maybe you should go back to Part 2 of the lecture, so I hope that this message does come across and I’m looking forward to the MAGMA practical that is planned for later today. Thank you for listening and see you later!\n\n\n\nFUMA\nTitle: FUMA: Functional mapping and annotation of genetic associations\nPresenter(s): Kyoko Watanabe\nKyoko Watanabe:\nHi everyone. I’m a PhD student at the Vrije Universiteit in Amsterdam. My work mainly focuses on understanding genetic associations in a biological context. Today, I’m going to introduce you to a web application I have recently developed, which is Functional Mapping and Annotation of Genetic Associations [FUMA].\nSo, I’m going to start with a very quick recap of what was GWAS [genome-wide association study] again. So, we basically start by genotyping a large number of individuals using SNP [single nucleotide polymorphism] arrays. Which, nowadays, we can tag around a million SNPs, and by performing imputation with reference panels, you end up with a maximum of 20 million SNPs. So, in a very simple case, when you have case and control groups in your genotyped individuals, you perform statistical tests to see if the occurrence of minor alleles in case and control groups are different from zero. So, in the end, you get the p-value for every single SNP you have. But, as you can imagine, the number of statistical tests being performed is the same as the number of SNPs you have. Of course, you have to correct for multiple testing, and the gold standard for a genomically significant p-value is 5 × 10-8. So, whenever you find SNPs with p-values less than that, those genomic regions are called “hits” or “significant.”\nSo, the very first GWAS study was published in 2005, and since then, the cost of genotyping has dramatically decreased, which allowed us to collect a much larger number of individuals. Nowadays, big consortia for meta-analysis usually use more than 100,000 individuals. And by increasing the sample size, we also increase the statistical power to detect relatively weak effect sizes. For example, the height study, using around two hundred thousand individuals, ended up identifying more than a hundred genome-wide loci. So, we’ve been conducting GWAS in the last decade, and nowadays, in the GWAS Catalog, we have more than 3,000 studies, including over 38,000 unique SNP-trait associations for over 600 phenotypes. So basically, we have a lot of risk loci spread all over the genome.\nHowever, especially for complex traits which are highly polygenic, we know that the association of single SNPs is very weak. To detect those effects, we need a much larger number of samples. And luckily, the UK Biobank was just released this month, and the QC2 database [UK Biobank dataset] contains information on 500,000 individuals and more than a thousand phenotypes. So, the UK Biobank has a potential to identify novel loci for many human complex traits, and we are expecting more and more GWAS to be published in the coming months.\nSo, the question is, what benefits do we gain from GWAS results? Ideally, we would like to identify the causal variants from genetic associations that can be used to improve diagnostics, prognostics, or even identify novel drug targets or biomarkers. However, an association isn't causal. Association doesn't tell anything about causality. And also, purely based on p-values from GWAS, you don't really know much about underlying biology. Identifying causal variants from GWAS results is not straightforward. So, to overcome this problem, we usually go through several steps.\nThe first step is to correct for LD [linkage disequilibrium], which is [a] non-random occurrence of SNPs. So because of LD, the most significant SNPs you find from a specific genomic locus doesn't necessarily have to be the one actually causing the phenotype. Instead, there could be other SNPs that are truly responsible for the phenotype, and these SNPs might have a higher correlation with the most significant SNPs. So, we don’t want to miss those SNPs just based on the p-value. So The first step is to include all the SNPs that have a higher correlation with the significant SNPs. Once you have the list of SNPs you’re interested in, the second step is to check the functional consequences on the genes. For example, if you have SNPs in exonic regions or in the non-coding regions, there are several software tools that can perform this task. However, more than 90% of GWAS findings are known to fall into non-coding regions. So just knowing that you have a hit in a non-coding region doesn’t really help you understand what is actually going on in a biological context. So you still need to annotate the biological functions.\nThere are several data resources you can use. For example, CADD score is a metric that assesses the deleteriousness of SNPs, and RegulomeDB is a categorical score that indicates how likely the SNP affects regulatory elements. Additionally, there are several eQTL databases, for example GTEx has details in 44 different tissue types. And, especially for non-coding regions, you will also want to check the epigenetic status. The data is available from Roadmap and ENCODE. I didn’t bring up any database names over here, but the 3-D genome, in the field of 3-D genome, more and more data is becoming available. So, including Hi-C data might also be another option to map SNPs to the distal genes. So, using this functional information at the SNP level, you can end up with a list of genes you’re interested in. Finally, you need to consider expression patterns in different tissue types and also cells that share biological functions, such as pathways.\nSo, we’ve been performing these multiple steps manually. As you can imagine, this requires you to install different softwares and download various databases, and sometimes reformat the data each time. So this is very time-consuming and elaborate. So, we hoped to make a single platform that can perform all of them.\nSo, we developed a web application named FUMA that basically optimises the four steps I showed in the previous slides into one single platform. So, in the FUMA, there are two main processes. The first one is SNP-to-gene, starting from GWAS summary statistics. We provide lists of candidate SNPs with annotations, and also the lists of prioritized genes. And these genes can be passed to the second process, which is gene-to-func[tion analysis], where it provides you [with] the further variant annotation at the gene level. And another advantage of FUMA is we also provide interactive visualisation in the web application, so you don't have to use external software just for visualisation.\nSo I’m going to go through what FUMA actually does in each process. So in the SNP-to-gene, starting from GWAS summary statistics, we first characterize genomic loci by correcting for LD. And here, we provide you with the list of lead SNPs and the genomic risk loci. All the SNPs which are in LD of lead SNPs are then passed to the second step, which is the annotation of SNPs. Here, we perform the ANNOVAR and annotate several variant scores and eQTL, and also the Hi-C. Using this information, we finally perform the gene mapping. So currently, we have three different criteria for gene mapping. The first one is positional mapping using annotations from ANNOVAR and eQTL mapping, and also, the chromatin interaction mapping. So before you perform this gene mapping, you can also filter SNPs based on the annotations you obtain from step two. And you can also combine different mappings together. You can specify lots of different parameters when you submit the job. And we provide a list of genes mapped by… based on the user-defined parameters.\nSo this is just an example of how the result page looks. We provide a Manhattan plot on the top.\nAnd the second one is… we perform gene-based tests using the MAGMA software. So this is the Manhattan plot based on gene p-values. And the summary results per genomic risk loci. And all the results are available as a table. And you can also create a regional plot with all the annotations and results together. And all the results and approaches are downloadable.\nSo this is just one example [of] how you can utilise the eQTL mapping. So this is one of the risk loci on chromosome 14, from schizophrenia GWAS. associated with schizophrenia. From the top, you’ll see a zoomed-in Manhattan plot, and the genes, CADD score, RegulomeDB, chromatin open chromatin states, and eQTLs. So as you can see, the risk locus itself spans multiple genes. So if you don't know, if you don't have any further information, you end up with listing all the genes, or you can manually check the function of genes and you can pick the one that has the most interesting function in the phenotype. However, by performing eQTL mapping, we prioritise the single genes which have eQTLs in the brain. So performing different types of eQTL mapping, you can also prioritise genes.\nAnd another example is for chromatin interaction mapping. FUMA currently uses Hi-C data from Schmitt et al. which includes 14 different tissue types and cell lines. As I already said, the field is growing very fast. We also provide [the] option to apply custom the chromatin interaction matrix, which isn't limited to Hi-C but can include Capture Hi-C and C5. So the graph shows the risk loci on chromosome 16 from BMI GWAS. The most outer layer is the Manhattan plot, and the second, the blue circle, is the genome coordinates. And the risk loci are highlighted in blue. And inside of the circle, the orange links are Hi-C links, and green links are eQTLs. So, as you can clearly see, the Hi-C can map SNPs to distal genes compared to eQTLs. So this can help you to identify novel candidate genes which you might have missed.\nSo finally, once you have the list of genes, you can use the gene-to-func process, where we provide a gene expression heatmap and tissue specificity, by performing overrepresentation tests for differentially expressed genes across different tissue types, enrichment testing for gene sets, and also, external links to OMIM [Online Mendelian Inheritance in Man] and DrugBank to further investigate the individual genes.\nSo, in summary, we optimise the post-GWAS annotation in a single platform, as a web application. So this might be the very first place to stop by for the very broad overview of what’s going on in the GWAS risk loci once you get the new GWAS results. But also, if you have a phenotype of interest, there are lots of GWAS summary statistics already available. So you can start, you can perform the FUMA for the available GWAS and start integrating with research. And for the future updates, we are thinking to extend FUMA to be able to accept the whole exome sequencing studies and also EWAS.\nAnd so finally, I would like to thank my supervisor, my co-supervisor, and FUMA is available online, so please feel free to visit the website. And I also have a poster this evening at location B-325, so if you want to know more details, please feel free to visit me. Thank you. [Note: please note that this is an archival recording; the FUMA website is available at https://fuma.ctglab.nl/]\nFacilitator: Time for a couple of questions?\nAudience member: So I have a question. I've seen a couple of cases where, even though the risk locus is associated with the same phenotype, there’s clear evidence of distinct haplotypes. It seems like FUMA would probably be able to show you cases like that where potentially you're getting the same phenotype from distinct variants that are affecting, say, the promoter of the gene or a nearby enhancer.\nKyoko Watanabe: You mean, like, pleiotropy?\nAudience member: Like, same phenotype but two different causal variants.\nKyoko Watanabe: In the same region?\nAudience member: In the same region.\nKyoko Watanabe: Um. Yeah. So it's more like the FUMA is just to annotate what's the functional information available, so it just provides you the options which SNPs you're going to look at in further. So it's not… It's not removing the information. So, you might get multiple SNPs that have functions from one locus, but yeah, we cannot distinguish which [one] is actually causal. But, I don’t think you’re actually going to miss that information."
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Welcome to the PGC Video Textbook!",
    "section": "",
    "text": "Hello! I’m Cathryn Lewis from King’s College London and I’m the Education and Training lead for the Psychiatric Genomics Consortium. I’m very pleased to welcome you to the PGC’s Video Training Textbook.\nThe aim of the textbook is to provide comprehensive training materials in psychiatric genetics. We start with an introduction to mental health disorders, give a background to genetics and the technologies used to generate genetic data, and then we step through the methods that are used to analyze that genetic data, from quality control, to genome-wide association studies, to polygenic scores, and pathway analysis.\nThese videos have been collated from online resources, including those produced for the Psychiatric Genomics Consortium, and others from external groups. They range in length from a few minutes, to an hour, and cover both teaching lectures, and methods tutorials. Some videos are marked as “Basic”, “Intermediate”, or “Advanced”, so you can plan your learning program accordingly.\nYou can use the textbook in any way you choose, working systematically through each section, or picking specific topics that you want training in. If you identify areas that we don’t cover, or that you want to suggest additional videos for, please let us know! The textbook is a flexible resource that we hope to update regularly.\nFinally, a huge thank you to the team, who have given their time, their energy, and their enthusiasm to create the textbook. This is an international team from the US, Europe, and New Zealand, and it’s been a great pleasure to work together.\nWe hope that this Psychiatric Genomics Consortium Video Training Textbook fills a need in building capacity in the psychiatric genetics community, and so, enables us to train a new generation of researchers worldwide. With the rapid expansion of available genetic data, the need for skilled analysts has never been greater! And that is an essential step, if we are to realize the potential of using genetics to improve the prevention, diagnosis, and treatment of mental health disorders. And I hope the Video Training Textbook helps you achieve that aim."
  },
  {
    "objectID": "software_MR_transcript.html",
    "href": "software_MR_transcript.html",
    "title": "Software Tutorials: Mendelian Randomization (Video Transcript)",
    "section": "",
    "text": "Title: Examine causality using Mendelian randomization\nPresenter(s): Jie Zheng\nJie Zheng:\nThank you for the invitation from the World Congress of Psychiatric Genetics 2021 conference. I appreciated all the support from the conference organizers. My name is Jie, and I’m a research fellow from the MRC Integrative Epidemiology Unit at the University of Bristol. Today, I’m going to present how to examine causality using Mendelian randomization.\nMendelian randomization is mother nature’s randomized control trial. Segregation of alleles at mitosis mimics the random allocation of individuals to the drug and placebo arms of a randomized control trial. If the number of disease events is lower in the drug arm, we conclude that the drug, by its protein target, was effective for reducing disease risk. Similarly, if we see fewer disease cases in the genotype AA subgroup, we conclude that the genotype, by its protein target, reduces disease risk.\nA major advantage of animal studies is that they are much cheaper and safer to run than randomized control trials.\nHow does the method actually work? In Mendelian randomization, we use genetic polymorphisms or SNPs to proxy for the protein or other exposure of interest. We have two effect estimates: the effect of the SNP on the protein in a protein GWAS and the effect of the SNP on disease in a disease GWAS. GWAS, also called genome-wide association study, measures the association between hundreds of thousands to millions of SNPs across the genome with a trait of interest, typically in very large sample collections. Thousands of such studies have been published in the past 10 years.\nIn Mendelian randomization, we combine the SNP-protein and the SNP-disease effect estimates to infer the effect of the protein on disease. In the example, the protein level decreased by 0.5 units per g allele in the protein GWAS, and disease risk increased by 42 percent per g allele in the disease GWAS, using the ratio method. We take the ratio between the two estimates to infer the effect of the protein on disease. The latter is the odds ratio of disease per unit increase in the protein and can be interpreted as a 50 percent lower disease risk for each unit decrease in the protein.\nCausal inference using Mendelian randomization requires the following assumptions to be true: First, the SNP truly affects the protein. Second, the SNP affects disease only through the protein. And third, the SNP is not associated with confounders.\nMR can be used to rapidly evaluate the potential downstream consequences of pharmaceutical interventions, including:\n1. Prediction of on-target beneficial effects.\n2. Prediction of on-target side effects.\n3. Opportunities for drug repurposing.\nTo support large-scale causal inference, Mendelian randomization, we developed the MR-Base platform and the TwoSampleMR R package. The MR-Base platform is both a database of GWAS summary statistics and an analytical platform for Mendelian randomization.\nAs an example of applying the MR-Base platform, we studied the causal relationship between proteins and diseases. Proteins are direct targets for most drugs and therefore have high priority for drug development. Using the MR-Base platform, we are now able to estimate the causal effects of thousands of proteins on hundreds of human diseases and risk factors using Mendelian randomization.\nThis large-scale Mendelian randomization study identified over 200 robust protein-disease associations, covering a wide range of disease areas, including psychiatric diseases, cardiometabolic diseases, autoimmune diseases, and cancers. This analysis also validated eight approved drugs for the same indication being investigated in existing clinical trials.\nMoreover, we systematically compare the MR estimates with clinical trial effects. We found that it is considerably more likely to predict drug trial success if a protein-disease pair shows MR and co-localization evidence.\nTo improve reproducibility, we made our protein-disease association results available through the MP-GraphDB platform. The MP-GraphDB platform is an analytical platform and database that aims to support data mining in epidemiology.\nWithin the platform, we are able to query the protein we are interested in, the outcome we are interested in, or even the SNP we are interested in, and the platform will return the graphical results and the MR results in a table.\nIn addition, we recently upgraded the MR-Base database into a centralized GWAS summary database. The new database is now launched as the IU OpenGWAS platform. Till now, the new platform provides summary stats for over 40,000 phenotypes.\nTo further extend the usage of GWAS summary statistics, we developed this ecosystem within the IU OpenGWAS project. The ecosystem includes both a database and analytical tools, which could be accessed by a few R packages, such as IU-GWAS and IU-GWASUtils. These packages could be used to support all types of genetic analysis using GWAS summary statistics, including LD score regression, fine mapping, and genetic co-localization.\nHope the ecosystem could make your research life easier. This is the end of my presentation about examining causality using Mendelian randomization. Thank you for your attention."
  },
  {
    "objectID": "software_gwas.html",
    "href": "software_gwas.html",
    "title": "GWAS",
    "section": "",
    "text": "Genome-wide Association Studies in PLINK\nTitle: Genome-wide Association Studies\nPresenter(s): Hailiang Huang\nLevel: Intermediate\nLength: 15:47\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets.\n\n\n\nGenotype QC\nTitle: How to run quality control on genome-wide genotyping data\nPresenter(s): Joni Coleman\nLevel: Intermediate\nLength: 16:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to PGC RICOPILI pipeline.\nLink to Coleman lab GitHub GWAS scripts.\n\n\n\nTractor: GWAS with admixed individuals\nTitle: Tractor: Enabling GWAS in admixed cohorts\nPresenter(s): Elizabeth Atkinson\nLevel: Intermediate\nLength: 18:38\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets.\n\n\n\nGenotype Calling and Imputation with MoChA\nTitle: Genotype calling/imputation on the cloud: MoChA pipleine\nPresenter(s): Giulio Genovese\nLevel: Intermediate\nLength: 15:13\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to MoChA GitHub and Webpage.\n\n\n\nSAIGE: Family-based GWAS\nTitle: Genetic association studies in large-scale biobanks and cohorts\nPresenter(s): Wei Zhou\nLevel: Intermediate\nLength: 27:17\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to GitHub and Wiki Tutorials for SAIGE and SAIGE-GENE.\n\n\n\nCC-GWAS\nTitle: Tutorial to use CC-GWAS to identify loci with different allele frequency among cases of different disorders.\nPresenter(s): Wouter Peyrot\nLevel: Intermediate\nLength: 21:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub and datasets."
  },
  {
    "objectID": "software_imaging.html",
    "href": "software_imaging.html",
    "title": "Imaging",
    "section": "",
    "text": "Title: Imaging Genetics\nDescription:\nPresenter(s): Xavier Caseras\nLevel: Beginner\nLength:"
  },
  {
    "objectID": "software_prs_transcript.html#sec-video2",
    "href": "software_prs_transcript.html#sec-video2",
    "title": "Software Tutorials: PRS (Video Transcript)",
    "section": "PRS-CSx to perform PRS analysis",
    "text": "PRS-CSx to perform PRS analysis\nTitle: Hands-on tutorial of using PRS-CSx to perform multi-ancestry PRS analysis\nPresenter(s): Tian Ge, Yunfeng Ruan, Stanley Center, Broad Institute\nYunfeng Ruan:\nHello everyone, this is Yunfeng Ruan. I’m going to introduce how to calculate polygenic risk score with PRS-CSX. PRS-CSx combines multiple GWAS to increase the accuracy of polygenic risk score. If you are interested in the algorithm, please refer to our preprint on MedRxiv. Our GitHub has a very detailed readme that covers almost all aspects of how to run PRS-CSx. If you are familiar with Python, you may figure out how to run it within no time yourself. Here, I will walk you through the whole process and clarify some details in addition to the readme.\nThis is a workflow of the method: The input is GWAS summary statistics from different populations. PRS-CSx adjusts the effect size of each populations’ GWAS using their corresponding LD reference panel. The adjustment of effect size is performed to SNPs that are shared by the input GWAS, LD reference panel, and the target data. Therefore, PRS-CSx needs the prefix of target data so that it can read the SNP list from the .bim file. The output is two sets of adjusted effect size.\nNext, you calculate the PRS of your target sample based on each set of the adjusted effect size. Usually, this is done by PLINK. Finally, you linearly combine the PRS from different populations’ GWAS using software like R or MATLAB and predict the phenotype with the final result in PRS. You can also add more GWAS so that hopefully you can have a better result.\nNow, let’s see how to run PRS-CSx step by step. PRS-CSx is a command-line Python software. You can use either Python 2 or 3, and have to install SciPy and h5py. You can use the command “git clone” to download and install in one step, or you can click the code button to download it locally and unzip the compressed file. You can test if the installation is successful by commenting “prscsx --help”. It will print a long list like this.\nNext, download the LD reference. We provide the pre-calculated LD reference panel link in the readme “Getting Started” section. We have LD references calculated from 1000 Genomes, and LD reference calculated from UK Biobank samples. You also need to download the SNP information and put it in the same directory with other LD reference files. After you download all the software and the LD reference, you can write your first analysis.\nYou can download the formatted test data and the user example command from the readme “Test Data” to run a test. The test will be finished in about one minute, and you will have two .txt files as the output. If you use your own data, the first thing to do is format your summary statistics file. The information is available in the “Using PRS-CSx” section. I want to highlight that the formatted inputs must contain a header line and have the right column order and the variable names.\nNow you can finally run the method. Here is a typical command, which contains the path to the PRS-CSx, the directory of LD reference, the prefix of the target data, a list of formatted summary statistics. Please note that you should put a comma between different items, and there should be no space in the list. Also, the list of GWAS sample size, and the list of the population of the GWAS. We highly recommend you run PRS-CSx per chromosome to allow calculation in parallel. You can specify which chromosome you want to adjust by specifying it in the “chrom” option, and then the hyperparameter file and the output.\nTo address the effect size of one trait, you need to run 88 jobs. For each job, PRS-CSx will print something like this on either screen or the log file. You will get two outputs, one for each population. If you adjust each chromosome in parallel, you can concatenate the results in one file using the command “cat”. Altogether, you will have eight sets of adjusted effect size. You can calculate PRS based on the adjusted effect size using an allelic score function in PLINK.\nThe last step is to predict the phenotype with the PRS. First, you normalize all the PRS to have a mean of zero and a standard deviation equal to one. Then you optimize the hyperparameter in validation data.  For each file, you predict the phenotype with the normalized PRS A and normalized PRS B, and get the R-squared. You compare R-squared from each file and learn the path file and the coefficient of the two PRS under that file.\nThen in the testing data, you calculate the final result.  With the knowledge of the path file and the coefficient of the two PRS, you choose the PRS under that path file and combine them with the learned coefficients. Then you use the combination of the PRS to calculate the R-squared. This will be your final result. If you have any other questions, you can raise an issue on the GitHub website. We thank Hailiang and Tian for supervising this project, and all team members and data resources."
  },
  {
    "objectID": "software_prs_transcript.html#sec-video3",
    "href": "software_prs_transcript.html#sec-video3",
    "title": "Software Tutorials: PRS (Video Transcript)",
    "section": "PRS in Ancestrally-diverse Populations",
    "text": "PRS in Ancestrally-diverse Populations\nTitle: PRS-CSx: Improving Cross-Population Polygenic Prediction using Coupled Continuous Shrinkage Priors\nPresenter(s): Tian Ge\nHost:\nIt’s half past nine, so I will, you know, start by introducing Tian. Uh, Tian is a good friend and a good colleague of me, of mine, at Mass general hospital and Harvard Medical School. Um, he’s an assistant professor in Harvard Medical School and, you know, trained as a mathematician. He’s contributed to many, uh, you know, domains in science, including the neural genetics, neural imaging, statistical matters, and the genetics of neurodevelopment. So for today, I’m very excited to have him talking about a PRS, a new PRS method that is able to jointly model GWAS summary statistics of multiple ancestries to improve the prediction accuracy. So without further ado, uh, Tian, let’s get started.\nTian Ge:\nUm, thanks for the very generous introduction and for having me today. So, I’m going to talk about our recent work that extends our base in Polygenic Prediction framework, which is known as PRSCS to PRS-CSx, which can now integrate GWAS summary stats from multiple populations in order to improve cross-population polygenic prediction.\nUm, so to start, I’d just like to briefly recap the idea of polygenic prediction and then talk about the motivation and intuition behind the PRS-CSx work, which might be useful to see how PRS-CSx works.\nAs many of you already know, um, polygenic prediction summarizes the effects of genome-wide genetic markers to measure the genetic liability to a complex trait or disorder. So a conventional method to compute polygenic risk score is pruning and thresholding, which is also known as Clumping and Thresholding. So basically, to apply this method, usually, we set up a p-value threshold or screen a range of p-value thresholds. And for each of these p-value thresholds, we only consider SNPs reaching the significance level. We then perform a procedure called LD clumping, which basically retains the most significant SNP in each genomic region and discards all the SNPs that are in LD with the lead SNP. So, for example, in this figure, we will just keep the top SNP in purple and then remove all the SNPs that are in red, orange, green, light blue because they are in LD with the lead SNP. Um, and then finally, we sum up the genotypes of the remaining SNPs, weighted by the effect size estimates from external GWAS.\nThis pruning and thresholding method is very widely used because it’s conceptually intuitive and also computationally very efficient. But it has several limitations. So, for example, it relies on marginal GWAS association statistics, and we know that most associations in European regions may not be causal. So a lot of times, we might be using sub-optimal tagging SNPs to build PRS. And because of the LD clumping procedure we use, the method also ignores many of the secondary and tertiary signals in each genomic region. And finally, when we sum up the SNPs, we use the effect size estimates directly from external GWAS without any adjustment or shrinkage. Large effect size estimates in GWAS may suffer from winner’s curse, and most non-causal variants will have noisy non-zero effect size estimates. So by including these SNPs and using these GWAS effect size estimates directly without any adjustment, we are adding a lot of noise to our PRS. And all these limitations limit the predictive performance of PRS.\nSo, to address these limitations and improve the conventional pruning thresholding method, a more principled framework is to calculate the polygenic risk score by jointly modeling the genetic markers across the genome without any arbitrary pruning and thresholding. And to do this, we are basically fitting this linear regression problem where the phenotype vector is regressed onto this genotype matrix X, and beta here is a vector of SNP effect size, and epsilon is a vector that captures non-genetic effects. So, if we can jointly fit this regression model and get the effect size estimates, which is denoted as beta hat here, we can take the estimate to the target dataset where the genotypes are and compute the polygenic score.\nSo the methodological challenge here is that in genomic prediction, we often have many more SNPs than the number of samples we have. So this is an ultra-high-dimensional regression problem, and we need to regularize the effect size estimates to avoid overfitting. Um, and so we know that this pruning and thresholding method can actually be considered as a specific way of regularizing and shrinking the SNP effect size because it, in essence, shrinks the effect size of discarded SNPs to zero and performs no shrinkage on effect size estimates of selected SNPs. But we have discussed that this shrinkage scheme may be arbitrary and sub-optimal. So there are many more principled statistical methods that can be employed for shrinkage. For example, the frequentist approach is to fit a regularized regression using methods like lasso or Ridge regression or elastic net, which often encourages sparse effect size estimates and penalizes large effects. So, if you have heard of this measure called lassosum, that’s one of the polygenic prediction methods that applies lasso to build PRS.\nIn the past few years, we also see many Bayesian polygenic prediction methods that have been developed, and the Bayesian approach to tackle this high-dimensional regression problem is to assign a prior on SNP effect sizes to impose shrinkage. So, all the models basically fit the same regression, and the difference is what prior distribution to use. The question here is: how do we design a prior, or which prior is optimal, for this type of genomic prediction problem?\nThe most widely used prior is what we call the infinitesimal normal model, which is also known as the Bayesian ridge regression. So, with the effect size of each SNP follows a normal distribution.  This model is very widely used in many classical statistical genetics methods, including like GCTA and LD Score Regression. So, all these methods assume this underlying infinitesimal normal genetic architecture, and one major advantage of this prior, and also that’s why this prior is so popular, is that it’s mathematically tractable, and there’s a closed form expression for the posterior.\nHere, Lambda is a penalty parameter or shrinkage parameter, which depends on these two variances. One is the per-SNP variance of the SNP effects, and the other one is the residual variance. So, you can see that if the noise, the residual variance, is large relative to the genetic signal, then we impose a strong shrinkage on the effect size, and beta is shrunk towards zero. So, in the extreme case, if you have no genetic signal, then the beta will be shrunk to zero. On the other hand, if the genetic signal is relatively large to the noise, then the estimate will be closer to the least square estimator, and with this penalty from the matrix is always invertible, so this is a well-defined estimator.\nWe also noticed that this is a multivariate estimate of SNP effect sizes, and X transpose times X here is proportional to the LD Matrix, so it’s easy to incorporate LD information in this estimator, and in practice, you can always divide the genome into independent LD blocks, and then within each block, you can do this joint estimate of SNP effects.\nSo, with this being said, there are also limitations of this prior. As you can see here, the shrinkage parameter is a constant, meaning that under this infinitesimal normal prior, all the SNPs are treated equal, and they are shrunk towards zero at the same constant rate. So, this is sub-optimal because ideally, we want to impose various strong shrinkage on small and noisy non-causal signals, but at the same time, we don’t want to over-shrink large and real signals. So, what we really want is the shrinkage scheme that is adaptive to different SNPs and different GWAS signals, but this cannot be achieved by this infinitesimal normal prior because the penalty parameter here is a constant, which is not SNP-specific.\nSo, an alternative way to look at this problem is to take a look at the shape of the prior distribution, which is normal. This non-adaptive nature of the prior is equivalent to say that for the normal distribution, when used as a prior, there isn’t enough mass around zero to impose strong enough shrinkage on noisy signals. And because of this, the normal distribution has exponentially decayed tails. So, these tails are too thin, meaning that a priori, we believe there’s very low probability of large effect sizes. So, we don’t have a prior that can accommodate those large effect sizes, which often leads to overshrink of real signals. So, that’s why the Bayesian ridge regression or this infinitesimal normal prior is not very adaptive to different genetic architectures and usually only works well for highly polygenic traits.\nSo there are many works, um, trying to design a more flexible prior so that the polygenic model is more adaptive to varying genetic architectures. And one idea is that, in contrast to using a single normal distribution as the prior, we can use a mixture of two or more distributions. So, for example, one pioneering approach in this field, LDpred, uses this Spike and slab prior, which assumes that a fraction of the SNPs are null, so they have no effect on the phenotype, while the rest of the SNPs are causal SNPs, and their effects follow a normal distribution.\nUm, if we take a look at a density prior, this prior has two components. So there’s a spike component, or which is a very narrow distribution centered at zero, which is often used to model small signals, and there’s a slab component, which is much wider and can be used to model large signals. And then, by varying this proportion of the chordal variance, which is coded in pi here, this model can cover a wide range of genetic architectures.\nUm, so although this prior is much more flexible than the infinitesimal normal prior, it also has two limitations. So number one, this is a discrete mixture of two components, so we call this type of prior discrete mixture prior. So in posterior inference, you can see that each SNP can either belong to this null component or this normal component, normal component. So you can imagine that if there are a million Snips, then we have a discrete model space of 2 to the power of a million possibilities, which is, you know, almost unlikely to fully explore. And number two, so unlike the infinitesimal normal prior, which has a closed form multivariate update of the SNP effects, the spike and slab prior does not allow for a multivariate effect estimate. So in posterior inference, one has to update effect size SNP by SNP, which makes it very difficult to incorporate LD information in this model estimation procedure.\nUm, so there are many other Bayesian polygenic prediction methods that have been developed and use different priors, but the majority of them are discrete mixture priors. So, for example, you can parameterize um, just two normal mixtures differently using an additive version or multiplicative version. So you can also do a null component plus a t distribution, which gives you a heavier tail to model larger signals. So S-Bayes in R, which is another method that receives a lot of attention recently, uses a mixture of four normals, and each of these normals captures the effect size distribution on a different scale, which makes the model even more flexible. And then finally, there are non-parametric models where the effect size distribution is assumed to be a mixture of an infinite number of normals, and in posterior inference, the data will determine the optimal number of mixtures. Um, so these are different variations of these discrete mixture normals, and they are all discrete mixtures of two or more distributions, so they largely share the same advantages and limitations of LDpred.\nSo just to quickly summarize, so we have discussed that the infinitesimal normal prior is computationally efficient and allows for multivariate modeling of LD dependence, but it’s not robust to varying genetic architectures. While discrete mixture priors, on the other hand, can create much more flexible models for the genetic architecture, they are computationally challenging, and it’s often difficult to incorporate LD information. So our motivation was to design a prior that can combine the advantages of these two types of priors.\nSo in our PRSCS work, we introduced this conceptually different class of priors, which is called continuous shrinkage priors. In contrast to the horizontal discrete mixture of normals, we use the hierarchical scale mixture of normals. Here, Phi is a global shrinkage parameter, which is similar to the penalty parameter in Ridge regression, and it is shared across all the SNPs, and models the overall sparseness of the genetic architecture. Different from the infinitesimal normal prior, we added this local shrinkage parameter. Here, J is the index of SNPs, so this local shrinkage parameter is SNP-specific and can adapt to different genetic signals.\nYou can see that if we integrate out these hyperparameters, the density function of this prior is continuous, which can also be seen in this density plot on the right. The dashed black line is the normal prior for reference, and the red and blue lines are the two components of the spike and slab prior, while the yellow line is the continuous shrinkage prior. So you can see that unlike the two-component spike and slab prior, the prior we used is one continuous density function, but it can approximate the shape produced by this discrete mixture prior.  And compared to the normal distribution, you can see we put substantial mass near zero, which can impose strong shrinkage on small, uninformative signals, and at the same time, this distribution has heavy polynomial tails, which can retain large and real signals. So the continuous shrinkage prior is almost as flexible as the discrete mixture prior, but because of its continuous nature, it also shares some advantages of infinitesimal normal prior, that is, it allows for the multivariate modeling of LD patterns, and it’s also computationally efficient.\nThese are the motivation and some intuitions behind PRS-CS. I’m not going to talk about other features of the method, but the software is available on GitHub, which you can download and test. So we have released both pre-computed 1000 Genomes and UK Biobank reference panels for major populations, which hopefully has made the application easier. In the initial application of the PRS-CS method, we applied it to some existing GWAS of six common complex diseases and six quantitative traits, and then predicted to the Mass General Brigham Biobank. You can see that PRS-CS substantially improved the prediction over the conventional pruning and thresholding method, and often outperformed LDpred.\nAnother application that might be relevant to this group is the polygenic prediction of schizophrenia. In this study led by Amanda, we aggregated the schizophrenia cases and controls across four healthcare systems, Geisinger, Mount Sinai, Mass General Brigham, and Vanderbilt, as part of the psychMerge Consortium. You can see this polygenic risk score calculated by PRS-CS correlates with the case prevalence and schizophrenia diagnosis, and can be used to identify diseases that are genetically correlated with schizophrenia using a PheWASdesign. So that’s the a review of the ideas behind PRS-CS and some of its applications. But one limitation of PRS-CS is that it was designed and tested in homogeneous populations. Now it is well-recognized that cross-population predictive performance of polygenic risk scores decreases dramatically, especially when the target sample is genetically distant from the training example, due to the predominant European samples in the current GWAS studies.\nSo there are many factors that can limit the transferability of PRS learned from European GWAS. So, for example, there may be population-specific variants or variation in the SNP effect size estimates across populations. The allele frequencies and LD patterns are different across populations, um, and also the differences in the phenotyping or environmental factors can all affect the prediction accuracy. So in the past few years, there have been many efforts to expand the scale of non-European GWAS and to diversify the samples in genomic research in general. Although, the sample size of most non-European GWAS remains considerably smaller than European studies, so they cannot, right now, they cannot be used to fully characterize the genetic architecture in non-European populations and dissect relative contributions of these factors to PRS predictive performance. But one question we can ask is, can we leverage these existing non-European GWAS to improve trans-ethnic prediction of the PRS, even if they are smaller than European GWAS?\nSo we have been working on this method called PRS-CSx, which is a very simple extension of the PRSCS framework. So, here, to model existing non-European GWAS, we assume that we have data from K different populations, and then we still use this continuous shrinkage prior to model SNP effects, but this time this prior is shared across populations. So you can see that these shrinkage parameters do not depend on K, which is the index of the population, so they are shared across populations. And to use this coupled prior, we have implicitly made the assumption that causal variants are largely shared across populations. So we think this is a reasonable assumption given that many recent studies have estimated the trans-ethnic genetic correlations for many complex traits and diseases to be moderate to high. And with this coupled prior, we can borrow information across summary statistics and increase the accuracy of effects estimation, particularly for non-European populations whose GWAS size are relatively small. And the other advantage of this coupled prior is that we can leverage the LD diversity across populations to better localize the GWAS signal. So this is very similar to the idea of trans-ethnic fine mapping. So although we are not doing any form of fine mapping analysis here, we’re actually using this idea,\nSo although we use this shared prior across populations, the effect sizes for SNPs are still allowed to vary across populations, and so we believe this gives the model more flexibility. So we don’t constrain effect size to be the same across populations, and we also allow for population-specific variants, meaning that if a SNP is available in one population but absent in other populations due to, for example, the low frequency in other populations, we still include the SNP in the model, although in this case, there’s no effects to couple, we still include the SNPs in the modeling. So finally, PRCSX incorporates many features from PRSCS, so it allows for this multivariate modeling of LD patterns using population-specific reference panels, and also computational efficiency.\nSo in practice, PRCSX takes the GWAS summary stats and the ancestry-matched LD reference panels from multiple populations. It then joins and models all these data, fits the model, and then outputs one set of the SNP posterior effect sizes for each discovery population. And these SNP effect size estimates can then be taken to a validation dataset and calculate one PRS for each population. And we then learn an optimal linear combination of these PRS in the validation dataset and evaluate the predictive performance of the final PRS in an independent testing dataset.\nSo, as a comparison, and also in the results I’m going to show, so we examined two alternative methods that can combine GWAS summary stats from multiple populations. One method, which we call PT-meta here, performs a fixed-effect meta-analysis of the GWAS and then applies the pruning and thresholding method to the meta-GWAS. And since the LD pattern is mixed after this meta-analysis, so it has different LD reference panels in this case, and then select the one with the best predictive performance to evaluate in the testing dataset. The other method, which we call PT-multi, this method was developed by Alkes Price’s group a few years back. So they applied pruning and thresholding separately to each GWAS summary statistic, and then the resulting PRS are linearly combined in the validation dataset and then taken to the final PRS for evaluation.\nOkay, um, so here are some results. So we selected um 17 quantitative traits that are shared between the UK Biobank and Biobank Japan. In this analysis, UK Biobank GWAS sample size is typically three to six times larger than the BBJ (Biobank Japan) sample size. We then train different PRS using BBJ GWAS only. These are the PRS measures that applied to the Biobank Japan GWAS only. And then these are the PRS that were trained on UK Biobank data only. The last three methods are those PRS that combine the GWAS summaries from the UK Biobank and Biobank Japan. And then we train these different PRS and predict into different populations in the UK Biobank that are independent of the UK Biobank training GWAS.\nSo, you can see, here in the first panel, when the target sample is the UK Biobank European population, you can see that PRS trained with the ancestry-matched UK Biobank GWAS performs better than PRS trained with the BBJ GWAS, which is expected. In this case, combining the UK Biobank and BBJ GWAS doesn’t help too much. You can see it is a very small, probably around five percent, improvement in prediction accuracy when we combine UK Biobank and Biobank Japan GWAS. That’s likely because the UK Biobank GWAS was already quite powerful, so adding a smaller East Asian GWAS doesn’t help too much in the prediction of the European samples. But when we predict into the UK Biobank East Asian samples, you can see PRCSx can increase the prediction accuracy. Here, the bar shows the median variance explained that was increased by about 25 percent when comparing PRCSx with these PRS trained on the European GWAS. And then if you compare with this ancestry-matched PRS trained in the Biobank Japan GWAS, the improvement was even larger, it’s around 80 percent. These results show that we can leverage this large-scale European GWAS to improve the prediction in non-European populations.\n[[about 30 minutes in]]\nWhen we predict into the UK Biobank African samples, the target population didn’t match any of the discovery samples, the Biobank Japan sample, or the UK Biobank sample. Both the European and East Asian samples are genetically distant from the African samples. So, in this case, the improvement in predictions was again limited, and predictions in the African population remain quite low relative to the predictions in European and East Asian populations.\nSo, we asked whether we can add some African samples to the discovery dataset to improve the prediction in the African population. Among the 17 traits we examined here, seven were also available in the PAGE study, which largely comprised African-American and Hispanic Latino samples. But you can see the sample size of the PAGE study was much smaller than the UK Biobank and Biobank Japan. So, the question here is whether adding a small African GWAS to the discovery dataset can improve projection. You can see in the right panel of this figure that when integrating this UK Biobank, Biobank Japan, and PAGE summary stats using PRN CSX, the prediction in the African sample was quite dramatically improved, and the improvement in median variance explained was about 70 percent when comparing with the PRS-CSX applied to UK Biobank and Biobank Japan GWAS only, and the prediction was also much better than the PRS trained on ancestry-matched PAGE study. So, these results suggest that we can leverage samples that have matched ancestry with the target population to improve prediction, even if the non-European training GWAS are considerably smaller than European studies. Adding the PAGE study to the discovery dataset also improved the prediction in other target populations, although the improvement was to a much lesser extent.\nIn the last example, we evaluated different PRS methods in the prediction of schizophrenia risk. In this analysis, we used the GWAS summary statistics derived from the PGC2 schizophrenia GWAS in the European samples and also the recent schizophrenia GWAS in the East Asians, led by Max Lam, Cheyenne, Hailiang, and colleagues. We have access to the individual level data of sixth East Asian cohorts, and we left out one cohort as the validation dataset. This is the dataset we use to learn hyperparameters or linear combinations of PRS. For the remaining six cohorts, we apply the leave-one-out approach, meaning that we, in turn, use one of the six cohorts as the testing dataset and then meta-analyze the remaining five cohorts with the other cohorts to generate the discovery GWAS for the East Asian population. We then again build different PRS using the East Asian GWAS only, or using the European GWAS only, or using methods that can combine these two GWAS. You can see that PRS-CSx can increase the prediction accuracy relative to methods trained on a single GWAS. The median variance explained here had approximately a 50% increase relative to GWAS using ancestry-matched East Asian GWAS and almost double the prediction accuracy when the PRS was trained using European GWAS.\nOn the right panel, you can see that at the tail of the PRS distribution, PRS-CSx can also better stratify patients at top and bottom PRS percentiles relative to other methods. Okay, so I think I’ll stop here and thank all my collaborators. In particular, Yunfeng has led many of the real data analysis in this project, and Hailiang has provided critical inputs in every aspect of the project. He also led the Stanley Center East Asia Initiative, which made the schizophrenia analysis in the East Asian cohorts possible. Our preprint is on medRxiv, and we have also released the software on GitHub. So any feedback or comments will be much appreciated. I will stop here and I’m happy to take any questions.\nHost: Great, thanks a lot, here, and that’s a great talk. So, um, we have 20 minutes for questions.\nParticipant: Terrific talk, Tian. Really appreciate it. So, um, I’ll just start things off. I’m sure there are other questions, um, but I guess one question I have is, it seems like with these methods, beyond pruning and thresholding, one of the big barriers is that some of them are harder to implement than others. Pruning and thresholding is so easy, um, and so I was wondering if you could just say a little bit about how difficult it is to implement this approach and what information people need to be able to use PRS-CSx.\nTian: Um, yeah, so, um, I guess in many of the analyses, we still use pruning and thresholding as a baseline because it’s computationally faster and also a robust approach. So you can use that as a comparison. So in terms of those Bayesian methods, um, depending on the different implementations and different methods, will require different computational costs and usually takes longer than pruning and thresholding. But, um, we have tried to, you know, hopefully make this software easier to use. So we have released these reference panels so users usually don’t need to calculate their own reference panels. And then we can parallel the computation of chromosomes. And usually, for the longest chromosome, it takes around an hour or one and a half hours to finish. So I think it hopefully doesn’t add too much computational burden on end user.\nParticipant: That’s really helpful, that’s really useful, right? And so, people just summarize statistics from multiple populations. Of course, the other things are provided with your software.\nTian: Yeah.\nParticipant: great!\nHost: So, on that topic, Tian, could you perhaps discuss a little bit about the mixed population? You know, I think not every population has a released reference panel. So, what are the considerations here and what are things you know these people can do if they want their analysis being done? You’ll see this matter.\nTian: Right, that’s a great question. There are a lot of challenges in terms of how to handle admixed populations because, um, you know, the LD patterns might depend on specific studies and the proportion of the samples in each study. So, I don’t think there’s a universal reference panel that can be used for all admixed populations. Um, so right now, for example, in this study when we try to model the PAGE GWAS summaries, it’s sort of a mixture of African-American samples and also Latino-Hispanic samples, so it’s kind of an admixture. So, we try to use an African reference panel to approximate in this situation and it turned out to work okay. But clearly, there is still a lot of work to do and think about how to better model the admixed populations and how to build reference panels in this case.\nParticipant: All right, maybe I can ask a question. So, solely dealing with summary statistics, it’s always simpler but at the same time more difficult, right, because you lose a lot of detailed information. But I’m curious, if you have individual-level data, will you be able to do that better for the admixed population because you should be able to have much higher individual-level resolution, incredible population local structure, right?\nTian: Um, right. So, I think there are two aspects of this question. Number one is, you know, do we lose any information when we use summaries that’s relative to individual data, whether it’s a homogeneous population or admixed population, right? Um, so the question, the answer to that question is, if you only look at, um, only use the second order information, it’s basically the LD information, and then you assume you have a LD reference panel that can accurately approximate the true LD patterns in your GWAS sample, and then there’s actually no information lost when we use the reference we use the summary stats data relative to the individual-level data, um. So, the question here is, can we find a GWAS ref, can we find a reference panel that can closely approximate the LD patterns in your GWAS sample? Um, and so a lot of times, you know, when the GWAS sample was conducted in a homogeneous population, we think, um, you know, the reference panel was accurate enough, but that also warrants, you know, if in the future it’s possible to release in-sample LD information with the GWAS summaries, then we can, of course, do better and get more accurate LD patterns, so we’ll have less information loss or less reference sample mismatch in this case.\nAnd then the second part of the question was, you know, if we have individual-level data, can we do better to handle admixed populations? Um, and I think, sure, because with individual-level data, you can go beyond LD, you can look at local ancestry and do those decompositions and build PRS using those local ancestry information, which can of course be much more accurate than treating the whole genome in a homogeneous way.  So, I think going forward, releasing LD information and local genetic information with the GWAS summaries, that might be the key to further improve the polygenic predictions in admixed populations.\nParticipant: Just to follow that, um, so do you have an implementation to deal with individual-level data?\nTian: We don’t actually, but if you have individual-level data, you can just compute the in-sample LD and then do a GWAS, so then I’ve tried a method to the GWAS summary stats, that should give you, um, you know, highly similar results to a method using individual-level information.\nParticipant: Thank you.\nParticipant: Did we have a question from Laura Slootman?\nParticipant - Laura: We did, but it was answered because I didn’t, uh, I thought the answer was uh, truncated before the individual-level conversation.  I was going to ask specifically about what you just clarified.\nParticipant: Could I ask a practical question? Um, we usually transform odds ratio to log odds ratio before calculating PRS based on LD pruning and p-value approach. So, my question is, do we need to also do the same process in PRS-CSX because we know that the posterior effect size in this approach is relatively small? If we don’t, um, process the ratio to log odds ratio before calculating posterior effect size\nTian: Ah, right. So, PRSCSx can take odds ratio estimates, but it basically just takes the odds ratio and takes the log and converts it to standardized beta. So, if your, you know, GWAS summaries stats is odds ratio, then it’s fine. PRSCSx can take that.\nParticipant: I think it was really encouraging to see how much better the prediction was with the page samples for the African ancestry individuals. Um, do you have any ideas about why that worked as well as it did?\nTian: Yeah, that’s a good question. So, I think, number one, so if we have population-specific variants and information contained in the PAGE study GWAS summary stats that’s not available in the GWAS of other populations. The other possibility might be, you know, I integrate these GWAS, so because the LD pattern and the LD block is smaller in African samples, so we have a better localization of the GWAS signal which also improved the production accuracy. But I think there’s much work to do to dissect these contributions and see, you know, whether we can improve on that.\nParticipant: I think it’s just, it’s very encouraging because I think sometimes when you see samples for European ancestries that are over an order of magnitude or more larger than the other ancestries, you think, well, maybe it’s not worth including these other ancestries, but it sounds like these data are suggesting that definitely you should.\nParticipant: I just have a question. I think someone said about the phenotype. I just wonder if you think that, you know, for the Latin population, it’s because, you know, I think the sample size is small, if you have a better phenotype, something like that, should you have more chance to, you know, to, I don’t know, but to predict? I think this meta-analysis or phenotype is something, it’s a variable that matters here or not, you know, it’s just I don’t know if you start a project in Latin Americans that is totally mixed and you don’t know and you have a sample, there’s more sample size, do you think it’s good to spend, I don’t know, money or time having a deep phenotype or this doesn’t matter?\nTian: So the phenotype typing definitely influences the prediction accuracy of PRS, and then if you have very different phenotyping in say, your training and target populations, that might reduce the production accuracy. And in many of our work, so when we try different phenotyping methods, so for example, try to predict depression, and then there are different ways to, you know, type, like using ICD codes. Using any rule-based or algorithm-based definition of depression cases and controls, and they do give us meaningfully different prediction accuracy. And a lot of times, um, you know, there’s a balance here because if you use the simpler ICD-based method, you get, you know, more cases and controls. But if you use the more stringent definition, you get higher specificity but sometimes lower power because the case number is reduced. So, I think again, there’s many factors that contribute here, the sample size, and how the phenotyping matches between the discovery and target dataset and how specifically the phenotyping algorithm is. Um, a lot of times, you know, when you conduct the PRS analysis, these phenotyping issues are beyond our control because we only use GWAS summary stats, or test the PRS in the existing cohorts. But if you have control over these genotyping algorithms, um, I think some phenotyping can sometimes boost the prediction of PRS.\nParticipant: Okay, so another question is just today I saw, you know, a talk in a conference that they use a family polygenic trios, or families, to predict this polygenic risk score. And then, you know, they have this analysis using trios. What do you think about that? Is it a good strategy?\nTian: I think Trio or family studies provide additional opportunities that can’t be done by PRS of unrelated individuals. For example, you can better control environmental factors and sometimes you can decompose and dissect the transmitted and non-transmitted of use. Interesting questions that you can only do in family or Trios. So I think both methods are so, so a lot of work we do is to do this PRS analysis in population-based cohort and trying to stratify patients, for example, but in terms of memory study, they also give a different or unique aspect where you can look at the relative contribution of genetics environment. So I think both study designs are useful. and can be used to answer different questions.\nParticipant: Should your method/ can it be used in this approach?\nTian: That’s a good question. So right now, probably not because when we build the model, we assume the GWAS summaries are calculated on a large unrelated GWAS sample. So if we want to conduct any PRS analysis that is specific to your family or Trio design, probably need to look into, you know, more specific methods that can better address the questions there.\nParticipant: I have a question about how do you handle the LD matrices and population predictions. … [unintelligible] .. I remember there was a paper about estimating the cross-population prediction accuracy and um, he said the most tricky part of the smart population predictions is the different LD matrices and those variances. So in your method, how are these two parts handled?\nTian: So I actually have some trouble here in this first part of the question.\nParticipant: Well, maybe I can speak a louder. Is it a little bit better now. I would say, can you hear me? The voice quality is not great. Speaking up, it seems like helps a little, but it’s still a bit difficult to hear.\nParticipant: Sorry, I think it’s better sending the question in the chat if that’s okay.\nParticipant: Then I will type the question. Sorry, I don’t know if you have time.\nTian: but yeah, no problem. If you have any questions, you can also, you know, email me afterward.\nParticipant: Yeah, the question was, how are the two matrices handled in your method? Two different LD matrices?\nTian: Um, right. So we use, so there are, you know, if you have GWAS summary stats from different populations, you’ll have one with the, some population specifically matching that ancestry of the GWAS. And then when we do this effect size estimates, they are actually taken care of.  And then, so different effect sizes in different populations were modeled by the matching LD reference panels.\nHost: And there’s a follow-up question that, how are the variants with different minor allele frequency being handled into different matrices?\nTian: Um, how minor allele frequencies are handled. So we don’t... well, so I’m not sure how to answer this question. So how minor allele frequencies are handled.  So when you come to the LD Matrix, they’re just using population-specific reference panel to compute that LD matrix, and that gives you the matrix for each population. And then when we model the effect size, and those effect size are, and the relationship between SNPs in each population, was mapped to the population-specific LD reference panel. Um, I’m not sure if that answers the question.\nHost: Alright, so I think we’re at the hour. Thanks, uh, Tian for giving this great talk, and thanks everyone for joining us for this, uh, for this meeting. And we look forward to seeing each other again in a month!\nParticipant: I think the next meeting is May 5th. And I think we’ll be back at the, uh, 1:05 PM Eastern.\nParticipant: Great, thanks so much, Tian, and great to see everyone. Bye, folks. Bye."
  },
  {
    "objectID": "chapter6.1_transcript.html#sec-video1",
    "href": "chapter6.1_transcript.html#sec-video1",
    "title": "Chapter 6.1: Polygenic Risk Scores (Video Transcript)",
    "section": "Polygenic Risk Scores",
    "text": "Polygenic Risk Scores\nTitle: Polygenic Risk Scores\nPresenter(s): CPM Oxford\nOur genes provide valuable insight into our family history, our ancestry, and also our health. As we learn more about our DNA, we identify new opportunities for healthcare. One such opportunity comes from using polygenic risk scores. In this video, you will learn what a polygenic risk score is, see how they can be used in healthcare, and find out how you can participate in research studies that use them.\nPolygenic risk scores: Why might you care about whatever they are? Knowing our risk of developing particular diseases can be a really powerful way to help us live healthier lives. Imagine you knew that you were at a high risk of having a heart attack. That’s pretty clear motivation to do something about it, such as changing your diet or taking risk-lowering medication. But how could you know that you’re at a high risk?\nA major risk factor for common diseases such as heart disease, cancer, and diabetes is our own genetic makeup. New studies show that we can now analyze an individual’s genes and actually measure that risk using something called polygenic risk scores.\nOur genes vary from person to person; it’s why we’re not all the same. But some of these genetic differences can contribute to our risk of complex diseases. There are a few rare diseases that are caused by changes in a single gene, but we now know that for the most common diseases such as heart disease and diabetes, it’s often not just one or two of these genetic changes that are important; it’s many of them, each having a small effect on the polygenic risk (hence poly (“many”) genic (“to do with genes”) risk scores - scoring a risk.\nScientists have compared DNA among gigantic groups of people with and without disease and have identified thousands of genetic variations that influence risk for hundreds of diseases. This important reference DNA from the studies can then be compared with individual patients’ DNA to calculate their risk.\nSo, how could polygenic risk scores actually help you? A doctor’s usual assessment of a patient might indicate one course of action, but when armed with a polygenic risk score, a different approach may become the better way to go. These scores could help doctors better identify specific medicines or therapies a patient is likely to respond well to, or whether they’re likely to benefit from earlier screening for specific cancers. And because our genes don’t change, as more of these variations are identified, your polygenic risk scores can be updated without having any more tests. Also, remember this same genetic information can be used to give you a polygenic risk score for lots of different diseases.\nSo, we’re stuck with the genes we have all throughout our lives. But even when they do mean our polygenic risk score of a particular disease is high, that doesn’t mean our fate is sealed. There’s lots we all can and should do to reduce our overall risk of disease, particularly if our polygenic risk is high. For example, patients with a high polygenic risk of type 2 diabetes can significantly reduce their overall risk through exercise and eating a healthy diet.\nSo, what’s next? Well, you can see how much promise polygenic risk scores have, but there’s still work to be done. Further studies are needed to assess the clinical impact of PRS in improving the diagnosis, treatment, or prevention of specific diseases. Also, studies must be extended to cover a wider global population. But already, polygenic risk scores are a promising new tool. By taking them into consideration along with other risk factors, they have the potential to help us live longer, healthier, and happier lives.\nTo learn more about polygenic risk scores and how you can take part in research studies involving PRS, please visit us at cpm.well.ox.ac.uk/prs."
  },
  {
    "objectID": "chapter6.1_transcript.html#sec-video2",
    "href": "chapter6.1_transcript.html#sec-video2",
    "title": "Chapter 6.1: Polygenic Risk Scores (Video Transcript)",
    "section": "What is a Polygenic Risk Score?",
    "text": "What is a Polygenic Risk Score?\nTitle: What is a polygenic risk score?\nPresenter(s): Till Andlauer\nTill Andlauer:\nIn this video, I’m going to introduce you to polygenic risk scores (PRS), also often now called only polygenic scores (PGS) because you can also calculate them on quantitative traits like, for example, brain volumes.\nPolygenic disorders\nAll complex common disorders are polygenic. If you want to quantify genetic risk for a complex disorder, you thus have to assess the effects of many genetic variants at the same time. The basis for a PRS is a GWAS, and there’s a separate video explaining what that is.\nCalculating PRS\nFor the calculation of PRS, this GWAS is considered the discovery or training dataset. The GWAS effect sizes are used as the weights when calculating a PRS. To get stable effect size estimates, you need GWAS generated on large samples, as, for example, conducted by the PGC. But there’s one problem: neighboring variants are correlated, because they get inherited together and thus show similar associations. The SNP density could thus bias the PRS. At any given locus, classical PRS thus only use the variant with the lowest p-value. Correlated SNPs are removed using a method called LD clumping.\nLD clumping\nHow many SNPs to use? That’s a difficult question. You might only want to use the SNPs that show, genome-wide significance - 44 in this study on depression. But what about these ones here? Are they not relevant? Likely, they would get significant if a larger sample size was used for the GWAS. Therefore, typically p-values of 0.05 or 0.01 are used as thresholds for the calculation of classical PRS.\nOther methods\nIn recent years, many other methods have been published that use Bayesian regression frameworks to model the linkage disequilibrium and thereby calculate LD-corrected weights. These methods, like LDpred, PRS-CS, and SBayesR, they have been benchmarked in a recent manuscript, and they perform much better than classical PRS.\nNo matter how you choose your weights, the next step is always the same. For each SNP, you multiply the weight by the number of effect alleles. If “A” is the effect allele, then the multiplication factor would be “0” in this case, here “1”, and here “2”. You do this multiplication for each SNP, and you sum over all variants to get a single score. A PRS by itself, for a single person, is just an arbitrary number that cannot be interpreted well.\nIn order to be able to interpret the PRS, you need a large group of people that share the same ancestry and possibly even the same socio-demographic background among each other and with the people used for the training GWAS. Only with this group, you’re able to interpret the PRS of an individual relative to that group. And if you calculate PRS for many people, you will see that the scores follow a normal distribution. The PRS of a single individual may then map to a lower, an average, or a higher percentile within that distribution.\nPatients suffering from a disorder will, on average, have higher PRS for that disorder than healthy controls, but only on average. Individual-level risk predictions from PRS thus have a very poor sensitivity and specificity. Basically, you can only say anything about the people mapping to the lowest or highest percentiles; they have significantly lower or higher risk for the disorder. And for everyone in between, you can’t say much.\nYou can, however, get fairly good predictions if you contrast the extreme ends of the distribution, but the odds ratios achieved are, of course, still much lower than for monogenic disorders. Nevertheless, PRS can be used for risk stratification to identify people at high risk that might need more medical attention.\nApplications\nIn research, PRS have many highly useful applications. From the assessment of the polygenic risk load for common variants in families, especially affected by psychiatric disorders, to the genetic characterization of bipolar disorder subtypes, and of course, many, many more. Also, many reviews and methods articles on PRS have been published recently, and you will easily find a lot of material to keep on reading."
  },
  {
    "objectID": "chapter6.1_transcript.html#sec-video4",
    "href": "chapter6.1_transcript.html#sec-video4",
    "title": "Chapter 6.1: Polygenic Risk Scores (Video Transcript)",
    "section": "Polygenic Scoring Methods: Comparison and Implementation",
    "text": "Polygenic Scoring Methods: Comparison and Implementation\nTitle: Polygenic Scoring Methods: Comparison and Implementation\nPresenter(s): Oliver Pain\nOliver Pain:\nHello, my name is Oliver Pain, and I’m a postdoctoral researcher working with Professor Cathryn Lewis at King’s College London. Today, I’ll be talking about polygenic scoring methods, comparing various methods to one another, and also describing resources we’ve developed for the calculation of polygenic scores for research and clinical purposes. I have no conflicts of interest to declare. My presentation is split into three sections. First, I’ll give a brief introduction to polygenic scoring, then describe our work comparing different polygenic scoring methods, and finally introduce an easy and reliable pipeline for polygenic scoring that we’ve developed, called GenoPredPipe.\nSo first, a brief introduction to polygenic scoring. A polygenic score is a way of summarizing an individual’s genetic risk or propensity for a given outcome, typically calculated based on an individual’s genetic variation and genome-wide association studies summary statistics referred to as GWAS sumstats. Polygenic scores are a useful research tool and can also be useful for personalized medicine, as their predictive utility increases.\nFor polygenic scoring, the GWAS summary statistics are typically processed to identify variants overlapping with the target sample, account for linkage disequilibrium between variants, and adjust the GWAS effect sizes to optimize the signal-to-noise ratio in the GWAS summary statistics. Therefore, an individual’s polygenic score can vary due to the genetic variants considered and the allele frequency and linkage disequilibrium estimates used to adjust the GWAS sumstats. Often, in research, these factors vary depending on the target sample, using the intersective variance between the target sample and the GWAS and using estimates of linkage disequilibrium and allele frequency from the target sample itself. Now, this isn’t ideal, and an alternative strategy is to use a reference standardised approach, whereby a common set of variants is considered for all target samples, and the linkage and allele frequency estimates are derived using a common ancestry-matched reference sample. This reference standardised approach is advantageous when using polygenic scores in both clinical and research contexts, as it allows polygenic scores to be calculated for a single individual, and it avoids variation in an individual’s polygenic score due to target sample-specific properties, which might influence the result.\nNow, I’ll be presenting our work comparing polygenic scoring methods. This table shows the leading polygenic scoring methods as reported in the literature.\nThe pT+clump approach is the traditional approach, whereby LD-based clumping is used to remove the linkage disequilibrium between lead variants and GWAS, and a range of p-value thresholds are used to select the variants considered. The others are more recently developed methods that use LD estimates to jointly model the effect of genetic variants and typically perform shrinkage to account for the different genetic architectures.\nLike the p-value thresholding approach, several of the newer methods apply multiple shrinkage parameters to optimise the polygenic score. When multiple parameters are used, a validation procedure such as 10-fold cross-validation is required to avoid overfitting, whereby the variance explained estimate is artificially high due to trying many different p-value thresholds or shrinkage parameters.\nIn contrast, some methods provide a pseudo-validation approach, whereby the optimal parameter is estimated based on the GWAS summary statistics alone, not requiring a validation sample. Another approach that doesn’t require a validation sample is to assume an infinitesimal genetic architecture. Though, this approach works best for highly polygenic phenotypes.\nA third option is to model multiple polygenic scores based on a range of parameters, using methods such as elastic net to account for the correlation between the polygenic scores.\nWe compared methods using a range of outcomes measured in two target samples: UK Biobank and the Twins Early Development Study, referred to as TEDS. We used two samples to ensure our results are not target sample-specific, and we selected the traits on the right as they have well-powered publicly available GWAS and represent a range of genetic architectures.\nAs I described previously, we used the reference standardised approach when calculating the polygenic scores. LD and allele frequencies were estimated using two reference samples of differing sample size to evaluate the importance of reference sample size across methods. And these reference samples were the European subsets of 1000 Genomes Phase 3 and an independent random subset of 10,000 European UK Biobank participants. HapMap 3 variants were used throughout as these variants are typically well captured by genome-wide arrays, after imputation, provide good coverage of the genome, and also reduce the computation times.\nSeveral methods already provide LD matrices, including only HapMap 3 variants, for use with their software. In line with the reference standardized approach, the predictive utility of each polygenic score approach was evaluated based on the Pearson correlation between the observed outcome and predicted values. The statistical significance of differences between predicted and observed correlations was determined using the Williams test, which accounts for the correlation between predictions from each method.\nThis figure shows the average performance of each method compared to the best pT+clump polygenic score, as identified using 10-fold cross-validation. The transparent points in the figure show the results for each target phenotype separately. I’m only showing the results based on the UK Biobank target sample when using the 1000 Genomes reference as the results were highly concordant when using TEDS or the larger reference. When comparing methods that you use 10-fold cross-validation [for] to optimise parameters (shown in red), you can see that the best methods are LDpred2, lassosum, and PRScs. All outperform the pT+clump approach, providing a 16 to 18% relative improvement in prediction.\nLDpred2 showed further nominally significant improvements over lassosum and PRScs on average.\nWhen comparing methods that use a pseudo-validation approach or [an] infinitesimal model (not requiring a validation sample), highlighted in blue and purple, you can see that PRScs and DBSLMM methods perform well, providing at least a 5% relative improvement over the other pseudo-validation methods. PRScs is better than DBSLMM, providing a 4% relative improvement over DBSLMM. It’s worth mentioning that SBayesR’s performance improved when using a larger LD reference on average then doing as well as DBSLMM.\nWhen comparing the pseudo-validated PRScs performance to 10-fold cross-validation results in red, the PRScs polygenic score performs only 3% worse than the best polygenic score identified by 10-fold cross-validation for any other method, and performs better than the pT+clump approach for all phenotypes tested, highlighting the reliability of this approach.\nThe multi-PRS approach, shown in green, which uses an elastic net model to model multiple polygenic scores based on a range of p-value thresholds or shrinkage parameters, consistently outperforms the single best polygenic score selected by 10-fold cross-validation, shown in red, with the largest improvement for the pT+clump approach, of 13%.\nLastly, we tested whether fitting polygenic scores across multiple methods improved prediction, and we found it did to a small degree, though the computational burden is obviously substantial because then you have to run all of the methods. In terms of runtime, these methods vary substantially. This graph shows the number of minutes to run the method on chromosome 22 alone, without any parallel computing. You can see the PRScs and LDpred2 take a lot longer than other methods. However, since our study, LDpred2 developers have substantially improved the efficiency of their software, halving the runtime, moving that down to around here, just under 25 minutes for chromosome 22. It’s worth noting that the time taken for PRScs when using a single shrinkage parameter is one-fifth of the time shown here, meaning its pseudo-validation approach is actually reasonably quick. DBSLMM is by far the fastest of the newer methods, taking just a few minutes when run genome-wide without any parallel computing, which is impressive considering how well it performed against other methods in terms of prediction.\nSomething I wanted to highlight is that when using SBayesR, I’ve been using the robust parameterisation option available in the newest version of SBayesR. As I found SBayesR was not converging properly for some GWAS, using this robust parameterisation was [the] most reliable and did not make the SBayesR performance worse, except for depression, using the smaller 1000 Genomes European reference, you can see this here in the figure.\nThe second point is that since our published study, I’ve also compared PRScs methods using GWAS summary statistics for 17 different phenotypes derived using only the UK Biobank sample, avoiding possible quality control issues that occur from large-scale meta-analyses. The results are highly concordant with the results when using the largest publicly available meta-GWAS results, which is reassuring that our findings are reliable. However, the performance of SBayesR did improve again, highlighting that SBayesR is especially sensitive to GWAS quality than other methods. But when the quality is good, SBayesR performs very well and appears to be the best method in this analysis.\nWe were also interested to see whether one particular method did better when predicting across ancestries, as some methods might highlight causal genetic effects better than others. Using the same 17 GWAS I briefly mentioned on the previous slide within the UK Biobank sample alone, I’ve tested the performance of PRS methods across populations. So, using the European GWAS but predicting in non-European subsets of the UK Biobank. As you can see, the results are very similar in each population, with the best method identified in a European target sample also being the best method in the non-European target sample.\nSo, the advice for future research regarding polygenic score methods is: for optimal prediction, we recommend modeling multiple parameters from LDpred2, lassosum, or PRScs. But if you’re looking for a simpler option, for looking at genetic overlap perhaps, I’d recommend using the PRScs pseudo-validation method, also referred to as its “fully Bayesian” method. Alternatively, if you need to compute polygenic scores for many GWAS or have limited time or computer power, then DBSLMM is a fast and good alternative. Although SBayesR does very well when the GWAS summary statistics quality is good, its sensitivity to that quality means it doesn’t always perform well when using the largest meta-GWAS results.\nOkay, so now I’m going to move on to the last section of my talk, which describes our recently developed pipeline for polygenic scoring called GenoPredPipe. So, most of the work I’ve just presented is contained in the study shown in the top left, where we also described the reference standardised approach polygenic scoring. In the study, we provide a schematic representation of this reference standardised approach shown in the figure on the right. Whereby, target genetic data is first imputed, if it hasn’t been already; then, only HapMap 3 variants are retained, as these are typically well-imputed and provide decent coverage of the genome. And then, the sample is split into ancestral superpopulations determined by comparison to the 1000 Genomes phase three reference. And lastly, polygenic scores are then calculated based on GWAS summary statistics processed in advance using a GWAS ancestry-matched reference.\nNow, when we were carrying out this study and writing the code for the reference standardised approach, we wanted to ensure the results were fully reproducible, and we wanted to develop a resource that other researchers could use to calculate reference standardised polygenic scores. So, we used a combination of R Markdown, Git, GitHub, and GitHub Pages to create a publicly available website containing a series of readable documents describing the code and results of our various studies.\nThe website is called GenoPred as it focuses on genotype-based prediction. And there is a QR code here if you’d like to take a look. And I’ll now briefly show you the website. [https://opain.github.io/GenoPred/] So, this is the homepage of the website, which shows a list of the pages available. I’ll now click on the link describing how to prepare the reference data for the reference standardised approach. At the top, it provides some information and then lists the required software below that. And then, it goes through each step, one by one, with code chunks for users to use to reproduce results or create their own data.\nNow, whilst we think this is a great resource for others to see what we’ve done and replicate the results, you can see there are many steps and many separate code chunks to follow, making its use quite lengthy and possibly prone to user error. So, I’ve recently written all of the steps to calculate reference standardized polygenic scores into a pipeline using Snakemake, along with what is called a Conda environment, which will automatically download and install all of the required software, meaning it’ll create fully reproducible results. The pipeline just requires three input files. First, a list of GWAS summary statistics that you want to use, indicating the population in which they were derived, and optionally, information regarding the distribution of the phenotype in the general population (i.e., prevalence or mean and standard deviation). Then, you provide a list of target samples you want to compute polygenic scores for, with the pipeline currently accepting PLINK1 binaries (say, BED, BIM, FAM) or BGEN files and also the 23andMe format. Lastly, you’ll need a file called config.yaml, which describes the location of the GWAS and target list files. Then, actually running the whole pipeline just takes a single line of code. I provided some test data for new users to experiment with, which can be downloaded. And then step two, here, is where the pipeline is actually run. Just writing snakemake, then two parameters indicating the computational resources you want to use and that you want to use [the] Conda environment, and then the name of the output that you want. In this example, I’m requesting the final output, which is an R Markdown report of the ancestry identification and polygenic score results for all target samples which involves running the full pipeline.\nHere is an updated version of the schematic I showed you earlier, based on what is implemented in the GenoPred pipeline. The only difference is that at the end, now we generate a report summarizing the results of the ancestry identification and polygenic scoring. I’ll show you these reports briefly now.\nThis is the report produced to describe the results of a single individual. First, there are some descriptives about the number of SNPs [single nucleotide polymorphisms] before and after imputation and the number of reference SNPs available. Then, the report describes the results of the ancestry identification analysis, first showing the probability of being from each subpopulation in the 1000 Genomes reference. In this example, the individual has a probability of almost 100% of being from a European population. You can also see the individual’s position on the first few reference-projected principal components relative to the 1000 Genomes population I’m showing now, and the target individual in this case is that black circle.\nThen, the individual’s ancestry is broken down into more specific populations within the assigned superpopulation. In this case, showing the individual was most similar to the Great British population and individuals broadly from Northern and Western Europe. And at the bottom, it shows the individual’s polygenic scores, in this case for body mass index and cardiovascular disease, showing the results in relative terms compared to an ancestry-matched reference. And then, in absolute terms for improved interpretation, by considering the variance explained by the polygenic scores and the distribution of the phenotype in the general population.\nThen, this is an example of the report produced to summarise the results for a sample of individuals. Again, starting with some descriptives about the overlap with the reference SNPs, then showing the number of individuals assigned to each superpopulation using a hardcore threshold of 50%, with the underlying probabilities shown as a histogram below. Again, you can see the position of these individuals on projected principal components compared to the 1000 Genomes reference. And then, each individual is assigned to specific populations within each assigned superpopulation. I’ll skip past these, though, and show you the polygenic scores, which are summarised at the bottom, just simply using histograms to show the distribution of polygenic scores for each population and GWAS.\nSo, I’ll conclude with why I think people should use this pipeline. First, it performs ancestry classification, which is really important, as non-Europeans are often discarded from studies, and as sample sizes increase, usable non-European populations can be identified and analysed. Also, it’s important to consider the ancestry of an individual when calculating the polygenic score. Second, it provides reference-standardised polygenic scores, which are scaled according to an ancestry-matched reference and are independent of any target sample-specific properties, which is useful for research and clinical prediction purposes. Third, it can efficiently implement any of the top-performing polygenic scoring methods using a single line of code, saving time and reducing user error. Fourth, it’s been tried and tested. I’ve put UK Biobank through it and other samples, and compared the PRS to the observed phenotypes, assuring me that it’s working as it should. Finally, it’s well-documented online and produces fully reproducible results, both of which are important for progressing science. I’m showing the QR code again for the GenoPred website on the right. Please do check it out if you’re interested.\nLastly, I’d like to thank my amazing colleagues for their help with this work, in particular, Cathryn for her brilliant supervision throughout. Thank you for listening."
  },
  {
    "objectID": "chapter6.1_transcript.html#sec-video5",
    "href": "chapter6.1_transcript.html#sec-video5",
    "title": "Chapter 6.1: Polygenic Risk Scores (Video Transcript)",
    "section": "Polygenic risk scores: PGS comparison",
    "text": "Polygenic risk scores: PGS comparison\nTitle: Polygenic risk scores: PGS comparison\nPresenter(s): Guiyan Ni\nGuiyan Ni:\nWelcome! The topic of this video is how to run polygenic risk score comparison. Until now, we have already watched individual talks on PRScs, LDpred2, SBayesR, and other methods. I believe you all have a good understanding of polygenic scores and each of those methods.\nSo, these slides here are just to set up the scene, to make sure that we are on the same page. A polygenic risk score of an individual is a weighted sum of the counts of risk alleles. Based on the GWAS summary statistical results, the basic method for polygenic risk score is p-value clumping and thresholding. This method is simple, but it doesn’t fully model different genetic architectures. So, there are many new methods trying to model the genetic architecture for the trait of interest. For example, using different parameters in the Bayesian regression, like LDpred-infinitesimal model, LDpred2, SBayesC, PRScs, and SBayesR. And also, methods like lassosum are using the LASSO regression. MegaPRS is another method that runs on different priors. For example, if it runs on a prior using a mixture of four normal distributions, it will be the same as SBayesR, or similar. And if a SNP has a contribution to the phenotype variance, then it will be similar to LDpred infinitesimal model or *SBLUP. It can also run a prior like BOLT-LMM and can also run lassosum regression. So, the difference between MegaPRS and other methods is that the expected per-SNP [single nucleotide polymorphism] heritability can vary by LD and minor allele frequency. So, in this talk, we will compare all those methods. And we know that when a method is proposed, [the method creators] already compared it with other methods. But the fundamental question we are trying to answer here is: Which method should we use in the PGC data?\nThen we use the cross-validation. We want a cohort out of cross-validation to answer this question and to compare the performance of different methods. Here’s the toy example showing, for the cross-validation, each cell here is one cohort, and the pink cell is for the discovery cohort, and the green cell is for the target cohort.\nIn the first round of the analysis, we’re using the four pink discovery cohorts as a discovery set, and then we assess the performance of each method in the target sample. And then we repeat this process in each of those cells, each of those cohorts serves as a target cohort. If it’s needed by the method, we have another cohort that will serve as a tuning sample to select the optimal hyperparameters.\nSo, in the real data analysis, we use the GWAS summary statistics from Schizophrenia 2 [PGC Schizophrenia GWAS data freeze 2] as a discovery sample, and all those data we actually have access to through a service, through the cohort, where the individual-level genotype is available. We use each of them as a target sample. For the tuning cohort, we use these four cohorts in turn to tune the hyperparameters. And then, we can predict the polygenic score into each of the target samples.\nHere, we used a few statistics to measure the performance of different methods. One is AUC [area under the ROC curve], another one is the proportion explained in the liability scale, and the third one is the odds ratio. I will go through each of them to show how to calculate each of those statistics.\nSo, let’s first start with AUC. Here’s a toy example on how to calculate AUC by hand. So, AUC is actually short for the area under the ROC [receiver operating characteristic] curve, which is shaded by the pink here. The ROC curve is made by plotting the true positive rate against the false positive rate at each possible cutoff. So, what does that mean? It means, assume that this is the density plot for the polygenic score in the control sample, and here is for the case samples. This vertical line is the current cutoff. In this case, this graph can be divided into four groups: true negative, false negative, false positive, and true positive. And then we can calculate the proportion of each group, and then we can calculate the true positive rates and the false positive rates, which are the coordinates used in the ROC curve. So, in the current cutoff we use here, it means that we have roughly about 17% of cases that are correctly classified as cases, and then there are about 10% of controls that are erroneously classified as cases, which gives us the coordinates for this dot here. And then, as we vary this vertical line (this cutoff), we will get this ROC curve, as shown in this slide here. And this, you see, is the first statistic we use to measure the performance of different methods.\nAnd the second one is variance explained in the liability scale when using ascertained case-control studies. So, this variance is a function of variance explained in the observed scale, this R2 observed in a case-control study, and another two parameters, C and theta (θ). The variance explained on the observed scale is actually a function of two likelihoods, from the null model and the full model, which is designed in these two equations.\nAnd this parameter C is a function of K, z, and P. This K parameter is actually the proportion of the population that is diseased, this also means the prevalence of the disease. The z parameter is the density at this threshold t here, and this curve is a standard normal distribution. And the P is the proportion of cases in your GWAS results or in your case-control study. And the θ parameter is a function of the same K, z, t, and threshold t, but with a different combination.\nSo, in these slides, I just give the final result of how to calculate the variance explained in the liability scale. The full derivation of this equation can be found in this reference.\nThe third statistic is called the odds ratio. An odds ratio is a ratio between two odds, where an odd is a probability of being a case over the probability of being a control.\nSo here’s a toy example showing how to calculate the odds ratio by hand. Let’s say that we are ordering the individuals based on their polygenic risk score from the lowest to highest, and we are interested in the odds ratio between the 10th decile and the 1st decile. So, with a number of cases and controls shown in this table, the odds of being the case in the 1st decile, it is 23 over a 103. And odds being the case in the 10th decile, it’s 83 divided by the 40. The odds ratio between the two deciles is 9.3. This value means that when we order individuals based on their polygenic score, the individuals in the top 10%, or in the 10th decile, have 9.3 times higher odds of being a case compared to the individuals in the bottom 10%. And this odds ratio can be easily estimated from the logistic regression using the logit link function.\nSo, using the one cohort strategy, we can access the AUC, variance explained, and also the odds ratio for each of those target cohorts. Here’s the result for AUC and variance explained for each method, and different colors here stand for different methods we used. The y-axis here is the AUC difference compared to the p+T, which is a benchmark we used. And as you can see, with different validation cohorts, there are lots of variations. And that’s why we think our comparison is more robust compared to other comparisons when they [researchers] only use one target cohort.\nIf we summarise these bar plots by each group by method, we can observe this bar plot. The y-axis here is AUC, and each of the groups stands for each of the methods we compared. And each of the bars in each of the groups stands for a different tuning cohort we used.And we noticed that the methods that have formally modeled different genetic architecture actually have quite similar performance. This is because the genetic architecture of psychiatric disorders is quite polygenic.\nIf we look at the results for Alzheimer’s disease, which is less polygenic compared to psychiatric disorders, we will observe a big difference across different methods. And then we also observed a similar pattern for variance explained in the liability scale and the odds ratio between the top 10 percent and bottom 10 percent, also the odds ratio between the top 10% and medium. But we observed that LDpred2, SBayesR, and MegaPRS rank the highest among most of the comparisons.\nTo summarise, in this talk, I showed how to calculate AUC, variance explained in the liability scale, and also the odds ratio by hand. And based on the comparisons we made, we observed that for psychiatric disorders, which are very polygenic, all the methods perform similarly, but some rank higher than others, for example, LDpred2, SBayesR, and MegaPRS.\nThe results I show here are part of this study, which was recently published. In this paper, we also did the comparison for major depression and also other sensitivity analyses. We also provide the code to run each method and for each comparison, as well as each of the statistics used for comparison.\nWith this, I would like to give a big thank you to Professor Naomi Wray, who always gave me huge support whenever I needed, and thanks to all other PCC members. And thank you all."
  },
  {
    "objectID": "software_prs.html#prs-csx-to-perform-prs-analysis",
    "href": "software_prs.html#prs-csx-to-perform-prs-analysis",
    "title": "PRS",
    "section": "PRS-CSx to perform PRS analysis",
    "text": "PRS-CSx to perform PRS analysis\nTitle: Hands-on tutorial of using PRS-CSx to perform multi-ancestry PRS analysis\nDescription:\nPresenter(s): Tian Ge, Yunfeng Ruan, Stanley Center, Broad Institute\nLevel: Intermediate\nLength: 8:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "software_prs.html#prs-in-ancestrally-diverse-populations",
    "href": "software_prs.html#prs-in-ancestrally-diverse-populations",
    "title": "PRS",
    "section": "PRS in Ancestrally-diverse Populations",
    "text": "PRS in Ancestrally-diverse Populations\nTitle: PRS-CSx: Improving Cross-Population Polygenic Prediction using Coupled Continuous Shrinkage Priors\nPresenter(s): Tian Ge\nLevel: Intermediate\nLength: 56:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter2.2_transcript.html",
    "href": "chapter2.2_transcript.html",
    "title": "Chapter 2.2: Genetic Variation (Video Transcript)",
    "section": "",
    "text": "Title: Genetic Variation and Mutation\nPresenter(s): Precision Health\nThe differences in our genes come about through a natural process called mutation. Mutation is extremely common. As a cell copies its DNA before dividing, it makes occasional typos. Most commonly, a single base is substituted for another.Sometimes a base is deleted or an extra base is added. The cell is able to repair most of these typos. But each round of cell division introduces a few changes that are not repaired. When DNA changes occur in cells that will give rise to eggs or sperm, they can be passed down to offspring. Each one of us has about 60 new variations that were not present in our parents.\nPeople commonly use the terms mutant and mutation to describe something undesirable or broken. But mutation is not always bad. Most DNA changes fall in the large areas of the genome that sit between genes and they usually have no affect. When variations occur in areas of the genome that code for proteins, they often have consequences. These changes can effect the protein product itself or they can effect when, where, or how much protein is made.\nMost of the time, mutation generates variations that are neither good nor bad, just different. While a mutation is defined as any alteration in the DNA sequence, biologists use the term “single nucleotide polymorphism” (SNP) to refer to a single base pair alteration that is common in the population. Specifically, a polymorphism is any genetic location at which at least two different sequences are found, with each  sequence present in at least 1% of the population. Note that the term “polymorphism” is generally used to refer to a normal variation, or one that does not directly cause disease. Moreover, the cutoff of at least 1% prevalence for a variation to be classified as a polymorphism is somewhat arbitrary; if the frequency is lower than this, the allele is typically regarded as a mutation.\nSNPs are important as markers, or signposts, for scientists to use when they look at populations of organisms in an attempt to find genetic changes that predispose individuals to certain traits, including disease. On average, SNPs are found every 1,000–2,000 nucleotides in the human genome, and scientists participating in the International HapMap Consortium have mapped millions of these alterations.\nThe DNA in any cell can be altered through environmental exposure to certain chemicals, ultraviolet radiation, other genetic insults, or even errors that occur during the process of replication. Sickle-cell anemia is a disease caused by a change in a single nucleotide, and it represents just one class of mutations called point mutations. The alteration of a single nucleotide in the gene for the beta chain of the hemoglobin protein, the oxygen-carrying protein that makes blood red, is all it takes to turn a normal hemoglobin gene into a sickle-cell hemoglobin gene. This single nucleotide change alters only one amino acid in the protein chain, but the results are devastating. Molecules of sickle-cell hemoglobin stick to one another, forming rigid rods. These rods cause a person’s red blood cells to take on a deformed, sickle-like shape, thus giving the disease its name. The rigid, misshapen blood cells do not carry oxygen well, and they also tend to clog capillaries, causing an affected person’s blood supply to be cut off to various tissues, including the brain and the heart. Therefore, when an afflicted individual exerts himself or herself even slightly, he or she often experiences terrible pain, and he or she might even undergo heart attack or stroke - all because of a single nucleotide mutation. \nMutations can also alter the way a gene is read through either the insertion or the deletion of a single base. In these so-called frameshift mutations, entire proteins are altered as a result of the deletion or insertion. This occurs because nucleotides are read by ribosomes in groups of three, called codons. Thus, if the number of bases removed or inserted from a gene is not a multiple of three, the reading frame for the rest of the protein is thrown off. \nChanges in the DNA sequence can also occur at the level of the chromosome, in which large segments of  chromosomes are altered. In this case, fragments of chromosomes can be deleted, duplicated, inverted, translocated to different chromosomes, or otherwise rearranged, resulting in changes such as modification of gene dosage, the complete absence of genes, or the alteration of gene sequence. The type of variation that occurs when entire areas of chromosomes are duplicated or lost, called copy number variation (CNV), has especially important implications for human disease and evolution."
  },
  {
    "objectID": "chapter3.1_transcript.html",
    "href": "chapter3.1_transcript.html",
    "title": "Chapter 3.1: SNP array genotyping (Video Transcript)",
    "section": "",
    "text": "Title: SNP Chips (Introduction to genomics theory)\nPresenter(s): Gábor Mészáros, Genomics Boot Camp\nIntroduction\nHi everyone. Welcome back to the introduction to genomics lecture series. We continue now with the second part, and we will talk about SNP chips. Before we do so, we do a bit of a refreshment from previous lectures. So we talked about the DNA and its structure that is based on certain building blocks, and the set of rules of how these building blocks connect. We also established that if we cannot look at everything at once, molecular markers are good surrogates, because we actually know their genotype and their exact position on the genome. And also, they are connected to genomic regions of interest, for example genes, that influence the traits that we are actually interested in. There are multiple possibilities for genomic markers, but the most widespread and most widely-used ones are the so-called “SNP markers”.\nSNP chips\nSo a bit of refreshment also on this. So the SNP markers are the single nucleotide polymorphism markers that are single base pair positions that are different between the genomes of two individuals. So, here we have Individual 1 and Individual 2, and we compare their sequence, and we will find that most of the sequence is totally identical all of the time, except some variants. And one of these types of variants are the SNP markers that are single base pair mutations. And the good thing with these SNP markers is that there are many of them throughout the genome, so we can cover the entire genome with these SNP markers and use them to our advantage. Now, there are really really a lot of these SNP markers, in the millions, and not all of them are consistently appearing within populations. So what we really want to do actually is identify just those SNP makresr that are consistently appearing, so we can genotype them all the time and analyze these consistent data from many individuals. If we have really a standardized, consistent set of SNPs, we can genotype these ones in a straightforward manner, and also in a cost-effective way. This cost-effective way is genotyping these standardized set of SNPs with the so-called SNP chips. These SNP chips have multiple names, or you can find multiple expressions for it, so the beadchip, beadarray, SNPchip, microarray - all of these are basically meaning the same thing.\nSo this is how the SNP chip looks like. As I mentioned they have multiple names, but one thing is common that the SNPs that are selected on them are biallelic by design. For example, there is allele A and allele B, so we have three possible genotypes: homozygous AA, homozygous BB, and heterozygous AB. If we look at these SNP chips from a very close perspective, we will find that on these SNP chips there are hundreds of thousands of tiny wells as shown on the right side of the screen. So we have these tiny wells and then in these wells there are these beads, and therefore the name beadchip or beadarray. What it is actually doing, that each of these wells and beads is coated with multiple copies of oligonucleotide probes targeting a very specific locus on the genome. Therefore, each of these wells, and each of these beads, is designed to capture a very specific SNP for the particular species for which the SNPchip is developed.\nNow how does it work? So obviously we need DNA that we want to genotype and these DNA fragments pass over the beadchip. Each probe binds to a complementary sequence in the DNA and stopping one base before the locus of interest. After that, they come single base extensions that incorporate one of the four labeled nucleotides. Now these nucleotides are very special because when they are excited by a laser, so when the laser shines on them or points on them, they emit a specific signal, and the intensity of the signal actually conveys information about the genotype on that particular locus or in that particular bead. So this is actually shown on the picture on the left side here. So here we see the wells, and also the beads, here is the sequence, and then at the end of each sequence there is the labeled nucleotide. And this will be three SNPs here with the RS code and for each of these beads there is a certain genotype that emits a certan signal. So if there is a homozygous one it emits one signal very strongly and not the other one. Similarly, for a different locus there is a different homozygous genotype so it, again, emits a different signal, but again, just on the one side, and if there are some heterozygous genotype there is the signal intensity somehow in between the extremes.\nNow this is how the SNP chips look closer to reality. so basically we have these lanes here, and you see that there is a tiny fraction of the lane is magnified and you see here these tiny dots that are each of them here is a well and the bead that it is emitting some kind of signal. of course these signals do not tell us anything just by looking at them, and they need to be analyzed in a very specific way, so that we know what is the exact meaning of the signal at each of these dots. This analysis is done by a specific genotyping cluster, so there is an algorithm in place that automatically clusters the samples into two homozygous and one heterozygous group. So there are circles around each cluster where the genotypes should fall, and also there are wider kind of shaded areas where we still accept the genotype calls, and then SNPs that are falling outside even these shaded areas are the ones that are not given a genotype. This is what i had in mind. So this is such a graph for a single SNP. Each dot here is an individual genotype for that particular SNP, and here are the circles. So this would be one homozygous, other homozygous, and in between them are the heterozygous genotypes. Whatever is falling into these circles is fine, so this is called as such genotype, and also you see these wider shaded areas, they are still OK, so the individual falls into this area is still called, for example here, as heterozygous. There are some individuals that are outside of these areas, for example this one, and this one, in this case, the algorithm is not certain about the actual genotype call, and this is how we get these so called “missing” SNPs or “missing” calls into our data. So just something went wrong and rather than giving a very inaccurate result, the genotyping algorithm determines that rather, it would not call this SNP, and put it as a missing one.\nOn a SNPchip we have pre-selected SNPs, so we have SNPs that are working very well as, in this case, so we can clearly determine the homozygous, heterozygous, and other homozygous genotypes. I show this example of a so-called bad SNP also just for comparison. So there are also cases like this. Again what we here we have circles, the homozygous, other homozygous, and heterozygous, but you see here that this is somewhat problematic. Here, some the of genotype calls are really really close to each another or even overlapping, so if an individual falls somewhere here, for example, its not really safe to determine if it is heterozygous or homozygous. So there are also SNPs like this, they are generally problematic, but they do not appear on the SNP chips, because, actually, on the SNP chips we will talk about and we will analyze, they are basically these sets of pre-selected well-working SNPs.\nAfter these genotyping process is done, then basically everything gets transferred to a text file that is called the final report. Now, I made a few videos already about these final reports, and you can find them on the channel. But the short story is that everything from the genotyping routine is saved in this final report, which is basically a large text file, and part of this final report are also the genotypes, and these genotypes then can be transferred to other file formats, for example standard PLINK files, and then these files and these data could be analyzed with either PLINK or various other software as you see also a bunch of examples of this on this channel.\nNotes on data handling\nThis series of videos is supposed to be more on the theory side, so I don’t want to spend too much time on this right now. If you are interested in the practical applications there are lots of other videos on the channel, but still I would mention that this is how the data then looks like. So here each line is one individual, and here we have the actual genotypes. And, of course, we know also the locations of these SNPs, we know which chromosome they are on, which exact base pair position they are on, and what is their name, so we can actually conduct routine analysis of various kinds. And afterwards, when we have our data, we can transform these data, by using appropriate methodologies with some kind of signals, and these signals might reveal something about the organisms, or the individuals, or populations we are interested in.\nNow, when it comes to handling of the genomic data we do not rely entirely on our knowledge of biology, because we are talking actually about large data sets and these large datasets are handled exclusively with computers and various softwawre, then i dare to say that some kind of or some degree of knowledge of computers or information technologies is also really really useful if you want to do serious research in this area. I’m not saying you need to be a hardware or software expert, but you need still you need to know the basic jargon, and know your way around computers and also computers servers. It is really useful to have this kind of knowledge in the long run. When it comes to genomics, in our daily work we deal a lot with software because, as I mentioned, its really not possible to analyze this type of data by hand. While the programming skills are useful, well i say here essential, maybe i would rephrase that in a way that , yeah its really useful, and maybe not programming but scripting. if you’re really serious about this kind of work, or type of work, you really need to know some kind of a scripting language, and you need to be able to write some really basic scripts that tailor the data as you want, or modify the data in a way you want, or you be able to run software that you didn’t really use before, all kind of things. So you need to have some kind of knowledge of the computers and how to use them.\nDepending on what you do you can rely on your own scripts, but there are also a ton of established programs that do all kinds of things. So, especially at the beginning, there’s really nothing wrong with relying on these established programs or packages that do the stuff that you want. For any given methodology or approach there is a large number of approaches and software solutions, so I would really encourage you to look around and see which ones fit your needs the best way. But at the end, we will all come back to the same thing, so we will come back to large text files that will have SNP genotypes in them, which can be homozygous, other homozygous, or heterozygous. So this is a different kind of graph, don’t worry about that, but basically what we are after are these SNP chips and SNP genotypes in a text format that we need to analyze.\nAllele and genotype codes\nIn these large text files with the genotype data, you might find alleles and genotypes in different types of coding and these different coding types I want to detail in this slide. One of the most common ones is of course nucleotide coding. So we know that the DNA consists of four nucleotides: guanine, cytosine, adenine, and thymine. And these are also the abbreviations G, C, A, T for this type of coding. Now, you might notice that there is in brackets here is a TOP format coding, because in the SNP chips for some reason there are 2 types of nucleotide coding, usually that is called TOP and FORWARD. Actually both of them are nucleotide coding, so you would see the same type of codes, but genotypes for the same SNPs could be denoted a bit differently when it comes to TOP and FORWARD coding. If you analyze a single population this is not a problem, so you actually don’t need to care too much which coding it is. This question, or the question of TOP and FORWARD allele codes, come into play mostly when you want to merge datasets. Again, there is a video on data merging on this channel, so if you are really interested in that, I would just encourage you to look up that video. But for right now, just information that there is nucleotide coding and there could be TOP and FORWARD coding on the SNP chips.\nNow I mention that each SNP chip is biallelic, meaning that there are exactly two alleles possible for each SNP. So you can actually simplify that, so actually you don’t need four letters, or 4 possibilities, because each of the SNPs is only biallelic, so you can actually recode or rename one allele as A and the other allele as B. So there is another type of coding, character allele codes, with so called AB coding. Also, sometimes you need to use programs, or software, or approaches, or otherwise its somehow beneficial to store the alleles codes not as characters, but as numbers. In this case, very often the numbers that are used for this purpose is 1 and 2. “1” is one of the alleles on the SNP, and the number “2” is the other allele on the SNP. Sometimes also you can find or come across numeric allele coding that uses “0” for one allele and “1” for the other allele. So in all cases, after you get the genotype file, you look it up, what is the coding style that is used, and also you need to ensure that you know what these allele codes mean, or what are the actual allele codes that are used in your particular case, because it could be different, and there is not one single rule or one single scheme that is used all the time. So there are some schemes that are used more often, but of course this doesn’t guarantee that the file that you have uses this particular allele or genotype coding conventions.\nAlso, while I mentioned all these allele codes, but there are also what i mentioned before are the missing alleles. These are often coded with “0”. Of course if the numeric coding is 0/1, then its coded something else, but most of the time, or many times, the missing alleles are coded “0” or something else. Also this is the other thing you need to check is that first what is the codes that is used for alleles, and the second thing is what are the codes that are used for missing genotypes. For example, for the final report it is customary, or I very often come across coding for the missing allele as a “-” or a minus sign.\nNow in the previous slide I mentioned allele codes, but again, I underline that the SNPs are biallelic, meaning that they are two alleles that make up a certain genotype, and this could be homozygous one, homozygous other, and the heterozygous. Again, depending on the allele coding type, there could be different codes for the genotypes. So this would be an example of a nucleotide coding. This would be the example of the AB coding, so AA, AB, and BB. In case of numeric coding when the allele codes are 1 and 2, then these are the numeric coding genotype codes. Here, I would underline that this is not pronounced “eleven”, “twelve”, and “twenty-two”, but actually we refer to these genotypes as “one-one”, “one-two”, and “two-two”. And there is also a different type of genotype coding when you use just one number for each genotype, and this is customary to have it as 0, 1, and 2. And these numbers are used, so the 0, 1, and 2, because this is actually the numbers of the so called “2” alleles. So the “0” is used for the genotype 1/1 because there are zero “2” alleles, the heterozygous is often denoted as “1” because it’s from 1/2, so there is just one “2” allele here, and the 2/2 is denoted by “2”, because there are two “2” alleles here. And obviously, if this type of genotype coding is used, the code for the missing genotype must be something else than zero because zero is already used for one of the homozygous genotypes.\nSNP chip types\nSo the SNP chips are specific for each species, and here I show possibilities of SNP chip types in cattle. I mention cattle as the first species because, well, I work mostly with livestock, and cattle are the most widely genotyped among livestock species. And because of this, it has also a lot of possibilities in terms of chip types. So what we have most commonly, or most often, is the so-called mid-density SNP chip. Funnily enough, it has about 54,000 SNPs, but it is still being referred to as 50K or mid-density, but anyways there is this chip that is very often used for many purposes, from population genetics, to genomic selection. there are also SNP chips that have a higher or lower density depending on what you want to use it for. so the high density SNP chip has around 800K SNPs and the low density around 7K, but this could also be different ones, I just really put it out as an example. also there are custom SNP chips that might have, for example any of these ones as a base, and adding additional, special SNPs that the people, or researchers, the breeding organizations, are very specifically interested in.\nThis is just a quick comparison of the 50k and the HD SNP chips in cattle. So you see that each of these coloured dots here is a SNP on all of the chromosomes in cattle, and you see that the entire genome is covered. Of course, we have much more SNPs in the HD, so it is much more covered, so the inter-SNP distances are much shorter, but all-in-all, both SNP chips do the job, and they are covering the entire genome, so we can use these data to conduct various types of analyses.\nOf course, SNP chips exist for a wide variety of other species, and here I just mention some of these species, and some of these chip types. So there is a lot more on the market, but for you, just to have an idea, I mention a few of these. So there is a human SNP chip with around 900,000 SNPs. In horses, ovine, porcine, companion animals (for example, dogs, cats, and birds), and all these kinds of stuff, there are SNP chips available. Also, there are SNP chips for mice used in all kinds of research experiments. Additionally, in plants, wheat being one of the major crops, and I just included strawberry because I found it funny that there are SNP chips already existing for strawberries. Well, I just wasn’t expecting to find it, so I included it here as a kind of a “cherry on top,” in this case, a strawberry at the bottom of the list.\nSo, to summarize, there are different SNP chip types, and there are SNP chips for many species. As I mentioned on the previous slide, there are also different manufacturers, so there is at least some kind of competition on the market, which is, of course, very good for price development. There are options you can go for if you want something very specific.\nThere are a lot of laboratories that are providing the service of genotyping. So actually, you don’t need to have these genotyping machines in your lab. Basically, what you need is just to get the DNA, send it to a laboratory, and they do everything for you, including DNA extraction and genotyping. Then, they send you back the genotype data in a text format that you can then analyze further on.\nI also want to mention on this slide the saying that sometimes comes up in relation to genotyping. The saying goes, “In the age of genotypes, the phenotype is King!” This actually points out that nowadays, getting genotypes is really easy. All you need to have is DNA, or even, you don’t need to have DNA but just a biological sample, and you send it into a lab, and you get back the genotypes in a relatively short time. But if you want to have some very specific phenotypes, you might have a hard time getting them.\nSo, while we are talking about genotypes a lot during these lectures, we shall not forget that phenotyping is also a crucial thing and is really, really important for a range of analyses that we might want to conduct. A general example would be, for example, a genome-wide association study when we want to associate the genotypes with the phenotypes. Obviously, we need those phenotype records. And if we remain in the livestock sector, for example, genomic selection is one of the large areas where we actually rely on phenotypic information as well, including recording and all this other stuff that we will detail in a specific presentation towards the end of this lecture series.\nSo again, just to summarize the entire process: You get the biological sample, and then you get the DNA out of that. You send it to a lab that uses SNP chips that generates the data, and you can use this data to get some kind of results out of them. And, of course, it depends on what kind of results you are after. You will use the appropriate methodologies, software, and so on. Some of these examples and tutorials are also on this channel, but of course, there is a wide range of possibilities that you could go for.\nAs for the applications of genomic data, as I mentioned, there are really, really many of them. I mean, when it comes to research groups, they tend to focus on certain types of analyses of genomic data. Some research groups are more after, let’s say, population genomics; others are more focused on genomics of diversity, and still, others may be interested in some kind of GWAS-oriented or selection signature-oriented analyses. So, it really depends on the personal interests of research groups and people.\nThere are lots of possible applications, and some of them we already mentioned on this channel, and certainly, we will mention others as well at some point. Also, during these lecture series, we will talk about some of these. So, you could use the genomic data to compute the admixture proportions between populations. In the case of crossbreeding, you can compute genomic relatedness. You can use it for genome-wide association studies, selection signatures, genomic selection, genomic inbreeding coefficients, and all kinds of stuff.\nWe will do everything eventually, but for now, we arrive at the end of this lecture, and I want to end it with a short summary. So, we talked about the SNP markers that are being genotyped with high-throughput machines that determine the genotype of these SNPs in a cost-efficient manner. At the end, what we get are large text files that could be further analyzed. While these text files have various ways of how the SNPs are expressed, or the genotypes are expressed for these biallelic SNPs, these could be the various nucleotide coding or numeric coding. There are also various possibilities of how the missing data is denoted.\nOverall, these SNP chips are a very standard way of how to deal with genotype data in basically all populations, and SNP chips with different densities exist for many species.\nSo, we end here today. I thank you for your time you spent on this video, and I’m looking forward to seeing you again at the next lecture. So, thank you again, and have a very nice day."
  },
  {
    "objectID": "chapter8.6_transcript.html",
    "href": "chapter8.6_transcript.html",
    "title": "Chapter 8.6: Quantitative Trait Loci (Video Transcript)",
    "section": "",
    "text": "GWAS Studies and eQTL Analysis\nPresenter(s): Xiaole Shirley Liu\nXiaole Shirley Liu:\nGWAS Studies\nOver the last decade, we see an increasing rate of GWAS studies. This is because people really want to see: “Based on my genetic information, can I predict a disease? How likely am I going to have a disease?” They are also interested in whether there’s an association with the patient’s response to drugs. This is facilitated by the drastic reduction of genome sequencing costs in early days by the microarray cost. So you can see the number of publications in the GWAS studies are really increasing. In the early days when the cost was high and people didn’t have enough data yet, it was collecting sample cohorts. It’s very time-consuming; it takes decades to collect enough patients for some case-control studies. In the early days, they were looking at cohorts that have only a few hundred patients. Sometimes they might find a SNP that gives them some association, and if they repeat the same study using another scientist, they collect another cohort of patients with a few hundred patients, they do the same study, they see some SNPs, and there could be very low overlap between the two studies. This is mostly because, in the early days, there were not enough patients, and most of these GWAS individual associations have very weak effects on the disease. And because we’re doing so many multiple hypothesis testing, you actually hit on things that are not really real. If you change a cohort, you’ll land in some other regions. You can see here the number of GWAS hits is directly associated with your discovery cohort size. In the early days, it could be a few hundred or a few thousand, and then 20,000, and even 200,000. And now, with more and more people doing genome sequencing, you might even have these huge cohorts that have close to a million individuals to look at associations. The nice thing about this is, once you sequence somebody’s DNA, the information is always there, and all you need to decide is which phenotype do I want to look at. You can see here this could be height, body mass index, this QT interval (is a heart QT interval), cholesterol level. You can look at hair color, eye color, disease other associations, you can look at their IQ. Yeah, there’s many, like skin color and things like that, that you can look at.\nPopulation Stratification\nSo in these types of GWAS studies, to make sure that you are getting robust results, having a bigger population, a bigger cohort, is very, very important. Another important thing is to make sure that you don’t have population stratification. For example, if I want to look for SNPs that can predict longevity, I collect people who can live over a hundred, you know centenarians, who are over 100 years old, versus people who died before they reached 100 years old. Of course, centenarians are hard to find, but when you collect those and you do the GWAS study, you might find, it turns out, hidden things you don’t know. For example, the centenarians are enriched in, say, this Japanese population, whereas the people who died younger are kind of a random mix. And so because of that, when you are looking at the SNP difference, what you might see is that, “Oh, the Asian SNPs are all showing up,” because, you know, it’s just, it has nothing to do with longevity – it just happened that in your cohorts, the longevity cohort has more people from Asian or from Japanese descent. And so in this case, in your case, you have overwhelming population whereas your control is different. And what happens is that when you do the p-value calculation of different SNPs, remember we are doing this for millions of chi-squared tests, we can look at the p-value of those chi-squared tests and then do a Q-Q plot to compare the expected p-value and the observed p-value. If you don’t have population stratification, you would say that okay, most of these SNPs are not significant, then suddenly we have some SNPs that are significant. These are real, whereas in other cases, if you see kind of a very early diversion from your expected, like this; this is an indication that your case-control populations have this population stratification. There are a lot of SNPs that are significantly different between your case and control that could be because of racial/ethnic groups that are differently populated in your case and control, and so for that, you have to correct that and only call those ones as your real difference.\nYeah, so this is one example. If you also just run the PCA on all the SNPs, you can already see that on this PCA plot, there are people from different populations. And so you know, correct for the population difference from your case-control studies that are arising from the uneven populations in the different cohorts. Another situation is there are some unknown relatives. Maybe you have a distant cousin in this study that you don’t know, or in a case-control, we don’t want family structures. And so you really want unrelated family members for the case-control study. And so there are also something called IBD, which kind of gives you the chance that these two individuals are from the same family. And so if on IBD tests you see, “Oh, you see a lot of people from the same family,” you might want to remove the siblings or the cousins from the study. You really want a representative member from unrelated families in your study.\nGWAS Catalog\nAnd so by this time, for example, this is a figure in 2019. In NCBI, there is a GWAS catalog, and they continue to collect the published GWAS studies and get their SNPs from them. To see, you know, this is chromosome 1, 2, 3, 4, and what are the current discoveries of SNPs that are associated with different diseases. And so there are like many of those summary statistics files that tell you this GWAS study identified the following SNPs as associated with that disease, and another is a different collection of SNPs associated with disease.\nMajority of the GWAS SNPs Are Located in the Non-coding Regions\nAnd based on our lecture, Dr. Fong, you have mentioned that a lot of these GWAS SNPs, people found that only a very small set of those are actually in coding regions, because coding region is only 2% of the genome. A lot of these GWAS-associated SNPs land in non-coding regions of the genome, and interestingly, a lot of them are enriched in DNA hypersensitivity peaks. There could be some that are not in the peaks but in perfect LD with those peaks. This is because sometimes they didn’t do genome-wide association using genome sequencing; they just use SNP arrays and type the one SNP, but they don’t have the SNP in the DNA hypersensitivity peak region. But these two are perfectly linked, so you are suspecting that the real effect is on the DNA hypersensitivity region, and only a quarter of them are not in LD, which suggests that a lot of these SNPs associated with disease are related to a transcription factor that binds to a region, which influences the nearby gene expression. This “nearby” could mean 50 kb or 200 kb, or, you know, quite a distal region that influences the expression of that nearby gene, which then has an effect on the phenotype. So, understanding epigenetics now becomes very important to interpreting these GWAS findings. You know, “Oh, there’s a SNP in here, it’s associated with the disease,” but what’s happening? We want to see what transcription factor binds to that enhancer, what kind of nearby genes are linked, and then we can understand why that’s related to the phenotype or the disease.\neQTL: expression Quantitative Trait Loci\nAnother type of study that’s really useful is called eQTL analysis and a national project, a consortium project called GTEx, is really, really quite exciting. So first, they look at a thousand individuals. These are not disease-specific individuals; they are just average people who have died from various causes, a lot of them could be unnatural causes. They donated their bodies to the project, and they took different tissues, in this case, brain, heart, and 50 different tissues from these individuals. And they did both the SNP typing, using SNP arrays to look at their SNPs across the genome, and also to do the expression of those 54 tissues across individuals.\nIn here, because we are not interested in disease per se, the trait, in this case, is no longer whether this person has blue eyes or black hair or whether they have fair skin or a high body mass index. Instead, we use expression data as the phenotype. We say, “Well, for example, in a 1000 people’s livers, for this gene, it’s expressed at a much, much higher level in some populations but lower in another population. What is the difference?”\neQTL Analysis\nSo the features are on the SNPs, and we say, “Oh, it turns out if they have, for example, these SNPs, the expression is much, much higher in the liver. But if they have another SNP, the expression of the nearby gene is happening much lower in the liver.” And so you can establish this association from something called expression quantitative trait loci, or eQTL. So, quantitative trait is like a height; it’s more quantitative, it’s not a black-white, 1 or 2. It could be just the expression level, and the trait, in this case, is not really eye color or physical trait; it could just be an expression, and we are trying to look for a particular SNP locus that’s associated and this locus is associated with a quantitative gene expression level. Using these types of eQTL analysis, again, a lot of people find that there are SNPs in nearby regions that are linked to gene expression differences, and a lot of those are also in the ataxic or DNA peaks, which means that gene transcription factor binding in the distal regions might influence this gene expression and eventually influence real phenotype or trait.\nSummary\nYeah, so that’s kind of the summary. In the genome, there are SNPs, they are often linked if they are in proximity on a chromosome, and there are also these haplotype blocks where a lot of SNPs are linked to each other. In order to really do the genome-wide association studies, you can collect big families where members associated are known to be associated with the disease to look at the allele that’s transmitted, which made the difference, or you can just do case-control studies, normal versus disease, and check every allele to see whether there is any difference between the allele frequency in the two populations. And in order to increase the statistical power of your analysis, you can look at haplotype association rather than individual SNP association. You could increase your patient sample population; bigger cohort, the better power. And also, you need to make sure to eliminate the population stratification and remove the too-close family members. This is in the case-control studies. And also, we can use the eQTL to identify SNPs associated with differential expression and also the GWAS SNPs are linked to some trait. And a lot of these eQTL sites and the GWAS SNPs are in the non-coding regions, and then we need to use the epigenetic data to help us understand what’s happening. Okay, that’s all for today.\n\n\n\nMPG Primer: Introduction to expression quantitative trait loci\nTItle: MPG Primer: Introduction to expression quantitative trait loci (2021).\nPresenter(s): Francois Auget - Broad Institute\nFrancois Auget:\nIt’s a pleasure to be here. I’ve learned a ton from this series in the past, so it’s really nice to have an opportunity to contribute to it. So, what I’d like to do with this talk is give a sort of broad overview of quantitative trait loci or QTLs (I put “expression” here in parentheses), although most of the talk will be focused on expression QTLs. Many of the concepts are really generally applicable to any kind of molecular QTL.\nIn terms of the structure of the talk, I’ll start with aspects of data normalization and covariate correction; then get into the details of QTL mapping for cis-eQTLs; talk about other QTL types, including trans-eQTLs, splicing QTLs, conditional independent QTLs; as well as context-dependent QTLs. And as I’ve mentioned, these concepts are really generally applicable. There are some topics that the talk won’t cover just for the sake of time. One of those aspects, which is really key to these analyses but basically another talk in itself, is how to perform quality control of the core data types, so both on the genotyping side as well as the RNA sequencing. It also won’t be a sort of step-by-step, really detailed walkthrough of how to do these analyses with code examples. Really more of a bit of a higher-level overview. It also won’t be a discussion of QTL results, although I’ll present several examples, these will really be more for illustration purposes.\nSo, before I start, I just want to acknowledge that pretty much everything that I’m going to present is built on work that I did and others did as part of the GTEx Consortium, also building on a lot of prior expertise of mapping QTLs in other previous studies. So, this is really the result of a large collaboration.\nAs most of you are probably familiar with, the large and still-growing number of genome-wide association studies conducted over the past decade or so have now revealed over 70,000 associations to common diseases and traits. A large majority of those are found in non-coding regions of the genome, rendering their interpretation highly challenging.\nBut so, the hypothesis is that the mechanism of action of these non-coding variants must, at least partially, go through modification of transcription. So, the idea behind quantitative trait loci and specifically expression quantitative trait loci is then to identify this relationship between genetic variation and molecular phenotype, that’s the total level of gene expression, or alternative expression of different transcripts of a gene, and I’ll get to that later in the talk. The general concept is that we want to scan the genome either locally around genes or looking for any variant genome-wide that might affect the expression of a specific gene. And often, these effects can be tissue-specific. So, what I’m illustrating on the slide is an example where, in the lung, for example, we find a strong correlation between genotype and gene expression in the presence of a specific variant and assuming that there’s some cofactor that steers this in the lung. Whereas it’s absent in the heart where the effect is not observed. And to be able to observe these effects, we need to study large populations. In practice, this means that we generally require at least 70 samples or so to be able to reliably detect these relationships for common genetic variation. And then, the goal is to, for each variant, look for correlations between genotype and gene expression.\nAnd so, there are many different types of these relationships we can try to identify. And just to start a little bit with a little bit of terminology, so when we try to identify local genetic regulation of gene expression, we call this cis-eQTLs. And this simply means that we’re looking, typically, in a 1 Mb window on each side of the transcription start side of a gene. We’re looking for a common genetic variance that affects the expression level of a gene. The other component of this is trans-eQTLs, where we’re looking for a now distal regulatory variation, and that can be both distal, meaning further away from the gene than that cis window of a megabase or, more robustly, when the regulation is coming from a different chromosome, as illustrated in the cartoon here. And in addition to total gene expression level changes, we can make the same observation in terms of splicing. So, there could be a genetic variant that switches the expression of a specific isoform of a gene, and this can also be detected in the same cis window as the total expression level as well as looking for these effects in trans.\nAnd so, the bulk of this talk is actually really just answering the question: How do we identify which of these relationships are statistically significant, considering that we’re doing this for every gene in the expressed genome, in the specific tissue or cell context, and genome-wide when we’re looking for trans associations? And so, I’m just showing two examples that you might be, sort of, in terms of the visualizations, familiar with if you’ve looked at the QTL data before. Typically, for the association of a single variant and phenotype, we visualize these in these types of box or violin plots, where you see that in the top example, the sort of very strong relationship between the genotype and the expression level. Also, in these locus plots where we see essentially the several thousand common variants in a genetic locus and their association p-values, and on top of that, illustrated with the LD level between different variants, so variants that are in strong linkage are highlighted in these plots. The second example I’m showing here is much weaker, and so there’s a slight trend towards lower expression levels as a function of genotype. And this is also much more ambiguous in this locus visualization. So, the goal of all these analyses is identifying which of these signals are significant. In the top case, the association p-value is so strong that it will survive any sort of genome-wide multiple hypothesis correction. But cases like the signal at the bottom here are much trickier. So, it’s really, how do we come up with a robust framework to determine what the significance levels of any of these associations are?\nOn this slide, I’m just showing a broad map of the workflow. So, starting with the study data consisting of genotypes, RNA-seq data, as well as sample metadata, we essentially have the first couple of steps to generalize normalized expression matrices as well as sets of covariates to correct for unwanted variation – I’ll get into this in much more detail. Once we have these inputs, then we can actually perform the various types of QTL mapping, including the discovery of cis-QTLs, trans-QTLs, conditional QTLs, and also context-dependent ones. And I’ll get into all of these aspects, and the outputs from that are typically a number of files, one summarizing which genes have significant QTLs as well as the lead variants for those and then summary statistics for all associations that have been found. And then using those results, there are many downstream analyses that can be performed, including replication, fine-mapping, and so on. And so, the focus of the talk again is really on the central part here. There have been other very nice resources for some of these downstream analyses, including in this primer series.\nSo, if there are no questions so far, I’ll start with the first section, giving an overview of data normalization and covariate correction aspects. Just very briefly, partly also because we’ve developed tools for this, there’s in terms of the RNA-seq quality control, it’s really important to remove obvious outlier samples that might indicate poor quality or identify sample swaps. In any sort of large-scale study like this, this will invariably happen that some of the samples might get swapped somewhere in the process, and identifying those is critical to avoid just loss of power because of the misassigned samples. So, if you’re interested in this particular topic, feel free to get in touch with me, and I can provide more details.\nThe first really critical step, then, once we have the RNA-seq data in hand, and that means essentially just a quantification of read counts for every gene in for every sample, the first real step is to apply normalization between these samples. Because, if you’re familiar with RNA-seq data, it’s really a relative measurement, and if you conduct the experiment for several samples, there’s no normalization between these samples. The only normalization factor is the library size, meaning the total number of read counts that you get for a specific sample, and that just experimentally can be variable. And some of the standard normalizations will just normalize the scale to units called transcripts per million, which is an absolute scale and ignores potential outlier effects. So, it’s important to actually have a normalization that mitigates these, and there are several methods for this. And some of the best-performing ones are called TMM or size factors from DESeq. The idea there is to normalize all the samples that you’re using to either a representative sample that represents the median or construct such a representative sample from the set and then identifying a scaling factor that rescales normalizations relative to this. Here, just illustrating why this is exactly important is the relationship between just raw library size, so the total read coverage, and the scaling factor. While the correlation is fairly high and the scaling factor is close to one for most samples, there are some clear deviations from this. So, making sure that these are well corrected for is important. In terms of some of these well-performing approaches, there’s really relatively little difference between them, so any of those would, in practice, be reasonable to use.\nAnd so, in terms of this normalization, this can be applied to samples that are largely similar or have weak perturbation between them. But I don’t want to give the impression that this between-sample normalization will enable comparing and combining samples from vastly different tissues or experimental contexts. There’s this really nice review from a few years back that really highlights these issues. But the general idea is that once the deviation and the perturbation of the transcriptome becomes too strong, these computational normalization approaches, which rely on having at least a medium range of gene expression that’s unperturbed between samples, that falls apart. And then, the normalization methods also start to fail.\nAfter normalizing the samples, the next important step is identifying potential confounding effects.\nAudience question: There was one question that came up based on your last statement, saying that eQTL is mainly for non-coding SNPs. And does that make eQTL and pQTL complementary to each other in some sense? Answer: That’s a great question, and it’s important to clarify that. So, what one could say, one of the main goals is really to facilitate the interpretation of non-coding SNPs. But because identifying a functional mechanism for those is much harder, but there’s definitely coding SNPs that can be both eQTLs or pQTLs. And the analysis, in itself, in terms of just the mapping, is actually agnostic to the specific characteristics and functional impact of a variant. So, we really take all common variants in a cis window around the gene and look for associations with a molecular phenotype. And if we do this with gene expression, we can do this with protein levels. And some of those QTLs will be the ones that generally regulate general expression levels, will likely be the same. So, the QTL set that modify that are at the same time modify the protein structure function might have other downstream effects. But in terms of detecting which are significant associations between variants and the selected phenotype, the general statistical approach is the same and agnostic to the type of variant.\nThe next step is trying to make sure that we’re not confounded by unwanted technical or population effects. As you might know, if you’ve worked with genome-wide association studies, many of these concerns are exactly the same. In terms of experimental batch effects, there’s many studies that have nicely shown under highly controlled experimental protocols how measurements can be robust or vary between different labs. Here’s just an example actually showing that when quantifying just exon-level gene expression, for example, sequence at many different centers, the sample identity remains very robust. But then, when looking at transcript level, there’s much more variation and also a little bit of a trend towards center-specific effects. So, that’s important to keep in mind that there’s really an experimental variation that can add a lot of variance to gene expression, and that needs to be corrected for when we map QTLs to a similar degree of population structure. We want to make sure we’re correcting for this not to be confounded by potential effects that are correlated with population structure.\nHere’s one of these examples of technical effects between centers, and one of the stronger ones is just minor variation in GC content that can arise as part of the library preparation protocols in RNA-seq. And the example here is where there’s just, in terms of looking at the fragment GC content distribution, there’s a bit of variation on the upper end of the GC percentage. But then when we look at individual genes, this can actually translate to quite massive differences in coverage. And so, in this particular example, the coverage on this exon is almost lost in one of the centers. And this means that now in terms of both global expression levels and potentially splicing differences, we might artifactually believe that this represents a change even though it’s entirely driven by a technical artifact.\nSimilarly, when working with degraded samples, such as some of the postmortem samples from GTEx, RNA can be at least partially degraded, and in extreme cases, this manifests as a strong 3’ bias. So, when looking at the 3’ UTR since it’s based on a poly-A selection protocol, we see a strong enrichment towards this end of the transcript. And that, too, is important to correct for, especially if there’s variability of these effects across samples. We want to make sure we normalize this out.\nListing these potential effects individually and trying to correct for them explicitly is really challenging because, for a large-scale study, there can be many of them and there’s absolutely no certainty that we’re capturing or aware of all of them. So, there are frameworks to do this automatically, in a sense, and try to identify latent factors that best capture this unwanted variation. There are two more points that are important to make here, and then one I’ll get into more in detail. The first is that usually, the strongest contributor to expression variants is actually the cellular composition heterogeneity of different tissue samples in a study like GTEx. But this can also be the case if just in terms of whole blood or PBMCs or other blood-based samples. There’s quite a bit of variability between individuals. So, in accounting for that and these various technical factors to try to understand what variance components we’re identifying with latent factors, having access to really good sample metadata is very important. So, we can actually then disentangle what’s captured by latent factors and assign this to cell-type-specific variation or technical effects like the ones I’ve shown before.\nAnd here’s just an example, sort of highlighting how strong the cellular heterogeneity differences can be between samples, shown for both heart tissue and colon in the bottom row here. So this is part of an analysis where we actually looked at the enrichment of specific cell types in these tissues, and I’ll just use the bottom example here to highlight where some of these differences are coming from. Just in terms of the tissue sampling, for these colon samples, there will be samples where almost all of the tissue collected is from the muscular layer of the colon, whereas other samples might be almost purely epithelial. So, the cell type proportions across these can vary drastically.\nAudience question: Francois, there was a question about why you use PEER for adjusting for latent factors rather than SVA. Answer: So PEER is essentially just a Bayesian framework for identifying such latent factors. There’s many other approaches that can be used, including principal components. In the PEER paper and then prior work from the same authors, they show that the PEER framework is more efficient in terms of capturing unwanted variation with fewer components. But I’ve seen several QTL studies that use principal components, and there’s probably not a very drastic difference between these approaches in the end. SVA is designed to capture variation that’s orthogonal to a variable of interest, and it’s more often used in the context of differential expression. So, I’ll talk a little bit about context-dependent QTLs later, and SVA could certainly be a framework that could be useful for these types of analyses. But overall, these different approaches try to do the same thing and are largely compatible.\nAnd so here, just sort of to belabor this point of, in terms of quality control, the variance captured by any sort of latent factor that we’re selecting, whether it’s PEERs or principal components, what we generally see is that the first couple of these factors are really strongly correlated with the cellular composition of tissue. And what I’m showing here is just sort of an enrichment score, a correlation between an estimate of the tissue heterogeneity and PEER factors. And at the same time, these effects are only weakly correlated with known technical variants.\nIn terms of selecting these PEER factors, what is the optimal number of factors or components that we actually want to remove from the expression? Here, it gets a little bit trickier because on one hand, it’s again this idea that with metadata that we collect and data that we can infer from the samples, such as cell type composition, can only inform us to some degree the correlation between these known aspects and latent factors. But that’s no guarantee that we’re really removing everything that’s a potential confounding effect. So the strategy that we employ typically is to actually select the number of factors that will maximize QTL discovery. And here, I’m just showing this from an earlier analysis in GTEx, where the plots are just an increasing number of PEER factors and then the number of genes with at least one significant QTL that we identify. And this is also to some degree sample size dependent. And this is a little bit heuristic in terms of how we choose these; generally, we cut off these curves as they start to plateau. One danger of selecting too many PEERs, and this will be stronger with principal components because it’s an orthogonal decomposition of variance and will just increasingly remove signal, but a bigger concern is when we’re using these same factors for discovering cis regulatory variation and trans regulatory variation is that as we increase the number of PEERs, they might actually start to capture trans effects. And so one important quality control is actually to test for this. And what we generally do is we just run a genome-wide scan of each PEER factor to try to see if there’s any enrichment for specific loci that are significantly associated with this factor. And here’s just an example showing that in a case where we don’t really detect such effects.\nSo that sort of covers the aspects of data normalization and covariate corrections. With this, I’ll get into more details of QTL mapping. The model itself is very simple, so it’s just a simple linear regression of genotype onto phenotype with a set of covariates that I’ve discussed. So, these are these PEER factors, genotype principal components, and some technical batches that we may want to include, especially if there were technical batches on the genotyping side. But importantly, typically, we don’t really care about the coefficients of these covariates, so we just residualize the phenotype and genotype relative to these covariates using the orthogonal projection matrix, which should be pre-computed. And this residualization can be done very efficiently. In practice, rather than regressing on these sort of box plots that you typically see, the residualized space generally looks like this. Here’s an extreme example again of a very strong association.\nAudience question: Francois, there was one question about when you add additional PEER factors to maximize the number of eQTLs found, how do you differentiate between adding true positives and false positive discoveries? Answer Yeah, that’s a good point. The advantage of the PEER factors is that they once you add too many, they tend to be correlated and will, in most cases, add noise and lead to loss of power rather than false positives. At least that’s what we’ve seen in practice. But it’s an important point. There’s no guarantee that this will never be the case. The best sanity check is really looking for trans signals that might get captured, in which case we’re knowing that we’re starting to remove real effects.\nAnd so then here I just wanted to give an intuition of going back to this question of how do we identify a significant signal. I wanted to give an intuition of how this is done. So, in a specific locus, we can look for the strongest association between any variant and gene expression. But how do we know that this didn’t just arise by chance for this particular gene? So what we do is just apply a permutation strategy where we permute the sample labels and then retest this. And ask under this permutation, how often do we actually see a strongest association that’s at least as strong as the one we saw with the unpermuted labels? And that’s sort of illustrated here. So, we typically run a few thousand permutations, and then based on that, we employ an approximation strategy that extrapolates from this distribution, which can be modeled with the β distribution, to get a more accurate estimate of the empirical p-value. And we use these p-values to determine which genes have at least one significant signal.\nHere’s a couple of examples of this. One case where there’s a very localized signal that’s strongly significant after permutation. Another case where there’s a lot of LD, so there’s on that essentially in almost two-thirds of the regions, there’s variants that are strongly correlated that tag the same signal. But this also, under the permutation, is very significant. And here’s a much more marginal association, where the nominal significance p-value is still relatively strong, but it’s even visually in the locus less clear. And then with the empirical p-value, we also see that this is not significant by any means.\nAudience question: Francois, there is a question just about how you actually do the permutation and compute the signal after the permutation has been completed. So, the first scan is in the locus. We have a few thousand variants. We look for ways to compute the correlation between the genotypes for each variant and the phenotype, and then we pick the lead variant. And this relies on an order of the mapping between phenotype and genotype sample labels. And the permutations are just a scrambling of these labels. And then we perform exactly the same calculation. So, in this plot, the orange line indicates the p-value we had for the non-scrambled labels, and each scrambled p-value contributes to generating this gray histogram. So this is this distribution. And then we extrapolate this to get more accurate p-values. But it’s really the permutation builds null distribution. And we do this because this varies highly. For you that ,these are all sort of on the same scale, these distributions vary highly across genes. We really have to do this for every gene individually.\nOnce we’ve done this, we can apply FDR (false discovery rate) across all genes, and we typically do this using Storey q-values. But other corrections such as Benjamini-Hochberg would also be appropriate. As I mentioned, we do this using these beta-approximated empirical p-values.\nThe same sort of framework can be used for replication. So, what this means is typically, and this also answers one of the theory questions about how do we know if we’re capturing a true signal or if the addition of PEER factors adds false positives, one way, which also has some caveats, is to look for replication in an independent study. So here I’m just showing examples from GTEx where, for tissues that had a paired tissue type in the Twins UK study, we wanted to see how the p-values from GTEx replicate in those. And what we’re looking here is essentially enrichment; assuming that the signals discovered in GTEx are real, we now want to see the enrichment of small p-values for all these gene-variant pairs and that’s what these histograms are showing. The histograms are showing these replication p-values, which tend to all be very small, so less than 0.05. But to quantify this, we can use the same strategy behind the q-values, which is to estimate the proportion of true positives.\nNow, linking this back to both in terms of the FDR thresholds and the permutation scheme to nominal p-values, I’ve shown you examples showing that these empirical p-values can vary drastically between different genes. But in practice, we often get asked, if we calculate the nominal p-value threshold based on that, couldn’t we just use this as a sort of general cutoff for genes to avoid having to do all these permutations? And the answer to this is shown in this graph and it’s essentially no, because there’s quite a bit of variability, especially for a small number of outlier genes in this space that have really very different cutoffs corresponding to a set FDR level. And this has a little bit of an association with sample size, which is increasing here, but only vaguely.\nEverything I’ve described so far in terms of the mapping relies on the normalized gene expression space that was described earlier. While this is great for performing the associations robustly, the effect sizes that come out of this are not necessarily meaningful or interpretable. So what we actually want to know and quantify is the relative cis-regulatory effect of the alternative allele compared to a reference. And the way we quantify this is as allelic fold change, which is the definition of this shown here. So, it’s essentially the log fold change of expression of the alternate allele over the reference. To compute this, we actually have to work with the untransformed gene expression data, just with the read counts that have been corrected for between-sample variation but without further normalization. And this means that we are essentially dealing with a model that has now multiplicative noise instead of the additive noise of the simple regression model. To solve this, we can apply iterative approaches, but these are not necessarily tractable for the large number of associations that we want to compute this for, and there are now efficient linear approximations to speed this up.\nHere’s just an example of what we can learn when we have these analytical change effect sizes in hand. One of these important takeaways is that as we increase sample size in QTL studies, we tend to discover increasingly small effects. So, the discovery of large effects saturates with a few hundred samples, and then as samples increase, we detect increasingly smaller effects.\nJust briefly now, after going over the bulk of cis-QTL discoveries, I just want to mention a few points about other types of QTLs. One is splicing QTLs, so instead of just quantifying total read counts for every gene, we can quantify splicing phenotypes. One really nice approach is described in a paper describing the LeafCutter method, which computes intron excision ratios and groups them by connected components. So, groups of introns that share pairs of junctions essentially get clustered together and quantified together. Then these ratios get computed for all of these clusters. In terms of actually mapping QTLs for these, it’s not fundamentally different from the expression QTLs, but we want to do this as a group phenotype because to identify is whether there’s a significant splicing QTL at the gene level, we’re now essentially testing each of these clusters for the gene. And since we’re always picking the smallest p-value, we want to make sure that in this case now that we’re testing multiple phenotypes for each gene, we’re correcting for the selection of the smallest p-value among n phenotypes also as part of the permutation scheme. And that’s really all that’s different for the first splicing QTL discovery.\nWithout going much into detail here, an important aspect of splicing QTLs is that there’s now a more subtle technical effect that can happen during the RNA-seq alignment, which is called the “allelic mapping bias.” So, if a specific RNA-seq read contains a variant allele, this might actually mean that it aligns to a different place in the genome than if it contained the reference allele. This is generally a weak bias, but specifically, if it affects a splice site, this could actually induce a false positive splicing QTL and needs to be corrected for. Just something to keep in mind if you’re interested in splicing QTL analysis.\nAudience question: So there was a question about the kind of grouping and clustering that you observe with the splicing QTLs. I guess, do you have any more insights into how they cluster or what the biology behind it might be? Answer: This is not really biological; this is essentially just a computational strategy to robustly identify these associations. We want to make sure that we’re not inflating the p-values by selecting the smallest among many. But the biological interpretation here is more complex. Essentially, what comes out of this analysis is a SNP that’s associated with alternative exon or transcript usage within the patterns detected by this method. So, it does miss some alternative splicing phenotypes, such as alternative UTRs and polyA tails, and so on. But within that framework, this will detect changes; however, to identify the specific change, one needs to go back to the raw data and isoform assembly data to figure out what the switch exactly corresponds to.\nNow, just very briefly about trans-QTLs. In a way, detecting trans-QTLs is simpler than cis-QTLs because we’re just performing an association scan, looking for a correlation between the specific splicing phenotype or gene expression and all variants across the genome. So here it’s just more about computational efficiency and being able to conduct these scans. But with studies with sample sizes such as GTEx, we’re still very underpowered to detect these effects. We have only detected a few dozen of these trans-genes in GTEx, and there are alternative strategies, such as employing eQTLs, because of this more limited power, where one can take trait-associated variants and then specifically test those for trans-effects.\nThe reason I wanted to quickly bring up some details about trans-effects is that there’s an important technical confounder that needs to be corrected for. Between two genes, there might be a region of homology, and reads that originate from one gene might erroneously map to another. In the case of trying to detect trans-QTLs, this means that a cis-regulatory variant that affects a gene containing such a region might then actually artificially drive a trans association just because the reads from the cis gene map to the trans gene, and that association gets picked up.\nAudience question: Thanks, Francois. So, everyone’s very interested in your approaches, and the question was just, you know, when looking at these trans-eQTLs, what’s the relative power you need to detect a trans-eQTL versus a cis-eQTL in terms of sample size? Answer: I don’t have a good answer for this. I actually don’t have this plot in my slides. I thought that it’s sort of the first figure of a typical QTL paper, especially from GTEx, usually shows the correlation between sample size and the number of eGenes discovered, which shows these growing and now plateauing curves. We didn’t observe something like this for trans. There’s a slight indication that this starts to grow for the two tissues highlighted here, thyroid and skeletal muscle, which are some of the tissues with the largest sample sizes. But it’s difficult from this to extrapolate how far we’ll really need to go to detect these effects. And there are other confounders that we’re just starting to explore. Many of these trans-eQTLs could actually be driven by cellular composition effects. This is definitely something that’s still being worked on, and hopefully, as large-scale studies get done, we’ll learn a lot more about these and be able to discover a lot more of these effects.\nSo, just being aware of the time, one more type of QTL mapping that I wanted to describe is conditional independent QTLs and how to detect allelic heterogeneity. By applying this type of conditional analysis, we’re able to detect many different independent signals. Here’s an extreme example of this: in the top row, I’m showing the bulk association signal, and then the bottom two rows show a conditional analysis. Just as a reminder, in this context, “conditional analysis” means that once we find a lead variant associated with the gene, we can then add this to the covariates, essentially regress out the effect of that variant, and in the residuals, now ask if there is another signal that remains that also passes our significance threshold. In this example, there are two LD blocks that are perfectly separated, so if we regress out the signal from the first variant, the second signal remains, and there’s a strong secondary signal. What’s nice in this particular example is that we’re actually then able to map this back to effects originating in different cell types.\nOn that note, I just wanted to briefly give a last section on identifying context-dependent QTLs. This can mean many things, so the focus of what I’ll cover will be trying to identify cell type-specific effects, but this could be really any sort of context, meaning differences between genders, dynamic QTLs responding to a specific stimulus, for example, during development, and so on.\nOne way to identify these effects is using interaction models. So now, complementing the original simple model where we just looked for correlation between genotype and the phenotype, we’re now adding an interaction term and this gene-by-interaction cross term here, with again a set of latent factors and covariates. In terms of detecting these effects, what this means in terms of intuition is that if, for samples where this effect is very weak or not present, you’ll essentially see no difference in terms of just total coverage of reads in the RNA-seq between three genotype groups, illustrated in gray, blue, and red here. But as this effect becomes stronger, for example, if there’s a strong enrichment in samples for a specific cell type, the coverage here and the QTL effect will become more apparent. We can also visualize these in scatter plots, whereas the cell type enrichment grows, we then see a separation between the different genotypes and the QTL effect becomes apparent. We’ve extensively applied this to try to identify cell type-specific effects, and this general idea of trying to identify context-dependent QTLs was already proposed in an earlier paper by Lude Franke’s group.\nHere’s an example of the cartoon I showed in the previous slide, where at the bottom, for samples where there’s a low proportion of keratinocytes, there’s a relatively weak difference in coverage across the exons of this gene for the different genotypes. This effect becomes much more pronounced with samples with high keratinocytes, and we also see this in the scatter plot.\nWhy is this important and why do we think this is an exciting direction to go in? Because when we’re actually starting to incorporate cell type-specific effects, we were able to find a lot more colocalizations with the GWAS signals. So, I’m rushing a little bit here towards the end because I just really wanted to give a broad overview of the different types of analyses. But what this talk leaves out is a very important aspect of downstream analysis. Once you have your QTL results in hand, meaning the different eGenes and the conditionally independent signals, an important next step in many analyses will be to actually fine map these signals and then find colocalizations with results from other studies, including GWAS. Here’s just an example showing this: typically, what we want to do is, if we did a locus for just an example gene, we would try to identify a QTL that underlies the association that we see with the GWAS signal. The example here shows that we were not able to identify such an association, such a colocalization, with the bulk QTL signal. But then when we actually apply these context-dependent analyses and try to identify cell type-specific effects, in this case looking for QTLs that depend on the enrichment of neutrophils, we then actually pick up a QTL signal that colocalizes very well with the GWAS. When we apply this across all the blood genes in GTEx and also across all tissues, we see a very strong gain in terms of colocalization that we are able to identify with these context-dependent QTLs, indicating that it’s an important direction to pursue here. Others have had results along these lines, of course. It will be to go beyond just bulk tissue studies and try to increasingly identify cell type-specific effects.\nVery briefly, all I’ve shown before was conceptual. Here’s a shameless self-promoting plug: we’ve built this software that allows conducting these associations extremely efficiently by leveraging GPUs, called tensorQTL. It builds on previous work, essentially extending what fastQTL, a previously widely used mapper, has been doing, but it implements a lot of different modes, essentially all the types of QTL mappings that I’ve described are implemented in the software. So, if you have any questions about this, feel free to reach out.\nI think the really exciting direction for the field now is to go after identifying more cell type and context-dependent QTLs. This is more on the experimental design side of QTL studies, but also increasing sample sizes, especially to identify more trans effects. There are large-scale biobank efforts such as TOPMed, which are generating a lot of multi-omic data, which will provide really exciting opportunities to do these analyses. At the same time, these new data types and larger study sizes will require improved statistical methods to deal with the challenges that will arise from these data types.\nI’ll just end here with the slide containing different resources to follow up on this. Of course, the GTEx portal contains all the results from GTEx and has nice visualizations that allow you to explore the QTLs. A really nice resource is the eQTL catalog, which aggregates a lot of QTL studies and makes all the summary statistics available. All the pipelines and software that are behind the analyses and methods that I’ve described in the talk are publicly available at these two links. If you have any questions, please reach out and get in touch. Thank you so much.\nAudience question: Francois, that was an excellent and tightly packed review of quantitative trait loci across many different situations. Just as one final question before we end: How do you think this applies to the study of rare variants and their contribution to human disease? Answer: So, I’ve actually glossed over this a little bit. In terms of the genotype QC at the beginning, these types of studies, depending on sample size, are typically restricted to common variants with a minor allele frequency of 1% or higher. If the sample sizes are small, even 5%, in terms of identifying rare variant effects, I think that for GWAS, I’m much less familiar with those approaches, but there are aggregate tests to gain power to identify the effect of rare variants and that’s certainly an idea that I think the QTL field will go into. So, as hopefully sample sizes from biobank-scale studies in blood will go towards tens of thousands of samples, those are models that we’ll be able to start exploring, including actually just lowering the minor allele threshold to go to slightly more rare variants in te\n\n\n\neQTLs, genes, and molecular networks\nTitle: Introduction to expression (e)QTL & their role in connecting QTL to genes and molecular networks.\nPresenter(s): Laura Saba\nLaura Saba:\nThank you, thank you. So, just to touch on a few housekeeping things, this is the third webinar for the “Quantitative Genetics Tools for Mapping Trait Variation to Mechanisms, Therapeutics, and Intervention” . It’s sponsored by the NIDA Center of Excellence and Omics Systems Genetics and the Addictome. Today, we’re going to be talking about expression QTL and how we connect those to genes. So, we sent out a survey a few weeks ago after our second webinar, and one of the really helpful suggestions that was given on that survey was to aim for a presentation that’s close to an hour and then add an extra 30 minutes for those who want to stick around and ask questions and have more of a discussion. So, that’s how we’ll work it today. Feel free, if you’ve only got an hour, to log off at the end of the talk, or feel free to stay on and have a discussion with us to talk about some more in detail about some of the things that we’re discussing in these webinars. And I do need to make the apology of sending out the wrong link to you for those of you that had to register twice on my email, lesson learned about copying and pasting. The slides are now available on GitHub and the video and the slides will be available online at the Oak Harwell website within the next couple of days. And we’ll do another webinar here in a couple weeks, so we’ll send out information all about that so that you can register and join in on that if you’d like. I think that about sums up the housekeeping items, except that I will be sending out another quick survey, just three questions, in case you want to give us some feedback on the format or give us some feedback on other topics you’d like to see covered. So, let’s just jump right into it. I’m Laura Saba, I’m at the University of Colorado, and I’m co-director of the NIDA Center for Omics Systems Genetics and the Addictome. I’m going to be talking today about expression QTL.\nQuantitative Genetics Tools for Mapping Trait Variation to Mechanisms, Therapeutics, and Interventions Webinar Series\nTo remind you what this whole webinar series is about, we’re trying to traverse this path in a forward genetics approach where we start off with a phenotype and genotype, identify the QTL, and then identify genomic regions, genes, and pathways that could be responsible for that association between the DNA variant and our phenotype of interest. And so, I wanted to start off by just recapping what we’ve already presented to get everybody on the same page and give you a reminder of the things we covered before. So, if you remember, if you attended the first webinar, that was Saunak, who gave us a great introduction to quantitative trait loci analyses in general. So, he was able to tell us what they are and teach us how and what genome scans are, and how we find these QTLs through markers or DNA variants to find that link between DNA and our phenotype.\nThen, for the webinar two, Rob built on this knowledge of QTLs and showed us how to do it within GeneNetwork. So, he gave us a review of rodent models and human models looking at substance use disorder. He walked us through on GeneNetwork: how we can take the phenotype data, do some initial preliminary glances at it, like looking at normalization and blocking and distribution, and then go ahead and do the actual mapping of the QTLs right on GeneNetwork. And then he ended the presentation by observing what genes were physically located underneath these QTLs that were identified. Sorry, I keep pushing the wrong button.\nOutline\nSo, now I’m going to take over here. I’m going to talk about a slightly alternative pathway to identifying genes underneath a QTL and start talking about how we can incorporate RNA expression to link that genomic region or those QTLs with genes. And so, we’ll go over the first half will be about expression QTLs for individual genes or individual transcripts. And then we’ll talk about taking that a step further and looking at co-expression networks and how we can link co-expression networks to regions of the genome or to QTLs of particular phenotypes.\nStrategies for linking QTL to genes of interest\nSo, we’ll start with an introduction to eQTLs. Like Rob mentioned in the GeneNetwork analysis, he looked at 15 genes physically located in…\n[audio cuts out for 3 minutes]\nWhy RNA expression?\nAnd so, a couple of reasons why we’re interested in the RNA expression is it really is one of the first quantitative links between DNA sequence and phenotype. So, DNA sequence differences there are usually yes/no, heterozygous or homozygous, where when we get to RNA, we really are talking about levels and slight differences that may have a large impact down the road. It’s one of the first steps where DNA sequence and environment interact, and we can see that right away with differences in RNA expression in different types of cells and different types of tissues. And RNA expression, to that point, can help us differentiate cell types and tissues within individuals. So, incorporating this information into our analysis of how that causal DNA variant can affect our phenotype is also going to bring in knowledge about relevant cell types and relevant tissues. And finally, what we’ll touch on at the end of this presentation is that also by using network analyses, we can, with these transcription levels and their natural relationships to each other, gain a little insight into genetic and environmental interactions.\nRNA as a mediator of the effect of a DNA variant on a phenotype\nSo, when we start bringing RNA into this analysis of translating our QTLs into genes and mechanisms, we need to think of RNA as a mediator of the effect of a DNA variant on a phenotype. So, in our phenotypic QTL, we see this relationship, this dotted line between DNA and phenotype, and we really don’t know what’s going on in between. By adding this element of RNA expression, we can start to understand whether the DNA variant causes differences in RNA expression that, in turn, influence the phenotype.\nSo, by incorporating this concept of RNA acting as a mediator of that relationship between DNA and the phenotype, we can add to our information about translating that QTL. Instead of looking at genes physically located under the QTL, what we can do is look for RNA expression levels that are controlled by the same QTL that controls the phenotype.\nSo, when we start to think about some examples of how this would actually work in the cell, we bring back those effects that we talked about that were physically located. So, again, a DNA variant in a transcription factor binding site or an untranslated region that affects levels, but we also incorporate information on what I’m calling, in this context, an “indirect effect”. So maybe the causal variant is in a transcription factor rather than in the transcription factor binding site, so the variant is causing differences in the transcription factor, which then causes differences in expression of many other genes.\nGenetic differences in RNA expression\nAnd so, one of our first assumptions if we’re going to bring in RNA into this model of how DNA affects a phenotype is the assumption that our DNA-RNA expression is genetically controlled. And so, from a very simplistic view, we think about heritability. A lot of times, we’ll calculate heritability on phenotypes for humans and for rodent models. In the most simplistic case, in an inbred panel of animals, we can calculate heritability relatively easily. We can look at the variation within an inbred strain - so within animals with the exact same genetic background - and compare that to the variability across strains. This gives us an idea of how much the variability in RNA expression is controlled by genetics or by differences in the DNA. If you look on the left-hand side of this graphic, we see a toy demonstration of a really strongly heritable gene expression. Along my Y-axis, expression levels, and then each one of my colors along the x-axis represents a different strain. We can see that there’s very little variation within a strain. When they have the same genetic background, the RNA expression levels are fairly stable. But then we see relatively big differences when we look across strains. So, when we do vary the genetic background, we get strong differences. Where in a non-heritable RNA transcript, we see a lot of variability within strain, and just as much variability within strain that we do see across strains. This RNA that has no heritability or very little heritability isn’t a good candidate for the mediator of a DNA variant on our phenotype. So, we’re looking for transcripts with this strong heritability characteristic.\nMapping expression (e)QTL\nWhen we go to map expression, we do it in a very similar way to what we did for the phenotypes that Saunak talked about a couple of webinars ago. So, we simply map the RNA as a quantitative trait against our DNA variants. Again, we test for an association between each one of our markers or SNPs with our RNA expression levels. The big difference here really comes in the computation of it all. Some of this is what we need to keep in mind when we’re picking out tools and things like this to run these analyses. Now, instead of running an analysis rather quickly, if we need to run 30,000 genes, then we have to be very intentional with our computational efficiency. I just wanted to make a little note; early on when eQTLs were first discussed, they were given the name “genetical genomics.” So, we hear that off and on here in more modern times, but originally, we called it genetic genomics, so studying the genetics that drive RNA expression.\ncis vs. trans eQTL\nWe can also classify these eQTLs into two different categories. So oftentimes, we call them cis-eQTL or trans-eQTL, but “local” versus “distal” is a more accurate term for it. In a cis-eQTL or local eQTL, the locus that controls transcription is near the physical location of the gene in the genome. It’s right in that promoter region, it’s right in that untranslated region. It’s something that has a direct effect on the gene of interest. Whereas a trans-eQTL can be further away on the chromosome or can be on a completely different chromosome than what the gene is physically located on.\nSo if we come back to our example of a transcription factor, if there is a variant that was affecting the transcription level of a transcription factor on a different chromosome, but that transcription factor was necessary for the gene of interest to be expressed, that would end up looking like a trans-eQTL.\nA couple of characteristics that we typically see on cis- versus trans- eQTLs: cis-eQTLs tend to be much stronger. So they tend to have lower p-values, higher statistical significance than our trans-eQTLs. We can rationalize that based on these mechanistic effects that we’ve been thinking about and use to conceptualize eQTLs. So, if the effect is more direct to the gene and doesn’t have to act through an intermediate, then it’s likely to have a stronger statistical effect than it would if it has to go through another transcript, another protein, or a more indirect route to controlling that expression.\neQTL hot spots\nWith trans-eQTLs, we also often see hotspots. It’s what it’s called: a hotspot is an eQTL that is in the same location for many genes that may not physically be located there. So, in this graphic up here, we have a single element on a particular chromosome that actually controls multiple genes on multiple different chromosomes. We can also conceptualize it, again, going back to my favorite example of a transcription factor. If the red square or red triangle here is a transcription factor physically located in its own cis-eQTL, it may be creating a trans-eQTL for many genes. So, these blue circles represent genes that aren’t physically located near the eQTL but are controlled from that region.\nTissue-specificity of eQTL\nWe talked a little bit about one of the benefits of using RNA expression to discover genes related to phenotypic QTLs. It’s being that we can gain information about tissue and cell type. This demonstration here is a graphic from the GTEx project that looked at cis- and trans-eQTLs across many human tissues. A couple of things that I wanted to point out about these graphics: first of all, this upper triangle here represents the cis correlations, eQTL effects across tissues, and the bottom triangle represents trans-eQTLs. A couple of things that stand out fairly quickly on this graphic are that tissues that are similar tend to have similar eQTLs, those cis and trans. So, this box here represents different brain regions, and some of these other boxes along the axis represent slightly different tissues that are highly related. And again, you can see the brighter colors in these boxes. The other thing that we see is that brain regions tend to be similar among themselves but tend to be rather different than the other tissues. And the final thing that this graphic alerts us to is, as we expected, the cis-eQTLs tend to be more conserved across tissues than the trans-eQTLs. You can see that by the brighter colors in the upper triangle than with the lighter colors in the lower triangles and the difference in the scales on this. The other interesting thing we see about tissue specificity is when they went to compare cis-eQTLs to look if they were shared across tissues, one of the things that was immediately apparent is that it was a very bimodal distribution when looking at the number of shared cis-eQTLs. So, cis-eQTLs tend to be present in all tissues or most tissues, or present in a single tissue or just a couple of tissues without very much in between. And they do tend to, in the final one is they do tend to have larger effect sizes in the cis than the trans. Just have larger effect sizes and tend to be more consistent across many tissues.\nTools for mapping eQTL\nTools, we’ve been touching on this a little bit throughout, that the tools for mapping eQTLs are similar to any of the tools that can be used to map a behavioral or physiological QTL. But really, efficiency is of utmost importance. So, you can still use R/QTL and there are other tools such as QTLReaper and Matrix eQTL, another R package that can do this rather quickly. And then when we’re doing QTL analyses with population structure, there are several software out there that work to make this more efficient computationally, so it can run these mixed models quickly.\nDatabases/Websites for Rat eQTL\nBecause this does require a lot of computation. If you’re thinking about 30,000 transcripts across 10,000 variants, there have been a lot of databases for the rat eQTLs, and we’ll talk particularly about rat because that’s what our NIDA center happens to be focused on. But both PhenoGen and GeneNetwork have several rather large datasets of these eQTLs and information. And that’s one of the benefits when incorporating it into these studies. So, if you’re measuring a phenotypic QTL, you can actually use eQTL information from different populations or different labs for linking it.\nSummary of intro to eQTL\nJust to take a pause here and summarize what we learned in this intro to eQTL. We know that expression QTLs are simply regions of the genome that influence RNA expression levels. So, the genetically driven variation in RNA expression levels. When we separate them into local and distal eQTLs, local eQTLs tend to have bigger effect sizes and are more likely to be retained across tissues than trans, and eQTLs are matched using similar methods and software.\nMotivation for Genetical Genomics/Phenomics Approach\nNow, how do we link this information we’ve gotten from eQTLs to our phenotypic QTLs and eventually to genes? So, what’s our motivation? And so, I’m gonna call this a genetical genomics phenomics approach throughout, just to give it some kind of terminology. So, use the first approach when we only identify candidate genes that are physically located in a phenotypic QTL, we may miss genes that are controlled by that QTL, whose RNA expression is controlled from that QTL, but they don’t physically reside there. So, we’re going to miss those genes with trans-eQTLs in this similar region. We also run the risk of identifying genes that are not expressed in our tissue of interest. So, for instance, if we’re looking at a dependence-related trait for alcohol or drugs of abuse, we may not find the gene expressed in the brain or a tissue that would be relevant to that disease. And the third thing is, we could miss critical information about how that causal variant influences the phenotype. But when we use this overlap of an eQTL with a phenotypic QTL, you’re more likely to focus in on those genes from the tissues you believe are important. You’re more likely to have a little bit more information about the mechanism on how that region influences the phenotype. And we might be able to identify broader biological and mechanistic themes by looking at genes that are controlled from the same region, although they may not all be physically located there. So, they may have more of a mechanistic reason for having that shared transcription control, and we’ll go into this a little bit more when we start to talk about networks.\nDefinition of Genetical Genomics/Phenomics Approach\nIn a genetical genomics phenomics approach, we’re combining three sources of information. We’re combining the sequence polymorphisms with the complex trait or diseases, so our phenotype here, with variation in transcription levels. So, in a very traditional genetical genomics phenomics approach, we look at each one of these comparisons and pairwise comparisons individually and then look at convergence of results.\nFor the sequence to the disease, we’re going to use a QTL analysis. We can look at the correlation between a transcript and a disease. And then finally, we can calculate these eQTLs and make sure that each eQTL transcript is associated with the same polymorphisms that are associated with a disease. So, again, we’re looking for the convergence of all three of these analyses to a single gene to make it a candidate in a genetical genomics phenomics approach.\nApplication of GGP approach\nAnd so, just to give you an example of the way this has been done in the past, in this particular analysis, we were looking at alcohol consumption in a RI panel of rats. So, we matched the phenotype of interest, in this case, it’s just the amount of alcohol consumed. And then we took individual genes and first looked at their correlation with the phenotype and indicated that they had to be correlated with the phenotype. And then we looked to see that their eQTL location overlapped the pQTL location, and that’s how we got our candidate list of the gene. And then we were able to look at similar biologically annotated functions among the genes and similar literature on these genes to get a better idea of the genetic pathways involved.\nSo, the quick summary here is simply that by looking at the convergence of this information, we’re getting more information about mechanism. We’re getting candidate genes that we can link through both DNA and RNA to our trait.\nWhy networks?\nAnd so, this is a genetical genomics approach. What we’ve showed you so far is looking at a gene-by-gene basis and then talking about a list of candidate genes in the end. But an alternative approach is to think of this in a network-type setting. So, why do we want to move from this single gene to a network-type approach? First of all, it really is logical to conclude that no gene product acts independently in the cell. Most, if not all, gene products interact with one another. We know that most tissues, especially the brain, are complex hierarchical networks. And so, we know that there’s cell types, there’s cell-to-cell communication, there’s cell type to cell type communication, there’s region-to-region communication, and all of these represent interactions and ways genes work together. Another thing that comes up often, especially in substance use disorders, is that we can conceptualize many diseases as a failure in network regulation - is that some kind of network breaks down to produce the disease, to produce the phenotype. So, it makes sense to model our data using that network structure as well. And then finally, it can provide more insight if we know that the genes involved, if we know how they interact with one another, so that we can get a better idea of predisposition to disease and how diseases evolve.\nMethods for defining networks of genes\nThere are typically lots of different methods for defining networks of genes or defining how genes and proteins interact. There’s protein-protein interaction databases and networks that look both at experimental results and computationally derived results about how proteins interact with each other to form protein complexes and other mechanisms. There are annotated pathways and gene ontology, such as KEGG and the GO ontology, that actually derive these networks through annotation in the literature and other mechanisms and means. And then, finally, one of the other methods, and again, this isn’t an exhaustive list, is RNA co-expression. So, instead of looking at relationships at the protein level or things that are derived from the literature, by actually looking at a mathematical or statistical mechanism for talking about the interaction between two genes or two RNA transcripts. And again, there are lots of different methods out there for doing that. Weighted Gene co-expression network analysis is just one. I’ll talk a little bit about it more later, just as an example. But there are also very classic methods like k-means clustering and more advanced methods like Bayesian networks and Gaussian graphical models, and more statistically advanced methods for identifying these co-expression networks.\nCo-expression as a measure of “connection”\nSo, for the remainder of this talk, I’m going to focus on co-expression as a measure of “connection” - I’m just going to talk about the co-expression methods for identifying networks, not that the other methods aren’t valid or don’t add a lot of information. It’s just a focus for today.\nWhen we think of co-expression and using it as a measure of connection, the theory behind it is that if the expression levels of two transcripts correlate over multiple environments, then the two transcripts are likely to be involved in similar biological processes. So, if gene A is increased under stress and gene B is also increased under stress, or if A is reduced under hypoxia and so is B, if they tend to react in a similar manner across these multiple environments, the likelihood that they’re doing similar things in the cell increases. So, when we talk about genetic panels like the Hybrid Rat Diversity Panel or HS rats or other types of animals, we’re talking about, instead of environments, thinking about stress and actual toxins and things like that, we’re classifying these different environments as different genomes. So, if gene A is high or low in a particular strain, then gene B tends to be low in that same strain. In this toy example, the blue gene and the red gene tend to differ in a similar manner across all the strains. So, we’re hypothesizing that they are involved in similar processes. They may have a similar method of transcriptome control. Whereas the green gene here does not follow any of the similar patterns as the other genes.\nWhat do we gain by building networks and identifying co- expression modules instead of considering each candidate gene individually?\nWe have found this in our research to be extremely helpful when we start looking at under-annotated or unannotated genes. This could be protein-coding genes, this could be non-coding genes, this could be small RNAs that we may not have the KEGG information, we may not have the GO information, it may not create a protein that we could look at protein-protein interactions with. But we can incorporate these genes into our co-expression modules and start to make hypotheses about what their biological function could be, based on what the other genes it’s highly correlated with do and what their known functions are. In a similar vein, many of the genes do lots of different functions in the cell. When we can look for when genes have multiple roles, if we can look at the overlaps of those roles between the different genes, when a network is associated with this particular phenotype, we may find out which one of those roles is important for this particular phenotype. When we start moving into making functional hypotheses about mechanisms and thinking about therapeutic targets, when we have a network, our most significant gene in our association might not be druggable. It might not be the best one to target when thinking about therapeutics. But when we have that network of genes, we can start looking for alternatives or things that already have a drug, or things that we know more about potential side effects with. And then finally, like knowing the biological function, we can also get information about cell types and brain regions that may be of interest in the heterogeneous tissue. So, if in our co-expression module, half of the genes are exclusive to dopaminergic cells, then those may be the cell type that’s important for all genes within this module.\nWeighted gene co-expression network analysis (WGCNA)\nSo, I’m just going to go into a little bit of detail about weighted gene co-expression network analysis (WGCNA). This method has been around for a really long time. It’s been used by many people. It’s rather simplistic from a statistical point, making it simple to use. When I have trainees who are just beginning to learn about co-expression network analysis, it’s a really good one to start with. They have great tutorials, they have an R package, and there are many people using it not only in the traditional way but also in new novel ways. Where WGCNA differs from simply looking at correlation is that we do some statistical and mathematical manipulations with traditional correlation coefficients to make our connections between our genes or our network resemble a scale-free network. We’ll talk a little bit about the motivation for wanting to do that in the next couple of slides. The second thing that weighted gene co-expression network analysis (WGCNA) does above and beyond just a simple correlation is that they also have created what’s called a robust measure of connection or co-expression between two genes done by not only taking into account the direct correlation between the two, but also looking other genes that are correlated with these two genes. So, from a social network perspective, they’re looking to see if these two genes’ friends are friends. This provides more evidence that there really is a robust association between two genes.\nScale-free network assumption\nNow, moving on to the scale-free network assumption. So, over here on the left-hand side, it’s just a visual demonstration of the difference between, here on the farthest left, what we consider a random network versus what’s on the right, which we consider a scale-free network. Just looking at the two pictures, we often make the analogy of a highway system for the random network versus an airport system or an airline system for the connections on the right. When we think about the highways or this random network, each node in the network has just a few lines or just a few links that are coming out of it. Each node, each dot in our analogy, each city, has about the same number of highways or roads coming into and out of it. So, if we look at the distribution of the number of links per node, we see that most of our nodes, or most of our cities in this example, have the same number of links, and we only see very few cities or nodes that don’t have any links, and very few cities or nodes that have a lot of links. On the other hand, in our scale-free network up here, we see these red dots or red nodes as kind of like hub cities for the airlines, where they have lots of lines going in and out. Some of the other nodes up here have very few, maybe one or two, that are coming in and out. The difference in the distribution of the number of links per node in the scale-free network is that most of our cities in this graphic only have one or two flights going in, and you have a couple of cities that have a lot going in and out of them. So, that’s a general description of what a scale-free network is. We have a couple of sources of information that lead us to believe that RNA may work in a similar manner. We don’t know that for sure, but one piece of evidence that we have is that we can see some of these what are called small-world properties in metabolic networks. So instead of metabolites being produced in a linear chain where one is needed to create two, which is needed to create three, and so forth, we see that this linear arrangement can be really inefficient from getting from metabolite 1 to the production of metabolite 778 takes a long time and is very inefficient. If any one of these nodes breaks down or disappears, there’s no way to get from 1 to 778. It just isn’t a very efficient biological model. Whereas when we think about hubs, in a scale-free network type of paradigm, we can get quickly by moving from hub to hub, from the beginning of the linear array to the end of the array, and that if we have any of the smaller nodes fail in between, we’re able to bypass them and still get the work done. The other source of evidence, which I don’t have a graphic for here, is if we think about it in evolutionary terms, genes get added to the DNA and to the system, so older genes tend to have a lot more connections and tend to be your nobes, whereas evolutionarily younger genes tend to have fewer connections.\nIntegration of indirect and direct correlations\nThe other part that we talked about, standing out in making WGCNA just a little bit different than correlation is this idea of indirect and direct correlations. And I realize that there’s some ambiguity around the terminology here—indirect versus direct. So, when we have high indirect correlations, if I’m looking at the association between the two grey genes or two grey nodes here, they have a strong correlation, but we also see that all of the green nodes, so all the other genes that they’re correlated with, they’re correlated with each other. So, this green node is correlated with those grey ones, and the other green one that they were both correlated with, so their friends are friends. Where if we look at this demonstration of a low indirect correlation, there’s a high correlation between the two greys, but none of the other genes that they’re correlated with are correlated with the other grey node. So, this tends to be a more robust correlation measurement, and this tends to be a less robust correlation measurement.\nModule eigengenes\nOnce we have our modules—so now we have gone from these measures of how to tell how connected two genes were—we use a modified hierarchical clustering method to then identify modules of co-expressed genes based on this new measure of connectivity or distance between two genes. But once we have the modules, one of the things we need to be able to summarize the association of a module with a phenotype, or if we need to map the module, is to get a summary of expression across the genes in a module. So, we’re going to call this a module eigengene. These graphics here on the right are simply giving you a general idea of what I’m talking about. If each one of these grey lines represents a gene’s expression level across 18 samples—you can see it goes up and down—then the black line would represent the module eigengene for this particular gene. It’s trying to summarize the relationship among the samples across all the genes. This is a similar graphic where you can see this module expression heatmap, where we have genes along the Y-axis and samples along the X-axis. It’s summarized into an eigengene expression value. For example, sample 3 tends to be high across all these genes, and sample 6 tends to be low. We see that reflected in the eigengene expression. So, we typically use PCA analysis to do that and look at the first principal component. Our end result, like this last graphic here, is eigengene expression, and we get one value for each one of our samples for a given module.\nSummary of co-expression networks\nAnd then, just like we talked about expression, we can map this module eigengene using all the same tools, tricks, and models that we had before. So, to give you a general summary of what I wanted to cover with the co-expression networks, is that by looking at genes as a group and talking about how they’re related to one another, again, we’re hoping to gain more information to develop those functional hypotheses down the road. Co-expression is one mathematical way for describing a network, and so this allows us to be more inclusive, but at times can be at the detriment of ignoring known information about interactions and assuming that there are things like specific relationships between types of relationships, whether it be linear or monotonic, between two transcripts. We talked about WGCNA very briefly, just because it’s a popular method. It’s a little bit easier to grasp and understand. And just like eQTLs, we can map our module eigengenes in that summary of those modules across the genome.\nSo, how do we link this all together? Again, all we’ve done is taken that genetical genomics phenomics approach and replaced an individual gene with a co-expression module. So, we’ve looked at genetic correlations, we’ve looked at QTLs, and we’ve looked for overlap with that module eigengene QTL.\nCandidate Co-expression Module for Alcohol Clearance\nJust want to check how we’re doing on time. I’m almost out but I’m almost close. And so, I just wanted to close with an example of when we’ve used this process of looking at networks and phenotypic QTL overlapping with eigengene QTL. In this particular example, I’m going to talk about today, we measured a couple of alcohol metabolism-related traits in an RI panel and then used liver RNA expression to identify candidate genes. And so, we did this very intentionally because we know a lot about what’s going on in alcohol metabolism. So, we had an expectation of what should come out of this procedure.\nWe did the typical analysis where we did our phenotypic QTLs and we were looking at alcohol clearance and circulating acetate levels or acetate area under the curve (AUC) in this particular analysis, we were able to identify several peaks—you can see throughout here. We then identified our co-expression networks in our liver and looked for those that were associated with our phenotypes. So, these were associated with alcohol clearance, these are associated with acetate AUC. And one of the modules I want to point out here is this orange 3 that’s associated with both acetate and alcohol clearance.\nWhen we overlap our maps of both the phenotypes and that orange 3 eigengene, which is a particular module, we can see that we see a similar peak here for both the orange 3 eigengene and our alcohol clearance module. So, indicating we do have that overlap. And when we look further into this orange 3 module, we see that the two most highly connected genes within this module are alcohol dehydrogenases. And so, that’s what we expected based on what we knew about the process of alcohol metabolism.\nSummary of pQTL to meQTL to network\nSo, in summary, at the pQTL to meQTL to network, we are gathering and looking again for convergence of information. We’re using not only the identity of these genes but how they relate to one another. And we were able to show that in a somewhat unsupervised approach, we get things that we would expect to get in this process.\nConclusions\nSo, I wanted to wrap up this portion or the whole talk, I guess, by saying that by bringing in RNA expression, we’re thinking about it being a natural mediator between DNA variants and behavior. And that by incorporating RNA expression information, we’re just broadening our possible mechanisms for how that QTL affects that phenotype. Because we would miss the differences in protein structure by only looking at RNA expression and eQTL overlap, but we would be missing some potentially critical genes by only looking at those physically located. So, oftentimes, it requires us to look at multiple sources of evidence for how to identify these genes. And oftentimes, these co-expression networks, not only from a statistical point of view, does it reduce some of that multiple testing burden, but it also allows us to bring in some of those genes that we just really don’t have a lot of information on. And if we would have done it in a fashion where we only looked at a gene-by-gene basis, our natural human instinct would have been to ignore them completely and just go with the genes that we know. And so, this is a way to kind of bring that information in and make sure that we’re keeping these under-annotated or unannotated genes in our radar when thinking about what the next functional test is.\nThe last one is just to remind you the NIDA Core who’s been sponsoring all this. And we’re here to develop, help provide some training, help to find some study design and analysis services. And trying to, we have some pilot grants that we’ve given out to kind of build up these databases and resources.\nAnd finally, you know, I have a great group in my lab. We have some funding from NIAAA as well as you saw by all those alcohol samples to generate some of these datasets that hopefully can be used by many, many people. So, I think I will stop my share there and see if we can attack some of the questions.\nHost: Thank you, thanks very much, Laura. There are some numbered questions in there. Joe had two towards the beginning, so if you just wrap, roll up the top of the chat, you can sort of follow along. Joe, if you want to unmute yourself and just ask your set of questions, go ahead. Let’s see, do I have to unmute Joe, or can you do that himself? There you are.\nAudience member: I have to go back and figure out what the first question was.\nLaura: Oh, I see it in the chat. And the question was about splice variants. So, I think you know, I find splice variants especially intriguing, and I think our ability to capture and add splice variants to these types of analyses – whether there’s some groups that have specifically looked at splicing QTLs, meaning genetic variants that could affect splicing – versus looking at eQTLs at the splice isoform level, rather than at the gene level. So, all of those things, I think, will continue to evolve and I think are really easily incorporated in these kinds of analyses. But, you know, this calculating things at the isoform level requires really deep sequencing and a great depth of knowledge about what splice variants are out there. So, it’s requiring some more informatics at the front end for this type of analysis. And we, as a rat community, are working towards that. But all of those splice variants aren’t annotated yet, and so that causes some ambiguity and some of the isoformal connotations.\nAudience member: Do you have any sense of how much biology we’re missing by not formally including splice variants in the tests that we’re doing? And second, I think that there are reasonable methods out there – there are methods out there to look at splice variants. And you said you’ve referred to those that are annotated, but you’d also do discovery of splice variants that could then be, if it’s related to the trait of interest, could motivate some secondary studies to validate the occurrence of the variant. And then you could do work that would, again, motivate further functional studies.\nLaura: Right, right, no and I totally agree. Some work on this – that the reason why I say that is because we have done some of the reconstruction with the RNA-seq. And so, those are some of the methods that identify these novel splice variants. And those types of methods do really well at identifying splice junctions, but they do so-so at chaining them in the correct order. And so, when we’ve compared that to things that we’ve gotten out of single-molecule sequencing, right – which actually defines the full transcript length – we see a few differences. So, the reconstruction is better than the annotation, and the small molecule sequence is even better than that. And so, that’s why I hedged my bets a little bit there. But I do think it has a huge influence. You know, with these datasets where we’ve used this single-molecule sequencing, especially on the brain, we’re capturing 60,000 splice variants in the brain, right, where we have, you know, twenty annotated, right? So, we know that a lot of that’s going on. So, it’s not only the splicing, but it’s also those differences in 5’ and 3’ ends too – which could be really important. And I think that as we build these datasets and as we, as these technologies improve, we can get more and more specific about the networks that we’re building. So when we build them at the gene level, we’re kind of grabbing at that low-hanging fruit, right? I don’t think we’re wrong in some of the things that we’re identifying, but I think we’re missing a lot more detail and subtle differences, like splicing.\nAudience member: Because with expression levels, you’re changing the amount of basically the same isoform. With splicing, you’re potentially changing functionality of the encoded protein.\nLaura: Right, right, and right now, most of - we’re trying to build these databases that are quantitating transcripts at that isoform level, but I think most of the results right now are at the gene.\nAudience member: Yeah, so my second question was related to something that you said later on about strain differences giving you a sense, a measure heritability, but basically, the strain difference is just a strain differences, anything that differs between strains. Not only the nuclear genome, but also mitochondria, the microbiome, if there are parental behavioral influences, stress and the parents affecting stress in the kids. It doesn’t, when you sit, when we say heritability in the narrow sense that we mean, we’re interested in genes because we’re geneticists, but heritability more generally is anything that’s transmitted from one generation to the next in that nice linear, strain-specific way. And we need to take that into account when we look at those data and just the way that you said, to geneticists, everything looks like a gene.\nLaura: And I agree, and I think that’s going to be some of the, you know, we’re re-deriving what we’re calling the Hybrid Rat Diversity Panel. So I think that will be interesting when thinking about some of these microbiome effects too, about where these strains were originally raised in different places, and if we resuscitate them in the same place and resuscitate them in the same environment, are we changing some of those microbiome effects? Right? if there were environmentally driven.\nAudience member: Ron Kahn has a great paper. I think his Cell Metabolism, maybe three or four years ago, where he looked at strains new from different sources, same strain, different sources, and then changes in the microbiome as they acclimated to a particular, the same facility. And there were remarkable changes in the microbiome, and they were correlated with changes in the host metabolic features.\nHost: It’s a good point. It really comes back to Suanak’s first comment about the method of differences. That was where we had the caveats of when that doesn’t work too well if you’ve got confounders you haven’t taken care of. And that, of course, led to a lot of frustration on computing heritabilities. The nice thing about getting down to the QTL level is that, at that point, you don’t have to really concern yourself too much about the heritability if you have a strong linkage.\nAudience member: What I was getting out too though, is that if we see as differences between strains, does it mean that it has a genetic basis? And there could be other features that could be just as interesting biologically, outside of the nuclear genome.\nHost: Join the discussion now. Just unmute yourself if you have any comments. It’s not just the three of us here, so weigh in if you feel like weighing in.\nAudience member: Should we go down the question list?\nHost: Um, yeah, I just want to make sure we’ve got plenty of time, so we’ll get to those. The next question is mine. So, Laura, a great introduction to cis- and trans-eQTLs. I ask a question, Carl gave an initial answer, just want your thoughts. When we declare a cis-eQTL or trans-eQTL, the criteria are somewhat different because one is a genome-wide search initially, one is sort of a regional search around a gene, and that has some important implications for declaring that you found a cis- or a trans-eQTL, so talk about that a little bit.\nLaura: It’s all how it’s all how you apply it, right? Or the context of the experiment, right? I mean, one option is that we’ve kind of, you know, in Colorado we’ve kind of taken the approach before. We do genome-wide analysis, identify the top variant, and then declare whether the top is a cis or trans, as opposed to separating it out into two separate analyses, right? Is one option for controlling a genome-wide rate, but there’s also a lot of value to only looking for the cis-eQTLs. But I do think that you can’t really compare the threshold between the two, is my opinion, unless you’re searching for them in the same manner.\nHost: Um, let’s see, I’m just going through this. I think that was, yeah, that was question three, and then we’re about halfway through your talk, and Joe has the question, “Why are tests for cis-eQTL limited to local rather than genome-wide?”\nAudience member: I’ve believed that was related to what you’re just saying.\nHost: I think about a genome-wide penalty for cis tests, you want to field that, Laura?\nLaura: The approach I was talking about, if I’m entertaining the question right. You penalize everything for a genome-wide search and then classify your hits as either cis or trans, rather than in testing directly per locus effect and only looking at cis genes.\nAudience member: Do cis effects get a multiple testing penalty?\nLaura: So it depends on how the QTL is applied.\nHost: My philosophy is a little bit more liberal, and it’s based upon sort of a, I think, a reasonable Bayesian prior that if any variant is going to control gene expression, it’s going to be a local variant. So that self-control is sort of a prior expectation, and from that prior expectation, you actually expect, in a sufficiently polymorphic population, every transcript to be a cis-eQTL at some level. It may not be true in a population that has a lot of regions that are IBD, but if you imagine a hypothetical population where every gene is highly polymorphic, then every gene should be a cis-eQTL, frankly. So, from that point of view, the initial barrier for declaring a cis-eQTL, in my opinion, should be quite… should not have any kind of genome-wide correction. It might involve a correction for the length of the segment that’s in the LD with the gene. In a human GWAS, that’ll be pretty small, that’ll be a hundred KB or less. But in that hundred KB, you might end up doing 2,200 tests. So, in that case, you would need an FDR correction for all the local tests you’ve done. So, you couldn’t just likely take a p-value of 0.05 as sufficient evidence of linkage. But it really, I think it depends a lot on your priors, and my prior is, again, that cis-eQTLs are, frankly, no-brainers. And I find them interesting but hardly surprising.\nAudience member: So, Rob, your point is completely consistent with Carl’s suggestion, in that the his suggestion of applying the FDR across all your, all the cis-eQTLs is exactly what you’re talking about, except that, you know, the FDR has an empirical-based interpretation. So, it is estimating the prior in effect from the data itself because you can kind of, based on the shape of the distribution of p-values, you can estimate what fraction of cis-eQTLs are actually null versus marginal. So his suggestion is completely consistent with your, you know, your intuitive idea of how they should be treated, but they, I think, you know, in some ways, applying the FDR has the advantage of, you know, taking out, to some extent, individual opinions, and, you know, you have a more formalized procedure, right?\nHost: Carl, do you have anything you wanna add? You’re still on mute. I’ve tried to unmute you, but maybe you’re not right in front of your monitor. We had a question from Herodotus, where is that question?\nLaura: I see it “How to appropriately test for gene overlap between two modules from data…” [indistinguishable] This is a really good question that we have been, is not struggling with, but investigating, let’s say - it’s a more positive way to put it. And some of the research that we’ve been doing computationally in the lab. It’s really to compare two networks, to say that a network has fallen apart or a network has been activated under different conditions. So, there are some tools from, for example, from WGCNA, where they’re looking at the robustness of a module. So, what they would do would take the genes from e-turquoise and look to their relationships are in the splicing dataset. So then they can assign a quantitative value to how well that e-turquoise is conserved in the splicing data, but it’s not necessarily meant to say, “I’ve identified these modules independently. How much do they overlap?” And so there are several, you know, statistical tools to say, “does this group of genes in the e-turquoise module have the same relationships when I look at them in a different dataset?” But not as much as for the overlap, because you know an overly simplistic approach would be to simply look for a for enrichment of genes from the e-turquoise and the s-turquoise, but what you’re missing there is the weightedness based on how connected those genes are in the module. We’ve anecdotaly found, as we’ve run these types of analyses on different technologies and with different normalizations and things like that, that our modules tend to be really robust among those genes that are highly connected with that module for liver, the alcohol dehydrogenases, those that were highly connected, tend to be really robust no matter what we do to the dataset. It’s the ones that were there on the periphery, that weren’t that connected with other genes, that kind of fell in and fall out. So strict, you know, Fisher’s exact test for the two models would miss that information, the prioritizing of that information, but I can definitely put the link in the chat discussion that we post on GitHub for the article from at least the WGCNA group that looks at, that has this z-score for replicability of a module.\nHost: There’s a question just came in from Priscilla. You might have a look at that to the last. Has anybody compared cis-eQTLs of genes in WGCNA?\nLaura: We’ve just done it anectodately not systematically. But it would make sense that those that are most highly connected, especially for those modules that have a strong eigengene QTL, that it would be a cis-eQTL at the hub. And so, in some of our work where we’ve looked at that alcohol consumption and brain, our most highly connected genes in the module are ones that are physically located in that area and have a cis-eQTL. And that sort of leads to, I think it was your question, Rob, about how to avoid LD artifacts. Implementation of WGCNA - we can’t - LD could be one of the reasons why they’re correlated.\nHost: Can you explain exactly what that means?\nLaura: LD is linkage disequilibrium, and so it’s saying that gene A and gene B, their expression is correlated, simply because they’re physically near each other on the genome. So during recombination, they tend to travel together, but the actual variant that’s controlling expression for those two genes are two different variants, right? They just happen to always travel together during recombination, so they appear correlated, but it’s just because they’re so close to one another on the genome. Do you agree with that definition, roughly?\nHost: How could I disagree?\nLaura: And some of the things that we’ve done in a post hoc manner with our co-expression modules has been to look at partial correlations among genes within the module once you control for that QTL for that module eigengene QTL. So if two genes are highly correlated and are physically next to each other, and we do a partial correlation where we account for that QTL and they’re no longer correlated in that partial correlation, then it’s a signal to us, or it’s evidence for us, that those two were just correlated because of linkage disequilibrium, rather than some mechanistic relationship between the two. And so, again, I’ve got a whole slide on that. We could do a whole webinar on it sometime about what the rationale is behind that and how the statistics behind it work, but we’ve just done it simply from, in a post hoc vein. I think I’ve seen other people do it where they adjust out a cis-eQTL effect of all genes and then do a WGCNA, which is there is another option on how to control for that.\nHost: I’m pleased we have Jake Lusis on, taking the class with us. He could be giving this, having coined the term eQTL in some papers in the early 2000s. Jake, if you want to unmute yourself and say hi that would be great.\nAudience member: Hi everybody good to see everyone.\nHost: He was Eric Schadt’s mentor, and so all of those very early corn human mouse eQTL studies, and of course, he’s been very busy in the field for the last twenty years since then, so good to be joining us.\nAudience member: Thank you.\nHost: And If you want to give one of these courses, just let me, Laura, and Sean know. There was one more from little Herodotus, but maybe that got answered. Module size, what’s the kind of, what’s module size?\nLaura: It depends. One of the clinicians in our department laughs, because she says that that standard statistician answer, “It depends.” In WGCNA, the creators of WGCNA, you know, Steve Horvath and Peter Langfelder, they suggested 30 genes being a module size, and some of their rationale for that is because if you want to do any kind of enrichment analysis afterward, so if you want to look for an enrichment of GO terms or enrichment of KEGG pathways, you need at least 30 genes to do that in any kind of reasonable way. In our lab, we tend to drop it down to five. And the reason why we’re down to five is because we know that of the genes that we’re able to, with our technology, we might not expect a really strong correlation between a ton of genes. And so, we’re willing to do the knowledge-based analyses of looking it up and doing things like that, and to us, a smaller number of genes is much more manageable for looking at each individual in the literature to make a story. We’ve tested it out with WGCNA. What happens is when a gene does not fit into a module, they put it into a clump that they call “gray.” And so, that just means that it wasn’t in a module that was over 30. When we adjust that minimum module size in our actual development of these modules, what we see is that “gray” tends to get smaller if we allow those smaller modules. So, we were willing to take the hit on the multiple testing correction for more modules to be able to dig down a little deeper on some of these small ones. So, it really depends on what you want to do next, and a lot of times it may even depend on what the phenotype you’re interested in is. Is a phenotype that you expect a thousand genes to be associated with or influencing, or is it a phenotype that you expect a smaller number of relevant genes on a smaller network to be involved?\nHost: Probably see that question from Saunak at the very bottom:\nLaura: How do your methods change for other omics data such as microbiome, metabolome, etc., versus the transcriptome? So that’s a good question. You know, one of the things that we’ve been struggling with — again, struggling is the wrong word, let’s be positive, which we’ve been investigating a little more — is once you start… In my mind, the philosophy of the scale-free network is easy to comprehend at the protein-coding gene level. When we start adding in things like non-coding or microbiome, we might not expect that scale-free network distribution. We might not expect the same relationships between a non-coding and a protein-coding RNA as you would between protein-coding RNAs. And so it’s that piece of the WGCNA that I’m more hesitant with when we switch to other omics. Is this idea of the scale-free network and whether or not that’s something we want to enforce.\nHost: To your question, one other obvious difference is that some of the “omes” are rooted in the genome: the epigenome is, the transcriptome is, and the proteome is, so the concept of cis- or trans-eQTL makes sense, at least for the proteome and transcriptome. They don’t for the metabolome, and they don’t for the metagenome, and they don’t for a lot of “omes” that were are new. So in some sense, they don’t have that built-in “rooting”. So the lovely thing about cis-eQTLs as they are is sort of a built-in standard for quality control and other basically data quality control. Obviously, there’s a lot of real biology there. I had a question for you, Laura, about WGCNA. So, it seems like when the scale-free, the exponent beta that’s used to raise the correlation coefficients of, you know, to 8, or which is kind of the recommended default, it tends to kill all the negative correlates, which typically have lower correlation coefficients than do the positive correlations. So if you look at a distribution of correlation coefficients that tend to be poly-skewed and tend to some power like 0.8. The negative correlations get washed out. But those negative correlations are really important. In your talk, you mentioned that you’re defining “friends” of transcripts, but I’m often more interested in finding antagonists. And I find for WGCNA, I almost always have to rummage around and rescue the negative covariates. I’m just wondering whether you have a strategy.\nLaura: That’s a good question I’ve never looked at in-depth about the bias and distribution of negative to the positive correlations. There are options in WGCNA, and there are differences in philosophy on whether you actually do what they consider a signed network versus an unsigned network. So in an assigned network, you’re forcing all of the genes within the network to have a positive correlation with each other, as opposed to allowing negative and positive correlations within the same network. I’ve always been a big fan of letting them both in. I think there’s not much value to only looking at one direction. You know, so many of our pathways have both, like you said, protagonists and antagonists, and, you know, we don’t know -that’s important to have that information.\nHost: So, in your Orange 3 module, could you tell me which ones were positive and which ones were negative?\nLaura: Yes, I can.\nHost: I just wanted to make sure that data was actually preserved. I haven’t followed that, but I am fairly sensitive to the imbalance. It’s way more acute in human datasets. So if you look at GTEx, it’s extreme. You can look at the top 500 covariates of a transcript and every one of them will be positive. In rodent expression data, we rarely see that. It tends to be more balanced. But even in the rodent datasets, the positive covariates are more common, and it kind of makes sense because any of your batch effects are going to add on the positive side. Almost all.\nLaura: That would be my interpretation of the overabundance of positive correlates.\nHost: I wish there was a way. I mean, there is a [indistinguishable] way to take the negative covariances, give them a little boost in the negative direction so that they’re perfectly balanced with the positives, and then raise everybody to the power of eight.\nLaura: So I think the one last question, given we only have five more minutes, and this was from [indistinguishable], and it was “what constitutes to independently validate an expression module?” And again, the answer is, “it depends”. It’s a really hard question to answer. And so I’ll go back to - so WGCNA, they have a method, and it’s published in a paper, it’s like “is my module replicable”? And so they actually take, I believe, nine different measures of replication. So it’s same genes, the same order, it’s the most connected one always, the most connected one, those sorts of things. So they have different aspects that’s a module that you can quantitatively measure to say that it’s been reproduced. And so they combine that into a z-score to do it. But there isn’t a globally acceptable way to say that this entire expression module is validated, right? And so, like we talked a little bit about anecdotally, and that we tend to validate the highly connected ones, and the low connected ones tend to not validate, well do you consider that whole module validated or not. So there’s a lot of it’s not an easy answer. We tend to use that overall and almost like a meta-analysis of different aspects to look at the robustness of our network. So we’ll take our WGCNA module, and then we’ll do a bootstrap sample and see what that conservation of that module is in the bootstrap sample and see how consistent that is when we do things like bootstrap or natural ways to [indistinguishable].\nHost: Anybody else unmute yourself and comments questions, suggestions for improvement all welcomed at this point. Countdown 10, 9, 8… All right, well, I guess we’re heading into the weekend. Actually, I and Saunak are heading into our next meeting.\nLaura: Thank you.\n\n\n\nFrom SNPs to Genes\nTitle: Linking SNPs with genes in GWAS.\nAuthor: Luke O’Connor and Dan Weiner\nSarah [Host]:\nGood morning, everyone, and welcome to today’s MPG primer session. We are very excited to have a talk by Luke O’Connor and Dan Weiner, who are here today to share their work on the intersection of common and rare variant disease. Luke O’Connor is a Schmidt fellow at the Broad Institute; his focus is on developing statistical approaches to understand the genetics of common disease. His work integrates data from multiple phenotypes, from common emerging genetic variation, and from diverse populations.\nDan Weiner is an MD, PhD candidate at Harvard Medical School who has just defended his PhD. Congratulations! He is studying the convergence of common and rare genetic variation, co-advised by Luke O’Connor and Elise Robinson. They will be speaking to us today about their recent work linking single nucleotide polymorphisms with genes and genome-wide association studies. So, thank you both for being here today. They’ll be happy to take your questions during the talk, so go ahead and post them in the Q&A or type in the chat, and there will be pauses to have them addressed.\nLuke O’Connor:\nOh, thank you very much, Sarah, for the invitation and the introduction and good morning, everyone. So today, Dan and I will be telling you about a sort of well-studied and well-known challenge in statistical genetics and functional genomics, which is linking SNPs to genes and GWAS. In particular, linking disease-associated SNPs with the disease genes that probably mediate their effects.\nSo I’ll just give a quick background on GWAS and the importance of this problem before telling you about a very direct approach to link SNPs to genes using eQTLs. And then I’ll pass it over to Dan, who will tell you about work that he led on linking SNPs to genes with proximity and this abstract mediation model, as well as some work led by our collaborators on combining multiple strategies to link SNPs to genes’ extensions.\nSo the challenge, of course, is that you observe, after performing a genome-wide association study, some associated variants, and you don’t know which genes it is that they regulate. In particular, GWAS hits are usually not in coding regions, and typically, the loci that are implicated in GWAS contain multiple plausible candidate causal genes. What it looks like these variants do is that they affect transcriptional regulation.\nSo, very early on, it became apparent that significant hits were non-coding, and that they instead localized to regions in the genome that plausibly affected transcriptional regulation. DHS (DNA size hypersensitivity) sites were sort of, that was an early assay of possible transcriptional activity. These are regions of the genome that are accessible to DNase, and therefore they might also be accessible to binding by a transcription factor, and a SNP that’s localized where this region might affect the binding of that transcription factor and thereby alter regulation of a nearby gene. So, there’s early evidence that a large fraction of GWAS hits were in DHSs or were in linkage disequilibrium (LD) with DHSs. And of course, heritability-style analyses (that was 2012) found the same thing. So these were some early estimates from Gusev et. al, Sasha Gusev, by the way, is local and has done a lot of work on eQTLs as well, looking at the heritability explained by coding as opposed to regulatory regions, so DHS regions in particular. So it’s about 10 or 20 percent of SNPs explaining a large fraction of heritability, maybe its estimates have been revised down a little bit since 2014, but a large fraction of heritability and coding variants, even though they’re actually more enriched for heritability, about 10x enriched. They comprise a much smaller fraction of the genome, so they explain less heritability.\nNow on the one hand, it’d be nice if a larger fraction of heritability localized to the coding variants, and we could identify the genes more easily. But on the other hand, one nice thing about regulatory variation is that it’s cell type-specific. So, a lot of regions of DNA look like they have regulatory activity in some cell types but not others. And this is a figure from Finucane et. al 2015, the original stratified LD Square regression paper where they looked at the heritability enrichment of the cell type-specific regulatory elements and found, sure enough, that they’re enriched in the diseases you’d expect. So, in schizophrenia, brain regulatory elements are highly enriched, in rheumatoid arthritis, immune and hematopoietic regulatory elements are enriched. One interesting finding here was that for BMI, BMI looks more like a brain behavioral trait than it does look like a metabolic rate, even though, of course, metabolic processes are also important. So this implies that disease-relevant transcriptional processes are probably usually cell type-specific, and any approach that measures those processes should probably recapitulate that specificity.\nAnd it strongly supports this model where what’s going on with most variants is that most disease-causing SNPs affect gene expression in some sort of causal cell type or cellular context, and that, in turn, has an effect on disease risk. I don’t want to give you the wrong idea, though, because protein-coding variation is also very important, and I’ll come back to transcription regulation in a second. But, in particular, for rare variants, rare coding variants are strongly implicated. They’ve been implicated for a long time in rare diseases, of course, in particular developmental disorders. And more recently, they’ve also been strongly implicated in complex traits, where they can also often have larger effect sizes.\nLow-frequency variants, you know, around one percent frequency or kind of in between, it looks like they have a much higher percentage of their heritability in coding regions compared with common variants. And for common variants as well, actually, fine mapping studies will often turn up a whole bunch of coding fine-mapped SNPs. So this was a study in inflammatory bowel disease where they found that 30% of their fine-mapped variants were in coding regions. And, of course, those types of variants are very valuable; they point to genes directly.\nOkay, so next, I’m going to talk about eQTLs. I just want to pause and see if there are any questions.\nHost: Thank you, Luke. I don’t see any questions yet, so.\nLuke: Okay. So, eQTLs are a very direct approach to link SNPs with the genes they regulate.\nSo the sort of hypothesis behind eQTL studies is that there’s some causal SNP that is associated with disease risk from each GWAS, and you look at all the nearby genes, and you see that, well, here’s this gene that this SNP actually regulates. The SNP is associated with the expression level of this gene, and maybe it’s associated with the expression level of this gene in some tissue or cell type that you think is disease-relevant. And then that would be good evidence that what the SNP is doing is that it’s affecting the gene, and the gene is affecting the risk of disease.\nSo, the GTEx project in particular is an eQTL study that, it’s not the first or the largest eQTL study, but it’s the most comprehensive in terms of the tissues that are represented in that study. Very much motivated by, right, the observation that, you know, disease-relevant transcriptional processes are probably cell type or tissue-specific.\nThere have been some challenges with this approach, though, and the first challenge is co-regulation, in particular, co-regulation across cell types and tissues. So, whereas a lot of regulatory elements, especially enhancers, will be cell type-specific, a lot if not most eQTLs are not tissue-specific. This is a figure from Urbut et al., who did some basically clustering of eQTLs by their pattern of tissue specificity. And the largest pattern, the pattern that explained most eQTLs was a pattern where basically these are variants that affect expression levels across all the tissues, and they are particularly correlated across different brain regions. And then they’re also strongly correlated across non-brain regions. And the result of that is that when you look at the relevance of these eQTLs in GWAS, it’s hard to pick out the right tissues.\nSo, this is a figure from Hormozdiari et al., who performed fine- mapping of these eQTL signals, identified likely, sort of, causally linked eQTLs (causal for gene expression, I mean), and then looked at the disease heritability enrichment in those variants, finding that actually, the strongest disease heritability is associated with the tissues with the largest sample size and not necessarily, sort of, the right tissues that you’d expect to be involved in various traits. That, of course is, right, a consequence of the fact that the eQTL signal is relatively similar across tissues, with important exceptions.\nAnother flavor of this challenge is co-regulation across nearby genes. So, it’s actually not always the case that you see a SNP and it’s clear that it always regulates this gene specifically and not the other ones. A lot of enhancers will activate the transcription of all of their nearby genes, and accordingly, like a lot of regulatory SNPs, affect multiple genes, resulting in these sort of patterns of co-regulation. So, this is a heatmap showing the correlation with the cis- genetic correlations between the expression levels of nearby genes that are locus. You get these blocks of correlated genes, and you also get these faraway correlations as well. Some of these are negative correlations, which are sort of equally problematic as positive correlations for identifying causal genes. So, it’s much less informative as to what’s the disease gene if your disease-associated SNP is regulating all of the genes at the locus.\nAnd another reason that you can get co-regulation is just LD. So, the variants themselves can be highly correlated, and then even if the causal variant for your trait only affects the expression level of one gene, it might be in LD with a different SNP that affects the regulation of a different gene. So, you can have co-regulation due to LD as well. And this is a figure from Chun et al., who performed co-localization. Co-localization is an approach similar to fine-mapping, trying to identify shared causal SNPs, so SNPs that are causal for both gene expression and the trait, as opposed to different SNPs. And here, they’re looking at autoimmune disease loci and they’re looking at immune cells where there is an eQTL at the same locus. And they found that most of the time, like 80 or 90% of the time, even though there’s an autoimmune association and an immune cell eQTL, the causal variants are actually different.\nSo the solution to this challenge, maybe I shouldn’t call it a solution, but the approach to address this challenge that has been most successful is to explicitly model co-regulation. So this is a figure from [audio cuts out][indistinguishable] tissues in terms of the eQTL enrichments for different traits, and most of the time, that turns up sensible tissues. And this is another figure from Mancuso et al., showing that this is an approach on, sort of, basically a fine-mapping approach at the level of keys. So, we can model co-regulation across genes and identify which genes look like their expression levels are causal. So, these blue triangles, these are sort of their credible set. This is a locus where lots and lots of genes look like they’re associated. So, this is like the transcription, this is like this transcriptome-wide significance threshold. Lots of genes look like they’re associated, but their model thinks that one of these five genes here is the causal gene. In particular, like this gene and this gene are strongly associated, but they’re probably along for the ride.\nHost: Luke, someone in the audience, Cal, has just raised a hand, so I’m gonna see if I can unmute Cal to ask a question. Cal, you should be able to speak now. Let’s see. I’m not sure that, um, oh, Cal can’t hear you, or at least I can’t hear you.\nCal: I have myself raised my hand by accident.\nHost: Oh, okay, great. Um, maybe we will assume for the moment that that’s the case, but, Cal, if you have a question, please do post it in the Q&A, and we’ll handle it that way. Thanks.\nLuke: Yeah, I’m definitely happy to take questions at any point. I’ll pause at the end of the section, which is just coming up in a few slides as well.\nOkay, so, and of course, these methods also need to account for LD, which is another major challenge.\nThe second challenge associated with eQTLs is that they often will, they may, measure the wrong thing. So, in particular, they might measure expression level in the wrong cell type or tissue. Well, probably not the wrong tissues, since you know GTEx measures like all the tissues, but it might be the wrong cell type or the wrong cellular context, the wrong stage of development. And, um, these are figures from Yao et al. Doug developed a model where basically you’ve got some observed expression levels, you’ve got SNPs, and you’ve got observed effects of the SNP on the expression levels and the SNP on your trait or disease. And basically, the model just estimates what fraction of heritability looks like it’s explained by these observed gene expression levels versus, is not mediated by that, must be explained by some other mechanism. And under this model, Doug estimated that only about 10% of heritability is actually explained by gene expression levels instead of observed cell types and tissues. That’s consistent, of course, with the findings from Chun et al., which is that oftentimes the eQTL and the disease-associated SNP are just different variants.\nAnd then the other flavor of the measuring-the-wrong-thing challenge is that you might actually be measuring the wrong, you might be measuring, you might be finding eQTLs for the wrong gene. And so, this is an interesting figure from Yao et al. again. So, surprisingly, genes with higher cis- heritability, so genes with more eQTLs or stronger eQTLs, seemed to mediate less trait heritability. So, they seem to be depleted of trait heritability relative to genes with just a few eQTLs or just with weak eQTLs. And that’s consistent with the possible effect of negative selection. So what might be going on is that important disease-associated genes are under selective constraints and they don’t have strong eQTLs or they don’t have a lot of eQTLs. And so then, you perform an eQTL study in a couple hundred individuals, and you mostly pick out the strong eQTLs, and the strong eQTLs are mostly those for sort of less important genes. Mostafavi et al, this is a recent pre-print, a really nice recent pre-print, that sort of explores this model where, okay, you’ve got genes with respect to trait, and those genes are under strong negative selection. And what happens to your eQTL signal? They find that, as a result of this type of effect, there are all sorts of differences between genes that look like they’re associated with traits and genes that have eQTLs. And they think that this is what explains this sort of limited overlap between eQTLs and genes that are disease genes. Explains why it’s been a challenge to use this approach.\nAnd then, of course, the possible solution to this is, well, I mean, for measuring the wrong cell types, a possible solution is to measure the right cell types. And if you’re not getting eQTLs for the right genes because they have vQTLs, and of course, a possible solution is to use larger sample sizes. So, there’s a lot of interest right now in single-cell RNA-seq, single-cell eQTLs in particular, and really context-specific eQTLs, and a lot of interest in scaling that up to large sample sizes.\nAnd before passing it over to Dan, I’ll just note that some people think that you should just pick the closest gene instead of using eQTLs. So Eric Fauman is very vocal on Twitter about this. He thinks that, you know, basically, you should just look at the closest gene or the closest couple of genes, you should forget about the eQTL signal and approach it, he really likes, as an illustration of this point, to use metabolomics GWAS, where, you know, metabolomics, we often actually know what the causal gene is. It’s often just known as we understand the biology, and GWAS will often, like, you know, pull out these whopping hits and then turns out that the closest gene will be the one. And that sort of approach to figuring out what’s a good SNP-to-gene strategy is the same sort of approach that we use, that Dan is about to tell you about, and also that our collaborators use in the other thing. So, now I’ll ask for a question before handing them over to Dan.\nHost: Thank you very much, Luke. I don’t see any questions yet. So, one thing I will be curious to learn more about is whether you have detected any patterns about whether the closest gene assumption as a starting phenomenon is a worse or better approach for some types of phenotypes than others.\nLuke: Yeah, that’s a good question. So, I suspect that those metabolomics GWAS, it’s especially good if those metabolomics GWAS, because it’s like they’ve got these whopping hits, they’re just much stronger than what you observe in other GWAS, and maybe those really strong hits, it’s like basically the way you get that is by being a promoter variant or being very close to the causal gene. So, I suspect that that’s the case. Yeah.\nHost: Great. Thank you.\nDan: Fantastic. Well, thanks, Luke, for that great overview, and I’m gonna first talk about our recent work on estimating gene-mediated heritability with the abstract mediation model.\nSo, as Luke talked about, it’s often unknown which SNPs regulate which genes, and this obscures something that we might be particularly interested in, which is this idea of gene-mediated heritability. Coming at it from the rare variant and sequencing perspective, we get coding variants, making estimation of this quantity of gene-mediated heritability much more straightforward. But from a common variant perspective, where the mapping between largely non-coding common variants and genes is uncertain, it’s much more challenging to estimate. So, as Luke mentioned, there are a number of potential approaches to this issue. So, we can estimate gene-mediated heritability or something close to it using proximity-based enrichment. These are approaches like LDSC and MAGMA, which can estimate the heritability enrichment of SNPs near a gene set of interest. But those SNPs may not actually be regulating those genes. So, in this toy example here in the bottom left, we have a SNP that regulates gene B. It’s closest to gene A. So, we might, using a closest gene approach, fully map its heritability to gene A when in reality that’s not the appropriate mapping.\nThe second potential approach, this is the same paper that Luke just talked about from Doug Yao and colleagues, is the idea of integrating molecular data like eQTLs to estimate mediation. But this paper found that a small fraction of disease heritability is mediated by measured expression levels.\nThe approach that we took was not to use measured molecular phenotypes, given the challenges that we’ve discussed, but instead to develop an abstract mediation approach where, instead of directly observed SNP-to-gene effects, for example from eQTLs, instead we used enriched proxies. So, SNP-to-gene proximity for the gene effect and membership of genes in an enriched gene set, and then the results from genome-wide association studies for the SNP-to-trait effects. And the idea is that we can estimate SNP-to-gene architecture by the decay in heritability enrichment as a set of enriched genes gets farther away from the those variants. And this allows us to partition heritability across SNP-gene pairs and then to estimate gene-mediated heritability in any combination that we would like, for example, in a gene set of interest.\nSo, to illustrate this a little bit further, we model the proportion of heritability, on average genome-wide, mediated by the closest gene in Pk, which is in expectation the case with distance, as well as membership of genes in an enriched gene set. For example, the set of constrained genes. And the expected SNP heritability is essentially a function of whether a SNP has a proximate gene in the enriched gene set and the proportion of heritability, on average, mediated by genes of that proximity. And then we can estimate all of this using an approach similar to LD score regression.\nThe approach behaves well in simulations, and there are a couple of important assumptions in this model that I wanted to go over.\nSo, the first is that the heritability enrichment of SNPs mapped to a gene set is actually mediated by those genes. So, for example, if we pick SNPs that are based on GWAS results, we can get spurious enrichment. So, this would be that genes in the set are in a disease-relevant region of the genome without being relevant themselves, which can give spurious enrichments.\nThe second is that this cis-regulatory architecture does not vary by gene sets. So, the SNP-gene architecture that we can estimate comes from our training set of enriched genes. So, primarily the set of constrained genes. And if this regulatory architecture varies outside of that set, then we won’t be able to see it. So, you could violate this by picking a training gene set with a particularly different cis-regulatory architecture.\nAnd then the third assumption has to do with cis versus trans mediation. So, we assume that genes in cis, nearby genes, mediate all heritability. But if there are some trans regulatory effects that are not mediated by those genes in cis, then that can give us bias.\nSo, we use the set of constrained genes meta-analyzed across 47 traits to estimate this SNP-to-gene architecture. We estimate that, on average, the closest gene mediates just under 30% of heritability, and that these fractions decay with distance as genes get farther away. But a detectable proportion of heritability is mediated by more distal genes. So, this is consistent with the idea that most heritability, or if you had to pick one gene that mediates the most heritability on average, it will be the closest gene, but there are substantial proportions that are mediated by more distal genes.\nWe checked to see that these SNP-to-gene architecture estimates weren’t specific to constrained genes, and we see qualitatively similar estimates using gene sets of specific expression and relevant matched traits, here from liver and from cortex. So now, with these SNP-to-gene architecture estimates, these proportions of heritability mediated by the closest gene, we can then go in and estimate gene-mediated heritability.\nThese are estimates from a range of gene sets, and we see a number of interesting things here. So, for example, in a small gene set of Mendelian lipid disorders and lipid drug targets, we see massive enrichment of LDL heritability, that these, just around 20 genes, mediate approximately a quarter of LDL heritability. In contrast, for genes implicated in Mendelian developmental disorders, we see much more modest enrichments, which is consistent with differences in negative selection affecting these gene sets and traits.\nWe also compare mediated heritability enrichments across different tissues based on the magnitude of gene expression. So, here on the left, this is the cumulative expression fraction for cortex and liver, where there are many fewer genes that are highly expressed in liver relative to cortex, which has a much more flattened distribution of expression across genes. And we see this difference echoed in the mediated heritability enrichments. Where for liver, we see strong mediated heritability enrichments in the genes that are most expressed, decaying quickly. While, for cortex, the mediated heritability enrichments are much flatter across the distribution of gene expression. And I think these observations are consistent with elements of the omnigenic model, where expression in the appropriate cell type is sufficient for enrichment of those genes as perhaps peripheral genes in that model.\nA couple of important limitations of the approach: the gene sets must be pre-specified and more broadly, and the approach doesn’t identify causal genes. We’re estimating snp-to-gene architecture on average across genes here. The cellular context and cell state, as Luke mentioned, is very important for many of these cis-regulatory elements and gene mappings, and AMM doesn’t identify the relevant cellular context. And similarly, the approach has limited power for individual genes and small gene sets. And this is different from the next paper that I’ll present, which does look at both genome-wide patterns but also the individual links between SNPs and genes.\nI’ll finish this section by mentioning that the AMM approach is available as a command-line tool and operates on summary statistics and gene sets. And as I mentioned before, estimation of gene-mediated heritability using AMM provides an apples-to-apples comparison for gene-mediated rare variant heritability from exome sequencing. And we used this comparison recently in a paper where we characterized the genetic architecture of rare coding variation and compared it to common variation.\nHost: Thank you very much again. Before you go on, there were two questions, both of them relevant to eQTLs, so I think they may pertain to both of the sections so far. So, the first is this: for eQTLs, the eQTLGen Consortium combines PBMC (15% of cohorts) and whole blood (85% of cohorts) eQTLs. The audience member would like to know if you think that the eQTLs may therefore represent different signals.\nLuke: So, if I understand the question correctly, I think it’s saying that there’s a mixture of different cell types. Is that right?\nHost: That is my interpretation too. Yes, although if the person who raised this question is here and would like to elaborate, of course, please do in the Q&A.\nLuke: That’s a good question. Of course, most tissues are a mixture of different cell types, and it sounds like eQTLGen might actually have a mixture of different approaches in there. I know eQTLGen has a massive sample size, so it might be sort of a meta-analysis. It’s definitely a challenge, especially in whole blood, that you have—which I think eQTLGen is whole blood—that you have a mixture of different cell types. And for some eQTLs, that’s not a problem because those eQTLs are actually shared across those cell types. So, for a promoter variant, in particular, you might expect that it’s going to have an effect in every cell type where that gene is expressed, and you’re going to maximize your power to detect it by munging all those cell types together. On the other hand, if you have some cell type-specific enhancer, that effect might be missed, or it might get swamped by stronger signals when you mix a whole bunch of cell types. So, that’s definitely a potential problem.\nHost: Great, thank you very much. And then the second eQTL-related question is this: for GWAS and eQTLs, the signals do not seem to always align well. And when the goal is to predict gene expression levels, the audience member would like to know your thoughts on using genotype data versus eQTLs to do that.\nLuke: When the goal—sorry, the question was—when the goal is to predict gene expression?\nHost: I believe the question may be your thoughts on using GWAS as compared to eQTLs to do that.\nLuke: So, if your goal is to predict gene expression levels, then eQTLs are great. So they—um, they directly, I mean eQTLs, you know, are associated gene expression levels. And so if that’s what you want to do, if you want to predict the expression level of the gene from genotypes, then eQTLs are definitely a good approach.\nHost: Great. I believe that’s the question, but again, if you’re in the audience and want to elaborate, please do. And that’s all the questions at the moment. So, thank you.\nDan: All right. So, for this last section, I’m going to present a recent paper from one of our colleagues, Stephen Gazal et al., and the Price Group on the prospect of combining different SNP-to-gene linking strategies to both connect individual loci to mediating genes, as well as for genome-wide characterization of gene-mediated heritability.\nAs we’ve talked about, there are many approaches to prioritize target genes from causal variants, and each has strengths and weaknesses. Previous papers have suggested that a combined approach could perform better than any individual SNP-to-gene mapping strategy, but it’s unclear how, first, to estimate the performance of any individual SNP-to-gene approach, and then to take those performance measures and consider how to combine approaches to optimize the performance of the SNP-to-gene mapping strategy. So, these were the two challenges that this paper took on.\nThey considered 13 primary SNP-to-gene strategies. These include a number that we have mentioned. There are some that are non-functionally informed, for instance mapping variants to their closest TSS or just looking at exonic variants and mapping to the corresponding gene. There are also functional approaches, including using data from GTEx and other resources to map to mediating genes through eQTLs. There are also Hi-C approaches and other more sophisticated approaches like the activity-by-contact model.\nEach of these SNP-to-gene mapping approaches, the authors synthesized into what can be thought of as a significant SNP-to-gene matrix, where for each SNP-gene pair, you have a numeric value that represents the strength of the connection between that SNP and gene. And then you can use that as a genome annotation to estimate heritability enrichments of that SNP-to-gene mapping approach.\nSo, the authors took a heritability-based approach to assess different SNP-to-gene mapping strategies, and there were a couple of key quantities that they looked at. The first is coverage of the SNP-to-gene mapping approach, which is the percent of SNP heritability that’s linked to genes. Then second is precision, essentially how efficiently does this strategy capture SNP heritability, with the intuition that a precise SNP-to-gene strategy is more likely to link a SNP to a critical gene. Here, like in AMM, they make use of the set of constrained genes as a benchmark for assessing the performance of different SNP-to-gene approaches.\nHere’s one of the primary results, we’re looking at the precision and recall plot. You can see, for example, approaches like promoter and exon which are highly precise but only capture a small fraction of overall heritability because only a small fraction of disease-associated variants are exonic, compared to closest TSS which captures a much greater fraction of trait heritability but often maps to genes that are less likely to be disease-critical.\nSo, one of the primary advances here is the development of combined SNP-to-gene strategies. They take the approach of using linear combinations of individual SNP-to-gene strategies, using these weights Wk across K SNP-to-gene approaches, where the weights are determined through optimization over recall thresholded on a certain precision. Here’s the composition of the combined SNP-to-gene approach, which is heavily based on exon and promoter annotations, but includes a number of functional approaches as well. And on the top right of this plot, you can see the performance of the combines-to-Gene approach, which performs better than the non-combined approaches.\nThere are a couple of applications that the authors use the combined approach for in this paper. They look at nomination of causal genes and disease loci, as well as genome-wide assessments of disease “omnigenicity”, comparing the number of causal SNPs and E with the effective number of causal genes, seeing a wide range from traits that have many relevance, like neuroticism, compared to traits with much different genetic architecture, like color.\nThe combined SNP-to-gene strategies are publicly available and are a great resource that will definitely continue to develop over time, as other constituent SNP-to-gene approaches come online. And I think that is it.\nHost: Great, thank you so much, uh, to you both. I don’t see any other, uh, questions in the Q&A at the moment, so I encourage anyone in the audience who has, uh, remaining questions to post those now. Um, so we’ll give people a couple of seconds to think about, think about that.\nSo, I have a question to get things started off, if that’s okay. Um, thank you both for an excellent talk today and just covering a huge amount of recent advancements, as well as some of the persistent stumbling blocks. What do you think, if you’re someone who’s, uh, as Dan is, moving forward to the clinic, what do you do once you have a SNP that’s perhaps associated with the disease gene? What do you see as the future for kind of moving forward? Where we see these SNPs that may have an incremental risk, what do you think we’ll be doing with this information in five years or ten years?\nDan: Yeah, I mean, I think that there are a range of different paths for it in that context. You know, there’s the approach of taking the SNP-to-gene link and then deeply studying the biology of that association, essentially using GWAS to get to a set of relevant genes and focusing in on those. I think, given the recent expansion of exome sequencing approaches as well, I think it’s really important to look at the overlap of evidence between common and rare variation and to see if, you know, in the context of gene loss-of-function or other variants that have potentially larger effects on that gene, whether we can learn about the potential efficacy and safety of a therapeutic targeted at that gene. But there have also been great recent examples in sickle cell disease and others of targeting the non-coding cis-regulatory elements as a therapeutic approach as well. So, it’s definitely not necessary to sort of, to just use the non-coding region as a tool to get to the gene, but there are, there are exciting examples of therapeutic modulation of the cis architecture to affect the gene as well.\nHost: Thank you. A new question has just popped up in the Q&A, and here it is: Do you expect cis eQTL heritability to change depending on different tissue developmental times and postmortem versus fresh, and how do we tease apart noise from true signal?\nLuke: Yeah, that’s a good question. So, I do expect that the cis regulatory architecture will be different across developmental stages and, just more generally, across different cellular states, and we know that, we know that especially during development, you know, cells do different things at different time points again go through stages where they, you know, have different roles. And I don’t think we understand very well, in particular, the genetic effects on those processes. Like, it’s easy, like we know that there are, like, you know, enhancers that are, like, you know, specifically active in fetal brain, for example, and I don’t think that we understand very well at all what regulatory variation does at those time points.\nHost: Great. Thank you. Well, I think that’s the last of the questions, so this seems like a good time to conclude. Thank you so much to you both, that was really, really an exciting and thought-provoking talk, so thank you, and we will talk with you and look forward to hearing more in the future.\nDan and Luke: Thank you very much. Thank you."
  },
  {
    "objectID": "chapter9.5_transcript.html",
    "href": "chapter9.5_transcript.html",
    "title": "Chapter 9.5: Family-based analysis (Video Transcript)",
    "section": "",
    "text": "Title: Univariate/MonoPhenotype Twin Modeling in OpenMx\nPresenter(s): Hermine H. Maes\nHi, my name is Hermine Maes, and I’m going to talk to you about univariate or mono phenotype twin modeling in OpenMx. Today, this is the first of several videos that I’ll be recording to help you understand how to go about modeling twin data, in terms of estimating sources of variance in a phenotype of interest. Now, this presentation, as well as various of the slides, wouldn’t have been possible without help from a variety of my colleagues, including Drs. Nick Martin, Elizabeth Prom-Wormley, Lindon Eaves, Tim Bates, Mike Neale, and many others.\nYou can find the files, as well as the code, with the OpenMx scripts on this website here that you can freely access, and we’ll talk more about how it’s organized and how to get to the various bits and pieces later on in this talk.\nSo what are we going to address? Question is, does the trait of interest run in families and if so, can this familial resemblance be explained by genetic or by environmental effects, or both? Which sources of variance contribute significantly to the variance of the trait and how much of that trait variation is accounted for by either genetic or environmental factors?\nRoad map\nHere, I’ll provide a little road map for what we call univariate or mono-phenotype analysis. These are analysis of a single phenotype using twin data, so there are basically different steps in this process. The first step is used the data to test some basic assumptions about the models, including whether or not means and variances between twin 1 and twin 2, as well as for MZ and DZ pairs, mono- and dizygotic twin pairs, can be equated. We call this a saturated model. Next, we will want to estimate the contributions of genetic versus environmental factors on the total variance of a phenotype. So we’ll delve into both ACE or ADE models? And I’ll just explain in a minute what these acronyms stand for. And then finally we test various sub models of these ACE and ADE models to identify and report significant genetic and environmental contributions. So these would be considered an AE, CE, or E only model. Hopefully this will become clear during the rest of this presentation.\nData\nThe practical example that we’ll be using relies on a data set from the NH&MRC twin registry in Australia, kindly provided by Dr. Martin. Data come from the 1981 questionnaire and we will focus on data of BMI which stands for body mass index and is measured as weight divided by height squared, and it’s a measure of obesity. Today we will focus on the young female cohort. Those are between the ages of 18 and 30 years old and you see the sample sizes here. We have 534 monozygotic female twin pairs and 328 by dizygotic female twin pairs.\nVariables\nThis is what a data set looks like, and this is obviously straight from R where we just are showing the first top rows of this data set, representing in each row, a pair of twins, as you can tell by the list of variables where we have, for example, wt1, wt2, representing the weight measured in twin 1 and twin 2 of the same pair. Of course we also have zygosity as well as age. “part” indicates participation, so we include both pairs were both have been completed measurements as well as those where only one completed measurements.\nNaming conventions\nHere are some of my naming conventions and these are my naming conventions, which means that you can change them and you can use your own, but I’ve tried to use a number of consistent ways to explain these scripts and use the same ones across, so it makes it easier to translate from one script to the next. So the name of variables is called “vars” and “nv” stands for the number of variables and “ntv” for number of twin variables, because most of the analyses here will deal with twin data. “selVars” for the “vars” or the variables that are selected for the particular analysis, “covVars” for definition variables which typically are used to include covariates, and “sv” for start values, “lb” for lower bounds. I mean, these are pretty obvious. Also, the model names are quite important as you’ll see and so we will try to name them specifically to the kind of model that we’re applying, and so we will change name with something that is more descriptive about each of the models.\nClassical twin study\nSo today we’re going to fit some basic twin models, so it makes sense to talk a little bit about the classical twin study and provide you a little bit of background for those who are not that familiar with it. The Classical Twins Study, CTS or also referred to as the classical twin design or CTD, uses MZ and DZ twin pairs reared together where MZ or monozygotic twins share 100% of their genes, while DZ twins share on average only 50% of their genes. So, given that we know this information, there is the expectation that genetic factors are assumed to contribute to a phenotype when MZ Twins or more similar for a trait than DZ twins. Let’s unpack that a little bit.\nVariance\nSo what we’re interested in is the variance, how much people differ from one another and whether those differences can be ascribed to either genetic or environmental factors. So we want to partition the phenotypic variance into genetic and environmental components, where the total variance, “V”, is the sum of the variance components assuming that these effects can be added up and are independent of one another. And we’ll talk about some ways in which that can be evaluated or tested later on. You will often hear about the concept of heritability denoted as h^2, which is the proportion of this total variance that’s due to genetic influences. It’s important to remember that this is a property of the group not an individual, and as it’s also specific to that group in place and in time.\nSources of variance\nSo you’ll hear me talk a lot about different sources of variance in the rest of this talk, and I’ve color coded them consistently. Such as the red refers to the genetic factors where we have additive genetic factors as the majority of the genetic factors, they’re basically are the sum of all the average effects of single alleles at all the individual loci. But there could potentially be some dominance resulting from the interaction between alleles at the same locus, which we refer to as D or VD or d^2. These are alternative ways in which people refer to them with they typically mean the same thing. The environmental factors can also be broken down into two sources, one that we refer to as C, the common environment. These are aspects of the environment that are shared by family members which contribute to similarity between relatives, and that’s the key part here. So they are shared and they increase similarity. In contrast, the other environmental factors that I denote in yellow here, the E component, these are unique environmental factors, unique to an individual that contribute to variation even within a family. They are also referred to as the specific, unique, or within-family factors.\nClassical Twin Study Assumptions\nNow what we’ll be doing is fitting models today, but every model comes along with a variety of assumptions, and so does the classical twin study design. And some of these will list here. The equal environment assumption is quite critical, and it assumes that MZs and DZs experience the same degree of environmental factors or that they create the same level of similarity in MZs and DZs, with respect to factors that have a direct impact of the trait of interest. Other assumptions include those of random mating, no genotype by environment correlation, no genotype by environment interaction, no sex limitation or no genotype by age interaction. Several of these can be addressed with more complicated models that we will address in future videos.\nClassical Twin Study Basic Data Assumptions\nNow, in addition, there are some basic data assumptions associated with the classical twin study. We assume that monozygotic and dizygotic twins are sampled from the same population. Therefore, we expect equal means and equal variances in twin 1 and twin 2, which are supposed to be randomly assigned. We also expect to find equal means and variances in monozygotic and dizygotic twins. Now there could be further assumptions needed that we will introduce when we introduce more complicated models that include other for example, male twins or opposite sex twins or a variety of other variables.\nDescriptive statistics\nNow let’s have a look at some actual numbers that you could get out of collecting data from twins. So what we have here is some descriptive statistics we could come up. We could calculate the means for both twin 1 and twin 2 in MZ twins as well as in DZ twins and we can work out what the expected covariance matrix looks like. Of course a covariance matrix is always symmetric with variances on the diagonal, and the covariances on the off-diagonal. Now when we look at these observed values, the question is, does it look like the means can be equated across twin 1 and twin 2 and across MZs and DZs? and the same for variances? Of course, we can do some old fashioned data checking, but we really need to test this properly, which is what we’re going to do next.\nSaturated model\nSo how do we test this? We do this by fitting what we call a “saturated model” that basically just estimates the means, the variances and the covariances in our data, and so because we’re dealing with two groups of data that potentially have different expectations, at least with respect to their covariance, we are inherently fitting a multigroup model, and so we have a separate group for MZs and for DZs. In each group we estimate the mean of twin 1 and twin 2, the variance of twin 1 and twin 2, as well as the covariance. This model is described in the code called oneSATc.R, “one” indicating we’re having one phenotype, “SAT” for saturated model and “c” because currently we’re dealing with continuous data. These models can also be applied to ordinal or binary data, but we’ll talk about that later.\nIntuition behind Maximum Likelihood (ML)\nNow before we go on and look at the script, let’s talk just a tiny little bit about how we go about fitting models and you’ve seen a little bit of this in a different video that introduces the various concepts of likelihood and parameter estimation. But just a quick refresher. Likelihood is the probability that an observation or data point is predicted by a specified model, which is basically what we’re doing here. So what we’re doing is trying to estimate by maximum likelihood, the best values for the parameters in the model. In this case for a saturated model, the mean, the variances and covariances, possibly some covariates as well. So how do we go about this? We typically define a model first. We then define the probability of observing a given event conditional on a particular set of parameters, and then we choose the set of parameters which are most likely to have produced the observed results.\nLikelihood ratio test\nAfter calculating a likelihood, we can also use this likelihood to compare it to the likelihood of a different model and construct what we call a likelihood ratio test, which is a simple comparison of the likelihoods under two separate models, and actually, in practice, it typically is a comparison of the log-likelihoods under two models, because that has slightly better properties. So we typically have an unconstrained model, which we call here Mu that has more parameters than the constraint model which has fewer parameters. And then the likelihood ratio statistic equals the difference or basically twice the difference between the likelihoods, or the log-likelihood, of the unconstrained and the constrained model. And this likelihood ratio is asymptotically distributed as a Chi square with its degrees of freedom equal to the number of constraints. Hopefully this will make more sense when we use this in practice.\nProbability density function\nSo it’s also useful to remember is that this is based on the probability density function, which describes the distribution of a range of values of a continuous measure that are considered to be normally distributed. So Phi of xi is the likelihood of a data point, for example xi, for a particular set of mean and variance estimates. So based on the values that we provide for the mean and the variance, we can work out the likelihood of any particular data point and then we can sum across all these likelihoods of all the different data points to get the total likelihood of our model. And basically for any particular data point, it’s the height of this probability density function that provides us the likelihood of that particular data point.\nMultivariate situation\nNow remember that we’re dealing here with not a single distribution of a single variable, but we have variables for twin 1 and twin 2, and so we’re talking about the likelihood of a pair of data points Xi and Yi, that we will work out in the context of a particular set of means, variances and correlation estimates, so we’re talking here about the multivariate situation or the height of the multinormal probability density function which is described here.\nConclusion\nSo this is our quick basic introduction with some background as to how we go about fitting or setting up models in OpenMx and the next video will show you the specific steps we go through in the code to set up a saturated model that estimates the means, variances, and covariances of continuous data in MZ and DZ twins. Thank you very much."
  },
  {
    "objectID": "chapter9.2_transcript.html",
    "href": "chapter9.2_transcript.html",
    "title": "Chapter 9.2: Mendelian Randomization (Video Transcript)",
    "section": "",
    "text": "Title: A two minute primer on Mendelian Randomization\nPresenter(s): George Davey Smith, TARG Bristol\nEpidemiologists are interested in understanding factors related to health and disease. The smokers tend to die younger than non-smokers; it certainly looks that way, but disentangling cause-and-effect can be difficult. Smokers are different on average from non-smokers; they’re more likely to drink heavily and have less healthy diets. But even if we measure these confounding factors, we may not measure them perfectly, or there may be others we haven’t measured. Also, as people become ill, they may cut down or give up smoking, which could wrongly suggest that reduced smoking leads to worse health.\nWe could randomly assign 50,000 people to smoke and 50,000 people to not smoke, and follow them up to monitor their health. This removes the possibility that any other factor could be responsible for any differences we see in their long-term health, but this is neither ethical nor practical.\nFortunately, we’ve all been recruited into an experiment without knowing it at the point at which we were conceived. Our genes, which have passed on randomly from generation to generation, influence how much we eat, drink, smoke, and more. These genetic influences are not affected by anything else you may or may not choose to do in your life; they’re not related to confounding factors.\nWe can use this knowledge to learn about cause and effect, grouping people according to their genetic code. This method is called Mendelian Randomization. For example, smokers carrying one version of a gene called CHRNA5 tend to smoke less heavily than those who carry a different version. When we group people according to which version of this gene they have, we find that the people with the version of the gene associated with heavier smoking do indeed die younger. But is the gene influencing how long we live in some other way? We don’t think so. When we look at the same gene in non-smokers, there’s no effect on life expectancy. So, that must be driven by smoking.\nUsing this method, you can show that smoking causes lung cancer, heart and respiratory disease, and many other diseases, but it has not seemed to influence depression or anxiety. Mendelian randomization has already begun to tell us about factors that influence our risk of disease. Now, we’re using the same approach in other ways to look at several risk factors together and to look at what influences disease progression, which may help us develop new treatments."
  },
  {
    "objectID": "chapter8.1_transcript.html",
    "href": "chapter8.1_transcript.html",
    "title": "Chapter 8.1: SNP Heritability (Video Transcript)",
    "section": "",
    "text": "Title: Heritability and SNP Heritability\nPresenter(s): Alkes Price, Broad Institute\nAgain, good morning, everyone, and welcome to our first virtually presented MPG session. We’re fortunate today to have Dr. Alkes Price from Harvard School of Public Health speaking. He’s a professor of statistical genetics in the Department of Epidemiology, and today, we’ll be talking about heritability and SNP heritability and the relationship to the genetic architecture of disease.\nA few notes about questions: Dr. Price has kindly offered to answer questions both at the end and also, importantly, during the talk itself, to sort of make sure that those are voiced in this virtual format. Please plan on typing them into the Q&A box, which, at least on my screen, is at the very bottom. There, you can type your question and also upvote questions posted by others that are of particular interest to you, and I will do my best to keep an eye on that and to interject as needed during the talk. Also, if you have questions that you prefer to save until the end of the talk, I will keep an eye on that, and we can aim to address those then. So, again, welcome!\nAlkes Price:\nThanks very much for joining, and I think we can get started with the talk. All right, well, good morning, everybody. I’m Alkes Price from the Harvard School of Public Health, and welcome to this first virtual MPG primer session. I’m going to talk about heritability and SNP heritability, which is basically an introduction to the genetic architectures of disease. And as Diane mentioned, I do encourage everybody to jump in with questions throughout the talk in the format that Diane communicated. So, let’s get started.\nI think that everybody in this audience will be aware that genome-wide association studies have already been very successful at producing important biological insights. This is just one example from the schizophrenia landmark study. Schizophrenia GWAS was published in the year 2014 in Nature, with a lot of contributions from people at Broad. This landmark study had a huge number of interesting and important findings. And yet, at the same time, I think everybody in the audience will be aware that even though genome-wide association studies have found a lot of things, they certainly have not found everything. And in the case of schizophrenia, we know that the findings of the 2014 paper explained about 3% of heritability, whereas the heritability of schizophrenia has been estimated at about 64% from twin studies.\nSo, there’s this big gap between what we found and what we believe is out there, and this gap is classically known as the “missing heritability”. And this story about missing heritability goes back to around 2008 or so. This is a commentary of Maher 2008, Nature, and in the year 2008, people didn’t really know what the cause of the missing heritability was. Now, I think it’s much better understood, and this talk introducing the concept of SNP heritability will also provide a review of what we know about the answer to this mystery of missing heritability.\nSo, with that in mind, here’s an outline of my talk: I’ll start with an introduction and a definition of heritability. Then, I’ll talk about genome-wide association studies and missing heritability. And then, we’ll delve into this idea of heritability explained by SNPs, also known as SNP heritability. And if we happen to have extra time, there are some extra bonus topics pertaining to heritability that we might have time to scratch the surface on. So, let’s start with an introduction to heritability.\nSo, heritability is generally defined as the proportion of phenotypic variance that is due to genetic effects. And most of the time when people are talking about heritability, they’re talking about narrow-sense heritability, which is the proportion of phenotypic variance due to additive genetic effects. You could also talk about broad-sense heritability, which could include epistatic or recessive-dominant effects, but those are generally harder to estimate. So, generally, when people are talking about heritability, they’re usually talking about narrow-sense heritability denoted as lowercase h squared.\nPeople have been trying to estimate narrow-sense heritability for a long time, at least back to the year 1886. One way that you could do this is you could take some relatives and see if they are somewhat phenotypically similar because if a trait is genetically heritable, then you would expect that relatives are going to be somewhat phenotypically similar. And there’s a method called Haseman-Elston regression, but I’m not going to describe in detail. But here, you’re basically regressing the phenotypic similarity on the genetic similarity or the expected genetic relationship amongst a particular pair of relatives. So, you know, a parent-child pair or a sib-sib pair have an expectation of about 50% shared genetics, and you could ask how phenotypically similar are those pairs? What’s their phenotypic correlation?\nAnd so on this graph, this is sort of an amalgamation of different results that were compiled in the paper of Visscher et al. 2010, in which each point represents one study. You can kind of see, this is a study that generally people who are very closely related to each other tend to have very correlated values of height. This is generally sex-adjusted standardized height that people are studying. Whereas on the other hand, people who are a little bit genetically similar to each other tend to have height that’s a little bit, you know, typically correlated, and that sort of stands to reason for what you’d expect for a trait that’s largely but not completely heritable.\nThe slope of this line in this paper is estimated at 0.747, which might mean that height is something like 75 percent heritable in terms of narrow-sense heritability. But there’s a little bit of a surprising finding here, which is that there’s an intercept. You know, as you’d think, as your familial genetic relationship goes to zero, the phenotypic correlation ought to go to zero, right? If you’re sharing close to zero genetics, you should have close to zero phenotypic correlation. So, you’d really expect this red line to go through the point (0, 0), and yet, surprisingly, this red line seems to not go through the point (0, 0). There’s this large intercept, and there’s a lot of speculation, different hypotheses why what could this be due to. Probably the simplest explanation is shared environments; that even people who are just cousins, they come from a similarly type of socio-economic background or something like that that makes them more disposed to be taller or more disposed to be shorter as a consequence of environmental effects. And that is one possible explanation for why this line does not go through (0, 0).\nAnd there are more complicated explanations as well, involving genetic ancestry and assortative mating, and so on and so on. But we just have to keep in mind as we think about narrow-sense heritability that it is complicated, and there’s a lot of room for different sorts of confounding and complex effects as we try to estimate this quantity that I have defined.\nProbably the most popular way right now to try to estimate narrow-sense heritability is using the classic monozygotic (identical) and dizygotic (fraternal) twin study. The idea here is, the hope (not the guarantee, but the hope) is that monozygotic twins versus dizygotic twins have basically the same amount of shared environment, and really the only thing that differs between monozygotic twins and dizygotic twins is that monozygotic twins share 100% of their genetics, whereas dizygotic twins share only 50% of their genetics.\nSo, the difference in how much phenotypic correlation you see between monozygotic twins, on one hand, versus dizygotic twins, on the other hand, should give you a sense of how, you know, how heritable the trait is. And this twin-based approach is really the, it’s widely considered as the gold standard in ways to estimate narrow-sense heritability. And it should work unless there’s a difference in the amount of shared environment between monozygotic twins, on one hand, and dizygotic twins, on the other hand, for example, due to effects in the womb or due to sort of societally, you know, effects or family influences, differences in the ways that monozygotic twins versus dizygotic twins are treated. And we have to take seriously this possibility that there could be still some confounding due to differences in the amount of shared environment between monozygotic twins and dizygotic twins, whereby monozygotic twins have more shared environments, and that might actually inflate the twin-based estimates of heritability, and I will provide some evidence later in the talk that this is, in fact, likely to be the case, that the twin-based estimates may, in fact, be inflated.\nAll right, and so, keeping in mind that these estimates might be inflated, there have been a lot of work published. You know, that probably the broad and recent reference worth looking at is Polderman et al. 2015, Nature Genetics. But, for example, for height, the most quoted value from twin studies is 0.8. And then, you know, recent studies of cancer have produced estimates of around 0.3 to 0.5. Generally speaking, most of the diseases and complex traits that people tend to be interested in (height is a bit of an outlier, that’s extremely heritable), but most of the diseases and complex traits that people tend to be interested in seem to have narrow-sense heritability estimates from twin studies that might be on the order of around 0.5 or a little bit less than 0.5, typically somewhere in that range.\nAll right, so that’s a brief introduction to narrow-sense heritability, and now I’m going to delve just a little bit more into the missing heritability problem from genome-wide association studies that I’ve already defined early in this talk.\nSo, again, the missing heritability originally was defined as this gap between what we discover from the genome-wide significant loci for GWAS, versus the estimated narrow-sense heritability from twin studies. And again, using schizophrenia as an example, 3% is the heritability explained by the 108 GWAS loci from PGC 2014, Nature, versus, on the other hand, 64% the narrow-sense heritability that’s been estimated from twin studies. That’s a very large gap. And in the early days of GWAS, people were really very interested to try to understand what the cause of this gap is. Why doesn’t GWAS find everything? Why is it so incredibly far away from finding everything?\nAnd there are a lot of explanations out there, but these are the four that I think are most worth discussing in this MPG primer format. So, I’m gonna go over these four explanations one at a time.\nSo, the first explanation is common causal variants of exceedingly low effect size. And so, we could imagine some different possible values of the genetic architecture of a disease or trait. One possible genetic architecture is that you have ten common risk variants, which each explained about 1/10 of the heritability, and this is what people thought, you know, a long time ago, way back, maybe early 2000s. This is what people thought disease architectures might look like. There’d be 10 loci you’d run a GWAS, you’d find the 10 loci, end of story, and that’s what people were expecting.\nBut it might be more complicated than that. You know, there could be 10, there could be 100, or a thousand, or even 10,000 common risk variants that each explained a tiny, tiny, tiny, tiny proportion of heritability. And in this extreme case at the bottom of this slide where we have 10,000 common risk variants that each explained on the order of one ten-thousandth of narrow-sense heritability, maybe more in some cases and less in some other cases, you can imagine that even at a very large sample size, GWAS is going to be very underpowered to find them. And you might only find a very, very small fraction of the true common causal risk variants, and that’s why you’re only going to explain a very, very small proportion of the narrow-sense heritability, with the ones that you’re actually lucky enough or well powered enough to actually find. And this is what a lot of people believe, that schizophrenia happens to be an example of a particularly polygenic trait with a lot of causal variants, and so this may be what we’re looking at. We may be looking at something like 10,000 common risk variants that each need to explain a tiny, tiny, tiny proportion of heritability. And even at very large sample sizes, you’re going to be underpowered to detect most of them. You’re only going to detect the ones with the largest effects, or you’re going to detect a few because you get lucky or something like that, but most of those common risk variants, you’re just not going to detect them as being genome-wide significant in the GWAS.\nWe can even go to a greater extreme, the so-called the infinitesimal model. Nobody believes that this is realistic, but it can be useful to think about as a theoretical construct. And so, the infinitesimal model is a model in which all of the common SNPs in the genome are causal risk variants with causal effect sizes, something like, you know, normally distributed with mean 0 and variance h squared divided by M, where M is the number of SNPs. And so, there you’re sort of spreading the narrow-sense heritability across literally all the common SNPs in the genome. So, this is one very plausible explanation as to what is going on with missing heritability. We’ve got a lot of causal, a lot of common causal SNPs of tiny effects. That GWAS, even at large sample sizes, are not finding most of them. And we do know that there’s a lot of traits that are extremely polygenic. I’ve already said that schizophrenia is a particularly polygenic trait. This is a Manhattan plot from the blood pressure GWAS of Evangelou et al. 2008, Nature Genetics, and this is another illustration of an extremely polygenic trait which clearly has an extraordinarily large number of causal loci.\nSo, a second explanation that people have been interested in for a while is the explanation of rare and low-frequency causal variants. And we know that GWAS are not well powered to identify rare and low-frequency causal variants because the power sort of scales with the allele frequency. If you have a really rare variant, it doesn’t occur very often in the sample, and so you’re not going to be well powered to detect it. Mathematically, we know that at a specific per-allele fixed effect size, the power scales something like with the sample size times P times 1 minus P, where P is the minor allele frequency. And so, if you have a really rare variant, then you’re definitely not going to be well powered to detect it as being genome-wide significant in a GWAS, where you’re conducting single-marker tests. I mean, I’m not going to get into the topic of multi-marker gene-level tests, which is a topic for a different MPG primer session. And so, GWAS are not well-powered to identify these rare and low-frequency causal variants as being genome-wide significant, and maybe that’s where a lot of the missing heritability is, and that’s an explanation people have been interested in.\nBut recent work from Zeng et al. 2018, Nature Genetics, as well as other work from our group (Schoech et al.), are suggesting that there’s not really a lot of heritability coming from rare and low-frequency SNPs. So, according to Schoech et al., it’s less than 10% of SNP heritability (I know I haven’t defined SNP heritability yet, we’ll get to that later) but less than 10% of SNP heritability is coming from SNPs with minor allele frequency less than 1%. Now, there’s a little bit of a complicated story here involving negative selection. Under simplified assumptions where you don’t have any selection and you might have to also assume equal a constant effective population sizes across time. But under some assumptions (I’m not going to get into), you might expect that SNPs with MAF less than 1% ought to explain about 1% of SNP heritability. So, there is some excess here where they actually explain, according to this paper about 9% of SNP heritability, which is a lot more than 1%. And that excess, whereby 9% is more than 1%, is a consequence of the action of negative selection, which sort of causes really important SNPs that have important effects to, generally, those important effects are usually bad for the organism, and because they’re bad for the organism, those SNPs cannot rise to high frequency and stay as rare SNPs. So, you might expect that rare SNPs will tend to have larger effects, and in fact, that’s exactly what you see, and that’s why SNPs with minor allele frequency less than 1% explain somewhere around 9% of SNP heritability, which is a lot more than 1% of SNP heritability. But even so, at the end of the day, according to these papers, these rare and low-frequency SNPs are not really explaining a lot of heritability, and they are probably not the primary explanation for missing heritability.\nNow, to be clear, I don’t, I specifically do not want to say that rare and low-frequency causal variants are not important and we shouldn’t study them, rather, what’s going on here is that if you’re interested in heritability or in explaining a lot of heritability, then rare and low-frequency causal variants don’t contribute a lot. And by extension, if you’re interested in polygenic prediction, which kind of piggybacks off of heritability, then rare and low-frequency causal variants aren’t very important. But on the other hand, if what you’re interested in is discovering interesting disease biology that could lead to a drug target, rare and low-frequency causal variants may be really important. You may identify a rare coding variant that explains a minuscule amount of heritability, but it has a really biologically interesting sort of mechanism behind it that might lead to a drug target. Totally fine if it explains a minuscule amount of heritability if it’s going to lead to an actionable drug target. So, let’s just keep in mind that even though they’re not so relevant for heritability, they could be very important for disease biology and developing drug targets.\nAh, the next explanation that people… I see that there’s a Q&A. Diane, do you want to read out the Q&A?\nDiane: Thank you. Yes, um, I will do that now. So, the question from Now Son is: “How is the heritability of rare variants estimated for a GWAS study that is underpowered?”\nAlkes: Okay, so the question is, “How is the heritability of rare variants estimated for a GWAS study that is underpowered?” And the topic of SNP heritability that I will delve into in the third part of this talk delves into estimating the aggregate heritability contributed by all the variants in the genome. Maybe, I mean mostly what I’m going to talk about is estimating the heritability explained by collectively explained by all common variants in the genome. But something related to that that you could do is you could estimate the heritability explained by all rare variants in the genome. And so, these studies that I’ve quoted on this slide, Schoech et al., Zeng et al., and others, they’re doing something sort of like that. They’re doing something, they’re sort of extending the methods to estimate SNP heritability or heritability explained by all SNPs in the genome, which I will be talking about in the third part of this talk. They will be extending that to estimate the heritability explained by all SNPs in a particular minor allele frequency class, such as the heritability explained by all rare variants. And I will aim to return to this question in the third part of the talk when I talk about SNP heritability, and hopefully, what I just said will become more clear after I delve into what’s going on with SNP heritability.\nAll right, so moving on to explanation number three on my list, which is copy number variation, and it is in principle possible that copy number variants are biologically important and contribute a lot to heritability but are not well tagged by common SNPs. And there’s another question.\nDiane: I don’t see the question in the Q&A box.\nAlkes: Okay, in that case, I will keep going.\nDiane: Thank you.\nAlkes: It’s possible that the copy number variants could be important for disease, but their effects may not be so well tagged by SNPs. So, if you’re only looking at SNPs, you won’t see them. And we have to take that possibility seriously. Way back in the year 2010, this paper that I’ve cited suggested that common copy number variants that could be typed on existing platforms (and that’s an important qualification) did not contribute much, but that may be more about the technology than about the biology. And more recent work of Sudmant et al. 2015 is suggesting that structural variants are enriched on haplotypes identified by GWAS. So some of the common SNPs that we identify as genome-wide significant in GWAS may be tagging causal copy number variants. So, I don’t think we have a conclusive answer to this question right now as to whether untyped and untagged or only partially tagged copy number variants are responsible for a lot of the missing heritability.\nI would like to go out on a limb and hypothesize if this story might maybe be sort of similar to this story with rare variants. And some of these copy number variants, of course, are likely to be rare because you can’t, you know, you can’t knock out a big chunk of the genome and have it not have a huge effect, which would then, due to negative selection, keep the variant rare. I hypothesize that it might be true that, just like rare variants, copy number variants might not explain a lot of heritability and might not be important for quantifying heritability or, by extension, for polygenic risk prediction. But, on the other hand, they might be really important for understanding disease biology, and they might lead us to examples where we understand the biological mechanism and can develop drug targets. So, I hypothesize that they might be not so important for heritability, but at the same time, very important for disease biology and drug targets.\nAnd then finally, the fourth explanation that I wanted to talk about is the possibility that narrow-sense heritability was overestimated in the first place. And I already alluded earlier in this talk to the possibility that narrow-sense estimates of narrow-sense heritability might be inflated due to shared environment. And there are some other complicated explanations whereby if you have G by G interaction (which is not supposed to be included in estimates of narrow-sense heritability, which is defined as including only additive effects and not G by G interaction effects), then according to Zuk et al. 2012, that could inflate your twin-based estimates. So, there are people out there who believe that the twin-based estimates may be inflated, despite the fact that they’re sort of the best, well, until recently at least, they’re the best thing we have.\nI’d like to highlight this paper of Young et al. 2018, Nature Genetics, which introduced a new method called relatedness disequilibrium regression, which is predicated on having at your disposal a really large data set with a lot of related individuals, which those authors happened to have because they analyzed the deCODE Genetics data set from Iceland. And they claimed that they have an estimation procedure that is robust to these types of effects of shared environment. And they claim that the narrow-sense heritability of height is only about 0.55. And we have to take those claims seriously. They are, generally, fairly consistent with a couple earlier studies of Zaitlen et al. from our group. And I think people at this point in time tend to believe that 0.8 for height, for the twin-based estimates of narrow-sense heritability of height, that 0.8 probably was an overestimate, and the truth might actually be some number closer to about 0.6 or so. I think that’s what people, most people, believe.\nAnd so, summing it all together, I think, in terms of heritability, the two explanations for missing heritability that are most prevalent are: Number one, you’ve got common variants of exceedingly low effect size that cannot be detected by GWAS as being genome-wide significant. And number two, the twin-based narrow-sense heritability estimates are somewhat too high. Obviously, this is not going to get you a slightly overestimated narrow-sense heritability is not going to explain a difference between 0.03 and 0.64 for schizophrenia, which is a humongous difference, and that’s probably more dominated by number one.\nSo where does this leave us? Well, there’s a sort of fundamental question that was asked originally in a landmark paper of Yang et al. 2010, Nature Genetics, from Peter Visscher’s group. Maybe we can try to estimate the heritability explained by all the SNPs in the genome, or maybe all the common SNPs in the genome, not just the SNPs that are genome-wide significant, but in fact, all the SNPs collectively. Even if we don’t know which ones are the causal ones, we can still try to estimate the heritability jointly explained by all of those SNPs together. And so, that’s this concept of SNP heritability, which is really the main focus of this primer. And I will start to now delve into that. And so, the distinction between, on the one hand, narrow-sense heritability, and on the other hand, SNP heritability, the heritability explained by SNPs specifically, really sort of rests on the distinction between two important ideas:\nIBD or identity by descent, and IBS or identity by state. So, identity by descent or IBD means two people are related; they have similar genetics. IBS or identity by state means two people who are not related may still, by chance, have genetics that are a little bit similar, and you might be able to do something with that to learn about complex trait architectures.\nSo let me delve into a little bit of detail. Let’s start with IBD. Let’s suppose you take two individuals who are related. So, the two individuals I depicted on this slide are brothers, and we could ask ourselves, what is the proportion of the genome that these two brothers share, identical by descent, which means inherited from a recent common ancestor? The answer, well, the answer is not exactly 0.5. The answer is approximately 0.5 because it can vary a tiny bit from one SNP to the next, but an expectation is 0.5. And you can imagine if you have sort of a large set of related individuals who you’re analyzing genetic data from, you can build an IBD matrix, quantifying the IBD for each pair of individuals. And for a pair of individuals who are brothers, the entry would be 0.5 because 0.5 is the IBD of those two brothers. And this K matrix or this IBD matrix, you could use it to estimate narrow-sense heritability.\nAnd no worries if you don’t feel like sorting through all these equations, but this is what it would look like in terms of math. That you have a vector Y of phenotypes, and you’re sort of decomposing the phenotypic variance into the part coming from genetic effects, called U, and the part coming from environmental effects, called epsilon. And the variance of U, the genetic effects, is proportional to the IBD matrix. And where’s the environmental effects? Well, that’s captured in the epsilon. And the variance of U, the genetic effects, isn’t it sort of proportional to the IBD matrix? And where’s the environmental effects? On the other hand, if we make a strong assumption of no shared environment, then the environmental variance is proportional to the identity matrix. That is saying we have no cross terms at all between distinct individuals. And then you sort of parameterize the overall phenotypic variance of V in that way, estimate these parameters, and estimate the narrow-sense heritability.\nThere are methods, you know, max likelihood methods or restricted maximum likelihood methods for estimating that parameter, or actually, you’re estimating the two parameters, Sigma square G and Sigma square E from the previous slide. I’m going to just kind of skip most of the mathematical details. You can read about it in these various papers. And you know, no worries if you’re not following all the math.\nSo, on the other hand, we could have two unrelated individuals, such as the two individuals depicted on this slide. And even though these two individuals are completely unrelated, it is possible that they might be just a little bit more genetically similar than average or just a little bit less genetically similar than average, just by statistical chance. And we can sort of quantify that using the equation on the right half of this slide. Where we can compute something, this is sometimes called the genetic relationship matrix or it’s sometimes called could be called an IBS matrix. But it’s basically you just sort of like compute the correlation across SNPs between these individuals’ genotypes, suitably normalized. Typically, if you have two unrelated individuals, you can practically guarantee that that number is going to be really, really close to zero. But in these standardized units, in which the mean is going to be zero on average, it might be a tiny bit bigger than zero, like 0.004, or it might be a tiny bit less than zero, like -0.004. And you might expect that if you have a heritable trait, then two individuals who just by chance are a tiny bit more genetically similar than average ought to have slightly concordant phenotypes, whereas on the other hand, individuals who are slightly less genetically similar than average ought to have slightly less concordant phenotypes. And this is something that you can use to estimate the heritability explained by SNPs or heritability explained by genotyped SNPs in the way that it was originally employed. And the real question here is, what is the set of SNPs? Well, the answer is related to the SNPs that you used to compute this genetic relationship matrix.\nIf you only use SNPs on chromosome 1 to compute the genetic relationship matrix, then you’ll get an answer that has something to do with SNPs on chromosome 1. If you use all common SNPs to compute the genetic relationship matrix, then you’ll get an answer that has something to do with all common SNPs, and so on, and so on, and so forth. And so, once again, we can model the phenotypic covariance, V, as a sort of linear combination of this genetic relationship matrix or IBS matrix, which is the genetic part coming from SNPs, and then everything else, which is the environmental part, or strictly speaking, it’s everything except the genetic part coming from SNPs.\nAnd then we can estimate this quantity called SNP heritability. Again, reverting to a little bit of math here, I feel that if we talk about estimating a quantity, we really want to define that quantity in the entire population. And this is a formal definition of that entire quantity in the population. We can skip the mathematical symbols here, and in words, this is just the maximum amount of phenotypic variance that you could explain using any linear combination of SNPs. And that’s the definition of SNP heritability. That’s a definition in the entire population. That definition does not depend on a particular sample, although it does depend on which set of SNPs you’re looking at. If you’re looking at, you know, just SNPs on chromosome 1, the answer will be smaller than if you’re looking at all SNPs in the genome. Or you might be looking at just a few hundred thousand genotyped SNPs or a large number of imputed SNPs or common SNPs or common and rare SNPs or whatever, and the answer will be different in each case. But this is a quantity that you can define in the entire population. And then, after you’ve defined it in the entire population, then you could obtain an estimate of that quantity, an estimate with noise, of that quantity in a finite sample.\nAnd this is what was done in this landmark paper by Yang et al. 2010, Nature Genetics. And this is the same math that I showed on an earlier slide for IBD. This is just the same math with the IBD matrix K replaced with the IBS or GRM matrix called capital A. And we’re now intuitively thinking about this in terms of unrelated individuals and SNP heritability, but mathematically, all the math is the same, and all the maximum likelihood or restricted maximum likelihood computations are the same.\nSo, I’m going to choose to not focus on the math in this talk, and I’m gonna focus more on intuition. Now, we have these two quantities: H squared, that’s total narrow-sense heritability, and that corresponds to the question: How phenotypically similar are two relatives?\nThere’s a question?\nDiane: Yes, thank you. I think the question is asking, or the question from Anna Lewis is asking, the following: When you say ‘the entire population,’ do you mean the entire human population?\nAlkes: Okay, so this is a good point. This is something I’ve really glossed over, and you know if I wanted to delve deep into the population genetics, that could be a separate MPG primer. But generally, when people talk about a population, they’re talking about a population of a particular continental ancestry, one example: the population would be European Americans. Another example: the population would be the British ancestry individuals from the UK Biobank, and so on and so forth. Now, if I want to be a strict population geneticist, I might define a population as a set of pandectically randomly mating individuals. Now, nobody believes that European Americans are a set of ethnically randomly mating individuals. Nobody believes that British ancestry individuals from the UK, or you know, East Asians from Japan, or you know, Nigerians, or whatever population you’re talking about, nobody believes that that’s strictly speaking a pandectically mixing set of individuals. We might just choose to pretend that’s the case as a sort of approximation. And I mean, I mentioned earlier at the beginning of this talk that there are opportunities for all sorts of confounding. One sort of confounding has to do with confounding due to population stratification. Do the differences in genome-wide ancestry amongst different individuals? That, again, could be a topic for another MPG primer. We should be aware of the possibility that if you are studying a population, such as European Americans or any population in which there are differences in genome-wide ancestry amongst different individuals in that population, we have to be a little bit careful, because there is the possibility for confounding due to population stratification. But the short answer is we’re thinking about a specific population of a specific continental ancestry.\nAll right, and so getting back to this slide, narrow-sense heritability corresponds to the question, you know, how phenotypically similar are two relatives? And it’s kind of implicit that two relatives could be phenotypically similar due to, like, whatever genetics they happen to be carrying. Maybe they’re carrying the same rare variant, maybe they’re selling the same copy number variant, maybe they’re carrying the same common variant; it’s going to include all of that. On the other hand, we have a smaller quantity, SNP heritability. Originally, this was called the heritability explained by genotyped SNPs because people liked to estimate this just using genotyped SNPs before imputation became kind of universally popular. So that’s why you’ll see this terminology on some of these slides, but you can apply this to any set of SNPs.\nAnd this, this corresponds to the idea: if I have two unrelated individuals and I use a specific set of SNPs (and the set of SNPs is important to quantify how genetically similar they are just by chance), then how phenotypically similar will they be? And again, this is a function of a very specific set of SNPs. And because it’s only capturing the heritability causally explained by a very specific set of SNPs, in general, it’s expected to be lower than the total narrow-sense heritability, which is capturing additive effects from all possible genetic variants.\nAll right, and then finally, there’s a third quantity that has already appeared in this talk, and I’m going to call that H squared GWAS. H squared GWAS is the heritability explained by genome-wide significant SNPs. And I mean if you’re not worried about LD (linkage disequilibrium), then it’s basically just the sum of the variants contributed by each of your genome-wide significant SNPs in turn. But it’s a little bit more complicated if you have LD, and strictly I can define it as the maximum proportion of phenotypic variance that you can explain with any linear combination of genome-wide significant SNPs. So, just focusing on those SNPs that are genome-wide significant, and that’s the number that was in a particular study at 0.3 for schizophrenia. And I have to be a little bit careful when I say that I’ve defined this; it’s not really a true population-level parameter because it’s a function of which SNPs pop up as being genome-wide significant in a particular GWAS at a particular sample size. So it’s really a function of a specific GWAS that identifies a specific set of genome-wide significance SNPs. It’s really a function of what the set of genome-wide significance SNPs is in a particular study.\nAnd so now we have, we have sort of an inequality with three different quantities. We have H squared GWAS, that’s the smallest number, which is just the genome-wide significant SNPs, with H squared G, that’s the SNP heritability, that’s all the SNPs in the genome. Maybe all the genotyped SNPs, maybe all the common SNPs, whatever flavor of SNPs you want to choose to study at a particular point in time, but it’s, in some level, the heritability explained by all the SNPs in the genome, including all the SNPs that are not genome-wide significant but might contain some signal. And then finally, the largest quantity, total narrow-sense heritability, that’s the additive heritability explained by all genetic variants, which includes not only every possible type of SNP but other types of variants as well, copy number variants, and so on and so forth. So this has been a little bit of a dry technical discussion.\nNow, we might want to look at some real data to sort of get a sense of having some intuition of how this works in practice. I’m going to start with height, which is probably the trait most well-studied by geneticists. In terms of narrow-sense heritability, well, from the twin studies, we have 0.8 (as I mentioned earlier, that’s probably an overestimate, and we’re probably closer to 0.6, but for now, I’m just going to say 0.8 from the twin-based studies). Then we have SNP heritability. This is the quantity that Yang et al. (2010 Nature Genetics) in their landmark paper estimated at 0.45. Then just the heritability explained by GWAS SNPs, well, way back in the year 2010, that was at about 0.10, although it’s gone up a little bit with some studies (Wood et al., 2014 Nature Genetics and Lango et al., I forget which year, 2018 or 2019, Human Molecular Genetics). It’s a little bit higher now, but you can see that this concept of SNP heritability can explain most, not all, but it can actually explain most of the missing heritability. And this goes back to explanation number one of my four explanations that I quoted earlier, where, there’s just a lot of SNPs with really small effects that GWAS do not detect as being genome-wide significant, and if you define and estimate this quantity H squared G, which quantifies the heritability explained by all SNPs or all SNPs of a particular category, such as genotype SNPs, not just the ones that are genome-wide significant, then you get a much, much, much larger number, like 0.45, than just the genome-wide significant ones, such as 0.10.\nI like to use the terminology “hidden heritability,” as do others, to explain this gap between H squared GWAS and H squared G. So, heritability, we know it’s there, we know it’s in those SNPs, we just don’t know which SNPs it is because we don’t have enough sample size to do an infinitely powered GWAS to figure out which are all the causal SNPs in the genome. But we know that heritability is there; it’s just kind of hiding. And then, on the other hand, there’s heritability that’s still missing, which is this gap between 0.45 and 0.8, although, as I said a moment ago, maybe it’s really a gap now between 0.45 and 0.6 if we believe 0.6 for height now, and that’s heritability that’s still missing, and then we don’t currently have an explanation for.\nSo, this is just a generalization published by Yang et al. 2011 to some other quantitative traits, and I’m not going to go over this in detail other than to say that, of course, these traits are less heritable than height, but qualitatively the story is pretty similar to what’s going on with height.\nDiane: We do have one question in the Q&A that I think it’s maybe timely at this moment, which is from Matthew Worman who says, “Hey Alkes, if one were to use a p-value of 10 to the minus 5 instead of 10 to the minus 8 for GWAS, what fraction of SNPs exceeding this less stringent threshold are false positives?”\nAlkes: Okay, so I do think it is of interest to say, “Hey, what happens if I choose some other threshold, let’s say 10 to the minus 5? And I could define a set of SNPs which are SNPs that come in at P less than 10 to the minus 5 in my GWAS”. And I guess I would like to say three different things about that.\nWell, the first thing that I want to say is that, just as is the case with the set of genome-wide significant SNPs at five times ten to the minus eight, the nature and characteristics of the SNPs that would come in at ten to the minus five is very much a function of your sample size. I mean, it’s a function of a lot of things, like the genetic architecture of the trait and so on and so forth, but it’s really very much a function of your sample size. So that’s sort of like very dependent on a particular study, you know, the SNPs that come in at P less than ten to the minus five have a particular set of characteristics. That’s the first thing that I wanted to say.\nThe second thing that I wanted to say is that, although it might depend on that particular study, on the particular genetic architecture, and so on and so forth, sample size, intuitively, it’s appropriate for us to think intuitively that most of the SNPs that come in at P less than ten to the minus five are going to be false positives. In other words, they’re not truly causal variants or they’re not truly tagging causal variants. In general, that is what I suggest is likely to be the case.\nAnd the third thing that I want to say is that even if most of them are false positives, a very important fraction of them will be true positives. Those true positives will be contributing more heritability. So if I define something called H squared GWAS less than ten to the minus five, which is the heritability explained by a particular set of SNPs that comes in at P less than ten to the minus five in a particular GWAS, then that will lie somewhere in between H squared GWAS, which is just the sum of genome-wide significant ones, and H squared G, and it would probably be substantially greater than each squared G was, because even though, I would say that in general, we expect that most of those coming in at less than ten to the minus five would be false positives, a very, very important fraction of them would be true positives, and that would get you a lot closer to H squared G, the true SNP heritability. Although I’m still going to say that, in general, you’d expect that there’s still a lot more signal that is not even captured by P less than ten to the minus five, and you’d still have a gap between H squared GWAS 10 to minus 5 and H squared G. So, H squared GWAS 10 to the minus 5 might lie somewhere in the middle between H squared GWAS and H squared G, with the details that depend on how polygenic the architecture is and what the sample size is and so on and so forth.\nAlright, very good. So, generally speaking, H squared G is less than H squared (it’s the narrow-sense heritability), that could be due to rare and low-frequency variants, as well as other types of variants like copy number variants, that are not captured by the SNP heritability of a very specific set of SNPs, usually either genotyped common SNPs or genotyped and imputed SNPs that are mostly common or something like that, that you’re estimating the SNP heritability of.\nAnd then, on the other hand, we have a completely different phenomenon: H squared GWAS less than H squared G. So that’s H squared GWAS less than SNP heritability, where we need larger GWAS sample sizes, or as Matt alluded to, maybe we need just a less stringent genome-wide significance threshold, or whatever. I mean, H squared G is just sort of in the limit of, you know, any SNP with P less than or equal to 10 to the 0 in your GWAS, right? And so in the limit of large sample sizes, we might hope that as our GWAS gets hugely large, then we’ll identify all the associated SNPs, and H squared GWAS will approach H squared G. But there is a bit of a caveat here that even if you’ve got a humongous sample size, like 758,000 samples for blood pressure, if it’s a really polygenic trait like blood pressure, 758,000 samples is not enough, and it’s not even remotely close to being enough, and H squared GWAS at 0.06 may still be a lot lower than this SNP heritability of 0.21.\nAlright, so I’ve been saying repeatedly that SNP heritability is a function of the set of SNPs, and that’s definitely true, but we should also keep in mind that it’s a function of the particular population that we’re looking at. And again, as I mentioned earlier, one population that some people sometimes study is British ancestry individuals from UK Biobank, which is mentioned on this slide. And another population that people sometimes study is European Americans, and that happens to be the population that was analyzed in reference two on this slide.\nAnd it turns out that if you estimate SNP heritability in British ancestry UK Biobank samples, you consistently get higher numbers than you do if you estimate heritability in other cohorts, such as European American cohorts. There are different possible explanations for this, but probably the most likely explanation is that SNP heritability is truly higher in a British ancestry UK Biobank population than it is in a European American population. That might be due to just less environmental variance in a set of British ancestry individuals residing in the UK, or more precisely, less environmental variance in a set of British ancestry individuals residing in the UK that the UK Biobank captures, which is actually not a perfectly random subset of British individuals, it’s British individuals who choose to respond to surveys and might have higher than average SES and so on and so forth. And there seems to be less environmental noise and thus larger SNP heritability in that population than in, for example, a general European American population. So let’s just keep in mind that SNP heritability depends on the population studied as well as on the set of SNPs. And, of course, I should say that total narrow-sense heritability also depends on the particular population that you’re studying.\nOne question that comes up a lot is, which assumptions do we need for estimates of SNP heritability to be valid? And one thing that the world now understands very well is that LD (Linkage Disequilibrium) dependent architectures can lead to bias in estimates. And because I see that I’m running out of time, I’m going to choose to just gloss over the details of that. I want to say very carefully what I mean by LD-dependent architectures. LD-dependent architectures - we’re not talking about tagging here. Of course, we know that a SNP that’s not a causal SNP can tag a different SNP that is a causal SNP. But LD-dependent architectures mean that causal effects vary with the amount of LD a SNP has. This could be just due to minor allele frequency - that’s kind of trivial - that rarer SNPs explain less heritability per SNP because they’re rare and they also have less LD, that’s kind of trivial. But even if you condition on MAF (minor allele frequency), even in a specific fixed MAF, we know that low LD SNPs actually have larger causal effect sizes than high LD SNPs. The reasons for this are quite complicated, we believe they have something to do with negative selection, but it turns out that that violates some of the assumptions of some of these estimation methods and it can lead to biases in estimates. It’s something that we have to watch out for, and it’s something that the world now understands well and knows that we have to watch out for. I’m just going to leave it at that and gloss over some of these other slides.\nAnd then a second question that comes up, which is the second bullet point now on this slide over here, is the question: Is it okay if effect sizes have a non-infinitesimal distribution? And so I haven’t really emphasized this point, but the estimation procedures that I’ve described, that were used to compute the genetic relationship matrix from unrelated individuals and then use restricted maximum likelihood to fit variance components to estimate heritability, there are some assumptions underlying that about an infinitesimal architecture, where infinitesimal, as I defined earlier, refers to a genetic architecture in which all SNPs are causal with a normal or Gaussian-distributed distribution of causal effects. The methods assume that. But we now know that even though that assumption is clearly an incorrect assumption, that does not lead to any bias. It’s merely the case that you might be leaving some precision on the table, whereby sophisticated methods that account for the fact that the distribution of causal effects is a little bit more sparse than that can actually produce more precise estimates. But that’s an issue of precision, of getting a lower standard error, more precise estimate. It’s not an issue of bias, and this does not lead to bias, and that is well understood.\nAll right, um, I do want to mention at least briefly the important point that whereas I’ve been talking about the math in terms of quantitative traits like height, people are very interested in case-control traits because most of the disease traits that we’re really ultimately interested in terms of medical actionability and drug targets and treatment or whatever it is we’re aiming for, you know, even polygenic prediction, we’re generally aiming for it in the context of medically important disease traits like schizophrenia or type 2 diabetes or whatever disease you like to study. And we need a little bit more math to get this to work right, and the type of math that is most commonly used is called the liability threshold model. And again, I’m going to try to avoid going over the mathematical symbols here, but I want to provide the intuition because this is so important. The liability threshold model models that there’s some unobserved underlying continuous number called the liability, and if the liability is above some number, you have the disease. Fortunately, we have an example which is type 2 diabetes, which provides some intuition here. Where maybe you’re doing a GWAS of type 2 diabetes, and all you know is that somebody has told you whether or not they have type 2 diabetes or whether or not they’ve been told by a doctor that they have type 2 diabetes. But there’s an underlying continuous quantity which we call the liability, which is your fasting blood glucose level, which is one of the diagnostic criteria for type 2 diabetes, and we could think of it as, you know, we don’t get to see the liability, but if your fasting blood glucose is above some level, then you have type 2 diabetes. And so what we want to do is we want to do some math that operates on this unobserved continuous-valued liability, and that’s what liability threshold modeling is about.\nAnd I’m not going to go into the details, but all the work that’s been done in this space on SNP heritability has relied on the liability threshold model. And contingent on using the liability threshold model, it has generally produced a qualitatively similar story to the story that I communicated earlier for height. And I’m not going to go into those results in detail.\nI do want to mention briefly that I’ve been talking mostly about methods that use individual-level genotypes. There’s been a lot of interest dating back to the year 2012 in methods that instead only require summary association statistics as input, and there are methods now that do input summary association statistics to learn about various flavors of heritability and SNP heritability.\nI just want to provide one point of intuition that this is related to the fact, an important observation made by Yang et al. in 2011, that the average chi-square statistic in a GWAS is very closely related to SNP heritability. You should not be surprised if your average chi-square statistic in a GWAS is above 1; that does not imply that you have confounding, but rather, that is directly related to the amount of polygenic signal in that GWAS. An increasingly sophisticated set of methods can use that observation to estimate heritability from summary statistics, and I’m going to gloss over all of the details of that.\nSo, conclusions: we can use family data, such as twin studies, to estimate the narrow-sense heritability of a trait. For even now, in the year 2020, most GWAS have the property that the genome-wide significant loci that they discover do not explain all the heritability, we have missing heritability, and there’s this concept of SNP heritability that can really close that gap.\nSo, these are some of the bonus topics, really within the heritability space, that I think are super interesting and that I have some bonus slides on, and, as expected, I’m not going to have the time to delve into in detail, but you can check out those bonus slides if you want to. As you know, the slides were sent out to the whole group, and I’d like to acknowledge all the members of my group who have figured all this stuff out and explained it to me. Also, I’d like to put in a brief advertisement for my course, “Advanced Population Medical Genetics,” which would be offered in spring 2021. With that, I’ll take any additional questions that people may have.\nDiane: Thank you very much. OK, as there are no questions in the Q&A box at the moment, please do post to the whole group. Please do post any if you have further questions.\nAlkes: There was a question that was just asked.\nDiane: Oh great, um, it’s from Matthew Worman who’s commenting: “Thanks, Alkes. I really appreciate the clarity your presentation, so I will second to that, and thank you. And I do have one question, which is, I’m curious about the with the identity by state definition, the concept of unrelated individuals. Does this require that the variant shared by the two individuals be of independent origin or …?”\nAlkes: Okay, so thank you for raising that point, and unrelated individuals, that this is another term, just like the term “population,” where it’s a little bit tricky to define it, and if you define it strictly, then it’s probably nothing like what you have in real life. So if I want it to be real strict about it, I could say unrelated individuals is like, let’s think, I have a simulation, and in my simulation, I’ve got some minor allele frequencies, and I’m, you know,or haplotype frequencies, generating the genome of individual number one, and then completely independently and generating the genome of individual number two, and the whole process is completely independent. But in real life, of course, it’s not like that. You know, you take any two people who think they’re unrelated to each other, no, you could probably go back, you know, five or ten generations and find some common ancestor; that’s usually the way that it works. So the detailed answer to that question is, if you’ve got cryptic relatedness, you’ve got this set of individuals from a GWAS; they’re supposed to be unrelated to each other, you don’t think there are any siblings in there, but there’s probably some cryptic relatedness. I mean, I believe that Yang et al. (2010) Nature Genetics came up with some threshold that I think might have been 0.05, or maybe they later they changed it to 0.025 or something like that. And, as long as you apply this threshold where you stringently check that there are no two individuals in the data set who are related at a level greater than 0.05, or greater than 0.025, then it is hypothesized that the method is sort of robust to that. And then, even though the people are not strictly unrelated, you should be okay.\nNow, on the other hand, what if you have a dataset that consists of like related and unrelated individuals, and you don’t want to throw away half of them because then you’d be zapping your power? Then, what do you do? Well, it turns out you can fit two variance components to jointly estimate narrow-sense heritability and SNP heritability. I’m not going to go over the details; there’s a reference (Zheitlin et al., 2013, PLoS genetics) from our group, so I think that, in summary, the shortest answer to your question is, as long as you make sure that there, you know, that any cryptically related people are only a little little bit related, then you’re kind of okay, and everything that I said still kind of holds.\nDiane: Thank you very much. There were three more questions posted, but I’m afraid we’re out of time because MPG talk will be starting shortly. So, I think we’ll keep those questions in reserve and share them with you Alkes if you, if you wish,\nAlkes: All are welcome to email me offline at aprice@hsp.harvard.edu.\nDiane: Great, thank you so much for your talk, and thanks to the audience for joining in today. So, thanks, Alkes."
  },
  {
    "objectID": "chapter9.html",
    "href": "chapter9.html",
    "title": "Chapter 9: Advanced Topics",
    "section": "",
    "text": "Chapter goals:\n\nGain a more extensive understanding of advanced genetic analyses.\nUnderstand what a copy number variant is and methods for defining CNVs in your data.\nUnderstand the concept behind Mendelian Randomization and methods for running MR.\nGain a comprehensive understanding of Genomic Structural Equation Modeling (Genomic SEM) and methods for performing Genomic SEM.\nUnderstand gene x environment interactions and how they are measured.\nGain understanding of Twin studies and how a twin-based analysis is run.\nUnderstand how drug target analyses and association tests are run on genetic data, and how this can help with the development of therapeutics for various disorders.\n\n\n\n9.1 Copy Number Variation\n\nThis video by Dr. Howrigan explains copy number variation: what is a copy number variant, and how is it detected in genetic data. Additionally, examples of CNV analyses demonstrate what a CNV file format looks like, as well as output from CNV analyses, and how to perform CNV burden and association testing on that data.\nThe paper referenced in this talk:\nMarshall CR, Howrigan DP, et al. Contribution of copy number variants to schizophrenia from a genome-wide study of 41,321 subjects. Nat Genet. 2017 Jan;49(1):27-35. doi: 10.1038/ng.3725. Epub 2016 Nov 21.\n\nTitle: How to run Copy Number Variation (CNV) analysis\nPresenter(s): Daniel Howrigan, Broad Institute\nLevel: Intermediate\nLength: 20:03\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n9.2 Mendelian Randomization\n\nThis video from Dr. Smith gives a very brief and simple explanation of Mendelian Randomization, and how it is an important tool in detecting causality.\n\nTitle: A two minute primer on Mendelian Randomization\nPresenter(s): George Davey Smith, TARG Bristol\nLevel: Beginner\nLength: 2:16\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n9.3 Genomic Structural Equation Modeling\n\nThe following six videos by Drs. Grotzinger and Nivard give a comprehensive look at Genomic Structural Equation Modeling (Genomic SEM) for examining shared and distinct genetic relationships across multiple disorders. The first two videos give an introduction to the concept and basic usage of Genomic SEM, while the latter four videos dive deeper into the methods and analyses that can be performed with this tool.\n\nTitle: Genomic Structural Equation Modeling: A Brief Introduction\nPresenter(s): Andrew Grotzinger\nLevel: Beginner\nLength: 10:43\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Short Primer on Structural Equation Modeling (SEM) in Lavaan\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 11:34\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: GenomicSEM: Input/Explaining how S and V are estimated and what they are\nPresenter(s): Michel Nivard\nLevel: Intermediate\nLength: 23:18\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Working through examples on the Genomic SEM wiki one by one: munge, ldsc, usermodel functions\nPresenter(s): Michel Nivard\nLevel: Intermediate\nLength: 22:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to tutorial GitHub here and Wiki here.\n\nTitle: Multivariate GWAS in Genomic SEM\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 30:08\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Using Genomic SEM to Understand Psychiatric Comorbidity\nPresenter(s): Andrew Grotzinger\nLevel: Intermediate\nLength: 1:01:02\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n9.4 Interactions with Environmental Factors\n\nIn this first part of Section 9.4, Dr. Lambert gives an introduction to the concept of interaction terms for statistical analyses: understanding how and when to use them, and how to interpret these terms. The first video gives a brief introduction, and the second video gives an introduction to interaction terms when using continuous variables.\nIn the second part of this section, Dr. Westerman gives an introduction to gene-environment interactions in the context of psychiatric genetics, describing how GxE interactions can be detected in genetics data, how statistical power can impact the detection of GxE interactions, and describing the difference between additive and multiplicative interactions.\n\nTitle: Dummy variables: interaction terms explanation\nPresenter(s): Ben Lambert\nLevel: Beginner friendly\nLength: 4:35\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Continuous variables: interaction term interpretation\nPresenter(s): Ben Lambert\nLevel: Beginner friendly\nLength: 4:53\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\nTitle: Gene-environment interaction analysis\nPresenter(s): Kenny Westerman, Broad Institute\nLevel: Intermediate (content will be easier to those with a statistical genetics background)\nLength: 42:28\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n9.5 Family-based analysis\n\nIn this video Dr. Maes describes univariate (one phenotype) methods of Twin modeling to determine familial genetic and environmental traits using OpenMx software.\n\nTitle: Univariate/MonoPhenotype Twin Modeling in OpenMx\nDescription:\nPresenter(s): Hermine H. Maes\nLevel: Intermediate\nLength: 18:21\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to OpenMx software.\n\n\n\n9.6 Therapeutic Implications\n\nIn the below video, Dr. Whirl-Carrillo gives a comprehensive overview of pharmacogenetics methods using the example of the Pharmacogenomics Knowledge Base (PharmGKB) database, which is a large database of clinical information, drug label annotations, and curated pathways for annotating genes. This talk describes how this database was put together, and how it can be used for performing pharmacogenomics analyses and augmenting genomics analyses.\n\nTitle: Pharmacogenomics knowledge for personalized medicine\nPresenter(s): Cristina Rodriguez-Antona, Michelle Whirl-Carrillo\nLevel: Intermediate\nLength: 44:27\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to the PharmGKB website."
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "Chapter 8: Post-GWAS bioinformatics",
    "section": "",
    "text": "Chapter goals:\n\nBe able to define SNP heritability and understand how it is calculated\nUnderstand the concept of “missing” heritability and explanations for why it is “missing”\nUnderstand methods of determining genetic overlap through calculations of genetic correlation"
  },
  {
    "objectID": "chapter8.html#magma",
    "href": "chapter8.html#magma",
    "title": "Chapter 8: Post-GWAS bioinformatics",
    "section": "MAGMA",
    "text": "MAGMA\nTitle: How do we go from genetic discoveries from GWAS/WGS/WES to mechanistic disease insight?\nPresenter(s): Danielle Posthuma\nLevel: Beginner friendly\nLength: 9:53\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to MAGMA software download and tutorial."
  },
  {
    "objectID": "chapter8.html#fuma",
    "href": "chapter8.html#fuma",
    "title": "Chapter 8: Post-GWAS bioinformatics",
    "section": "FUMA",
    "text": "FUMA\nTitle: FUMA: Functional mapping and annotation of genetic associations\nPresenter(s): Kyoko Watanabe\nLevel: Beginner friendly\nLength: 14:19\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\nLink to FUMA Website, GitHub, and Tutorial."
  },
  {
    "objectID": "software_MR.html",
    "href": "software_MR.html",
    "title": "Mendelian Randomization",
    "section": "",
    "text": "Title: Examine causality using Mendelian randomization\nPresenter(s): Jie Zheng\nLevel: Intermediate\nLength: 9:26"
  },
  {
    "objectID": "chapter8.8_transcript.html",
    "href": "chapter8.8_transcript.html",
    "title": "Chapter 8.9: PheWAS (Video Transcript)",
    "section": "",
    "text": "PheWAS: Discovering gene-disease associations\nTitle: PheWAS: Discovering gene-disease associations\nPresenter(s): Dr. Joshua Denny, All of Us Research Program\nJoshua Denny:\nThank you very much. It’s a pleasure to be here, as always, and talking about, just, the amazing stuff that’s happening here in the UK Biobank. Let's see… Alright, great. So to start with, we’ve talked a lot about genome-wide association studies and sequencing, and we’ve also talked about phenome-wide association studies as well. That’s going to be the focus of my talk. And just to orient us, essentially what we’re doing is thinking about an independent variable and exploring what phenotypes and the range of phenotypes that are available and associated with that. We’re really anchoring on the fact that we have richly and systematically phenotyped sets of individuals, such as in the UK Biobank, and other electronic health record datasets, which is where this started.\nUsually, that’s based on things like billing codes, but I don’t want to limit us there. You can think about laboratory values, you could think about natural language processing, and things like that as well. Most of it has been based on billing codes. So to start with, I want to orient us to a discovery study out of the Electronic Medical Records and Genomics Network, eMERGE, in the US. It was five sites that worked together for a carefully validated phenotype. We have used codes, labs, medications, natural language processing to find these cases and manually validate who was a case for presumptive autoimmune hypothyroidism and controls. We'd identified the thyroid transcription factor that was associated and replicated this. Then, we did a… we took a variant, the same variant that was found here, and did a pheWAS on that variant in a slightly larger population – a much larger population, you know, that was unselected for any given phenotype. And hypothyroidism was the highest associated phenotype there, but we also had some other thyroid diseases that came up, and things like atrial flutter were associated, which we know that hypothyroidism is less likely to manifest with atrial flutter.\nAnd so, this gives an opportunity to look at the performance of these two methods. And so, you know, on the left, we have the schematic of the algorithm we use, and then we use these mappings called pheWAS codes, or phecodes. And we usually say there have to be two or more that map into that phecode. And, you know, you can see the odds ratios are essentially the same between the two approaches within our population of individuals within eMERGE. You know, we identified more cases with the pheWAS codes than we did with the algorithm.\nSo, you know, there's many approaches to pheWAS. Just talking about that, you know, most used billing codes in the U.S. That’s been historically ICD-9 [International Classification of Diseases, Ninth Revision] with the clinical modifications, and now ICD-10 after 2015. There’s about 65,000 ICD-10-CM [International Classification of Diseases, Tenth Revision, Clinical Modification] codes. And on the right, you can see some of the ways this works. So the phecodes have numbers that kind of look like ICD-9 codes, but they’re actually not. And then what we do is we group like codes now across ICD-9 and ICD-10, and ICD-10-CM codes to a given phecode. And so, all the Type 1 codes come together, which is not obvious from the ICD-9 coding group system. And then each of those also defines ranges of control groups in addition to the Fee groupings or other groupings in the U.S.\nThere’s some, the AHRQ has released some software that groups things into about 300 diseases. TreeWAS is another thing you can also use. Raw ICD codes, for instance, that gives you a challenge, of course, mapping between ICD-9 and ICD-10. And you can do many other things. Survey data has been run across the UK Biobank and other things that I’ve talked about, like the procedures.\nSo here’s another example: pheWAS driven by EHR data looking at imputed HLA types into the two and four-digit types of HLA. And you can see, you know, quickly it highlights the fact that there are different associations between class one and class two HLA alleles. And what helps you think about the range of associations. And overall, I think, there were a hundred or so significant associations, most of which were known and a few new ones. But what’s more interesting is, by doing it in a single population, you can actually look across those phenotypes. And then look for pleiotropy and see, you know, if you adjust and condition on one and the other, do you see, you know,  that they’re truly independent associations? And you can also see, we can take a given HLA type that, you know, one or two HLA types that may put you similarly at risk for rheumatoid arthritis may have a differential effect on your risk for type one diabetes, for instance. So, that is a tool that you can rapidly explore using this kind of technique.\nAn important aspect is validating its efficacy. So, one of the early things we did using our ICD-9 codes across the eMERGE was replicating known associations in the GWAS Catalog. We found 86 phenotypes that were… could be represented in the electronic health record and a number of SNPs [single nucleotide polymorphisms], about 750 overall SNP-phenotype pairs. Overall, we replicated 210 of them across a number of disease classifications and 66% percent of those for which we were adequately powered in this population of 13,000 people, as well as finding some novel associations, the top of which we replicated.\nIt also allows us to actually compare the effect size. So here, you might see something that you would expect to see in that the effect sizes from the GWAS studies are typically a little bit lower than what’s in the GWAS Catalog. Now, some of that’s probably due to the winner’s curse, but some of it’s also due to the phenotype being not quite as accurate, and it helps you think about the ones where you have the most error. The most common error, and it was really, universally, type 1 diabetes is often mis-coded. In fact, 96% of the time, we found type 1 diabetics had type 2 diagnosis codes. So, it made it, it made it, and the reverse is true, 56% of the time. So, it caused a lot of inaccuracy in the type 1 diabetes phenotype, and we had trouble replicating some of those SNPs. And we’ve actually instituted methods to fix that problem, and we can recover those associations.\nHere’s a way you can use pheWAS in concert with aGWAS. So, we did a GWAS in eEMERGE, looking at the longitudinal risk of cardiovascular disease on a statin and found variants that are tied to expression of lipoprotein(a) were associated with that outcome as a longitudinal analysis. And that risk is increased for those that have, you know, kind of ideal cholesterol levels of less than 70.\nSo, we looked at a pheWAS of this locus and, you know, as you’d expect, you see coronary atherosclerosis near the top. And fortunately, we see most of the phenotypes are ones we would expect to see, which gets us to the question, if you were to target this with a medication, you know, what potential effects would you see? One of the things that’s interesting and wouldn’t have been on our radar screen is this point over here, which is not quite statistically significant, was lung cancer. So, you know, this is a relatively small population of 13,000 people. As it’s explored more, maybe that will turn out to be true or not. But it is a rapid tool for highlighting, especially, when you think about the scale of the UK Biobank. I mentioned mapping these to ICD-10 and ICD-10-CM codes. It just shows a little bit of a process and the vocabularies and systems that we used in the process, with some manual validation. It is still in what we call a beta form, but you can see it covers about 90% of the billed ICD codes in the UK Biobank. And amongst the 10% that aren’t there, most of those are not actual disease codes. Only a small fraction of those represent true disease codes. And we did an evaluation using our data with ICD-9 and ICD-10 codes in terms of pheWAS, and you can see that the effect sizes between this phenotypic population were essentially the same for these two known associations with that SNP.\nI want to give a few examples. Actually, Kristen showed this earlier, doing a pheWAS in the UK Biobank and just tons of associations associated with atrial fibrillation genetic risk score for AFib. And when they condition for the phenotypes, the cardiovascular phenotypes essentially, those associations went away. But it shows the power of a huge population to show lots of things you expect to see.\nHere’s another example for systolic blood pressure using a large GWAS done across the Million Veteran Program, as well as the UK Biobank. And just a number, number of associations showing up with systolic blood pressure. They also did the same with diastolic blood pressure and pulse pressure, to show that some of these phenotypes overlap. And you also see phenotypes that are not exactly associated with a cardiovascular disease in here coming out as well, endocrine being one of the more common ones.\nHere's a resource, Kristen also talked about the SAGE approach, using a saddle point approximation to create an efficient and accurate way of calculating these kinds of results at scale for the UK Biobank. They have produced a website where you can explore these phenotypes, calculated using the same approaches for phecodes across the UK Biobank. And this [slide] just shows a particular AFib SNP in that website, and the URL is there at the bottom.\nSo, you know, we’ve talked about this, and in looking at individual phenotypes, I want to spend the last few minutes talking about phenotypes in clusters and how we think of them. So, if you think about Mendelian diseases, they are a classic example that are often syndromic, presenting with many different features. These features may be what we bill in the electronic medical record as physicians, but it doesn’t necessarily represent the disease, you know. The disease is not always recognised or may be recognised later into the disease course, as we heard about earlier with hemochromatosis. And so, through the Online Mendelian Inheritance in Man resource and the linked Human Phenotype Ontology [HPO], you know, we can go from a Mendelian disease to a list of features of that disease. These features have a vocabulary behind them. Then, so… So, our lab mapped those HPO features to phecodes, so basically allowing you to translate OMIM features into EHR phenotypes. Similar to a polygenic risk score, you know, creating a phenotype risk score, that looks similar in process, so, aggregating phenotypes up by their weights to produce a score for individuals. Essentially, you can crank this out across anything for which you have a map and do it at scale.\nSo, let’s look at cystic fibrosis. We have a number of features from OMIM, and each of those is mapped to a Human Phenotype Ontology code. And so, we're using our phecode ontology of around 1,800 phenotypes and you can map the ones that line up fairly well to CF. They’re not all exact matches; some are better matches than others, and then some that we don’t have in the EHR, which, you know, we’re familiar with.\nAnd so, let’s play that out on a couple of hypothetical individuals, hypothetical different conditions. I mentioned they’re weighted, so features like bronchiectasis have a higher weight than features like asthma. And so, when you go across these individuals, they get different scores. What you find is that you can separate cases and controls for cystic fibrosis, you know, just using the features of the disease. So, we're not using the disease label and, in this example, we use manually validated cases versus controls who don’t have any evidence of the disease in the text record. And we see a very significant result. And we’ve actually done this for 15 other diseases now, and in every case except for one, we’ve seen very strong separation between cases and controls. The one exception is phenylketonuria, which, as you know, in the US is on essentially every newborn screening test. And if you avoid phenylalanine exposure, you don’t actually see the manifestations of the disease. So it sort of gives you a test of the effectiveness of newborn screening in removing the features of the disease in the population because they [those who avoid exposure] generally do not have elevated scores.\nSo we turned this on a population of 21,000 people who had exome array genotyping and looked at 6,000 variants that were rare at a 1% level or less. And we found 18 significant associations, most of which were novel, and, importantly, we were able to change the ACMG [American College of Medical Genetics] clinical interpretations for eight of these variants towards likely pathogenic or pathogenic. So, this, using our population as a paradigm, I think this approach can be explored with larger rich phenotype populations such as what is in the UK Biobank.\nSo I want to end with a recognition of some of the many people contributing to this work. The middle row is probably the most important row, as these are the folks actually doing the work. Thank you very much.\n\n\n\nPheWAS and EHR\nTitle: Using EHR-based genomic approaches to understand the relationship between mental and physical health\nPresenter(s): Dr. Lea Davis, Vanderbilt University\n[the recording starts mid-sentence]\nLea Davis:\n…and the biobank that’s attached to them at Vanderbilt. And so, this opened up a newer area of investigation for me – something that I’ve been interested in for a long time but hadn’t had the resources to investigate. And so, that’s basically using EHR-based [electronic health records-based] genomic approaches to try to better understand the relationship between mental health and physical health. That’s the story that I’m going to be talking to you about today. So, if I can advance my slides... oh, there we go. Okay.\nSo this relationship between mental health and physical health is well-known – that it’s important. In one of the nice summary statements about the importance of this, it comes from the World Health Organization. On their website, they state that there is no health without mental health. And it’s been documented for some time that poor mental health is a risk factor for a number of physical conditions, particularly chronic conditions. People with severe psychiatric illness are at a much higher risk of experiencing chronic physical conditions, and vice versa – people with chronic physical conditions are also at risk of declining mental health. And so, I’ve actually come to think about this much like a disparity, in fact, that people with severe psychiatric illness experience these kinds of healthcare disparities. Oh, there we go. So, this is really sort of punctuated by the observation that the lifespan for people with severe psychiatric illness – schizophrenia, bipolar disorder – but also neuropsychiatric disorders like autism spectrum disorders and cognitive impairment, on average, the lifespan is shortened by 10 to 12 and a half years. It’s thought that this is due to many possible reasons. One being a difference in access to healthcare – related to employment and insurance issues here in the States. And that’s certainly been shown to play a role, particularly for psychiatric illnesses such as schizophrenia, where there’s a much higher rate of homelessness.\nBut also, issues related to difficulty in expressing pain. For example, for people who are non-verbal, there’s also some suggestion that there may be altered interoception among people with neurodevelopmental disorders. So, for anybody who’s not familiar with that term, interoception is kind of your internal perception of pain, discomfort, hunger, thirst, even sensing your own heartbeat. And that appears to be somewhat altered in people with developmental disorders.\nBut it’s also certainly possible that there’s some increased genetic or biological risk that’s actually related to the genetic risk for the psychiatric disorder itself. So, imagine sort of pleiotropic mechanisms. And then, of course, increased exposure to environmental risk factors: poor diet, lack of shelter, lack of access to medication, and those kinds of things.\nI think it’s also important to highlight that this is really an understudied area for a couple of primary reasons actually. One is that until fairly recently, a lot of people with severe psychiatric illness and neurodevelopmental disorders received most of their healthcare in an institutionalised setting, whether that was a psychiatric institution, prison, or some other kind of group institutionalised setting that was separated from community-based healthcare. And so, this population has been understudied in epidemiological and community-based studies.\nAnd then, there’s also, as I’m sure everybody on this call is probably well aware, a historical separation between psychiatry and the rest of medicine. And so, it ends up being a kind of functional separation. That refers to the fact that often, mental health facilities are separate from primary care facilities. So you might have a community-based mental health clinic and a community-based primary care clinic. And even at a hospital, there’s often a separate psychiatric hospital in a completely different building from the general hospital. So, there’s this functional, cultural, and financial separation of psychiatry and medicine that I think has also contributed to this kind of healthcare disparity and the lack of research about it.\nOkay, and so, like many of us in this field, this is also an area of personal interest for me. So, this is a picture of me with my son, Dylan. Dylan is 24 years old; he has autism and severe cognitive impairment. He’s nonverbal, requires 24-hour support staff, and he lives in a community-integrated group home. And so, even with a whole team of people who are focused on Dylan’s health, we still really struggle to get proper healthcare for him. And so, this is an issue that’s important to me personally, as it is to many families of individuals with developmental and psychiatric disorders who are getting older and ageing, and we don’t really know what chronic health conditions they may be at risk for.\nSo, while, on the whole, this has been an understudied area, there have been a few areas of focus, particularly related to cardiovascular disease and cerebrovascular disease. And in recent years, there have been some really large meta-analyses looking at the prevalence and cumulative incidence of cardiovascular disease in people with severe psychiatric illness, primarily bipolar disorder, schizophrenia, and major depression. This is actually a recent paper with a meta-analysis of, I think, 92 studies that had an impressive sample size: over three million patients with one of those three disorders and over a hundred million controls, where they [study authors] investigated the increased risk of both cardiovascular disease and cerebrovascular disease in these populations. And so, basically, the take-home is that there is a significantly increased risk for both diseases, cerebrovascular and cardiovascular disease. This risk persists even after adjusting for some of the health behaviors that may be related to the incidence of disease, so things like smoking, poor diet, or BMI.\nSo based on these epidemiological studies that are just now starting to be published, we’re also interested in asking the same kinds of questions and better understanding the relationship between mental and physical health using our EHR. In particular, we want to understand if there is some shared biology; if it’s the case that poor mental health causes poor physical health or that poor physical health causes poor mental health. Really, our model is that it’s going to be all of the above. But understanding what the primary risk factors are for each type of chronic disease and where their shared biology is, yeah, I think this is going to be an important area of research.\nAnother question that we have is: Do these relationships between mental health and physical health transcend our diagnostic boundaries? Is it really the case that it’s just people with severe psychiatric illness or diagnosable neurodevelopmental disorders who are at risk? Or is it the case that the risk is continuous? That actually, across the entire spectrum of genetic risk for these traits, that there’s also an increased risk for these chronic health conditions?\nAnd then, are there particular health conditions for which people with developmental disabilities and severe psychiatric illness are at high risk? So there’s been a lot of work done on cardiovascular disease and cerebrovascular disease, and I think actually a fair bit of work on type 2 diabetes. But, outside of those primary chronic health conditions, there really hasn’t been much, and so we really want to look  phenome-wide at these relationships.\nAnd then finally, we want to see if this can help us understand the best point of intervention and, of course, to identify any interventions that are typically used in healthy populations that may cause particular problems in patients with severe psychiatric illness or developmental disorders. So maybe I’ll pause there for just a minute if there are any questions.\nFacilitator: Don’t forget to unmute yourself if you have one. And I mean, one short question. To move to the first slide here, so this is the shortening of the lifespan for severe psychiatric diseases, and so the one thing that was… not missing on this list is suicide. So, big toll to the psychiatric disease here, is it not the main cause for that, or…?\nLea Davis: No it isn’t, actually. It’s definitely a contributory cause, but actually, the primary causes are chronic health conditions. Suicide is definitely an increased risk in this population, absolutely. But it doesn’t account.\nFacilitator: So, you also mentioned already, being in facilities, being in, like, prisons, etc. So, are there other comparisons between, for example, also the US and Europe? Are there comparisons between Caucasians and other ancestries? Do you have any idea here already?\nLea Davis: Honestly, the literature is pretty scant, so I’m not sure if that’s really been investigated systematically and with sufficient sample size. It really hasn’t been. I mean, I think most of the papers looking at these associations have been published from the ’90s and on, so there’s not the kind of 80-year body of literature like there is for cardiovascular disease and in healthy populations. So, yeah, I guess the short answer is I’m not sure that it’s been systematically investigated.\nFacilitator: Interesting. Do others have some questions? It says that there are two or three [individuals] on the call that are not muted. So again, if you’re on the call and not muted, please mute yourselves. I can’t right now.\nLea Davis: So, we believe that EHR-based genomic approaches are actually a great way of investigating several of these questions that we have. So, the EHR, the electronic health record, allows us to investigate the relationships between phenotypes, and the biobank that we have also facilitates investigation of the genetic relationships between these phenotypes. And so, we can then also compare the genetic correlations to the phenotypic correlations to better understand, you know, where there are environmental risk factors that contribute to the phenotypic relationships and genetic risk factors that contribute to the genetic relationships between traits. And it also allows us to utilise the polygenic architecture of these complex traits and develop quantitative models, so we’re not necessarily relying on diagnostic categories but we can look at how genetic risk as a quantitative trait is related to risk for various phenotypes.\nSo, maybe some of you have heard me talk about the Vanderbilt EHR and the biobank, but just in case you haven’t, it can be kind of thought of as three entities. We have what we refer to as the synthetic derivative, which is this de-identified and continuously updated mirror image of the EHR or EMR [electronic medical record], that, as of now, has a little over 2.8 million individuals. If we look across just that set of 2.8 million individuals, the median length of the EHRs is only about a year, even though the EHR has been in existence now for 20 years. And part of the reason for this is that we’re a tertiary care centre. So we get people coming in from all over the state of Tennessee, Kentucky, and sort of all over the Southeast, particularly, you know, if they have the need to come see a specialist in a specialty care clinic. And so we end up drawing pretty sick people from all across the state. So, in comparison to, like, the UK biobank, that has maybe a healthier, on average, population, I think at Vanderbilt, we have a sicker, on average, population. That said, there is also a population of people who make Vanderbilt their medical home, so to speak, and they get most of their primary care at Vanderbilt as well. And so, our biobank consists of DNA samples that have been collected from just routine clinical blood draws, is enriched for that population of people that actually make Vanderbilt their medical home. And so, this is illustrated by the median length of the EHR, or BioVU subjects, which is about 10 years.\nAnd so, we have now somewhere around 270,000 DNA samples that have been collected, and a little over 50,000 of those subjects have been genotyped with some kind of GWAS platform. And on average, the age of those subjects is around 58 years old, but we are trying to genotype and accumulate pediatric care as well. So, any quick questions about the structure of the BioVU biobank or EHR that’s central to the rest [of the presentation]?\nFacilitator: So this is, like, all different kinds of diseases, right?\nLea Davis: Yep.\nFacilitator: All cases? Or are there also healthy subjects here, or is this all cases?\nLea Davis: Well, I mean, yeah, so there are people who don’t have, you know, chronic diseases certainly, and there are people who come in for, you know, routine health care, and they are in the biobank as well. So there’s no ascertainment at the biobank level. That said, it’s a hospital-ascertained population, so, I think, most people will likely be, you know, a case for something.\nFacilitator: Okay. And so, how many of these 50,000 are psychiatric patients?\nLea Davis: Um, how many of the 50,000 have psychiatric codes? I’m not exactly sure. Yeah, I actually don’t have access yet to all 50,000 samples. It’s not enriched for psychiatric codes, that. But we’ve… So, you know, the genotyping data, it’s actually a large project. Eventually, we’ll have a 100,000 people genotyped. And so, the data is coming through in waves, and we actually were involved in pushing several of the psychiatric diagnoses through, but we haven’t gotten those genotype samples yet. So, they’re, yeah…\nFacilitator: Right. Where are they genotyped right now?\nLea Davis: Where are they being genotyped?\nFacilitator: Yeah.\nLea Davis: Here, at Vanderbilt.\nFacilitator: Vanderbilt. On GSA [Global Screening Array]?\nLea Davis: No, on the MEGA platform [Multi-Ethnic Genotyping Array].\nFacilitator: Okay. Thank you. Great.\nLea Davis: Okay, so, as I mentioned before, we were very interested in taking advantage of the fact that most complex traits have a complex genetic architecture with a measurable polygenic component. So we can look at how the polygenic risk for all kinds of psychiatric disorders is also related to disease status for other chronic health conditions. So I think probably everybody on the call is familiar with this approach, but just in case, one of the methods that we’re using is basically to calculate polygenic risk scores for everybody in our biobank. So, using some kind of large discovery GWAS and taking the effect sizes from that GWAS to create a linear weighted sum of the number of risk alleles. And then looking at how those polygenic risk scores segregate cases from controls across a number of different phenotypes phenome-wide.\nSo we started with actually investigating this in both psychiatric disorders and in some of the previously published chronic health conditions. And so, I am starting with showing you coronary artery disease polygenic risk scores that are significantly associated with the EHR definition for coronary artery disease. So here we took the beta weights from the CARDIoGRAMplusC4D Consortium study, which had about 60,000 cases and 123,000 controls, and applied it to a small subset of our MEGA data target sample, just to make sure that indeed, coronary artery disease, as defined in carefully ascertained research samples, was related to the EHR diagnosis for coronary artery disease. And so, this included covariates median age across the EHR, sex, the top 10 PCs [principal components], and actually it’s not listed here, but also genotyping batch. So you can see that our polygenic risk score accounted for almost 3 percent of the variance in CAD diagnosis within our EHR.\nSo, this was encouraging. And we actually applied this model also to the lipid traits that have been studied by the Global Lipids Consortium – so HDL, LDL, and triglycerides. And we tried to model the relationship, the known relationships between those risk factors and coronary artery disease. Oops, so I’m going to take just a short methodological detour here because one of the other things that’s come to my attention in working with the Department for Biomedical Informatics, you know, often, when I present polygenic analyses, I get the question from people who do a lot of machine learning, “Where is your feature selection step?” Right? So, in your genome-wide association study, training the weights, but then, how do you know which SNPs to include in your model and the target data? And typically, really, what we often do is just work across a number of thresholds, and, you know, see how the R2 might change if we include just the genome-wide significant SNPs or, you know, everything at a p-value of less than 0.5 or less than 1. And so, kind of investigate across different thresholds.\nBut we wondered, really, how well we would do if we actually took a training set and used it to select a threshold for including SNPs in the polygenic risk score, and then applied it to a validation set. Because this is actually, you know, I think, ultimately what we would do. Really, just out of curiosity we looked at that now. The slide is a little busy but I will walk you through it.. We have our discovery GWAS phenotypes in the first column and the target phenotype in the second column. So the discovery GWAS was either the Global Lipids Consortium for HDL, LDL, or triglycerides. Can you guys see my cursor there?\nFacilitator: We can, we definitely can, but there is someone not muted in the background. Not sure if this is this one person who is not muted or if it’s you, Lea, I’m not sure.\nLea Davis: No, it’s not me. I heard it, too. It’s okay.\nFacilitator: But we see your cursor, yes.\nLea Davis: Okay, all right, great. So, right, for these lipid traits, this was Global Lipids Consortium, across CAD, this was the cardiogram study, and then our target phenotypes were all measured in the biobank. And so, we set up a training sample of about 9,000 people and a validation sample of about 16,000 people.\n[Note: There are noises in the background of the call due to an individual not being muted, 24:39 to 25:14]\nFacilitator: Gerome, if you hear us, please mute yourself! I don’t have the claim to power. They can’t hear us.\nLea Davis: Yeah. [laughs lightly] So we had a training sample of about 9,000 people, a validation sample of about 16,000 people. And so, you can see the p-value threshold that was the best fit in the training sample, the number of SNPs that was included in that best fit, the adjusted R2 value, or the proportion of variance explained. And the p-value for that…\n[Note: There are noises in the background of the call due to an individual not being muted, 25:35 to 25:55]\nLea Davis: Sorry, it’s hard to…\nFacilitator: I mean, they seem to be like they don’t have the speaker on, they don’t even listen, so it’s a little bit annoying. So please, everybody, again, here, if you hear us, please mute yourself or just, if you’re not listening, just leave the call. That’s fine as well.\nLea Davis: Okay, so we used the same threshold that was identified in the training sample to define the PRS in the validation sample. There’s a, you’ll notice, there’s a different number of SNPs included, and we think that this is because the training sample was on the Omni [genotyping] platform and the validation sample was on the MEGA platform, even though they were both imputed to the same reference panel, and they’re both European populations. The MEGA sample seems to have better overlap with the original discovery GWAS, and so the R2 for, you know, both the training and validation samples, particularly for the lipid traits, are really pretty impressive and, and actually start to approach the SNP-based heritability, for, in particular, for HDL.\nAnd so, I’ve put an asterisk here because the p-value threshold in the training sample was actually the same as the best-fit p-value threshold in the validation sample. So, I can pause there for any questions. We were actually kind of surprised that there were any traits where the best-fit threshold was the same across multiple samples. And so, I think this is actually really interesting and potentially indicating that we’re starting to approach, kind of, maximum information for HDL and LDL in our GWAS.\nRight, in the interest of time, I’ll just move on. So, we took the same best-fit thresholds and polygenic scores developed from them, and applied them in a pheWAS analysis. So what you’re looking at here is a Manhattan plot from our pheWAS where, along the x-axis, we have, kind of, classified phenotypes, and along the y-axis, the -log10 of the p-value for the PRS. The direction of the arrow indicates the direction of risk. So, if there were higher polygenic risk scores among cases, you’ll see an up arrow, versus control, you’ll see a down arrow. So, this pheWAS was for LDL, sorry, for HDL, and we see a protective effect of HDL on type 2 diabetes. And this has been previously shown. So, this was a good proof-of-concept.\nWe did a similar type of analysis for our LDL polygenic risk score, where, again, we see a strong association with dyslipidemias and coronary artery disease. So these are also known associations. Interestingly, so I don’t have it here in this presentation, but there is, the phenotypic correlations between LDL and the diseases tested in our phenome-wide analysis were much stronger. So, we saw lots of different phenotypes associated with the measured LDL, but these are the only phenotypes that are associated with genetically predicted LDL.\nSo, getting into our primary interest, which is the genetic risk for psychiatric disorders associated with diseases across the phenome, we asked whether genetic risk for MDD [major depressive disorder] was associated with heart disease codes and, actually, all phenome-wide codes. And so, we took the most recent 2018 MDD meta-analysis results that are posted on the website [note: of the PGC: https://pgc.unc.edu/for-researchers/download-results/] and did the same kind of thing where we calculated a genetic risk score in all of our 16,000 people in the MEGA sample. And we see really strong associations with mood disorders and depression, which we expect. Associations with bipolar disorder and anxiety disorders. But then, we also see some of these cardiovascular traits rising in significance as well. And so, we see an association with nonspecific chest pain, which is really a catch-all, you know, as it states, nonspecific, code, but was definitely interesting to us. And so, we wanted to investigate this a little further.\nAnd we asked whether we saw similar associations for related mental health traits. So, related to major depression is also an individual’s perception of loneliness. And this is something that we’ve been working on with a consortium group, Abe Palmer, Dorret Boomsma, and myself, and others. We call ourselves the Lonely Consortium. And we’ve amassed almost 500,000 samples. And I think those of you on the MDD call have heard some reports of this already. So, we looked at the relationship between polygenic risk scores for loneliness and phenome-wide associations as well. And so, this is our Manhattan plot for our loneliness GWAS, and you can see also that we are observing some enrichment of gene expression for our loneliness loci within tissues that we expect to see some enrichment in, brain tissues in particular. And we have also observed several genetic correlations. I just have a few posted here, but these are genetic correlations with phenotypes that are associated with poor mental health, including general tiredness, lower self-rated health, and coronary artery disease.\nOne of the reasons that we were really interested in looking at the genetic relationship between loneliness and these other traits is that loneliness has, in and of itself, been identified as a risk factor for increased morbidity and mortality. And so, there have been several epidemiological studies looking at the temporal relationship between a person’s self-reported loneliness and later health consequences. And so, often, the causal mechanism is inferred from that temporal relationship. But I think that’s a little tricky because, of course, by the time someone actually has a heart attack or has diagnosis of coronary artery disease, that disease has been developing for many years. And so, having that temporal relationship is not always an indicator of a cause-and-effect relationship.\nA graduate student in my lab, Julia Sealock, has led the effort on this work. She looked at, again, the innate propensity to loneliness — the polygenic risk scores for loneliness — to see whether they were associated also with poor health outcomes. And so, in this case, because we didn’t have loneliness measured anywhere in our biobank, we weren’t able to do this kind of hold-out training on a separate sample for a best fit. So, we just took everything at a p-value of less than one. And even with that, we actually see a strong association with mood disorders and depression, with tobacco use disorders. But then also, you can see a whole host of coronary artery disease-type phenotypes. So this is also getting at the question that we had about the relationship between sort of dimensional traits and diagnosis itself. So again, these are not necessarily… The sample is not enriched for major depression, and we’re not looking at the diagnosis of major depression or the diagnosis of chronic loneliness. We’re just looking at the genetic risk factors.\nSo, we actually… one of the, I think, benefits of having this type of data, as opposed to just looking at genetic correlations with summary stats, is that we can do a lot of conditional and sensitivity analyses to try to tease out some of the relationships. And so, we looked at how BMI and diagnosis of major depression might influence associations. And so, when we adjust for BMI, definitely we see an attenuation of the signal for coronary artery disease, although we do still see some of these phenotypes rising above phenome-wide significance. And we also see, again, a strong association with mood disorders, depression, and tobacco use disorders.\nWhen we adjust for the diagnosis of major depression, we still observe a significant association with obesity. And, even though our coronary artery disease codes kind of fall below phenome-wide significance, they’re still, of course, enriched among our results. And so, this was actually a really important analysis because it’s also well-known that after having a coronary event, people become much more susceptible to a major depression episode. And so, we wanted to make sure that our associations weren’t completely driven by the major depression that may be diagnosed after the fact. And while we do see definitely an attenuation of the signal, I think with increased sample sizes, these associations will probably still remain phenome-wide significant.\nWe were also interested in seeing if there was a difference between males and females. And so, we stratified our pheWAS sample and looked separately. And while we do see some qualitative differences – so, in females, definitely the depression and mood disorder codes remain phenome-wide significant, and in males, the MI [myocardial infarction] and atherosclerosis codes remain phenome-wide significant – these differences between them were actually not statistically significant. So I think this is really just reflecting the fact that more males have myocardial infarctions and more females are diagnosed with depression. Stephan, how am I doing on time?\nFacilitator: It depends on how many slides you have left. [laughs]\nLea Davis: Well, what time is it?\nFacilitator: Sorry, we have still another 17 minutes? But let’s leave another 5 minutes to the end at least. So you have good… 10 to 12 minutes?\nLea Davis: Okay, alright. Okay, great. So, since we were primarily interested in following up the associations between polygenic risk for loneliness and the coronary artery disease codes, we focused in on the males and looked there at the association after adjusting for, again, MDD or BMI. And so, again, we see that myocardial infarction remains significantly associated after adjusting for either BMI or MDD, and actually also remains associated after adjusting for both MDD and BMI. And so this, I think, now provides a really nice substrate for a Mendelian randomisation analysis as well.\nThe second part here is actually just an introduction to some of the work that we have planned. And so I wanted to just briefly go over it and invite any ideas or collaborations if people are particularly interested in certain biomarkers for phenotypes that they’re studying. So, within the EHR, we have access to actually thousands of labs, but many of those are kind of unique, special snowflakes, so they may have only been ordered on, you know, a small handful of patients.\nSo, when we start looking at labs that have a larger sample size, it turns out we have about 350 labs with over a thousand individuals. And actually, I should say that all of these labs have at least a thousand observations. So, if we say that we have 350 labs with at least a thousand individuals and at least a thousand observations, then it means everyone has been measured at least once. But then, we’ve got a larger number of labs where we have a smaller number of individuals but a larger number of observations per individual. And so, this data is also, you know, really rich for looking at longitudinal associations between the relationship with, you know, psychiatric illness and changes over time in various biomarkers. So in total, we have about 500 labs with at least a hundred people, at least a thousand observations.\nSo, this work… Sorry, this data source really has not been utilised very much in the EHR space, partially because it’s really messy data. So it’s taken us close to two years actually to really carefully QC [quality control] all of the labs that had sufficient sample size. It’s also challenging because, again, it’s a hospital population, and so, you know, a lot of the times, the labs are being drawn because somebody is actually sick. And so, this can be a challenge to interpretation. But, at the same time, the fact that we have the entire EHR allows us to investigate the relationship between diagnoses and changes in lab values.\nSome of the benefits of using this data are that we have a really large sample size that is really rich for clinical data. So it’s longitudinal. We’ve got over 20 years’ worth of data. And we can, as I’ve mentioned a couple times now, test the effects of many possible mediating and moderating variables. And then, we can also go into the charts themselves and validate by chart review. So, developing tools to both QC and visualise this data has been a really tremendous effort by Peter Straub, a programmer in my lab. So he has developed a Shiny app and a whole set of tools that we’re actually planning on making publicly available. And we’ll be able to make the summary data for all these different labs available, so that we can look at how they vary by age, by sex, by race, and that groups with other bio banks and labs, you know, can locally also download these tools and apply them to their data.\nAnd this is sort of the concept map for what we would eventually like to do. So this is, you know, that we would take, you know, the beta values from GWAS of many psychiatric illnesses, calculate polygenic risk scores across everybody in BioVU, and then look at how these risk scores are related to median values in, you know, hundreds of routinely collected labs. And, as I mentioned before, we can also use longitudinal models to see how they’re related to change in lab values over time.\nSo we’ve started doing this a little bit for some of these traits that I’ve been talking about so far. So this is just focusing in again on HDL, LDL, and triglycerides, and looking at how genetic risk scores for coronary artery disease, loneliness, or major depression is associated with median lab values. And so, in each of these plots, we’re looking at the R2 values, the proportion of variance explained in HDL, LDL, and triglycerides, on the y-axis. And then, the discovery GWAS p-value threshold on the x-axis. So that we can see kind of across the board how well do, you know, the MDD risk scores, CAD risk scores, and loneliness risk scores predict HDL, LDL, and triglyceride levels. And so, interestingly, we see that,  for HDL, our loneliness risk scores actually tend to outperform our CAD risk scores. And for LDL, you know, it looks like there is some differences, but I don’t think these are actually really meaningful because below 0.1% variance [explained], it’s not, not a significant association between the risk score and the median value. And then, in our triglyceride analysis, we do see the CAD risk score outperforming the loneliness risk score, which is what we would kind of think about, maybe intuitively expect. But the loneliness risk score does actually significantly predict triglyceride levels as well.\nSo, like I’ve said a couple of times… Sorry. We also are interested in testing the mediating effects of these quantitative traits and the moderating effects of sex and medications, and other diagnoses on these relationships as well. So this is kind of our… an example of a general model we’re interested in testing—the relationship between polygenic liability and disease diagnosis, that may be, again, mediated through quantitative traits.\nSo, kind of, our future interests are to mine this lab data for potential biomarkers for neuropsychiatric disorders, using genetic risk scores from our publicly available GWAS data. And I should say that, you know, we don’t think it’s likely that we will identify a biomarker for, you know, major depression out of all of these routinely collected labs, but that actually, we may identify several quantitative traits that together may be predictive of anindividual’s genetic risk for major depression or schizophrenia, for example. We also are really excited about using bi-directional Mendelian randomisation analyses to better understand some of these possible causal mechanisms. And to compare the phenotypic correlations with the genetic correlations, to try to identify comorbidities that may actually be more of a consequence of environment than of genetic causes—which, you know, we’re also equally interested in understanding.\nSo, I think that is it! I’ll wrap up with an acknowledgement of everybody in my lab, a really great group of people to work with and wonderful students. And the work of the Lonely Consortium has just been a phenomenal collaboration that, I think, has yielded some really interesting results. And so, I think, with that I conclude and I hope there is some time for questions.\nFacilitator: Thanks so much, Lea. Fabulous presentation, really, it’s a lot of data that is coming, therefore, it’s really exciting that you’re diving into that with full speed here. So is there… I mean, we have, like, around 30 people on the call. They probably have questions and they don’t know how to unmute themselves. I still have questions, but I don’t want to always step in, so… Did I hear somebody? Not yet. So, okay, so, Lea, I still have two or three more short questions, more on the technical side. There are two things here. So, first of all. Because you’re speaking about, like, longitudinal stuff, because you have multiple measurements from the same individuals here. But also, is there a chance to actually, given the fact you might have seen some people with especially high polygenic scores for schizophrenia also. Is there a chance to get in touch with them [the patients] again on the next visit or so? Or is this something so enormous there’s no chance to get in contact with these individuals again?\nLea Davis: No, they’re, yeah, it’s completely a de-identified dataset. Yeah, there is also a what’s called the research derivative, so we work with the synthetic derivative because we’re also working with the genetic data. If we were to restrict ourselves just to the phenotype associations, we could apply for access to the research derivative, which is an identified dataset. But even then, I think there are rules—I don’t know that we could actually recontact patients. And there’s no, although I don’t understand why this is the case, you can’t work with genetic data in the identified environment.\nFacilitator: Is this special to Vanderbilt or is this general to the US?\nLea Davis: Well, I think most of the biobanks within the US are de-identified. That issue about not working with genetic data in the context of an identified environment, that might be specific to Vanderbilt. I’m not actually not sure what the, you know, rules around that [are] or where those came from.\nFacilitator: Very interesting, and a little bit sad, of course, because that would have been the gold example for that. And so, the other issue—a little bit more on the technical side—I mean, we all know that, like, especially polygenic risk scores are relatively sensitive to sample overlap. So how is… is there any chance for you? How are you dealing with this issue?\nLea Davis: Yeah, you know, it’s a good question. And we’ve been thinking about this a lot. So, there’s a method… I can’t remember the first author’s name now, but I think it came from Peter Visscher’s group, to… In the context that we’re using, where we have some individual-level data and GWAS summary statistics. They’ve published an approach to actually first just check to see if there is, yeah, what the probability of overlap actually is. Everything up until now, I have to be honest, is based on just the knowledge that no investigators have contributed actively from Vanderbilt to these GWAS. But I mean, we haven’t yet done the careful checking to make sure that there’s no sample overlap. But that is on our list.\nFacilitator: Yeah, I think it’s something that you need to think about.\nLea Davis: Yeah, absolutely. Yeah, it’s, it’s really something that, I think, we have to just actually bake into the pipeline. And we’re also part of a collaboration with other biobank sites through the eMERGE Network, where we’re doing a similar kind of thing—you know, replicating these polygenic pheWAS associations in other biobanks. And so, we’re really trying to develop a, you know, robust pipeline for doing that across several EHR environments. And one of the things we need to make sure to build into the pipeline is this kind of checking.\nFacilitator: Yeah, I mean, especially within [the] PGC, it’s good for just testing reasons, try to see if we see some unexpected overlap there, and if we don’t, then this gives you a little bit more security that there isn’t, and that it isn’t like, suddenly 200 cases coming up that were shared from somebody else to [the] PGC, something that you need to be super careful [about] with all the other phenotypes. At least in [the] PGC, with everything in place, we could test. We could test that even without, you know, we have the checksum method where we can do it without sharing the genotype. So, it’s something that could be done on a relatively low level.\nLea Davis: So, in a phenome-wide type of analysis, how would you then, basically, for each pheWAS category, conduct a GWAS and check all of those sumstat relationships? Because it’s also possible that we have, let’s say, somebody with schizophrenia who doesn’t actually have the diagnosis in our electronic health record. Right? They have a diagnosis of type 2 diabetes in our health record, but maybe they’ve been included in a GWAS somewhere else. So, I think…you know, I mean…\nFacilitator: Exactly, that’s why I would be very interested to see, like, to check on the summary statistics level works for, like, a significant overlap for, for… if you have the same phenotype. I think you’re raising exactly the right questions that, I think, if you want to be really sure, you need to really test it on a genotype level, right?\nLea Davis: Yeah.\nFacilitator: And then… But I think that at least, if you can do this, like, with a couple of these consortia. You can, at least, mean that if there’s something unexpected or something expected, and I think this already provides some support here. And maybe when you actually see this on the summary statistics level, when you see actual significant overlap there, find it somewhere. Then you probably want to go there, ask these guys, “Can we actually test this on genotype level? Who are these individuals?” Is this actually correct? I think we can test this on a couple of different levels.\nLea Davis: Yeah, yeah, that’s… that’s a good idea. And that’s basically what we wanted to start with—was just to see, first, like, do we see any evidence of overlap? And then, if we do, you know, how should we go about dealing with it?\nFacilitator: I mean, to be honest, the overlap cannot be really, really big because then it’s so quick. We’ll see really exploding R2 values that’s really impressive. Even if, like, you have a small data set, like couple hundred individuals, in our big meta-analysis, all of these, it’s just 300 cases in the meta-analysis, the R2 are exploding. So, it is probably… Definitely… If there is something, it’s definitely very small. Of course, even this small thing can have this small impact on these… that seemed to fit so well, right?\nLea Davis: Yeah. So, do you have an intuition of, like, what you expect if there’s an overlap of, say, you know, a dozen people? You know, like a really small overlap?\nFacilitator: No, this is always something that I really wanted to test. Yeah, somebody wants to go for it? Please do so. [laughs] No, I don’t have that intuition. I have these experiences that if there’s like a couple hundred overlap, it’s really, it’s really strong. So much surprise… surprisingly strong, really. That’s why I’m always careful about this. It’s not that you see, like, R2 that’s just jumping a little bit around, it’s really exploding. And so, that’s why I think… My hunch is that even, like, a dozen or 20 could have a small impact, not big—I mean, some of your p-values are really just passing this phenome-wide threshold there, right? So that’s why, especially for these cases… But yeah, let’s keep this up, this discussion, and I’m very happy to be a part of it.\nLea Davis: Yeah, and so, and for anybody on the call who’s interested in looking at particular phenotypes, and we’ve now pheWAS polygenic risk scores for all of the publicly available summary statistics from [the] PGC. And so, anybody who’s interested in talking more about what schizophrenia looks like or what bipolar looks like, or whatever, please feel free to get in touch with me, because I would love to kind of have more folks to discuss these with.\nFacilitator: Fabulous. I’m sure the people will be more brave. Okay, so we are actually over the hour, so I will have to close the call now. Thanks so much, Lea, for presenting.\nLea Davis: Yeah, thanks, guys.\nFacilitator: And I hope this will continue here. All the best to everybody here. See you in a short time. Bye-bye.\nLea Davis: Bye."
  },
  {
    "objectID": "chapter2.4_transcript.html",
    "href": "chapter2.4_transcript.html",
    "title": "Chapter 2.4: Linkage Disequilibrium (Video Transcript)",
    "section": "",
    "text": "What is linkage disequilibrium?\nTitle: What is linkage disequilibrium?\nDescription:\nPresenter(s): Gábor Mészáros, PhD\nHello, everyone. Welcome back to the Genomics Boot Camp to a series about linkage disequilibrium. In this video, we just start to talk about this genomic phenomenon, so therefore, the question is what it is exactly. So what is linkage disequilibrium? And you will know it from this video.\nBefore we start, let’s activate some of our prior knowledge that you might know from this channel or from other sources. And the two most important issues to highlight here is one that the single nucleotide polymorphisms, or SNPs, exist. These are some markers that are widely used and currently, in my opinion, the most important marker types as for the genomics. Tens of thousands of such SNPs are being genotyped in a cost-effective way on the entire genome, so basically it is a very good way to get to know something about the genomes themselves. And the other bit of information that is important here is the existence of the recombinations or recombination events that are of major biological importance. This means that the nucleotides on our genome are not being inherited independently but in a form of shorter or longer genomic segments from the paternal and maternal side.\nMendels Law\nSuch statement about the non-independent inheritance is in conflict with the Mendel’s law of independent assortment, which says genes do not influence each other with regard to the sorting of alleles into genes and every possible combination of alleles for every gene is equally likely to occur. Mendel’s law is, of course, valid when the genes or the parts of the genome are far from each other or, for example, on separate chromosomes, but as you will see, it’s not valid when these genes or SNPs or parts of the genome are very close to each other.\nLinkage Equilibrium\nThe next few slides I took from the presentation of Professor Henner Simianer from University of Göttingen to demonstrate what happens in such occasions and what, in fact, the linkage disequilibrium is. So, for the sake of simplicity, let’s have an individual with two alleles and we ensure that this individual is entirely heterozygous. Then we mate it with another individual, which is entirely homozygous for these alleles A and B. And when the law of independent assortment is valid, then we get the four possible genotypes and, if our sample is big enough, then we have all these possible genotypes with a 25% probability. So, in our population, we would see that all four genotypes appear in a proportion of 25. In some cases, however, we might notice that these four genotypes are, in fact, not appearing in an equal proportion but very, very differently from each other. In this case, the alleles denoted by the capital letters A and B seem to appear much more frequently together, and also the alleles denoted by the lowercase a and b also appear to occur much more frequently with each other. So, in this case 45% in comparison to a situation when the lowercase a for example and a higher case b would be together in a single individual. The reason for this is, of course, recombination, so we have a non-recombinant gametes on the one side and the recombined gametes on the other side. So, in other words again, in case a recombination happened between these two loci, then we have a different occurrence for these for these gametes. The degree of recombinations is measured by the recombination rate.\nLinkage Disequilibrium\nSo what we see is a departure from this equal distribution, denoted as a linkage disequilibrium or LD, which is the very frequently used abbreviation. I wanted to underline that LD is, in fact, a parameter of the population, so you need more individuals and, in the best-case scenario, a large number of individuals to determine the LD between two loci in a population. It is, in fact, a non-random association between the loci within this population, which could be measured so we can tell that the two loci are strongly linked or weakly linked or not linked at all together. In other words, the LD tells us something about the strength of the information if we see an allele in a certain locus and what it can tell us about the occurrence of another allele on another locus. There are various methods to measure LD, and we will talk about these measurement possibilities in the next video.\nHeatmap\nSo, here is an example of a part of a genome with a heat map, so here is basically the darker colors are high LD and lighter colors are low LD. So it’s measured with D’ in this example - don’t worry about this right now. But basically, what I want to show you that the SNPs that are close to each other tend to have a higher LD between each other, and there are, for example, SNPs – for example, #9 and #20 - you see that there are relatively lower LD between them. Also, there are segments on the genome that are so called LD blocks where all the pairwise combinations of the SNPs yield a high LD, so basically this whole block is inherited in one piece. I really like this picture about the LD block structure, in this particular case, in a chicken genome, that demonstrates a similar thing as in a previous picture or previous slide but on a much larger part of the genome. Also, it compares to chicken lines where we see that the same part of the genome could be very different even within the species when we talk about different breeds or, in this case, the different lines. And the reason for this could be that, for example, in this chicken breed, in this particular part of the genome, there are some genes that are very important for this breed, therefore this part of the genome is quite well conserved with very high pairwise LDs in this part and the same genes are, of course, present also in the other breed. In this breed these genes are not important or at least not selected for. Therefore, we do not see such a strong LD block occurring in this breed.\nWhy is LD important\nThis simple picture supposed to illustrate the use of LD and why is it important in the genomics as such. So what we have here is let’s say a chromosome and we have a SNP on them, and let’s say we did some kind of analysis where we found the significance of each SNP, so this would be the higher SNP is, the higher the significance level. Now we see that there we have some kind of a signal here. So the question is: is this the gene of interest that influences our trait? Of course, because we are speaking about SNPs that are themselves just markers and not the causal variants, so this is, in fact, not the gene of interest but actually just shows what region of the genome is interesting. For our case, so we have a region of interest here and most likely the exact gene of interest resides somewhere here. Now the size of this region is determined by the linkage disequilibrium so LD, because most likely this SNP, the highest significant SNP, is connected to all of these nucleotides around. Also that, the parts of the genome that were not observed and, therefore, helps us to find the gene that is actually interesting for us.\nSummary\nSo, to sum up why LD is important, it actually defines how far we are allowed to or supposed to look from the detected markers when we are looking for the causality on our genome, and it shows the association strength between the observed SNPs and the unobserved genes or QTLs. Also, I want to underline that the LD itself is actually dependent on a population or species, so most likely these association strengths or LD in these regions or the size of the regions might differ between the different populations.\nApplications\nAs for the applications of LD in genomics, it is really, really wide-ranging, so I just mentioned a few examples here. So in evolutionary biology, it allows us to reflect on past events and gives insight into evolutionary history. When it comes to genetic diversity, it’s a similar as in a previous point, but we often compute the effective population size for our populations, and one of the computation methods is via LD itself. Then we can tell something about the artificial or natural selection events on the genome when it again comes to the population with the so-called selection signatures and the genetic hitchhiking - these are events when the actual selected gene drags along part of the surrounding genome and appearing in a population as a selection signature. So this size is determined by the LD itself. My small schematics showed an example of a genome-wide association study where we could search for causal genes and, as it was mentioned, there we could locate our search to these interesting regions where the most important or most significant SNPs reside. And LD is also used in a genomic selection in an indirect way, so here the question is that, how many SNPs we need that are more or less equally distributed on the genome so the whole genome is covered. So we could detect these very small QTLs and very small genes that affect our trait of interest, which is usually a quantitative trait, so it is influenced by many genes of small effect. We need to ensure that the whole genome is covered properly, so that all of these small genes are connected to at least one snip in a high enough LD and, therefore, taken into account.\nWhen the genomic LD value is calculated, so again a slide to give an answer to the initial question: what is linkage disequilibrium? So, linkage disequilibrium is a parameter that quantifies the non-random association between the loci. It shows if the frequency of a different alleles between two loci is higher or lower than it would be expected if the loci were independent from each other and associated randomly.\nConclusion\nAnd the very last summary slide for the entire presentation we talked about SNPs, markers that allow us to track the associations between parts of the genome, and such associations and such connections could be non-random, especially if the SNPs or markers or parts of the genome are very close to each other. Such non-random association is called linkage disequilibrium or LD in a shortened abbreviated form. And there are wide-ranging applications of LD within genomics. In the following videos, we will continue with the exact measurement techniques of the linkage disequilibrium. Also, I will provide a very nice hands-on example but, of course, also show the way how the LD is computed for large data sets using PLINK. For today, I thank you for your time and see you at another video on the Genomics Bootcamp Channel. I wish you a very nice day.\n\n\n\nMeasuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium?\nPresenter(s): Gábor Mészáros, PhD\nHello everyone, welcome back to the Genomics Boot Camp. Today we will continue to talk about linkage disequilibrium. Before we start, however, I want to thank you for the 500 subscribers or the over 500 subscribers on the Genomics Boot Camp channel. It seems that there is a continued and ever increasing interest about the contents of the channel, which is, of course, very good news for me and good motivation to move forward. Of course, if you are not subscribed yet. you can use this opportunity to do so. So thank you, thank you, thank you, and without further delay, let’s move on to further discussions about linkage disequilibrium.\nLinkage disequilibrium\nSo let’s start with a little bit of activation of our prior knowledge. Namely that single nucleotide polymorphisms, or SNPs, exist, and these are actually the main marker types we speak about on this channel. Linkage disequilibrium, as such, also exists, and we spoke about this in the previous video. And in short, it actually measures the non-random associations between SNPs, and there is wide-ranging applications for LD in genomics, but for today’s question: how to measure linkage disequilibrium?\nWe saw this graph in the previous video, and basically what we see is that some of the allele combinations are appearing much more frequently than others, thus leading to linkage disequilibrium.\nThis non-random association between some alleles of a different loci can be visualized in such a two by two, table where we have on one side locus A and with the alleles, capital A and small a, and on the top side, locus B with alleles, capital B and small b. Each of the combinations is then represented with a proportion, so basically the pAB is the proportion of the combinations for these alleles. If we are interested, for example, in the total proportion of the allele A, then we have it in the right side. The same we can tell about the proportion of all the other alleles, so the  lowercase a, the capital B, and this lowercase b. There is one rule if we are talking about biallelic systems that the sum of the proportion of the two alleles sums up to 1.\nIn case of linkage equilibrium, when the loci A and B are not linked, we can actually compute the genotype proportions based on the allele proportions. This is shown, for example, in the first line, and we can actually do the same for all the other genotypes. One additional comment here, so we use this feature of this previous 2 x 2 tables. So I told you that basically the proportions for the two alleles are summing up to 1, so basically the pb we can also express as 1 - pB. This is, of course, a preferred setting, because we reduce the number of unknowns from four to two. So basically, we just have to work with two variables: pA and pB. Now this situation is valid for the linkage equilibrium as I mentioned, so when the alleles are unconnected.\nIn case of linkage disequilibrium, however, these equations do not hold true. In other words, from the proportions of the alleles, we cannot compute the proportion of the genotypes. We cannot compute this, because there is a difference between these two metrics. That is the disequilibrium coefficient or D, and it can be computed as the proportion of the AB together minus the proportion of A times proportion of B, and when we include this equilibrium coefficient to the equations as before we get the proportions of the genotypes. So in other words, this D or this equilibrium coefficient is a measure of the linkage disequilibrium or a coefficient that can be used to express it.\nD prime\nBut we have a bit of a problem with this this equilibrium coefficient because it’s a bit hard to interpret and the sign is arbitrary so it’s depending on which allele actually you consider first. Of course, there is a common convention to set the capital A and capital B to be the most common allele and the smaller case a and b to be the rare allele. But of course, the rarity of the alleles, or the minor allele frequencies, can change from population to population, even within the same species, so that is not such a big win after all.\nNow there is a better version of this D or the disequilibrium coefficient called the D’. That is a scaled version that is computed as shown in this slide. So basically, it is divided by the minimum of these two values, in case the disequilibrium coefficient is lower than zero, or the minimum or of these two values if the disequilibrium coefficient is higher than zero. Now we end up with a coefficient, which is another, a more popular version of the linkage disequilibrium and ranges between minus one and plus one. Extreme values, in this case, imply that at least one of the haplotypes was not observed.\nIt has several advantages that is the D’ = 1 or D’ = -1 means that the two SNPs are not separated by recombination, so that they are in a complete LD. It also means that if the earlier frequencies of the two loci are relatively similar then the high D’ means that the markers are good surrogates for each other, but in this case, we have also some disadvantages. namely that the D’ estimates are inflated. In case, the sample size is small, and the D’ estimates might be also inflated when one of the alleles is rare.\nCorrelation\nThen there is a follow-up question if there is a more intuitive way to measure linkage disequilibrium, to which the answer is yes. And that it can be measured straight with the correlation coefficient, or an adapted value anyway. This correlation coefficient as we would expect expresses the mutual relationship between the alleles of the two loci.\nNow a little bit of refreshment on correlation, so this is the correlation as we know it from the statistics. So, it has a standardized abbreviation of lowercase r and ranges from -1 to +1. Basically there could be different values for the correlation coefficient, which describe how the two variables behave in relation to each other. So basically, we have an x and a y, and if, with the increasing x, the y decreases, then we have a negative correlation coefficient. And if the two values are increasing, so, with the increasing x, also y increases, then we have a positive correlation coefficient. In between -1 and 1, so the extreme values, so the correlation coefficient expresses the strength of such relationships. And of course, it could be situations when the two values are not correlated, so then the correlation coefficient is zero.\nNow for linkage disequilibrium, we use the squared correlation coefficient, so this is the r2. So the squared correlation between the markers, and therefore, it ranges between 0 and 1. r2 of one implies that the markers provide exactly the same information, and the r2 of zero implied that the two markers are not connected at all, so independent from each other. So, in other words, the r2 measures a loss of efficiency when a marker A is replaced with the marker B.\nNow the r2 value could be computed from the allele frequencies themselves and what we need is actually just 3 values from here: the joint appearance of A and B, the proportion of capital A, and the proportion of capital B.\nBased on these values, we can put them into this very nice equation, and we can compute the correlation coefficient r2. Now I realize that there is quite a jump from the correlation itself to this equation, and I provide actually all the details in an additional video, which I call the advanced or “advanced” video, so you can look at it if you’re interested. It is already uploaded on the Genomics Boot Camp channel.\nAnd once we have our measure of linkage disequilibrium, of course, we can compute the pairwise linkage disequilibria between markers.  Such it was done in Qanbari et al., 2010, for this example. So in here, what we have on the y-axis is the linkage disequilibrium, measured in r2. On the x-axis, there is a distance between the markers, in this case, measured in morgans, which can be, of course, translated to megabases. So this would be one megabase, two megabase difference, three, four, and five megabase difference. Each dot on this graph is a linkage disequilibrium value for a marker pair, so you see that if the two alleles are very close to each other, then the linkage disequilibrium could be very high up to 1, but then this value quickly goes down to a lower value as the distances between the markers or the genomic distances between the markers increase. This sudden decrease of the average linkage disequilibrium between markers is shown with this dashed line, and in fact, its shape is very typical for all the organisms. We refer to this sudden decrease as linkage disequilibrium decay.\nThe shape of this LD decay is, in general, similar between the organisms, but the starting values and the further developments can be different when it comes to different species or, in this case, as shown in Pérez O’Brien, 2014, it could be different even when we consider Bos taurus and Bos indicus breeds. This, of course, has some relevance when the LD is used for further genomic analysis as discussed in the last video.\nSummary\nSo, to summarize this video, the LD characterizes the degree of relationships between the nearby loci. There are various methods and possibilities to measure it. One of them is the disequilibrium coefficient denoted by D or as a D’ - that is a metric to use to quantify the LD but it has some disadvantages. And then the other more commonly used metric is the r2, or the squared correlation coefficient. That is a robust measure of the LD. So this is the end of this video. If you’re interested how we arrive to the equation to compute the r2, then be sure to check out the next one, a follow-up to this, and if you say that is enough for today, then of course, I thank you for your time and wish you a very nice continuation of the day.\n\n\n\n(Advanced) Measuring linkage disequilibrium\nTitle: How to measure linkage disequilibrium? (ADVANCED)\nPresenter(s): Gábor Mészáros, PhD\nHello, everyone. Welcome back to another video on the Genomics Boot Camp. This time is a follow-up or an add-on to the previous video how to measure linkage disequilibrium. This video is entitled “advanced,” because we have a bit more equations here as on the average of the channel. The reason I did not include these ones into the previous video is that perhaps not everybody is interested or they don’t like to see, you know, too many equations in an educational video. Therefore, I thought it makes sense to divide the two, but you are here, you are interested, so let’s get to it. I repeat here a few slides that were in the original video in order to provide context, but we will go through these ones in a quicker manner.\nR-squared\nWe start with this video where we ended last time, and this is our prior knowledge now. And the most important part is the r2, so there we have the commonly used robust measure of LD in the previous video. I was just including the summary equation, but in this video, we will have more information on how we arrive to this r2.\nSo again we have the correlation coefficient from a general statistics and abbreviated as a lowercase r, ranges from -1 to +1, and there are various values of the correlation coefficients that are expressing the relationships between the two variables x and y.\nOf course, the correlation coefficient is also computed somehow and it is actually computed as a covariance between x and y divided by the square root of variance of x and variance of y. If we want to write out this one in a further detail, so we have this would be the covariance between x and y and the variance of x and the variance of y in these brackets. Basically, the goal of this video is to provide you with the link between this equation and the r2 equation to compute the linkage disequilibrium that was shown at the end of the last video and also will be shown more times also in this one.\nSo for refreshment we have the r2 value that is the squared correlation between the markers, therefore ranges between 0 and 1. And the value of 1 implies that the markers provide exactly the same information, while the value of 0 for the r square implies that the markers are not connected at all to each other. Therefore, it measured the loss of efficiency when a marker A is replaced by a marker B.\nThe r2 itself could be computed based on the allele frequencies from this 2x2 table, so we have a locus A and the locus B and each of them has two values capital B and small b and a capital A and a small a. So these values are valid for a biallelic system - so these kind of computations are valid only in a case that the markers have a two alleles, but because the SNPs on the SNP chips are biallelic by design, then we are good to go. There are, in fact, only three values that we need; that is the joint occurrence of A and B and the respective frequencies of capital A and capital B.\nNow we have the r2, which is the measure of the linkage disequilibrium and that can be computed with this equation. Previously, it was stated that the correlation itself, so the r, could be computed with this equation. So basically, the main question of this video is how to get from this to this.\nNow to answer this question, we have to reformulate our example a little bit. And we consider two loci A and B again on one gamete but each with a possible random realization. We give the value of 1 to locus A with the probability of pA and the value of 0 with the probability of p 1 minus p­A , so this is basically the other allele. So this is the pa and pA and then this is the pa, but because pa and p­A equal to one, so the sum of them is equal to one. So then the pa could be expressed as 1 minus pA. The very same thing happens with the locus B, so we assign a value of 1 with the probability of B and the value of 0 with the probability of one minus pB.\nSo this is a very particular example that we will follow up on in detail in the next video, but still I thought it’s useful to show it here so perhaps it’s eases some understanding. So basically what we have are two loci here: locus 1 and locus 2. So we have alleles capital A, lowercase a, and for locus 2, it’s  alleles B and lowercase b. And basically what we do is we replace with the capital A with that ones and the lowercase a is with the zeros, so then it looks like this. So basically. we just count how many capital A’s we have, so that would be a 5. And how many capital B’s we have that would be a 6. And the joint occurrence of capital A and capital B would be 4 in this particular example.\nNow, of course, we are interested in in a general example, so we introduce a population size of N and we can compute the proportion of the allele A as the sum of occurrence of allele A divided by the population size N. The same way, we can compute the proportion of B with the sum of the allele capital B divided by the population size N. And the joint occurrence of A and B we can compute when the jointly occurring A and B together divided by the population size N.\nFollow up\nOkay then the follow-up is what is the correlation between the realizations of 0 and 1 of a random variable at the two loci, and here comes again the equation for the correlation coefficient so that is the r. This equation basically has three parts, so that is one in this top, so basically the covariance, and the variance of x and the variance of y.\nAnd basically we have all these parts figured out already in the previous slide as shown here, so it can be replaced by N times the joint occurrence of A and B. And the sum x can be replaced by N times the proportion of A and the y could be replaced as the N times proportion of B. And similarly, all the other parts could be replaced and then, in some cases, also adjusted, so we end up with such a beautiful and much more simple equations. To provide you a link to the next slide, I also color code them, so basically we have this orange, the green, and the blue part, which we then put back to the original correlation coefficient equation.\nSo this is the top part here is the correlation coefficient, and then we put in the orange, the green, and the blue part. And then you see here that the N is present all the way, so it can be actually removed from the equation and then only this part remains. So this part is basically the simple correlation between two loci 1 and 2 containing the alleles A and B, and if we square that equation, because we are interested in r2 rather than a simple r, so then we have this correlation or a squared correlation coefficient that is the measure of the linkage disequilibrium as we have shown in the previous video and also in this one.\nSummary\nSo, for the summary, the r2 is a commonly used and robust measure of LD, and the computation of the r2 is adapted based on the equation for the correlation coefficient. And for the allele and genotype frequencies in reality, of course, we talk about computation of linkage disequilibria between a large number of markers or marker pairs, so we use software for it, for example, PLINK or other software. And also, in the follow-up videos, we will actually show how it is done using PLINK itself. For today, I thank you for your time, thank you for your interest also in this advanced content, and I wish you a very nice continuation of the day.\n\n\n\nComputing linkage disequilibrium\nTitle: Compute linkage disequilibrium (Part 1)\nPresenter(s): Gábor Mészáros, PhD\nHello, everyone. Welcome back to another video on the Genomics Boot Camp back again with another video on linkage disequilibrium. This is the first part of the finishing two when we are actually computing the linkage disequilibrium this time by hand. I believe this is an interesting piece of knowledge, so it’s worthwhile to look at.\nSo what we know so far. So LD characterizes the degree of relationship between two loci as always, and r2 is the commonly used measure of the LD computed with the equation below.\nAnd here we are at the first of the two examples I will show you today. The first one is the very basic one, and actually you might recall this 2x2 table from the previous videos. When we look at the r2 equation, it is clear what we need to compute, and that is the proportion of A, proportion of B, and the proportion of the joint occurrence of A and B. Therefore, we are interested in what is in this segment or cell, in this other one for the B, and the joint occurrence of A and B together. Because we are speaking about proportions, we need to divide them with the sum of all alleles. So, this is already how we compute the proportions. So, the proportion of A is the 5 divided by 10, so 0.5. In this case proportion of the B allele is 6 divided by 10, so 0.6. And the joint occurrence is 4 divided by 10, 0.4. When we have our usual equation, we basically have all the unknowns, so the proportion of A, B, and AB and we put them in, and after the computation, we get the LD between these two loci A and B as 0.167.\nSo after this warm-up example, I show you the real deal. That is this LD calculation exercise that was shown by Professor Henner Simianer at the “Livestock Conservation Genomics Data Tools and Trends” workshop back in 2012 in Pag Island, Croatia. I really like this example, because it puts the LD calculation into context of haplotypes and all the allele combinations using SNPs. So first, I give you the initial example how it was given back in 2012. Then I give you the breakdown of the example, and you will have the chance to stop the video and do the computations yourself and, after that, you can look up the solution in this video.\nSo the exercise looks like this, so we have our genome here and we have a four SNPs. First one is an AT SNP, second one a CG SNP, third one is also a CG SNP, and the fourth is an AT SNP. We have also our population, and these are already the haplotypes, where this haplotype for our genome or part of the genome appears 17 times, this other one 14 times, 3 times, 3 times, 2 times, and this haplotype appears just 1 time in the population. The exercise is to calculate the LD between all pairs of loci. Now in this video, I will show you how to compute LD between locus 1 and 2 in detail, then I will show you the solutions for the two other pairs of loci and the last three you can follow up yourselves if you want.\nSo to give you a bit of a head’s up information, I list here the loci that you need to compute the proportions for, so for locus 1 is the A, the locus 2 is a C, locus 3 is also a C, and locus 4 is an A. You need to compute it for these alleles specifically, because these are the major alleles for these loci. So, if you count them up, so the A is the major allele for locus 1 and the C is for locus 2 and the C is also for locus 3 and A for locus 4, and obviously then you need to compute the joint occurrences between the two loci that you are following up and you want to compute LD for.\nSo, if you actually have all these proportions, you can compute the LD between all pairs or actually for pairs 1-2, 1-3, and 1-4 as we will list it in this video. And putting these proportions into the well-known equation, then you get the r2 value for that pair of loci. Now, if you want to go ahead and do the calculations yourself on with pen and paper, then this is the moment to pause the video and well just go ahead with the computations. And after you are done or at least you are done with the locus, for example locus 1-2, then to resume the video and see if you have done well or if there is something to be corrected. Okay, so we are continuing with the solution in 3… 2… 1…\nNow actually what needs to be done is pretty similar to the previous smaller case, but in this case, you need to build up the 2x2 table yourself, so here we have a locus 1 with an AT SNP, a locus 2 with the CG SNP. And because we told that the A is the actual major locus, then you actually need to count them up, so basically, you need to fill up this spot in the 2x2 table. For the locus 2, the C is the major locus, so you need to fill up this spot on the 2x2 table. And of course, the joint occurrence as well.\nSo how to do that? Of course, we will use the actual haplotype numbers as they are given in the exercise. Actually, right now, we are looking at locus 1 and locus 2, so basically these two columns are interesting for us, so we can actually hide the other ones so that they don’t bother us too much.\nNow perhaps the first thing you might consider is that, well, we need to calculate the proportions, so we basically need to know how many haplotypes are there total. So basically, we add up 17 plus 14 plus 3 plus 3 plus 2 plus 1 and, when we do that, we actually arrived number 40, so this is the total number of haplotypes now.\nFor locus 1, we need to count how many times the allele A appears, so in this haplotype, there is an A so, it’s 17 times. So it’s 17. Here in the locus one is a T, so that is we don’t count. This also there is an A here and A here, here. There is a T here, so we don’t count this, and the A here. So basically, we have a 17 plus 3 plus 3 plus 1 - that is together 24.\nFollowing the same logic for locus 2, in this case, we count the C allele. So here we have a C, so we count this. Here we have a G, that so we don’t count. This count, this, this, and this, so everywhere where is the c, so there is 17 plus 3 plus 3 plus 2 and that is together 25.\nThe last thing we need to find out that how many times the A and C appear together so here the A and C appear together, so that we count, this we don’t count this, but also AC here, AC here. Here although there is a C, but there is not an A, so we don’t count this part. And also here is an A but there is a G here, so it’s not an AC combination, so we don’t count this. So basically, we count this 17, these three, and this three, which is together 23.\nAnd because we are actually interested in proportions rather than the numbers themselves. So we divide for the locus 1, the proportion of A is 24 divided by 40 - that is 0.6. And therefore, locus 2, the proportion of C is 25 divided by 40 - that is 0.625. And the joint occurrence the same way, so the 23 divided by 40 and that is this number 0.575.\nSo with this, we already have everything. So we have a proportion of A, proportion of C, and the joint occurrence, so we have our beautiful equation here. So this is the joint occurrence and this would be the proportion of A and proportion of C, so we put them in into the equation itself and then we end up with an LD between locus 1 and locus 2 as 0.711, so this is the LD between these two loci.\nOn this slide, I give you all the results for the other loci, so locus 1, 2, 3, and 4 and the joint occurrences of these three pairs. And if we fill them in into the equation, the well-known equation, then we end up with – well, this is just a repetition from before - and then there is the LD between locus 1-3 and the LD between locus 1 and 4, so this is basically it this is how you compute LD by hand.\nSo, for the summary, an example of the LD computation by hand was shown, which is not a very widespread piece of knowledge, so I argue that is really good to have it in your inventory. But on the other hand, we also have to note that this manual mode of computation is not very effective. For that, we need to use software solutions that are able to handle computations of LD between thousands and hundreds of thousands of loci, and such computations will be shown in the next video. For today, I thank you for your time and have a very nice rest of the day."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Tools/Software Resources",
    "section": "",
    "text": "Below is a table listing various software and other tools that are commonly used in genomics analyses. Most are mentioned throughout the PGC Video Textbook. Please use the outline below to navigate through the table.\n\nGeneral Tools\nGWAS\n\nQC\nImputation and haplotype phasing\nRelatedness/Population stratification\nAssociation testing\nPower testing\n\nPRS methods\nPost-GWAS analysis\n\nSNP heritability\nGenetic correlation\nTWAS\nPheWAS\nGene/Gene set identification\neQTLs\nGene set enrichment\nFunctionally-informed risk scores\nFine-mapping\nConditional analyses\nEWAS\n\nWGS and WES analysis\n\nRaw count QC\nPreprocessing\nAlignment\nPost-alignment processing\nVariant calling (SNPs/INDELs)\nVariant calling (CNVs)\nVariant annotation\nAssociation testing\n\nDatasets and data access\nOther Genomics tools\n\n\n\n\nAnalysis\nStep\nSoftware\nAdditional Tutorials\nLanguage\nPubMed ID\n\n\n\n\nGeneral Tools\n\n\n\n\n\n\n\nGeneral\nFile management/formatting\nSAMtools\nManual\nC; Python\n19505943; 21903627; 21320865\n\n\nGeneral\nFile management/formatting\nBCFtools\nManual; GitHub\nC; Python\n19505943; 21903627; 21320865\n\n\nGeneral\nFile management/formatting\nBEDtools\nGitHub\nC; C++\n20110278\n\n\nGWAS\n\n\n\n\n\n\n\nGWAS\nGenotype calling and Imputation\nMoChA\nGitHub\nC; R\n29995854; 32581363\n\n\nGWAS\nRapid Imputation and Computational Pipeline for GWAS (RICOPILI)\nRICOPILI pipeline\nRICOPILI download; Tutorial\nPerl; R\n31393554\n\n\nGWAS\nGeneral; QC\nPLINK1.9/PLINK2\nPLINK website\nC; C++\n17701901\n\n\nGWAS\nRelatedness/Kinship\nKING\nManual; GENESIS documentation\n\n20926424\n\n\nGWAS\nPopulation Stratification/PCA\nGENESIS/PCAiR\nManual; Tutorial; GENESIS GitHub\nR\n20926424; 25810074; 31329242\n\n\nGWAS\nPopulation Stratification/PCA\nGENESIS/PC-Relate\nTutorial\nR\n26748516\n\n\nGWAS\nLocal Ancestry and Admixture Inference\nRFMix\nGitHub\nC++\n23910464\n\n\nGWAS\nLocal Ancestry and Admixture Inference\nTRACTOR\nGitHub\nPython\n33462486\n\n\nGWAS\nImputation panel\nTOPMed\nImputation server\nNA\n33568819\n\n\nGWAS\nImputation panel\n1000 Genomes\nImputation server\nNA\n26432245\n\n\nGWAS\nImputation panel\nHRC\nWebsite\nNA\n27548312\n\n\nGWAS\nImputation algorithm\nminimac\nGitHub\nC++; C\n27571263\n\n\nGWAS\nHaplotype phasing\nEAGLE\nManual; GitHub\nC++\n27694958; 27270109\n\n\nGWAS\nPhasing and Imputation\nBEAGLE\nWebsite; Tutorial\nJava\n23535385; 30100085; 34478634\n\n\nGWAS\nHaplotype phasing\nSHAPEIT\nWebsite; Tutorial; GitHub\nC++\n22138821; 23269371; 24094745; 24743097; 25653097\n\n\nGWAS\nPower calculations\ngenpwr\nManual\nR\n32721961\n\n\nGWAS\nAssociation Testing\nSAIGE\nGitHub; More documentation\nR; C++\n30104761\n\n\nGWAS\nAssociation Testing\nBOLT-LMM\nDownload; Manual\n\n25642633\n\n\nGWAS\nAssociation Testing\nREGENIE\nGitHub; Tutorial\nC++\n34017140\n\n\nGWAS\nCausal Mixture Model\nMiXeR\nGitHub\nC\n32427991; 31160569\n\n\nGWAS\nMulti-Trait Analysis of GWAS\nMTAG\nGitHub\nPython\n29292387\n\n\nGWAS\nqqplot and Manhattan plot\nqqman\nGitHub\nR\ndoi link\n\n\nGWAS\nQC of GWAS summary statistics\nDENTIST\nGitHub; Yang Lab Website\nC++\n34880243\n\n\nGWAS\nGWAS Quality Score\nGQS\nGitHub\nPython\n36651666\n\n\nPRS Methods\n\n\n\n\n\n\n\nPRS\nPRS methods\nPRS-CS\nGitHub\nPython\n30992449\n\n\nPRS\nPRS methods\nPRS-CSx\nGitHub\nPython\n35513724\n\n\nPRS\nPRS methods\nPRSice2\nTutorial\nR\n31307061\n\n\nPRS\nPRS methods\nLassosum\nGitHub\nR; C++\n28480976\n\n\nPRS\nPRS methods\nLDpred2\nManual\nR\n33326037\n\n\nPRS\nPRS methods\nSBayesR\nManual\n\n31704910\n\n\nPRS\nPRS methods\nMegaPRS\n\nPython\n34234142\n\n\nPost-GWAS Analyses\n\n\n\n\n\n\n\nSNP heritability\n\nGCTA-GREML\nTutorial; GitHub\nC++\n27457963\n\n\nSNP heritability and genetic correlation\nLinkage Disequilibrium Score Regression\nLDSC\nTutorial; GitHub\nPython\n25642630\n\n\nGenetic correlation\nGenomic Structural Equation Modeling\nGenomic SEM\nTutorial; GitHub\nR\n30962613\n\n\nGenetic correlation\n\nLAVA\nGitHub\nR\n35288712\n\n\nTWAS\nFunctional Summary-based Imputation\nFUSION\nGitHub; Tutorial\nR\n26854917\n\n\nTWAS\nTranscriptomic Imputation\nPrediXcan\nGitHub; Tutorial\nPython\n26258848\n\n\nTWAS\nTranscriptomic Imputation\nS-PrediXcan\nGitHub\nPython\n29739930\n\n\nTWAS\nTissue models (GTEx, CommonMind, PsychENCODE)\nPredictDB\nGTEx models; PsychENCODE models; CommonMind DLPFC models\nNA\n33499903; 30545856; 30911161\n\n\nTWAS/MR\nSummary-data-based Mendelian Randomization\nSMR\nSoftware\n\n27019110\n\n\nPheWAS\nPhenome-Wide Association Studies\nPheWAS R package\nGitHub; Tutorial\nR\n20335276; 24733291; 24270849\n\n\nGene/Gene set identification\nGene Set Enrichment Analysis\nGSEA\nDocumentation; README\nJava; R\n16199517; 12808457\n\n\nGene/Gene set identification\nMolecular Signatures Database\nMsigDB\nDocumentation\nJava\n16199517; 21546393; 26771021\n\n\nGene/Gene set identification\nGeneralized gene-set analysis of GWAS data\nMAGMA\nSoftware; Tutorial; Manual\n\n25885710; 27070863; 30218068\n\n\nGene/Gene set identification\nHiC-informed gene-based tests\nH-MAGMA\nGitHub\nR\n32152537\n\n\nGene/Gene set identification\neQTL-informed gene-based tests\nE-MAGMA\nGitHub Tutorial\n\n33624746\n\n\nGene/Gene set identification\nFunctional Mapping and Annotation of Genome-Wide Association Studies\nFUMA\nGitHub; Tutorial\nBlade; Java\n29184056\n\n\nGene/Gene set identification\nSet/Pathway-based analysis\nPRSet\nTutorial\nR\n36749789\n\n\nGene/Gene set identification\nGene set enrichment\ng:profiler\nWebsite; R package; Documentation\nR\n31066453\n\n\neQTLs\neQTL and GWAS CAusal Variants Identification in Associated Regions\neCAVIAR\nWebsite; GitHub\nC++\n27866706\n\n\neQTLs\nCalling eQTLs\nfastQTL\nGitHub; Documentation\nC++\n26708335\n\n\neQTLs\nCalling eQTLs\nmatrixQTL\nTutorial; GitHub\nR\n22492648\n\n\nFunctionally-informed Risk Scores\nPolygenic Transcriptomic Risk Score\nPTRS\nGitHub; Calculate PTRS for UKBB\nPython\n35027082\n\n\nFunctionally-informed Risk Scores\nTrans-ethnic polygenic risk scores\nPolyPred/PolyPred+\nGitHub; Tutorial; Documentation\nPython\n35393596\n\n\nFine-mapping\n“Sum of Single Effects” model\nSuSiE (used with coloc)\nR package\nR\n37220626; 34587156\n\n\nFine-mapping\nCo-localization analysis\ncoloc\nGitHub; R package\nR\n24830394; 32310995; 34587156\n\n\nFine-mapping\nCAusal Variants Identification in Associated Regions\nCAVIAR\nWebsite; GitHub\nC++\n25104515\n\n\nFine-mapping\n\nFINEMAP\nBenner lab website; FinnGen pipeline\nPython; WDL\n26773131\n\n\nFine-mapping\nIntegrative Genetic Association Analysis using Deterministic Approximation of Posteriors\nDAP-G\nGitHub; Manual\nC++\n27236919\n\n\nConditional analyses\nMulti-SNP-based conditional and joint association analysis using GWAS summary data\nGCTA-COJO\nWebsite; GCTA GitHub\nC++\n22426310\n\n\nConditional analyses\nMulti-trait-based conditional and joint analysis using GWAS summary data\nGCTA-mtCOJO\nWebsite; GCTA GitHub\nC++\n32398722\n\n\nEWAS\nEpigenome-Wide Association Study\newastools\nGitHub; Workflow\nR\n\n\n\nSequencing Tools\n\n\n\n\n\n\n\nWGS/WES\nRaw count QC\nFastQC\nGitHub\nJava\n\n\n\nWGS/WES\nRaw count QC\nMultiQC\nGitHub\nPython\n27312411\n\n\nWGS/WES\nRaw count QC\nNGS QC Toolkit\nGitHub\nPerl\n22312429\n\n\nWGS/WES\nRaw count QC\nQC-Chain\nGitHub\nWDL\n23565205\n\n\nWGS/WES\nPreprocessing\nTrimGalore\nGitHub; Documentation\nPerl\n\n\n\nWGS/WES\nPreprocessing\nCutadapt\nGitHub; Documentation\nPython\n\n\n\nWGS/WES\nPreprocessing\nBtrim\nWebsite\nC\n21651976\n\n\nWGS/WES\nPreprocessing\nTrimmonatic\nGitHub\nJava\n24695404\n\n\nWGS/WES\nPreprocessing\nleeHom\nGitHub; Documentation\nC++\n25100869\n\n\nWGS/WES\nAlignment\nBWA/BWA-mem\nGitHub\nC\n19451168; 20080505\n\n\nWGS/WES\nAlignment\nMinimap2\nGitHub\nC; Java; TeX\n29750242\n\n\nWGS/WES\nAlignment\nBowtie2\nGitHub\nC++; Perl\n22388286\n\n\nWGS/WES\nAlignment\nGATK\nWebsite; GitHub\nJava\nBook citation: Van der Auwera GA & O’Connor BD. (2020). Genomics in the Cloud: Using Docker, GATK, and WDL in Terra (1st Edition). O’Reilly Media.\n\n\nWGS/WES\nAlignment\nCactus\nGitHub\nC; Python\n33177663\n\n\nWGS/WES\nAlignment\nGiraffe\nGitHub\nC++\n\n\n\nWGS/WES\nPost-alignment processing\nPicard (MarkDuplicates)\nGitHub; Website; Tutorial\nJava\n\n\n\nWGS/WES\nPost-alignment processing\nGATK (see above links in “Alignment”)\n\n\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nWeCall\nGitHub\nC++; Python\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nGATK HaplotypeCaller\nWebsite\nJava\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nPicard\nWebsite; GitHub\nJava\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nFreeBayes\nGitHub\nC++\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nDeepVariant\nGitHub\nPython; C++\n\n\n\nWGS/WES\nVariant calling (SNPs/INDELs)\nSlivar\nGitHub\nNim; HTML; Java\n\n\n\nWGS/WES\nVariant calling (CNVs)\nParliament2\nGitHub; Documentation\nPython; C++; C\n33347570\n\n\nWGS/WES\nVariant calling (CNVs)\nmuCNV\nGitHub\nC++; C\n33760063\n\n\nWGS/WES\nVariant calling (CNVs)\nTT-Mars\nGitHub\nPython\n35524317\n\n\nWGS/WES\nVariant calling (CNVs)\nTruvari\nGitHub\nPython\n36575487\n\n\nWGS/WES\nVariant calling (CNVs)\nXHMM\nManual; Tutorial\nC++\n24763994\n\n\nWGS/WES\nVariant calling (CNVs)\nCODEX\nGitHub\nR\n25618849\n\n\nWGS/WES\nVariant calling (CNVs)\nDECA\nGitHub\nScala; R\n31604420\n\n\nWGS/WES\nVariant calling (CNVs)\nCLAMMS\nGitHub\nC\n26382196\n\n\nWGS/WES\nVariant calling (CNVs)\ncn.MOPS\nGitHub\nR\n22302147\n\n\nWGS/WES\nVariant calling (CNVs)\nGATK-gCNV\nGitHub; Tutorial\nPython; WDL\nBabadi, et. al (2022), BioRxiv\n\n\nWGS/WES\nVariant calling (CNVs)\nReadDepth\nGitHub\nR\n21305028\n\n\nWGS/WES\nVariant calling (CNVs)\nERDS\nGitHub\nPerl; C\n22939633\n\n\nWGS/WES\nVariant annotation\ngnomAD\nWebsite\nNA\n32461654\n\n\nWGS/WES\nVariant annotation\nSnpEff\nGitHub\nJava\n22728672\n\n\nWGS/WES\nVariant annotation\nHail\nWebsite\n\n\n\n\nWGS/WES\nVariant annotation\nSnpSift\nGitHub\nJava\n22435069\n\n\nWGS/WES\nVariant annotation\nClinVar\nWebsite\nNA\n\n\n\nWGS/WES\nVariant annotation\nHuman Gene Mutation Database (HGMD)\nHGMD Website\nNA\n\n\n\nWGS/WES\nVariant annotation\ndbSNP\nWebsite\nNA\n\n\n\nWGS/WES\nVariant annotation\ndbNSFP\nWebsite\nNA\n21520341\n\n\nWGS/WES\nVariant annotation\nSIFT\nWebsite\nNA\n22689647\n\n\nWGS/WES\nVariant annotation\nPolyphen2\nWebsite\nNA\n20354512\n\n\nWGS/WES\nVariant annotation\nMutationTaster\nWebsite\nNA\n24681721\n\n\nWGS/WES\nVariant annotation\nCombined Annotation Dependent Depletion (CADD)\nCADD website\nNA\n30371827\n\n\nWGS/WES\nVariant annotation\nCONsensus DELeteriousness (CONDEL)\nCONDEL website\nNA\n\n\n\nWGS/WES\nVariant annotation\nMultivariate Analysis of Protein Polymorphism(MAPP)\nMAPP website\nNA\n15965030\n\n\nWGS/WES\nVariant annotation\nMutationAssessor\nWebsite\nNA\n21727090\n\n\nWGS/WES\nVariant annotation\nANNOVAR\nWebsite\nPerl\n20601685\n\n\nWGS/WES\nVariant annotation\nVariant Effect Predictor (VEP)\nVEP Website\nPerl\n27268795\n\n\nWGS/WES\nVariant annotation\nSeattleSeq\nWebsite\nNA\n19684571\n\n\nWGS/WES\nVariant annotation\nClinGen\nWebsite\n\n\n\n\nWGS/WES\nVariant annotation\nX-CNV\nGitHub; Documentation\nR\n34407882\n\n\nWGS/WES\nVariant annotation\nAnnotSV\nGitHub; Documentation\nTCL; Python\n29669011\n\n\nWGS/WES\nAssociation Testing\nEPACTS\nGitHub; Documentation\nC++\n\n\n\nWGS/WES\nAssociation Testing\nBOLT-LMM\nDownload; Manual\n\n25642633\n\n\nWGS/WES\nAssociation Testing\nREGENIE\nGitHub; Tutorial\nC++\n34017140\n\n\nWGS/WES\nAssociation Testing\nSNP-Set (Sequence) Kernal Association Test (SKAT)\nSKAT Documentation; Website\nR\n23684009\n\n\nWGS/WES\nAssociation Testing\nSAIGE\nGitHub; More documentation\nR; C++\n30104761\n\n\nWGS/WES\nAssociation Testing\nGeneralized linear Mixed Model Association Tests (GMMAT)\nGitHub\nC; R; C++\n27018471\n\n\nWGS/WES\nAssociation Testing\nCoCoRV (Rare variant tests)\nGitHub\nC++\n35545612\n\n\nWGS/WES\nAdditional tools\nGLnexus\nGitHub\nC++\n\n\n\nDatasets/Data Access\n\n\n\n\n\n\n\n1000 Genomes\n\n\n\n\n\n\n\ndbGAP (US)\n\n\n\n\n\n\n\nFinnGen\n\n\n\n\n\n\n\nGTEx\n\n\n\n\n\n\n\nGWAS Catalog\n\n\n\n\n\n\n\nNIMH repository (US)\n\n\n\n\n\n\n\nPGC Summary Statistics\n\n\n\n\n\n\n\nPsychENCODE\n\n\n\n\n\n\n\nPsychSCREEN\n\n\n\n\n\n\n\nOther Genomics Tools\n\n\n\n\n\n\n\nEnsembl\nGenome browser for annotation, alignment, and other tools\n\n\n\n\n\n\nBioMart R package\nR tool for pulling annotation information from Ensembl and other large databases\n\nDocumentation\nR\n\n\n\nLiftOver\nTool for converting genome coordinates\n\n\n\n\n\n\nUCSC Browser\nTools for visualizing genomic data, aligning sequences\n\n\n\n\n\n\nGenecards\nDatabase for information on annotated and predicted human genes"
  },
  {
    "objectID": "software_correlation_transcript.html",
    "href": "software_correlation_transcript.html",
    "title": "Software Tutorials: Genetic Correlation and SNP Heritability (Video Transcript)",
    "section": "",
    "text": "LAVA\nTitle: LAVA Tutorial\nPresenter(s): Josefin Werme\nJosefin Werme:\nHi, everyone! My name is Josefin Werme. I work at the Complex Trait Genetics Lab at the VU in Amsterdam, and today I’ll do a small tutorial to show you how to run LAVA.\nSo, LAVA is a tool dedicated to local genetic correlation analysis, and as the name suggests, this is simply a genetic relation but sort of for a local region that you have specified. And today, I will not have any time to go into technical details about this method, but if you’re interested in those, I suggest you check out this pre-print on BioRxiv.\nToday we will go through four different analysis options. First, we have a univariate test, which is used to confirm the presence of local genetic signal for any phenotypes of interest. Then, we have a standard bivariate test, which is used to evaluate the local genetic correlation between any two phenotypes of interest. And then, we also have two types of conditional tests. First, a partial correlation, where you can (which you can use to) evaluate the local genetic correlation between two phenotypes of interest conditioned on another phenotype or a set of other phenotypes. And then, we also have a multiple regression approach that can allow you to model the local genetic signal for an outcome phenotype of interest using several predictors jointly.\nSo to run this, you need basically just summary statistics for your phenotypes of interest and just, you know, standard columns: SNP, effect, low, and some effect size, p-value, or a z-score, and a “N” column. And then, what you also need, of course, because we’re dealing with summary statistics, is some reference data for the estimation of the LD between the SNPs. Then you also need something called an input info file, which just specifies some variables that are required, such as, you know, phenotype ID, file name, and for binary phenotypes, it’s important to have the number of cases and controls. If you have continuous phenotypes, you can just set these to “NA”, and the program will know that this is a continuous phenotype and treat it as such.\nSo something that may bias your genetic correlations is if there is potential sample overlap between the data sets that your summary statistics were derived from. And in order to account for this, you can first run bivariate LD Score regression (LDSC) for your phenotypes and just extract the intercept from that correlation, put that into a matrix, and then LAVA will account for any potential bias due to sample overlap.\nThen finally, of course, because we are analyzing local regions on the genome, we also need a file that specifies which regions to consider. And for this tutorial, we will use a file that was created basically segmenting the genome into smaller regions on the basis of LD. They’re not entirely independent blocks, but the LD has been minimized between them. But you are free to use whatever you want here if you have something else in mind.\nBasic processing\nRight. So, for this tutorial, what I will do is I will run through all the basic processing and input functions interactively in R. You can download the program from the GitHub, and on this GitHub, you can also find info about, well, more detailed information, about how to install the program, about the different types of input data, and also the analysis functions.\nRight. So first of all, we have here the basic input processing functions. First, we just read in the locus file, and then we process all the summary statistics, this input info file, sample overlap file, if you provide one. If not, you can set this to null, but I would only do that if you’re really confident that there isn’t any potential sample overlap.\nAnd this also specifies the prefix for your reference data and reads in the bim file because, so here we essentially align all the summary statistics to the reference so that the direction is consistent, subset the common SNPs, etc. And by default, this is going to read in and process all the summary statistics files that are listed in this input info file. But if you want only a subset of phenotypes, you can specify this ‘phenos’ argument here, and it’s going to only process those for you. So that’s the first thing you need to do.\nThen, for any locus that we want to analyze with these subsequent analysis functions, we need to do some additional processing to convert the local marginal SNP effects to their joint counterparts while basically correcting for the LD between the SNPs and the locus. And this is done using this function here, and this function will also compute a bunch of other parameters that are relevant for the analysis. And yeah, what you do is you basically just pass this, a single locus and this input object with all the processed summary statistics, and, if you inspect this, you’ll find that this is actually an environment, and that’s just because it’s a bit more memory efficient to work with environments. But, yeah, so if you want to look at what’s inside here, you need to use ‘ls()’, and the same is actually true for this input object. It’s also an environment. But you can then subset this, just if you want to extract things like the start, oops, like the start-stop site or any other information from this locus. Yeah. Then once we’ve done this, we can proceed with analysis. Now, of course, here I am showing you, how to do this sort of locus-by-locus basis for a whole-genome bivariate and genetic correlation analysis. I would do, I, I mean, I would do this on a cluster computer, and I will, of course, provide scripts that show you how to do this, and you can adapt those however you want.\nRight, so, yeah, to proceed with the analysis, once we’ve defined our locus object, we can then run this univariate analysis function. And this evaluates the local heritability for all your phenotypes. And, yeah, first you have the, you see, which, you know, just a list of phenotypes, you get the observed sample-estimated sample local heritability for the phenotype. And you get a p-value for this. If you’re interested in the actual estimated population-level h2, so for binary phenotypes, you would then need to provide a column that specifies the population prevalence of your phenotype, but for just evaluating significance, this isn’t relevant.\nSo, what do you consider sufficient to proceed with a bivariate analysis? It’s up to you, and it depends on what your aims are. Some people might just want to filter out completely non-significant loci, while other people might not even be interested in interpreting a genetic correlation from loci without genome-wide levels of local heritability.\nSo, yeah, there’s no, there’s no kind of gold standard here. It’s up to you, but I would definitely recommend filtering out really unassociated loci because, you know, it’s the same principle as with LD score regression. If there’s just no heritability, you wouldn’t proceed. But yeah, here, clearly, there is lots of signal. And so, to proceed with the bivariate genetic correlation analysis, we can run this function here. So, you only really need to provide this ‘locus’ argument, but now, for the purpose of this tutorial, I’m going to speed up the processing time slightly by turning off sort of additional iterations for the simulation p-values at lower thresholds, which would otherwise normally be on. And I would recommend keeping those on as well for your analyses.\nRight. So here, what you get is, for each unique phenotype pair, you get the estimated local genetic correlation with 95% confidence intervals for this estimate. You also get the squared of this correlation, which can be interpreted as the proportion of local genetic signal in one phenotype that can be attributed to that of the other and vice versa, and then you also get your p-values, of course.\nSo, yeah, by default, this is going to do all the unique combinations of phenotypes. But if you just want a subset of the phenotypes you processed, you can do that using this ‘phenos’ argument. I forgot to say, but this also applies to this univariate function. And then, if, for example, you have only a target phenotype of interest, in this case, let’s say MDD, we can set this ‘target’ argument here, and it’s only going to compute the pairwise local genetic correlations with that phenotype and ignore those of your other phenotypes.\nSo, yeah, like I mentioned, I do think you should do some filtering based on univariate signal. And instead of writing an “if” statement, you can also use this combined function, which does that in one go. So, this will only do the bivariate local genetic correlation analysis conditioned on that there is some level of univariate association signal. So, this is my p-value threshold I selected, and as you can see, here bipolar disorder has now been excluded from any subsequent analysis because it didn’t reach this threshold. If you set a more stringent threshold, it’s just going to return null. And yeah, if all the phenotypes pass, by default, it’s going to do all the combinations between them. But you can, again, pass the ‘phenos’ or the ‘target’ argument, and it’s going to subset accordingly.\nConditional analysis\nThis is the basic univariate and bivariate association tests. So, like I mentioned, I will provide scripts for how to do this on a whole-genome level as well. But yeah, now I’m going to go through the conditional analysis.\nSo, if you find that there are significant bivariate genetic relations between some phenotypes, and you have a particular conditional hypothesis you might want to test, you can apply these two approaches to do so. And, as an example, say that here I’m particularly looking at MDD (that’s my phenotype of interest), and I want to model the local genetic signal in MDD using a bunch of psychiatric phenotypes. In this locus, I can see here that um, bipolar disorder and MDD are really strongly correlated, but there are also significant correlations with schizophrenia. And now, yeah, depending on your how many tests you did, but let’s just say all of these passed multiple testing correction. And then, maybe my question might be, well, does this relation actually still hold if I account for bipolar disorder? Because bipolar disorder and schizophrenia are, of course, also correlated, and perhaps this correlation is just confounded by that association.\nAnd then I might run the partial correlation. So here, I specify my target association of interest is that between schizophrenia and MDD, and by default, it’s now just going to treat any remaining phenotypes as those that the condition on, as you can see here. Right. And here the output that you get is, first, which ones for you to target phenotypes, z is those that you condition on. Here, you just have the bivariate r squared for your on-target as a reference, and then the partial correlation with confidence intervals and p-value. And as you can see, there’s quite a noticeable reduction in the partial correlation between schizophrenia and MDD compared to the marginal or the bivariate association between those. And also, this is far from significant, which suggests indeed if we account for the bipolar disorder, the correlation between MDD and schizophrenia is no longer significant.\nRight, so that’s one type of conditional test that you can do. There’s also the multiple regression, and this is if you want to model the genetic association signal for an outcome phenotype of interest using several predictors jointly. And again, we will do the target phenotype. It’s MDD. And yeah, now you get first your predictors, which is the outcome, and then you get your joint multiple regression coefficients standardized for your two predictors, with confidence intervals for those as well as a joint r2 for this association. And so, this r2 can be interpreted then as the proportion of local genetic signal in MDD that can be explained by both schizophrenia and bipolar disorders jointly. And so, that means that this then accounts for the correlations, the genetic correlations between these predictors as well.\nYeah, and what you might notice here is that the p-values are, of course, not very significant compared to what they were before, but that is typical of like multiple regression when you run it with correlated predictors. And this shouldn’t be interpreted as the fact that, you know, these phenotypes aren’t jointly relevant. It’s really typical and that’s why you need to always sort of interpret this in the context of the bivariate relations as well. And yeah, so although here we’re dealing with genetic correlations or genetic relations, sort of the interpretation of a partial correlation and multiple regression is the same as you would normally do.\nYeah, that’s quickly how to run uh these basic functions. Like I said, I will provide scripts for whole genome bivariate genetic correlation analysis. If things are still unclear, there’s more information on the GitHub about the individual functions. You can also check the function manuals. Also, you are very free to email me about any questions you might have, and also please join the Q&A if you like.\nSo I hope that this was at least somewhat informative, and yeah, if you have any more questions, email me, and thank you very much for watching.\n\n\n\nGCTA-GREML\nTitle: Estimating SNP-based heritability with GCTA-GREML\nPresenter(s): Jian Yang\nJian Yang:\nOkay. First, I’d like to thank the organizer for this opportunity to talk and thank you for watching this video. In this presentation, I’ll be talking about the use of GCTA-GREML to estimate SNP-based heritability from GWAS data.\nWe have three modules in GCTA. The first module is mainly about heritability estimation, genetic correlation analysis, and phenotype prediction. In the second module we have method to genome-wide association analysis and in module number three we have a couple of other methods.\nSo, in module number one, we have methods to compute the genetic relationship matrix (GRM), to estimate or partition genetic variation, to estimate dominance variance, or variance explained by chromosome X. Um, we can run genetic correlation analysis or phenotype prediction analysis using BLUP or summary based BLUP. We also have a version of Haseman-Elston (HE) regression implemented in GCTA.\nIn model number two, we have a couple of GWAS methods, and there are two mixed linear model-based GWAS tools, called MLMA and fastGWA. So, if your sample size is not so large, you can use MLMA or MLMA-LOCO (leave-one-chromosome out). If your sample size is as big as the UK Biobank or even larger, you can use fastGWA or fastGWA-GLMM, if the phenotypes are binary. We also have COJO and mtCOJO for summary-based conditional analysis. We use COJO to do condition analysis conditioning on SNPs and mtCOJO for conditional analysis conditioning on traits. We have also included gene-based test methods, such as fastBAT and ACAT\nIn model number 3 we have GWAS simulator, doing GWAS simulation based on real genotype data. You can do PCA or PC projection if your discovery GWAS data set is too large. And we can also compute fst values for genetic variants and inbreeding coefficients for the individuals. We can run our LD score cmputing or to find LD friends for a given set of variants. We also have a version of GSMR implemented in GCTA to run Mendelian randomization analysis.\nSo, the main character today is GCTA-GREML. There are two steps to run GCTA-GREML. The first step is to create a GRM. Here is a command to run the analysis, and we tell the program to read in a genotype file in Plink format. We also tell the program to run the analysis on the autosomes, and we also have options to include or exclude a specific set of individuals or variants. If your data set is big, then you can parallelize the analysis onto each individual chromosome. In this example, we can separate the analysis on chromosome number one.\nIf this is still too slow, like the analysis in the UK Biobank, you can further divide the analysis or the computation on chromosome 1 into different parts. Here we divided analysis into 100 parts, and once all the parts are computed, you can simply use the command “cat” in Linux or Unix shell to merge all the parts together. You can further then merge the GRMs for the chromosomes into a single GRM.\nOnce you have the GRM, then you can run a GREML analysis by typing this command. Basically, the flag “--reml” to input the GRM and the phenotype file, and we also have other flags. For example, to include a specific set of individuals or some covariates. In this case, we have 10 PCs included as covariates. If your data come from case-control studies, then you can specify the prevalence of a disease in the population to convert the estimate of SNP-based heritability from an observed zero-one scale to an unobserved underlying liability scale. We also provide an option to run a genotype by environment interaction analysis, which is this one “--gxe”. So, you can tell the program to run REML with this flag to input the environmental information and then ask the program to run a likelihood ratio test for the GxE component specifically.\nSo you can also run a GREML analysis with multiple components. In that case, you can basically stratify the variants by different information. For example, you can divide the variants into different chromosomes or gene regions, intergenic regions, or minor allele frequency or LD scores of the variants. So, basically, the variants can be divided into many different groups, and then you run a GRM computation for each group, create a lot of GRMs, and then fit all the GRMs together in a GREML analysis to partition the genetic variants into different groups of variants.\nHere is a very specific example. This method called GREML-LDMS is specifically designed to estimate SNP-based heritability from whole genome sequence data or imputed data with rare variants. There are four steps: in the first step, you will need to compute the LD scores for the variants. You can then stratify the variants by both minor allele frequency and LD scores of the variants. You then compute a GRM for each variant group by the following commands. And finally, you run a GREML analysis with all the GRMs fitted together. And of course, you can include the covariates in this kind of analysis.\nWe also provide options to do an analysis to estimate non-additive genetic variance, here, in this case, dominance variance. So, you may need to start with a computation of a dominance GRM together with an additive GRM, which is the default for the GRM I mentioned before. So, you have two commands here. One to compute an additive GRM, and the other to compute a dominance GRM. Once you have the additive and dominant GRMs, you can fit them together again in a GREML analysis to estimate additive and dominance genetic variance for a trait.\nWe also provide you an option to do genetic correlation analysis, which essentially is a bivariate GREML analysis. So, the command is actually very simple. You tell the program to run REML bivariate analysis with the input of GRM and the phenotype file. So I call this “balanced design” because, in this analysis, we assume you have two phenotypes measured on exactly the same set of individuals. But in some cases, the design can be unbalanced. For example, in this case, if I want to estimate the genetic correlation between males and females for human height, I treat the male height and the female height as two different phenotypes. So, in this case, I have phenotypes: male height measured on males only, and the female height measured on females only. So, if you input a phenotype file like this into this bivariate GREML analysis, the program can also estimate the genetic correlation, which is essentially the male and female genetic correlation.\nI guess this is the end of the, kind of, tutorial talk, but I also want to take this opportunity to mention a few frequently asked questions. For example, a commonly asked question is the sample size required for a GCTA-GREML analysis. For common variants, I would suggest a sample size of at least 5,000; of course, it would be better if you can have more. And for genome sequence data or imputed data with reference, I may suggest tens of thousands. In terms of computer memory requirements, we have provided equations on our website to approximately compute the computer memory required for GRM computation and REML analysis.\nSo another question is about whether we can apply GREML analysis to family data. The answer is yes. We often have two strategies. The first strategy is to remove one of each pair of individuals with estimated genetic relatedness larger than a threshold, for example, 0.05. Another strategy would be to use a so-called big-K-small-K analysis. So, it’s essentially a multi-GRM analysis with one dense GRM and another sparse GRM to estimate SNP-based heritability and the pedigree-based heritability simultaneously. In terms of this relatedness threshold, we often use 0.05, but in real data applications, we don’t really see too much difference if we use 0.025 or even 0.1 for GRMs computed from Hapmap3 SNPs or a similar set of common SNPs.\nSo, we also have feedback about issues related to convergence or sometimes estimates are too big or too small. For example, in the REML iteration, the estimate can hit the boundary of the parameter space, which is zero or one. In this case, we think that it’s likely because the sample size is not large enough. We know that the variance of estimates (var(estimate)) is inversely proportional to sample size squared, so if the sample size is too small, the estimates vary a lot and by chance, it can hit the boundary if the true parameter is either too big or too small. In case-control studies, this may also indicate that you have some technical differences between cases and controls, and often more stringent quality control needs to be applied.\nAnother issue is that the GCTA-GREML analysis did not converge. Unfortunately, we don’t have a universal solution to this issue yet. It’s often to do with errors in the data or analysis script, and sometimes you may just have to increase the sample size.\nYeah, the other question is about whether we can use GREML to estimate the variance explained by a subset of variants. The answer is yes, but the estimate will be inflated if the SNPs are selected from the same data. So, one suggestion would be to select SNPs from one dataset and to do the estimation in another independent dataset. We also have flags in GCTA to estimate the effect size of the covariates or to estimate the fixed effects, and this is sometimes useful if you are interested in testing the association between a covariate and the phenotype, conditioning on the polygenic background.\nFinally, we also have an option to do prediction, to predict the random effects. So here is an example: we can use this “—reml-pred-rand” to output all the BLUP solutions for the total genetic values of the individuals captured by the SNPs. We can then use this option “--blup-snp” to input the genetic values of individuals and then to convert individual BLUP solutions to BLUP solutions for the SNPs. And then you can use this SNP BLUP solutions in PLINK to compute a polygenic risk or PRS in an independent sample. So, it’s essentially a sort of individual-level data based BLUP analysis.\nI think this is the end of my presentation, so if you have any questions, you can visit our website or drop me an email. And thank you again for watching this video.\n\n\n\nGenetic Architecture\nTitle: Tracking 12 years of genetic architecture estimates of schizophrenia\nPresenter(s): Naomi Wray\nNaomi Wray:\nHello. So today in this short talk, I’m going to talk about tracking 12 years of genetic architecture estimates of schizophrenia. So, my target audience for this talk are really analysts from the PGC. The work I’m talking about comes from, is part of the latest PGC schizophrenia paper, which is currently on MedRxiv, and the work I’m talking about is particularly working with Vasa [Trubetskoy] and Mick [O'Donovan], as we drill down into some results, and the results are found in the main text section—SNP-based heritability and polygenic prediction—and also supplementary note. So, my motivation was really that schizophrenia is one of the flagship disorders for the PGC, often because the sample size is larger earlier on, some of the things that we do and see, are later seen by other disorders.\nSo, 12 years of tracking. So I’m considering the papers that started in 2009 with the International Schizophrenia Consortium paper, which was the first large genome-wide association study for schizophrenia with 3,000 cases. We then had the PGC Wave one in 2011 with 9,000 cases, PGC2 Wave 2 in 2014 with 37,000 cases, and the latest Wave 3 with 69,000 cases. So, this graph is just illustrating how the discovery over those Publications with on the x-axis number of cases and on the y-axis the number of independent associated loci going from actually zero from the 2009 paper to now nearly 250 with the Wave 3.\nSo before I start, I’m going to just give some basic concepts. So, with this bar, I’m representing the variation from all factors that contribute to a trait, in this case, risk of a disorder—risk of schizophrenia. This bar represents the variation attributable to genetic factors, and the ratio between these two is then the heritability. And we estimate this from family data. This bar represents the variance that’s attributable to genetic factors, which are tracked on our common variant genotyping arrays, so in our GWAS datasets. So obviously, our GWAS arrays don’t track all variants, just some of them, and so the variation that’s associated with them is smaller than the total genetic variance. And so, this ratio—the ratio between this one and this one—is the then the SNP-based heritability, which is less than the heritability. And then, this bar represents the variation that’s explained in out-of-sample prediction in polygenic risk prediction, and for that, we use the R2 term. So our expectation is that as this value is less than the SNP-based heritability, but as sample sizes increase, this value will get closer to this one.\nJust a small word about R2. We're used to the term R2 variance explained by a regression when we think about linear regression. When we have a binary trait, it’s a little bit more complicated, and so we have this statistic, the Nagelkerke R2.\n One of the problems with this statistic is that it actually depends on the proportion of cases in the sample. So in this graph, I’m showing all the points on these lines explain the same variance in liability, but the actual estimate of the Nagelkerke actually depends on the proportion of cases in the sample.\nSo back to 2009, in this very first paper, this was the first paper where we actually made estimates of what we now called SNP-based heritability and the first paper where we looked at out-of-sample prediction. So this was the iconic figure from that paper. On the y-axis, it was the Nagelkerke R-squared. We’re predicting from the International Schizophrenia Consortium into other independent samples: here in another European sample called the MGS sample. The different color bars reflect that we were using the basic p-value clumping and thresholding method with the different thresholds for the p-values. In this talk, I’m not explaining polygenic risk prediction. I’m assuming everybody knows what it is, and if you don’t, there’s another PGC talk or several talks which are specifically about polygenic risk prediction.\nSo, in this paper, we asked two questions. We said, \"what genetic architecture is consistent with the observations that we saw? and how do we expect the out-of-sample prediction to change as sample size increases?\" So many of you might have heard me say before that my favorite figure in this paper is this supplementary figure eight. So, on the y-axis, we have the Nagelkerke R-squared, and on the x-axis, we have sample sizes, and these are results from simulations. So we considered a sample size of 3,000 cases and 3,000 controls, and we chose that to mimic the real data. And then we thought about increasing sample sizes going up to 20,000 cases and 20,000 controls. Back in 2009, we thought it was absolutely ridiculous to think about going to larger sample sizes. We even thought they’d be laughed at for going to 20,000 cases. But of course, 12 years on, we’re now three times that size. The different colored bars are again the different p-value thresholds, and the different graphs represent different genetic architecture models. So those genetic architecture models differed in the number of causal variants, the frequency of the alleles of the causal variants, and the effect sizes. And what we found was that we could model many different genetic architectures, which are really very different in terms of the number of causal variants, but each of them generated observations which would very be similar to what we actually saw in real life. But we predicted that as sample sizes increase, we might see differences in the shape of the p-value thresholding results depending on genetic architectures. But you can see we went up to a Nagelkerke R-squared of, you know, like 20 percent.\nSo what do we expect with increasing sample size, first of all, for SNP-based heritability? So, we have to have some caveats for making comparisons as sample sizes increase. First of all, we might have to make sure that we’re thinking about the same definition of phenotype. We need to have our samples drawn from the same ancestry. We need to be using the same SNP set for these comparisons, and the same methods to estimate the SNP-based heritability. There are some other third-order assumptions which we don’t need to worry about. But basically, our basic expectation is that with increasing sample size, we expect that the estimate of the SNP-based heritability to not change with sample size because we expect it to be an unbiased estimate whatever the sample size. Of course, sample size comes into the standard error, so we expect that as sample sizes increase, the standard errors will get smaller, and so the estimates might bounce around as sample size increases, but only with respect to the standard error.\nSo, what did we actually see? So here on the y-axis, I’ve got the estimate of SNP-based heritability on the liability scale. On the x-axis, the different sample sizes. So, this is the first International Schizophrenia Consortium sample of 3,000 cases, and this was the estimate we made from a simulation of 0.34. We were then able to directly go in and estimate from individual-level data using the GREML method, and then our estimate was 0.27 with this 95% confidence interval. We then went to the Wave one data, and then the estimate was 0.23, which was not statistically different. We did observe, though, that when we made those estimates, we made them separately for the ISC sample, the MGS sample, and the other cohorts, and those estimates individually were all larger than the overall estimate. So we had 0.27, 0.31, 0.27, but when we put them together, it was 0.23. So, what was going on there? Well, this could be that some of those assumptions I had on the previous slide weren’t upheld. So, for example, maybe a slightly different phenotype definition, not quite sampled from the same population, etc.\nThese are the estimates then from Wave 2 and Wave 3, again not statistically significant. This says 69k, but it was actually just from the European subset. I’ve got a comparison here with the methodology comparison. So these ones were by GREML. When we get to the larger waves, we didn’t have access to all the individual-level data, so the estimates are then based on summary statistics. We can use LD score regression, which is perhaps more commonly used, but we know from many simulation studies that LD score regression actually gives slightly biased estimates. It slightly underestimates the true SNP-based heritability, but the estimates using more modern methods, such as SBayes-S, are less biased. And so, this is now our best estimate of SNP-based heritability, at 0.24, but basically, the estimates haven’t really changed with increasing sample size as we would expect.\nSo, what do we expect with increasing sample size for out-of-sample sample prediction? Well, first of all, we, again, have to make comparisons like with like. So, we, you know, some of the same assumptions as for SNP-based heritability also hold for the GWAS Discovery sample, which makes our polygenic prediction. But then we also have some additional assumptions for the Target sample. So, we need to be comparing based on the same method to generate the polygenic risk score. We need to be using the same kind of SNP QC, which maybe is also part of the methodology. And to have an apples-and-apples comparison, we need to be using the same test cohort. So, under these conditions, what do we expect? We expect that the out-of-sample sample prediction statistic will increase as the sample size of the Discovery sample increases, and we’re going to have a maximum, and we know that there’s a maximum of that R2 statistic, which is the SNP-based heritability. So the sample size of the Target sample comes in not to the estimate but to the standard error. So the key thing is the sample size of the Discovery sample, the out-of-sample sample prediction R2 goes up, the sample size of the Target sample affects the standard error of the estimate.\nSo these are the results from the Wave 3 study, and this is a figure that’s in the paper that’s on MedRxiv. So on the y-axis here, we’ve got the R2 on the liability scale. On the x-axis, we’ve got those different p-value thresholds, again using the basic p-value clumping and thresholding method. In Wave 3, we have cohorts of different ancestries, and so these results show the leave-one-cohort-out predictions. So we leave one cohort out and predict into it. We have samples of different ancestries shown by the orange dots, which are African-Americans, going up to the green dots, which are the European cohort. So we can see we have better out-of-sample sample prediction in the European cohort. It’s not surprising given the majority of our Discovery cohort were also European. So we got the highest out-of-sample sample prediction for the p-value threshold of 0.05 and the median value across those cohorts was 7.3 percent. And so, this talk was really kind of motivated by this initial observation we had, that this out-of-sample sample prediction median across the cohorts of 7.3 percent, which for me sent up a slight flag warnings because I knew that from Wave 2 we had an out-of-sample sample prediction of seven percent, and I felt that this increase from 7 to 7.3 was not as much as I might have expected. So then, we did a bit of digging.\nSo what did we find? So this is now an out-of-sample sample prediction comparing apples with apples, going into a single cohort. This is now the MGS cohort, and here we’re looking at the Nagelkerke R2, and I’ve done that because that was the statistic used in that first paper in 2009. So now we do see when we’re comparing on a single cohort that with sample size increasing, we do get a good increase in variance explained, going from that 3 percent Nagelkerke's R2 in 2009 up to 6%, and we had 9,000 cases, 18% for 37,000 cases, and 21% now. And this is then translating those estimates to the liability scale, so we’re now at a point of 9.9% liability variance explained. And, in fact, we know that if we use more advanced methods for generating the polygenic scores, this is now going above 10%.\nAnd the other thing that we looked at was again thinking about like with like. So on the previous slide, I said that in the Wave 2 when we had the Wave 2 Discovery predicting into the left-out PGC Wave 2 cohorts, that the statistic of out-of-sample sample prediction median was 7%, and I showed on the previous slide when we have the Wave 3 predicting into the left-out Wave 3 cohorts, that median was 7.3%, which is not very big difference. But we realized that if we take the PGC3 Discovery and predict into the PGC2 cohorts, so now again a like-for-like comparison, that’s going up to 8.5%. So part of the low difference here was actually in the nature of those PGC3 cohorts.\nSo that was all I wanted to show in this talk and hoping that it’s useful for analysts in other PGC disorders. So just left to give my acknowledgments. Nothing I do goes without funding, so thank you for the funding, and particularly a shoutout to the PGC Schizophrenia Consortium and all the collaborators that make that happen, and particularly to Vasa [Trubetskoy], Mick [O'Donovan], and Jian [Zeng], who helped provide, you know, really investigate what’s presented in this talk. Thank you."
  },
  {
    "objectID": "chapter9.3_transcript.html",
    "href": "chapter9.3_transcript.html",
    "title": "Chapter 9.3: 9.3 Genomic Structural Equation Modeling (Video Transcript)",
    "section": "",
    "text": "Genomic Structural Equation Modeling: A Brief Introduction\nTitle: Genomic Structural Equation Modeling: A Brief Introduction\nPresenter(s): Andrew Grotzinger\nIntroduction\nIn this first video, I want to provide a brief overview of genomic structural equation modeling, including some of the background and motivations that led us to develop this package.\nGraphs\nI’m going to start here by showing these two different graphs that show on the X axis, the discovery sample size from different GWAS studies and on the Y axis, the number of hits identified in these studies. And what you’ll notice is that as the sample size have increased, we have begun to identify hundreds if not thousands of different genetic variants associated with both complex traits like height and BMI and disease traits, like Crohn’s disease and prostate cancer, which really reflects this gradual realization that we’ve had in human complex trade genomics, that many of the outcomes that we’re interested in are highly polygenic, meaning that they are associated with many genes and not just some small handful of core genes. For those of us that are interested in the shared genetic relationships across traits this comes with the caveat that it’s not simply a matter of identifying the five or six overlapping genes.\nLimitations\nSo for my self, as somebody who’s interested in clinical psychology outcomes, I couldn’t simply look at these two Manhattan plots for schizophrenia and depression, which just to orient you to these, to the chromosome on the X axis and the negative log 10 P values with values that are higher up over here indicating genetic variants that are more significant. That I can’t just count up the ones that are above this red dash line here for genome-wide significance. Cause there’s simply too many. So we needed at some point to develop methods that find ways to estimate the aggregate shared information across these really polygenic outcomes.\nLD Score Regression\nAnd thankfully a team from the Broad, including some people who are talking at this workshop developed this method called LD score regression which can be used to estimate genetic correlations across participants samples with varying degrees of sample overlap, using what is often publicly available, GWAS summary data. As in, you can go online right now and directly download that data without going through any sort of strenuous request process or going through an IRB.\nGenetic Heat Maps\nWhen LD score regression is applied, it can be used to produce what is often referred to as genetic heat maps. Two of which are shown here. So on the left, we have the genetic correlations estimated across psychiatric phenotypes with the squares that are shown with darker shading, indicating stronger levels of genetic overlap. And of course the squares on the diagonal and all are shown in dark blue because that indicates the genic overlap of the phenotype with itself. So you see that across a number of these disorders, that there are high levels of genetic correlation. And that this is also reflected in general brain and behavioral, cognitive phenotypes shown this heat map over here on the right. And so this reflects one of the second things that we’ve realized in human complex trait, genetics, which is that there is both pervasive, polygenicity, which is say that the traits are affected by many genetic variants and there’s pervasive pleiotropy, which is to say that many of those variants are actually shared across traits. And this shared genetic architecture that we can see depicted here in these heat maps is really what motivated myself and others to come together and develop methods that allow us to actually analyze that joint genetic architecture. So again, genome-wide methods are clearly suggestive of these two things, high polygenicity necessity, and pervasive pleiotropy. And we’ve viewed genetic correlations as data to be modeled. We want it to be able to ask what kind of data generating processes give rise to these correlations and how can we use publicly available data, to examine systems of relationships, to really start to interrogate that data more. And so that what led us to develop genomic structural equation modeling, which we introduced in this nature, human behavior paper that involved a lot of key members on the team. But I’ll specifically highlight Michel Nivard and Elliot Tucker-Drob, who supervise this project and continue to work with me to develop extensions on it and of course, Michel is also one of the people presenting here.\nGenomic SEM\nSo our solution to this problem of how we model the genetic correlation matrix is genomic SEM which applies structural equation models to these estimated genetic covariance matrices, and then allows the user to examine traits that oftentimes could not be measured in the same sample and it provides a flexible framework for really estimating a limitless number of models using only these GWAS summary statistics. That again, can be applied to summary stats or what is often referred to as sumstats with varying and unknown degrees of sample overlap.\nExample\nI just want to focus on one specific example for time reasons related to my main interest in clinical psychology and this reflects our most recent application of genomic SEM to psychiatric outcomes, where we produce this genetic heat map across 11 major psychiatric disorders. And what you see is there’s a pretty high level of genetic overlap across these disorders. And what is really unique about this is that many of these disorders cannot be measured in the same sample. So bipolar disorder and schizophrenia, the way that we structured our clinical diagnoses, you can’t have that same set of disorders within the same person. You’re either assigned one or the other based on your presentation. And so that means we’ve been limited in clinical psychology research to making inferences about what is shared across these disorders based on patterns of findings from separate samples. Where now for the first time, genomic SEM offers this unique opportunity to actually model the set of relationships across these rare and even mutually exclusive clinical presentations. Because again, the summary statistics that were used to produce this heat map are not from the same sample, but instead reflect the aggregate summary level data from these different cohorts .\nWhen we then applied genomic SEM to model these relationships, we found that a four factor model fit the data best. And this actually mapped on pretty well too some level of what we might expect based on the clinical presentations of these disorders. So for factor one, we have anorexia obsessive compulsive disorder and Tourette’s syndrome that we might characterize as being really defined by this kind of compulsive, like presentation. With the psychotic disorders of schizophrenia and bipolar disorder. What we’re calling a neurodevelopmental disorders factor because it’s defined primarily by ADHD and autism, but also notably includes PTSD and alcohol use disorder. And then this internalizing disorders factor over here that includes MDD, anxiety and PTSD. And I would note that these factors are all correlated, but they also segregate into these more tightly defined clusters of sets of disorders, giving rise to some level of understanding about what is shared across these specific subsets of psychiatric presentations.\nWe can also use genomic SEM to examine individual SNP effects on the factors. And so when we then apply what we referred to, as multi-variate GWAS to these factors, it can be used to produce these four Miami plots here. Where I’m depicting with the black triangles, the GWAS hits that overlap with the univariate hits and in red triangles, the GWAS hits that were actually not hits for the individual disorders. And this is reflecting that by leveraging the shared information across the disorders that define the factor, without collecting any new data, we can actually discover new genetic variants associated with these factors. On the bottom half of these Miami plods, I’m showing the effect of Q SNP which is a heterogeneity index that we’ve developed that is designed to pick out those genetic variants that do not fit the factor model. As a classic example, we generally pull out the ADH1b gene that is associated with alcohol use disorder, specifically as something that is a Q SNP variant, which is to say that this is something that does not operate via these more general factors, but it’s highly specific to alcohol use disorder. And we’ll talk more about Q SNP in some of these later videos.\nAnd so as a sales pitch to genomic SEM I think one of the really amazing opportunities that genetics and genomics SEM building on methods like LD score regression presents is the ability to examine systems of relationships across a wide array of rare traits that could not be measured in the same sample. To give an example of a research question, you might have, that you could not do outside of a genetic space.\nLet’s say that you’re interested in the genetics of early and late onset schizophrenia and anorexia nervosa. You have some sense of research literature that indicates that a particular onset stage of anorexia nervosa can sometimes contain a psychotic-like presentation and you also recognize that these things can have distinct developmental onset periods that means something about the course of these diseases. And so you want to look at this cross-lagged model that examines the systems or relationships across these presentations within the disorder and across these two disorders.\nYou could collect a sample where you look at the relationship between early onset schizophrenia and anorexia nervosa and in the same vein, late onset schizophrenia and later onset anorexia nervosa. But that would take you a long time to collect just because of how rare these two disorders are. What you could not do is look at the relationship outside of a genetic space between the early and late onset versions of the same disorder. But again, with GWAS summary data, you could split the GWAS data by age of onset and then you could estimate the genetic overlap of the signal between the early and late onset versions of this disorder and so now with genomic SEM You can actually estimate this model and answer these sorts of research questions that again, would not even be possible outside of a genetic space.\nSummary\nThis final slide is just to highlight the different ways in which genomic SEM has been used. Including some work that I’ve been involved in, but also works from other outside research groups that are published in high-impact journals, like Molecular Psychiatry, Nature Genetics, Cell, and Nature Human Behavior. I really hope that some of you see some opportunity to use genomic SEM in your own research. In the next videos, we’ll talk more about how structural equation modeling works and provide some hands-on examples of how to apply genomic SEM to actual genetic summary data.\n\n\n\nShort Primer on Structural Equation Modeling (SEM) in Lavaan\nTitle: Short Primer on Structural Equation Modeling (SEM) in Lavaan\nPresenter(s): Andrew Grotzinger\nIn this video, we’re going to talk about some of the basics of structural equation modeling and more specifically how to do  structural equation modeling and Lavaan which is the art package that genomic SEM uses to estimate the models.\nTo start, let’s go over some of the basic model syntax for Lavaan, beginning with how to specify a regression relationship, which you could write as Y ~ X, which visually depicted as a path diagram would mean X predicting Y with this single headed arrow. And depending on how you think about variables in your model, you might think of that as reflecting A ~ B, or dependent variable ~ the independent variable, or outcome ~ predictor. But of course you would name these according to the actual names of the variables in your data set. This is just to drive home the point that the outcome is on the left-hand side of this equation and the predictor is on the right-hand side of the tilda (~) over here. To specify a variance of a variable, you would write the name of that variable twice with two ~ in between. So X ~~ X would specify the variance of X or the covariance of X with itself. To specify the covariance between two different variables you would also use two ~ and on the right-hand side, you would write the name of that second variable, in this case, Y, which in a path diagram would reflect this two headed arrow between X and Y. And then the standardized case would of course reflect the correlation between these two variables.\nTo specify factor loadings which refers to the regression relationships between this unobserved latent factor and these observed variables, A through E, which we’ll talk a little bit more about later in this presentation, you would write F and then equal sign ~ and then A through E.  You can name the latent factor however you want. In this case, we’ve just called it F and of course, A-E should again, reflect whatever the names of the actual variables are in your data set but this is generally speaking the schematic for how you would specify the factor loadings in a model. Just a note to say that the default behavior in Lavaan  is to fix the loading of the first variable to one and this is necessary because since F is not a variable in your dataset, Lavaan and structural equation modeling more generally needs some sort of anchoring point so that it knows what kind of scale to put this latent factor on. So again, unless you tell it to do otherwise, Lavaan we’ll fix that loading for the first variable to one. So it knows how to scale the latent factor. And as another note, just in general and structural equation, modeling squares are used to depict observed variables, such as the variables here for A through E and circles are used to depict an observed or what is often referred to as latent variables. And that can include latent factors like the latent factor F over here or, for example, the residual variances of observed variables where the observed variable is something that is observed in your data set and the residual variance is not. And so for that reason is generally depicted as a circle.\nTo fix a parameter in a Lavaan model, you would write the value that you’re fixing it to on the right-hand side of the equation, followed by an asterisk, and then whatever variable name comes on that side. You always put the value you’re fixing into on the right-hand side. So you would write one star X, ~~ Y but what this specifically does here is it fixes the covariance in between X and Y to one.\nYou can also name a parameter in lavaan and to do that, you also put an asterisk on the right hand side of the equation, but then, you would also include whatever you’re naming that parameter, which should just be some set of letters. So I could call this dog, cat, a, rain drop. You just have to name it something that doesn’t overlap with the actual names of the variables in your dataset. So this names the covariance between X and Y to be a, and that’s useful because then you can, in the later part of your model use what is referred to as model constraints for a particular parameter. So let’s say that for some reason, you know, that X and Y is covariance should be above zero, but you don’t know exactly what that number should be. So you’re not fixing the value to anything in particular, but you are telling the lavaan that that value should be above this particular number of .001.\nJust a cautionary note on naming parameters, I’ve seen a number of people use the same parameter label for multiple parts of the model and often times this was done unintentionally on their part and the reason that that can is a problem is because when you name it with the same label, a here, that’s not just naming the covariance between X and Y, and the covariance is between Y and Z. It’s also telling Lavaan that you want to fix these two covariances to be estimated at the same value. So if you do want to name multiple parameters in the model you just want to make sure to name them different things, unless you are intentionally trying to use what is referred to generally as inequality constraint. So this is what not to do unless you are trying to make these parameters be the same.\nHaving reviewed all of these Lavaan basics, I just want to go through some of the concepts and basics of structural equation modeling, and then continue to use this lavaan syntax along the way as we review those basics.\nTo start, I want to begin in this hypothetical space where we’re talking about a situation that we would not run into in which we actually know what the relationships are between the variables. So here let’s say that we know that the regression relationship between X and Y is 0.4 and that Y has a residual variance of 0.84, which would imply this particular set of relationships where Y equals 0.4 times X plus this residual u. If we extend that out to say that, right we know that Y causes Z of 0.6 that would add on this additional notation of Z equals 0.6 times Y plus u. And so this would imply a particular covariance matrix in the population. This set of relationships where this isn’t a standardized space on the diagonal we have the variances of the variables, which are one, and then on the off diagonals, we have the Relationship between X and Y at 0.4, between Y and Z of 0.6 and the relationship between X and Z in a structural equation model. And this case would be 0.4 times 0.6 or .24.\nIn practice, we, of course don’t have access to the covariance matrix in the population, but instead we have a particular covariance matrix within a sample, which is intended to be a rough approximation of the population that we’re interested in studying.  And so again, in practice, we have this observed covariance matrix in our sample. Instead of knowing these relationships, we’re then flipping that process on its head, and we’re saying that we want the model to pick the values within this system of relationships that we’ve specified that most closely, approximate the covariance matrix in our sample. With a structural equation modeling, you’ll hear people talk about the degrees of freedom at the model, and that refers to how many parameters you estimated relative to how many parameters you could have estimated given the unique elements in the covariance matrix. So for a three variable covariance matrix, you have six unique elements which refers to the three variances on the diagonal and the three covariances on the off diagonal. And in our model, we’ve only estimated five of those parameters where specifically we did not estimate the regression or covariance relationship between X and Z. So that means we have one degree of freedom and can be thought of as specifying a model that provides a more parsimonious or simplified set of relationships relative to what we observe in the data. To specify this model in Lavaan using that notation that we just went over for the regression relationship between X and Y, we would write Y ~ X over here in blue and for Y and Z the same thing, but Z ~Y.\nSo in practice again, what lavaan and what SEM softwares are doing more generally is they’re taking this observed covariance matrix and the system of relationships that you’ve specified and they’re picking the numbers that get as close as possible to that observed covariance matrix, which in this case would be.35, and 0.61. And that would imply a certain covariance matrix. And the level of difference between these two matrices is often what is used to produce what is referred to as model fit, which  is something we’ll talk about in later videos.\nAs we talked about earlier, you can also specify latent factors in a data set, which reflects the shared variation across a certain set of variables, such as Y1 through Y5 here. And I would just note that we’re actually employing a different notation than we used earlier, where we are specifying the factor loadings again with the name of the factor F =~ and then the name of the variables that define the factor, but we’re overriding that default Lavaan behavior to fix the indicator of the first variable to one by writing NA*Y1, and then we’re instead anchoring that part of the model or scaling it by telling Lavaan that we want to fix the latent variance of the factor to one. So we’re using what is referred to as unit variance identification instead of what we used before, when we fixed the loading of Y1 to one, which is referred to as unit loading identification. That’s not something that you need to keep in your working memory at all times, but just something for you to know as an option. And this particular part can be important if you’re specifying, let’s say correlations between factors, where if you set the variance of the factor to one, then that means they’re on a standardized scale and you can interpret those relationships as correlations.\nNow here, we’re just showing what if you want to expand this out to include the relationships between two latent factors. And so again, in Lavaan syntax, you would write for the latent factor F1 here this part of the model here in blue F1 =~Y1+Y2+Y3+Y4+Y5 the regression relationship between F2 and F1 this part here in purple and for the latent factor model for Factor 2, F2=~Z1+Z2+Z3+Z4+Z5.\nWhat genomic SEM does is it uses the principles of structural equation modeling to fit a model to the genetic covariance matrix. So it’s a two-stage process where in stage one, that genetics covariance matrix and its associated matrix of standard errors and their co-dependencies are estimated where we specifically use LD score regression and stage two, we fit that structural equation model to the matrices from stage one using Lavaan and the basic principles of structural equation modeling. I know this was a fairly brief overview, but hopefully it provided the basic notation to use Lavaan syntax and understand a little bit about what the SEM process looks like. And so in the next talk, we’ll go over how it is that we actually estimate this genetic covariance matrix and that set of standard errors.\n\n\n\nGenomicSEM: Input\nTitle: GenomicSEM: Input/Explaining how S and V are estimated and what they are\nPresenter(s): Michel Nivard\nWhat goes into genomic SEM before you can actually fit a structural equation model. So it’s basically a class on how the sausage  is made, right? So some of these things are details that are good as background, Good for you to understand, once you use genomic SEM. But basically stuff that’s largely handled by internal functions of genomic SEM. So it means you don’t have to do anything like this by hand. And to discuss what goes into genomic SEM, it’s good to discuss two other things. Namely, what kind of information goes into a structural equation model in general? Right. You can use raw data, but you could also use the covariance matrix of the raw data to fit the structural equation model or a regression model, as we’ll see in the example. And it’s good to discuss what LD score regression is exactly. It’s a very commonly used technique to estimate genetic co-variance and or heritability. And it actually is one of the main ways genomic SEM can estimate from raw GWAS data. So raw, summary statistics, the genetic covariance between traits and the heritabilities, which you can then use to fit a structural equation model too. Okay.\nSo first we’ll discuss what kind of information goes into a structural equation model and, show you that basically you can either use raw data to get a structural equation model to work, but you can also use the covariance matrix of that raw data. And that’s an important conceptual step, right?  Because if you understand that you can later understand why we can fit structural equation models while we don’t observe any raw data in genomic SEM. And with raw data, I mean, phenotypic observations. We don’t observe those at all in genomic SEM. The only thing that goes in is the GWAS summary data.\nSo now we’ll switch over to R and I’ll go through a little example in which I create a dataset and then fit a structural equation model. And then I’ll take only the covariance matrix of the dataset and fit the same structural equation model to illustrate what goes on in such an example.  I didn’t really clear my workspace before, so let’s do that so we have a fresh start. The first thing we’ll do in this, R script, this  will require some packages I need to generate some data and to fit structural equation model. So the MASS package allows me to generate data from distributions and lavaan will allow me to fit structural equation models. Okay, then we’ll run these, we’ll also run this line, which states that variable N is 10,000 we’ll use the variable N for sample size all throughout the example. Then we’ll generate three random variables, X1, X2, and X3. And we’ll also generate random variable, Y which is a function of X1, X2, X3, and some residual variance that’s not attributable to all those variables.\nSo what does variable Y look like? Something like this. What does X1 look like? Well X1 was normal, right? This, we just generated from a random, normal variable. This is what it looks like. Okay. We’ll combine all those variables into a dataset, right? The dataset just keeps together all the variables in one place. Now the variables also have a relationship because that’s the way we defined them. Right. So we could make a plot of Y against X1, and we’d see there be some sort of  relationship where if X1 is higher, then Y is also higher, which is exactly what we expect given that we just made Y a function of X1. Right? Okay. Now on these data, we can fit a linear regression model, which I do right here in which we regress Y on X1 on X2, and on X3. And we know what will come out of the linear regression model. Why? Because we just defined the relationship between Y and X. Right. We defined the relationship as Y being equal to 0.4 times X1, .5 times X2 and minus 0.2 times X3.\nSo if you run the model, Oh, I forgot to run the ‘dataset’ line, this is instructive. You know, when you, when you, when you’re programming, you’ll always make mistakes. I’m not going to cut it out of the video. Cause I think it’s instructive to know just that we also had just, you know, messed this up all the time. Maybe it’s not that instructive, but I messed this up all the time. Okay. So now I’ve run the lines we need to, to run a linear model, as you can see, this is the way you would define a model in R. Andrew has a video up on how you can define models in lavaan, but in R, the regression model uses a tilde as an equation sign and “+” to add additive terms. And so this linear model using the function LM fits linear regression, and we can get a summary using the summary function, applied to the object in which the linear model is stored.\nNow, as you  may expect the coefficients we get from the model are very close to the values we use to generate the data. They’re not exact because generating random data, also introduces a slight bit of noise. so X1 gets an estimate of .40, which is its true value, thereabouts. X2 gets point 49 , while the true value is 0.5 and then X3 gets minus point 16 while the true value is minus 0.2 so they’re all close to their true value, which is great. The linear model works, which is, I guess not entirely unexpected. You can fit the linear model. The same model; whoops, spelling error. You’ll get this code by the way. I’ll make sure it’s available.\nYou can fit the same model in lavaan using this as a descriptor of the model. Okay. Andrew went through the syntax for lavaan, so you should be able to understand this, but I’m going to read it out anyway. It basically defines one regression. Y is regressed on X1, X2 and X3, and then explicitly defines the covariance between X1 and X2,  X1 and X3. And actually I just noticed it should also include the covariance between X2 and X3 okay. And then using “sem()”, which is the lavaan function and adding the dataset, we created, the raw data the the observed data, to the data argument we can fit the SEM model and let’s have a look.\nNow, a structural equation model is capable of estimating the same parameters as  a regression model, so here you go. Y regressed on X1, 0.4, Y regressed on X2 .49 and Y regressed on X3, -0.16.\nSo, this is just a very convoluted, indirect way to fit the same model. Now notice, because this is a structural equation model we could fit many, many more models with these four variables, we’d be flexible. We could, we could say: “well, actually, Y is an outcome of X1, X2 and X3, but we suspect X2 to be an outcome of X3.” Right. And we could fit a mediation model in which Y is regressed on all three variables. And the X2 was regressed on X3. we could define that model and we could see what the parameter fit is. That’s the flexibility you have in lavaan or a structural equation model, or later in genomic SEM, but which you don’t have in a linear regression model. That’s not the point of this tutorial. So let’s get back to the point.\nI can create the covariance between the variables in the dataset using cov() function in R right? And I’ll get this object sigma, which has the covariance in it. Now, if we look at sigma, we’ll see a matrix with covariances between Y and X1 and X2 and X3. Okay. We can actually feed that covariance matrix to lavaan so we use the same SEM function, the same model. Now, instead of giving it raw data, we’re giving it the covariance matrix and we’re giving it the number of observations because it needs to know how precisely all the elements in the covariance matrix are fit. And we can run these and we’ll get exactly the same or very similar outputs.  Right. I am running this on SEM model 2. Right. So is this the model that doesn’t know about the raw data just knows about the covariance and, you get the same regression parameters.\nNow, this works because in a SEM model you defined a covariance, the cells of the covariance matrix in terms of the regression parameters. And then you ask the model to seek out the regression parameters that minimize the distance between the observed covariance matrix in the data and the covariance matrix implied by the model. And so we don’t need the raw data we could do with the covariance. Now it’s nice to have the raw data, because sometimes you have missing data points or other sort of things going on where there’s extra information hidden in the raw data. That’s not hidden in the covariance matrix. So it many cases it’s far more valuable to have to raw data, but if you don’t, you can fit these kinds of models on the covariance matrix.\nOkay. Let’s go back to the presentation. So we have covered that r to that, you can fit structural equation model based on the covariances only and that’s a valid input for a structural equation model. Now the input for genomic SEM are genetic covariances, which we get from GWAS summary data, right? So only from a vector of each SNP rs-number, the effect allele, so which of the two alleles is actually increasing or decreasing the trait, the Z statistic associated with that allele (so that’s the statistic of the linear regression association in the GWAS), and that’s all we’re putting in to getting the covariances. Now we’ll get those genetic covariances and heritabilities using LD score regression.\nNow, what is LD score regression? LD score regression actually tries to explain the signal that is created in a GWAS. So this is a Manhattan plot of a GWAS of schizophrenia and these hits highlighted in green, they only explained 4% of variability, but the heritability, according to twin studies is 80%. So how do we go about explaining the rest? So if we visualize the same GWAS as a QQ-plot, we’d see that the observed P values are much smaller.so therefore the minus log P values are bigger than what is expected under the expected distribution of a Z statistic, and so the question we ask is, is this, this inflation? Is this true signal or is it type one error that we have messed something up that we’re getting all these false positives? And that’s the question we’re asking ourselves when we’re using LD score regression. And so just really important ingredient for LD score regression is the LD structure of the genome. So what I’ve tried to depict here is a part of the genome and each dark blue square is a SNP and the lighter blue squares depict the correlations between the adjacent SNPs and LD introduces correlations between adjacent SNPs, which I’m sure has been covered in the days before this presentation. This is also what creates these towers in a Manhattan plot, right? Because if one of these SNPs is associated truly with the trait, then due to LD the other ones become correlated with the trait too.\nSo you can summarize these LD patterns into a score, which is basically for every SNP, just the sum of its LD with all its neighbors. That’s basically to reflect how well the SNP is correlated to all the SNPS around it, and then consider there is a true genetic effect. Right, so on the left you can see I called this beta. So it’s a true effect. So none of these SNPs has an effect, except for this one, it has a tiny effect. If this were the true effect and we were to do a GWAS, we would get estimated betas, and so we get something like this where we estimate the beta  for the SNP with the true effect to be non-zero, but also for all the SNPs that are in LD with the SNP with the true effect, we’d expect a estimate of beta. Those are sort of raised. Now, what happens on the genome wide scale is that those test statistics you could derive from the linear regression of a trait on every SNP, they go up for SNPs that have more LD and they’d go up precisely because so many SNPs in the genome are associated with the traits, right?  That if you “tag” more SNPs as a SNP, you are more likely to tag more true causal SNPs, and therefore your signal goes up and your test statistic goes up and it goes up in a very specific fashion. It goes up proportional to the heritability, to the number of SNPs with a true effect, M, and with the sample size, N. right? Because Power goes up when the N goes up. And so if the sample size goes up, the relationship between test statistics and LD scores goes up as well. So, and by the way down, there are Hilary Finucane, Brandon Bulik-Sullivan, Ben Neale, and Alkes Price, who basically wrote the first few papers on this relationship.\nSo going back to the little squares, I made the Chi square statistics you get from your GWAS, they are regressed on LD scores, which reflect how well each SNP tags its neighbors, and  SNPs that tag more neighbors are expected to have higher test statistics. And then the slope of that regression is reflective of the heritability because the other big unknowns in that equation - sample size and M - are known to us, right, because we know how big the GWAS was. And then there’s an intercept which actually reflects things that do not correlate with LD scores such as population stratification. Which is a very neat feature. So now we can separate the true signal from the signal introduced by population stratification.\nSo how did they go about validating this? They actually did a GWAS of Swedish controls versus UK controls. So neither of these sets of people have this disease, but they different mainly in their ancestry. And as you can see on the right hand side the QQ plot of the GWAS, it does look inflated. It does look like there is signal there that is inconsistent with the distribution of test statistics under the null. However, if they plot the LD score of all the SNPs, in bins against the test statistic in these bins, then there’s no relationship. In other words, the test statistic doesn’t go up with the LD, which means the signal is probably not a function of heritability, but a function of something else in this case, population stratification.\nNow, if you do it with the real  GWAS, like the schizophrenia GWAS we’ve discussed, you see that there is a steep and consistent correlation between the LD score a bin a SNP is in and the mean chi square the SNPs in each of these bins have. So the relationship is strong and it’s actually consistent with a SNP heritability for schizophrenia of like 40%, and 90% of the signal is true. Okay. So that’s how we get an estimate of heritability from GWAS summary statistics - the slope estimates the heritability.\nAnd if we have two traits, we get two slopes, but can also, instead of using chi square test statistic use a product of the Z statistics of the two traits, regress that on the LD score to get an estimate of genetic covariance between traits.\n So, this is an example where the rg is 0.5. We got a slope consistent with that rg (genetic correlation). And this is an example of why the rg is zero. We get a slope consistent with there being zero correlation. Okay. And it’s robust, this, this entire technique to sample overlap between the GWAS. Now, why is that? Because this intercept in the case of genetic covariance will actually absorb the sample overlap. So it’s  great, we can use GWASes from the same sample to estimate genetic correlation between traits or we can use entirely different samples, so we can correlate some MRI study to some metabolite study. Right. And that’s insightful because it’s really expensive to measure MRI and metabolites in the same people and not always feasible.\nOkay. Phase three of this lecture,\nWhat kind of information goes into a genomic structure equation model? Well, so these heritabilities and genetic covariances we have estimated are actually assembled into a matrix S which holds the genetic variance covariance matrix. So the top left entry is the heritability of the first trait and all across the diagonal, we get heritabilities of the other traits and the off diagonal entries are the covariances between the traits. Now as we’ve seen at the very beginning of this lecture, a covariance matrix is sufficient to estimate a structural equation model. However, if we use a covariance matrix to estimate a structural equation model in lavaan, we need the sample size. Now these estimates don’t necessarily really have a sample size. Yes, the underlying GWAS has a sample size, but that doesn’t translate directly into a precision or standard error for these heritabilities. And so we need that information to be presented in a different way. So for every entry in this matrix, S, we actually need to know its standard error or its variance, and we also need to know the covariance between the different estimates. Right. So imagine I estimate the heritability of height and BMI in one sample, then those two heritability estimates are interdependent, right? Because it’s the same people in that sample that feed into those two estimates, and that dependency needs to be taken account of.\nThat’s what we do in constructing this matrix V which basically has the squared standard errors or the variances of the heritabilities and genetic covariances on on the diagonal. So all the elements of S have a diagonal element in V that corresponds to their standard error or their squared standard error, or variance. And then the off diagonal elements in V are actually the dependencies between the elements of S.  And this matrix takes a while to compute, but it’s basically computed in a very smart fashion that’s called jack knifing in which we basically estimate the LD score regression in chunks of the genome, 200 chunks of the genome. And then we sample from those 200 chunks of the genome. We sample combinations of 199 chunks, and we estimate the matrix S 200 times, each time omitting a different part of the genome, which gets us an estimate of the variance of all these elements in S and their covariance. Those, we store in V and then S and V are the matrices that actually go into genomic SEM.\nNow that sounds really hard and complex and computationally it is luckily others have solved the wonders of how to compute this for us. And so we can very easily implement it. Let me just show you in a browser how is this done in genomic SEM. It’s in the Wiki page of the genomic SEM GitHub Chapter 3: models without individual SNP effects. And it basically starts by preparing the GWAS summary data we get, which is couple of lines of code. Right, which gets us like the summary statistics in a standardized format. So LD score will know how to read them. And then after that’s done all, we need to tell LD score regression within genomic SEM is where to find, in this case, the summary statistics for psychiatric diseases, what the sample prevalence is of these diseases are in the respective GWASes, what the population prevalence of the traits are in the population, where it can find these LD scores I’ve been discussing, and  what I want my traits to be named. And then I just run the function “ldsc()” and everything we just discussed, the estimating of S, the estimating of V is done automatically, and it’s stored in an object, which you can then use to start running genomic SEM models. It also means you don’t need to rerun all those steps. You can store the object, you created, so you don’t need to rerun LD score all the time.  Okay. Thank you for joining us for this lecture And catch you in the next one.\n\n\n\nExamples on the Genomic SEM wiki\nTitle: Working through examples on the Genomic SEM wiki one by one: munge, ldsc, usermodel functions\nPresenter(s): Michel Nivard\nMichel:\nHi, and welcome to this tutorial on genomic SEM, and specifically a tutorial on fitting genomic SEM models without an individual SNP effect. And today’s video is on how to perform those models. We also have written a Wiki page on GitHub. Right, so, navigate to Github.com/GenomicSEM/GenomicSEM, then click on the header Wiki. And you’ll find a number of tutorial pages, or pages with instructions, on how to perform analyses in GenomicSEM, and actually, the third chapter of the Wiki is about models without individual SNP effects. And that’s what we’re discussing in this video, and actually, we’re running the code from this tutorial.\nRight, and what the code will do, it will first download GWAS summary data for a number of psychiatric disorders: specifically bipolar, schizophrenia, depression, PTSD, and anxiety disorder. We’ll clean those summary statistics. In the sense that, we’ll take them from their format as they’re uploaded by the authors, and those formats, can vary quite a bit between authors and groups, and put them into a single uniform format. This step, we call “munging”.\nThen we’ll take a step in which we’ll compute the genetic covariance matrix using LD score regression, and we’ll compute the matrix of standard errors associated with genetical variances and covariances, as discussed in a previous video, right. We had a video about computing the matrix S, the covariance matrix, and the matrix V, which is the covariance matrix of all the elements in the covariance matrix. Once those two steps are done, we can actually start fitting models.\nWe’ll first fit the Common factor model. And then we’ll manually fit a model with a common factor that loads onto schizophrenia, bipolar, MDD, PTSD, and anxiety, and add a second factor as an illustration. And basically, allow you guys at home to follow along. Now, to follow along, you’ll need to download GWAS Summary data. And this is a neat trick: if you’re on a Mac or Linux machine, you can actually run command-line scripts in a terminal in “R,” right? So I’ve written this script to download the LD scores we’ll need for LD score regression, a list of HapMap SNPs which is the list of high-quality SNPs we’ll use for LD score regression. And also the GWAS summary data from either the PGC website or from James Walters’ group who did a ClOZUK GWAS of schizophrenia. And unzip all those GWAS summary data. Now, this will take quite a while to run because It’s like three gigabytes of data you have to pull in from the internet. I’ve already run it. So that’s the step we’re going to assume you do yourself at home, okay?\nWithout further ado, let’s move over to the script for this video. And as a first step in this script, I’ll require “GenomicSEM” and the package “data.table”. I will require “data.table” because one of the files has a column that’s sort of not easy to work with. In its SNP column, it actually has RSID and basepair and chromosome and allele A1 and allele A2 concatenated together as one variable. And that’s not what we want. We want a specific variable that’s only the RSID for the SNP. So step zero is to prepare specifically that schizophrenia GWAS and pull out the RSIDs from that one column. I’ve written a bit of code here to do that, and it will take quite a while to run, and it will heat up my computer a bit. So there we go, and I’ll then have to patiently wait for this code to run before we can actually take the next step.\nOkay. That code has managed to run, and we’ve prepared the summary data for further processing. Now we are ready to munge. So the “munge” function takes a number of arguments. Let’s look at the help page real quickly. And we try to be good about updating the help pages, but all of us developers for GenomicSEM have an actual career on the side in which you have to do science, so they may lag behind sometimes or they miss certain information. So basically, munge is a function that processes GWAS data and prepares them for LD score regression. So we’ll take in those large GWAS summary data files, and it’ll write out smaller processed files which are ready for use in LD score regression.\nOkay, so the first argument is called “files”, and it actually is just a vector of file names, which are the names of the GWAS files you’re going to process. Right? And the second argument is HM3, HapMap3. And it’s basically… I don’t know why we call it HM3, we could have called it filter or, you know, SNPs or selection or whatever. But it’s an argument you use to provide a list of SNPs you think are high quality SNPs or highly efficiently imputed SNPs. And for LD score regression, people have commonly used those HapMap3 SNPs because they’re well imputed. They behave well. So we trust LD score regression analyses based on these SNPs will be a good reflection of heritability or genetic correlation between traits.\nOkay. The next argument is a vector of names for your traits. Gotta be said: pick memorable names because you’ll need those names later. They’ll be the prefix for the file names. Okay? So very long strings of unintelligible things are probably not useful, especially if you return to a project in seven months and you’ve called something flip flop, flip. Then you won’t know what it is. Whereas if you call it SCZ for schizophrenia or BIP for bipolar, you might still recall what the file was you processed, though it’s better to rely on scripts than to rely on file names. Then a vector of sample sizes, an info filter for files that, if these GWAS files have an info column, it will filter SNPs with an info below 0.9, and a minor allele frequency filter, if these files have a minor allele frequency column, it will filter SNPs with a minor allele frequency below 0.01. Now, I say “if” because not all GWASes come with info and MAF columns. And so you will have to work with what you’ve got, right? And this is another reason to consider those high-quality HapMap3 SNPs, because sometimes you are simply unable to filter on things like info, which reflects imputation quality, or minor allele frequency.\nOkay. Let’s run this code. I should have started running it while I was talking, but I didn’t. So now we’ll have to wait a bit too. And as you can see, while it’s running, it will keep you up to date via the terminal. And it will also write a log file. And usually, it takes a few minutes, about a minute per file, which isn’t that long, but if you’re fitting models with like 50 traits, it will take 50 minutes, right? So as a rule of thumb, as many minutes as you have traits, and depending on how many traits you have, you may go for a cup of coffee, you may just check your Twitter, or you may just want to, you know, take the afternoon off and have a nice long hike.\nOkay, so the munge step in LD score regression is done; it took 12 minutes, a bit longer than I expected, but that’s probably because I’m screen recording at the same time. And if you scroll back up a bit and look at the log file it has produced, you’ll see it tells you what it’s doing and munging five summary statistics files, time it started, names of the files, which may be important if you have a lot of file names that are similar, make sure the right files are read in, and it will then tell you for each column in each file, what it’s interpreted in it as. So it finds a column “SNP.” And it says, “I’m interpreting that as the SNP.” And other files, I may find something like SNP ID. And it will actually know that that’s probably SNP. Makes sense for you to go through this file and check whether the things it’s doing are correct, right? So important to check whether these column names are what you want them to be or are interpreted as you want them to be.\nThen it will start filtering, it will remove rows that are in the summary statistics file but not in the HapMap3 SNP lists, the SNPs we think are high-quality SNPs we want to keep. It will then determine the effect column to either be an odds ratio or a beta. It will do this by determining the mean or the median of the column, of overall SNPs, if the median is around one or two, means around one, we’ll think, “Well, that’s an odds ratio.” If the mean or the median is around zero, it will assume that that’s actually a beta, right? And it makes sense for you to double-check whether it’s doing this correctly. Okay? And so usually for a paper, you would take an hour, half an hour to go through these log files and make sure everything’s done correctly.\nOnce you’ve run LD score regression, your working directory will contain “sumstats.gz” files which are like 12 or 13 megabytes. They’re far smaller than GWAS summary statistics because they only contain five columns, removing all the excess information you don’t need. And they only contained 1.3 million SNPs. Now mind you, if the overlap between your GWAS and the HapMap SNPs is only like 150,000 SNPs, you don’t want to run LD score regression, you want to figure out why the overlap is so small, right? So it’s also one of the things the log will report to you: how many SNPs are deleted for what reason. Now, in this example I’m in here, 260,000 SNPs were removed from the bipolar GWAS because the info doesn’t cross the threshold of 0.6, right? So that’s the reason for exclusion. You can go back to the bipolar file and manually check whether that’s true if you somehow suspect there’s something amiss with your bipolar file or your analysis.\nFor LD score regression, you’ll need to define some arguments. First argument is which traits we’re going to use, the sample prevalences of the GWASes. Now, Andrew has been so kind at some point to check these out and look this up, and, well, he had to - we were using them in the original Genomic SEM paper - and then also the associated population prevalence of these disorders. So here, we’re assuming schizophrenia as a 1% population prevalence, bipolar has a 1% population prevalence, and for example, MDD has a 16% population preference. And then you will also need to point LD score regression to where it can actually find the LD scores, which as we’ve shown before, have been downloaded from the internet, and then you define trait names. At this step, the trait names you define will be your variable names in your model later. So definitely here you want to choose memorable names.\nOkay, let’s run this, which again will take a few minutes. But, you know, the magic of video editing, I can omit those minutes and make this video move along rapidly. But you may want to go and have a cup of coffee. Okay, that finished running. And if you scroll back up a bit, you can see that it will produce sort of information for you to consider, right? It will tell you, “Okay, the mean chi-square across the SNPs for the MDD sumstats is 1.26”, Lambda GC, a metric of genome-wide inflation, the LD score intercept, which is supposed to be one, an excess of one is actually a metric for population stratification in your GWAS. It will report the heritability on the observed scale and the heritability’s Z statistic. Now, it will do so for all the heritabilities and genetic covariances, right? And at the bottom, I will report genetic correlations and heritabilities. All of this is written to a log which you can consider at your leisure and make sure that everything’s correct before you consider even publishing Genomic SEM results.\nNow let’s have a look at what was created by this LD score regression function. So we’re going to do some ad hoc inspection of this R object. The LDSCoutput is where you’ve stored the output of your LD score regression, and it contains a matrix V and the matrix S, and the matrix S is actually 5x5, and it’s a genetic covariance matrix. So let’s have a look. The row names of the matrix are the variable names, and on the diagonal, you’ll find the heritabilities of the traits on the liability scale because it converted the observed scale heritabilities to the liability scale using the population and sample prevalences. You can find the details of that in Grotzinger et al. on how that does that. And on the off-diagonal elements, you’ll find the covariances between these traits. Now, as Andrew has discussed in another video and I have discussed in the other video as well, a structural equation model actually takes the covariance matrix as observed in the data and the covariance matrix implied by your model and tries to pick the parameters for the model such that it minimizes the distance between those two covariance matrices. And that’s what we’re going to do right now.\nThe first model, which I’m going to save this output for, so we don’t have to rerun all those slow steps again. The first model we’ll fit is a common factor, which is basically the model you’ll see here, which says that a single latent factor explains the covariance between the psychiatric disorders we’re considering. Now, such a model, in some people’s eyes, and I think that’s a really reasonable perspective, carries a strong causal implication, namely that there is a shared cause of these traits. Right? And we can get into how you can actually test whether that’s a reasonable hypothesis later. Now we’re just going to run the model. Okay, let’s run the common factor. Now running the model doesn’t really take that long, 0.7 seconds to be exact. Now you can imagine if you run a GWAS with 10 million SNPs, 0.7 seconds times ten million - It’s actually quite a bit of time. And then we’d want to parallelize it and run it on a cluster computer perhaps. But right now for this SNP-less model, it doesn’t matter.\nAnd so the output you get from LD score regression is an object within it, some model fits that you can consider to compare two models. For example, the difference in chi-squares and degrees of freedom. You can test significance, test differences between your models. If the models are not nested, you can consider the AIC to compare models, and then there is the CFI and SRMR, which are not relative metrics of two models, but absolute metrics. And I refer you to the literature or the Wiki page or our articles for details on how to interpret these metrics and whether you consider something good. This will also depend on your application and many factors. I also encourage you to be strict on yourself and not just consider something good because you want the model to be good. But I can only encourage you to do so. And if you pick me as a reviewer, I will compel you to do so. So, you know, be careful what you wish for in listing me as a reviewer on your Genomic SEM article. I’m pretty sure the other Genomic SEM authors are similarly inclined. So, you’ll have to deal with someone’s neuroses about fit statistics.\nThen the other part of the model output is actually the parameters, right? So this is the loading of schizophrenia for the first factor, F1, and this is the value of the loading. It’s a standard error, and this is the standardized loading. The standardized loading also has a standard error, and there’s a P value, a very significant parameter. If you are interested, you’ll get factor loadings, but you also get residual variances. And so, residual variances are variances in the traits that are not explained by the factor, and they’re on the scale of the original heritability. So, 9% of the variance in bipolar disorder is not explained by the latent factor. Okay, good to know. Good, let’s move on.\nThis was a single common factor model, and we’ve been so kind to define a function specifically for you to use to fit the common factor model. But in many cases, you may want to fit a different model, right? So, to get into the habits of fitting your own models, we’re going to start by defining a common factor model. Now, you should really, if you haven’t watched Andrew’s video on lavaan syntax, you should pause now and go watch that video, and then come back, because we’re going to use lavaan syntax, and I’m going to assume you understand it because he has already explained it, right? So please do that and come back.\nOkay, well, if you’re still here and you’ve watched the video already, you’re just back. Welcome back.\nHere we define a common factor model, we define it as a latent factor F1, which is measured by, that’s what this equal sign plus tilde means, measured by schizophrenia, bipolar, MDD, PTSD, and anxiety, right? Factor causes variance in those disorders. And then we also define factor variance to be one. We fix it to be one. Now, it’s an identifying characteristic. We need to fix the variance of the factor to one, or we need to fix one of the loadings on one of the indicators for disorders to one. And let’s run the common factor model using the code for your model. The codes are actually designed to read your model, which may be different than a single factor model, and fit it to the data. And so we’ve already seen the fit of the model, and those of you who’ve seen model fits before will know that SRMR (Standardized Root Mean Square Residual) of 0.22, which is pretty high, and CFI (Comparative Fit Index) is 0.85, which is pretty low. So the fit of the model to the data is not great, and we can try to improve the fit to the model.\nOkay, so we can try to fit another model, and to determine what kind of model we should fit, we should definitely try to check out what the residual covariances are in the model. And so one of the arguments we can add to the user model is “imp_cov”, and it will give us the implied covariance matrix. We’ll set that argument to TRUE, and we’ll rerun the model. Doing so gives us the model implied covariance matrix and the difference between the model implied and the observed covariance matrix, right? Which we calculate as observed minus implied. And it will tell us where there is still covariance between traits that is not explained well by the model. And if we look at this matrix, we’ll see that there’s actually a slight, or actually maybe a substantial residual correlation between MDD and anxiety, which are very similar disorders, and their covariance isn’t explained by only the common factor. So, a model we could consider is a model in which we allow a residual covariance to exist between MDD and anxiety, and we’ll call it common factor model two. We’ll call its output common factor model two, right, to distinguish it from the previous model. And we can check whether this model fits the data better. Let’s go, let’s run it, and scroll up to figure out whether there’s anything in terms of model fit that improves.\nWell, wow. So the CFI of this model was 0.98, which is way higher than the one before. We can look back, it was like 0.85. And SRMR (Standardized Root Mean Square Residual) is way lower. So those are in the correct direction. That’s what you want - the model fits the data better if we allow for a residual covariance between MDD and anxiety.\nSo the last bit of this video went a bit chaotic, but that’s what you get if you ad hoc try to fit a new model. You can try to improve the model further or, yourself, you could consider whether there is still residual genetic correlation between schizophrenia and bipolar - maybe you removed the genetic residual correlation between anxiety and MDD. All kinds of things you can consider, and I encourage you to try. And some of those things we may introduce in live practicals in June. Thank you for watching and catch you in the next one.\n\n\n\nMultivariate GWAS in Genomic SEM\nTitle: Multivariate GWAS in Genomic SEM\nPresenter(s): Andrew Grotzinger\nAndrew Grotzinger:\nIn this video, we’ll be talking about how to perform multivariate GWAS using genomic SEM.\nMultivariate GWAS consists of four primary steps. The first one being munging the summary statistics, which we’ll talk about in just a second. The second to run multivariable LD score regression within genomic structural equation modeling to obtain the genetic covariance and sampling covariance matrices across the GWAS summary statistics. And note to say that these first two steps mirror the steps that you would go through to estimate a model without individual SNP effects, including for the user model and common factor functions that Michel talked about in the previous video and do not need to be run again just for the purposes of running a multivariate GWAS. And the third step, you’ll prepare the summary statistics for multivariate GWAS using the sumstats function. And finally, you’ll actually run the multivariate GWAS using common factor (“commonfactorGWAS”) or “userGWAS”.\nFor this example, we’re going to use the five psychiatric traits from the GitHub example for the P factor across schizophrenia, bipolar disorder, major depressive disorder, PTSD, and anxiety. And these are all publicly available summary stats that are directly available for download.\nThe first step again is to munge the data, where munge literally just refers to the general process of converting raw data from one form to another. Munge is primarily converting the summary statistics, specifically Z statistics. It’s aligning all the summary stats to the same reference allele, and it’s restricting them to Hapmap3 SNPs, both because these tend to be well imputed, and even with just those 1.1 million Hapmap3 SNPs, you tend to get a reasonable estimate of the heritability. So, sometimes people will be really concerned when they have this large set of eight to 10 million SNPs, and then they run it through munge, and they only have about one million SNPs left. But this isn’t cause for concern because that is enough to get an accurate estimate of heritability using the LDSC equation.\nWhen you run munge, it’s going to produce a “.log” file for each of your traits. And this is something that’s important to check, just to make sure all of your columns are being interpreted correctly. I think in general, there can be this push and plug forward with the results and not really take a look at your data or some of these log files that are produced by different packages, but you definitely want to make sure before going through all of the additional steps that the data is being read in appropriately. And one particular thing that I’ve highlighted here is for case-control traits, you really want to make sure that the effect column is being interpreted correctly as either an odds ratio or not. So, for MDD, I know that this is an odds ratio, and I see that the munge function is interpreting that as such.\nI just want to go over to R now just to walk through this code as we go along. So, up here, I’ve just set the working directory to where I’ve stored all of these files. This will look a little bit different in terms of how you do this if you’re on something that’s not a Mac operating system. Then I load in genomic SEM and also this function “data.table”. Before running munge, something I want to highlight is that I actually have to do something to this schizophrenia summary statistics so that munge can read this data appropriately. So I’ve already read in the schizophrenia summary statistics using “fread”. And if you look at a particular row within schizophrenia, you’ll see that within that SNP column, it’s not just the RSID identifier for that SNP but it’s in the format of “RSID:base_pair:A1:A2”. And that’s not something that munge is going to know how to read. So, prior to actually running munge, I use these two lines of code to first split that SNP column, so, it’s just pulling out the RSID using string split and sapply, and then writing out a new GWAS file titled “scz_withRS”. And then for munge, we list the files, the Hapmap3 lists the names of the traits, and then the total sample size before running munge. This is not something I’m going to do right now, just for time reasons and because Michel will have gone over it in the previous video. But just to show you what the code looks like for this first step.\nSwitching back over to our slides, the next step is going to be to run LD score regression, which computes that genetic covariance and sampling covariance matrix that was discussed in one of Michel’s videos. So, this is the level of genetic overlap across these different traits is estimated using LD score regression, and then also the standard errors and dependencies across those estimates, as will be the case when there is sample overlap. And this sampling covariance matrix is what allows genomic SEM to produce accurate estimates, even in the face of unknown levels of sample overlap across your traits. I’ll just note that before going on to steps three and four, I would highly recommend pausing at step two and actually fitting what I sometimes call the “base model” using the “usermodel” or “commonfactor” functions. That model doesn’t include the effect of an individual SNP on different parameters in the model, just to make sure that you’re getting reasonable estimates, that it fits well, and that lavaan or genomic SEM don’t produce any warnings or errors about this particular model. Because odds are when you then carry that model forward to multivariate GWAS, a lot of the same problems are going to start to show up. So you just want to diagnose that, make sure you’ve got this solid base model, and then carry that forward to multivariate GWAS in step four.\nSo going back over to the R script, LD score regression takes the names of the munged summary statistics for the case-control traits. It takes the sample prevalence of cases over the total sample size, the population prevalence (which can be pulled from the research literature), the LD scores, and the weights used for LD score regression, oftentimes, this will be the same folder for both of these, and the trait names for your summary statistics. This particular argument, “trait.names”, is important because this is how you’re going to name these traits when you specify the model in lavaan. So, you want to make sure you don’t name it something with a bunch of upper and lowercase characters, something that’s easy to write out when you go to write your model in later steps. And then you just run LDSC; this’ll take about a minute with only five traits. Again, I’m just not going to do it here for time reasons, and I’m going to load in that LDSC output that I created before, which is something that I just saved using this command here.\nThe third step is sumstats, and before I go back over to the slides to talk about some of the arguments for sumstats, I just want to read in these arguments and set sumstats up to run. So we’re going to just let this run and go back over to the slides to talk about what sumstats is actually doing.\nSo, just like munge, sumstats makes sure that in all cases, the GWAS summary statistics are aligned to the same reference allele, and further, the coefficients and their standard errors are transformed so that they’re scaled relative to unit-variance-scaled phenotypes. What that means is that it makes sure that the GWAS estimates are scaled relative to a standardized outcome, or what is sometimes referred to as “stdy” or partially standardized regression coefficients and standard errors. We are not standardizing with respect to the predictor (i.e., the SNP), but just to the outcome. And the reason that’s important is because we’re going to take this sumstats output, and we’re going to add it to the genetic covariance matrix from LD score regression that we just created in step two, and that genetic covariance matrix from LD score regression is itself on a standardized scale, where the heritabilities on the diagonal are, by definition, scaled relative to a standardized phenotype. So we want to make sure that when we add this sumstats output to that LDSC matrix (which I’ll show you visually in just a couple of slides), that they’re on the same scale so that we can produce the appropriate estimates. In order to do that rescaling appropriately, sumstats needs to know a little bit of information about the scale of the outcome and how the GWAS was specifically run. So, this takes a number of arguments in order to make sure things are done appropriately. And I just want to walk through those arguments here.\nThe first argument for sumstats is the name of the summary statistics files. This is not the munged files and should be the same as the name of the files used for the munge function. So it’s the raw summary stats that you provide to munge, and it should be listed in the same order that you listed them for the LDSC function in step two. The second argument is the reference file that’s used to calculate SNP variance and align to a single reference allele across traits. Here, we’re going to use a 1000 genomes referenced file from a European population. The third argument is the name of the traits. This’ll probably be the same as how you’ve been naming the traits for the ldsc and munge functions. And the fourth argument is “se.logit”, which is a vector that includes TRUE or FALSE for each trait, indicating whether or not the standard errors in those GWAS summary statistics are on a logistic scale. The reason that we make this a required argument is because oftentimes, GWAS summary statistics somewhat counter-intuitively will list an odds ratio but then they will list a standard error of a logistic beta. So we want to make sure that the user is being sure to double-check this. And this information, if you’re unsure, can often be found in the README file for the GWAS summary statistics for those case-control outcomes. The fifth argument is whether the phenotype was a continuous outcome analyzed using an observed least squares or what is referred to as an OLS or more commonly linear estimator.\nFollowing this argument is “linprob”, which refers to an instance where a phenotype was a dichotomous outcome analyzed using an OLS estimator. This is referred to as a linear probability model and is often run just for simplicity’s sake because it’s computationally much easier to analyze a dichotomous outcome using OLS. But in order to do that rescaling, we need to know whether or not this particular situation is occurring. Proportion (“prop”) is something that is specified in conjunction with the lineprob argument, and it’s necessary to perform the linear probability model, i.e., LPM conversion above. So, it takes the proportion of cases over the total sample size. “N” is a provided sample size in the order the traits are listed, and it’s only needed when OLS or linprob is true for any of the traits. Info and MAF filter are standard filters used to filter on imputation quality for info and to filter on the minor allele frequency, with package defaults of 0.6 and 0.01. “keep.indel” refers to whether you want to retain insertions and deletions, with the default being FALSE. “parallel” refers to whether or not the function should be run in parallel and utilize multiple cores on the computer, with the default being FALSE. And if you are running in parallel, you can specify the “cores” argument that indicates whether you want the computer to use a certain number of cores. The summary statistics or “sumstats” argument will typically only take in this case. It’ll take about eight minutes for 30 traits; it might take upwards of an hour. So you certainly can run in parallel and speed things up, but it’s not necessary to run in parallel by any means.\nSo I know that the sumstats argument can be a little bit confusing, and for that reason, I’ve created a schematic on the GitHub Wiki. So, this is on the second page of the Wiki, important resources and key information, that just walks you through how to think about specifying these arguments. And it starts with this first question: Is the GWAS outcome binary or continuous? And it just lets you know what and how you should specify these different arguments.\nIf we go back over here to R, you can see that I’ve specified those file names, the name of that reference file, which is available in a Box link listed on our Wiki. The trait names for all of these - these are case-control outcomes and they are reporting standard errors in logistic scales - so I set se.logit to TRUE for all of them, and I use the default info filters. For completeness, I’ve listed all of the different arguments here, but you can certainly write this in a more compact form. You don’t have to write “OLS = NULL”, “linprob = NULL”, “prop” = NULL” if you don’t have any OLS or linprob outcomes. And here, I am running in parallel using four cores. We’re just going to let this finish up here, and while that’s happening, we’ll move on to talking about the GWAS functions.\nBefore doing that, a note that the sumstats function will also produce a log file like munge. So, again, it’s imperative that you look at these log files and just make sure everything’s interpreted correctly. And much like major depression that I showed you for munge, we want to check here that for bipolar, the effect column is, in fact, appropriately interpreted as an odds ratio.\nSo, I’m going to first talk about the commonfactorGWAS function, and then I’ll end by talking about the userGWAS function. What commonfactorGWAS is doing is it’s automatically specifying a common factor model where the SNP is specified to predict the common factor.\nWhat’s happening behind the scenes for both of the GWAS functions is it’s automatically combining the output from step two from LD score regression and step three, which we’re running right now from sumstats. So what it does is it, one by one, takes the LDSC matrix, it takes a particular row for an individual SNP from sumstats, and it adds it to that matrix. So now that you’ve got the LDSC matrix and then this appended column or vector of individual SNP covariance effects between the SNP and these five psychiatric traits. And what it’s going to do is it’s going to create this matrix, run the model, and then discard the matrix. And so it’s going to create as many covariance matrices as there are SNPs across the traits.\nEffectively, if we then take that matrix and run the model, it’s then specifying this model where the SNP is predicting this general factor that indexes risk sharing across these five psychiatric traits. So this is an example of just one model that’s being run, but again, this first vector here is swapped out as many times as there are SNPs, and it’s re-estimated to get that updated estimate of the SNP effect on the factor.\ncommonfactorGWAS takes a couple of arguments. The first is “covstruc”, which is the output from LD score regression. The second is SNPs, which is the output from sumstats. The third optional argument is estimation, which specifies whether models should be estimated using DWLS, which refers to diagonally weighted least squares, or ML, which refers to maximum likelihood estimation, where the package default is DWLS. The way to think about the difference between these two is DWLS will look to produce model estimates that are trying to recapture the parts of the observed covariance matrix that are estimated with greater precision. This does not mean that DWLS is going to automatically produce things like larger factor loadings for the GWAS traits with a larger sample size. Instead, if you’ve got a particularly well-powered GWAS that goes into this model and that GWAS does not correlate very highly with the other traits, then the model will actually prioritize producing, in the context of a common factor model, a particularly low factor loading for that well-powered trait. So again, it doesn’t mean that the well-powered trait dominates the model per se, in the sense that it’s producing larger estimates. It just means that DWLS is taking into account the information that’s available. Depending on how you think about statistical modeling, you might have a different preference between them, but to our mind, if you’ve got more information about a particular cell in that covariance matrix that reflects a GWAS that’s better powered, why not use that information appropriately and let the model appropriately favor reproducing that part of the matrix. Cores is how many computer cores to use when running in parallel, where the default is to use one less core than is available in the local computing environment, but you can specify as many cores as you want using this argument. Tolerance (”toler”) is only relevant if you start getting errors or warnings about matrix inversion, but beyond that, it’s not something that you need to be concerned about.\nParallel is an optional argument specifying whether you want the function to be run in parallel or to be run serially. GC is the level of genomic control you want the function to use. The default is to adjust the univariate GWAS standard errors by multiplying them by the square root of the univariate LDSC intercept. What that does is it takes this univariate LDSC intercept, which is intended to index uncontrolled for population stratification, and that it appropriately corrects those standard errors by the intercept so that you’re producing estimates that pull out that uncontrolled for pop Strat. If the LDSC intercept is below one, I’ll just note that what the package is going to do is not correct for the intercept at all. So it’s never going to produce more liberal estimates than a univariate GWAS going in, but it’s going to be more conservative as a default. MPI is whether the function should use message passage interfacing, which allows you to use multi-node processing. We’ll talk a little bit more at the very end when we talk about runtime considerations for these different functions.\nSo now, if we go back over to R, we can see that the sumstats function has finished up running. It took about seven minutes. And now going on to step four of actually running the common factor GWAS, just for the purposes of this exercise, I’m just going to subset to the first 100 rows of that sumstats output, just so we can see how the common factor GWAS functions are running. So we’re just going to let this run here. And as it’s doing that, I’ll just show you that we’ve got covstruc that lists the LDSC output, SNPs that list, that subset output from sumstats, that we’re using DWLS, we’re using four cores, and we don’t need to set the tolerance lower. We’re not changing the SNP standard error that it uses, we’re running in parallel, we’re using the standard GC correction, and we’re not using MPI. SNP standard error (SNPSE) just refers to whether or not you want to change the standard error of the SNP. This is just set to a really small value to reflect the fact that that’s coming from our reference panels, so we essentially treat it as fixed, but it is not something that really affects your model estimates out to the fifth decimal place.\nSo that finished running. Let’s just take a look at the first five rows. And what you can see here is that it’s pulling the SNP RSID, the chromosome, the base pair, the minor allele frequency from the reference file that you fed to sumstats, A1 and A2, just the run number, the particular estimate from the model that was saved, which for common factor GWAS is always going to be the effect of the SNP on the common factor. The corresponding estimate for that parameter, the sandwich-corrected standard error, the Z statistic, the P value, and then this Q statistic and its degrees of freedom and P value. There’s also this “fail” and “warning” column at the end. That can be good to check using something like the table argument in R, just to see if any warnings or runs were failing to produce any output. A zero indicates that there were no warning runs. And we can see here that for these hundred SNPs, there were no issues that were raised. Before I switch back over to the slides to talk about Q some more, I’ll just highlight that for a lot of this code, I’m just including, for completeness, the arguments that are listed, including their defaults. So MPI is automatically set to false, GC is automatically set to standard. So we could produce the same output in a much more compact form, writing this code here below. This will just produce the same results, and it’s just to highlight that if you’re setting the arguments to the default behavior, you don’t have to list them.\nSo what you saw in that output was three columns related to this QSNP statistic, which is an estimate of SNP-level heterogeneity. And the way to think about QSNP is it asks the extent to which the effect of a SNP operates through a common factor. It’s a chi-square distributed test statistic that is essentially comparing the fit of a common pathways model, in which this SNP operates strictly via the common factor, to an independent pathways model in which the SNP directly predicts the indicators of that common factor. If this independent pathways model fits much better than this common pathways model, then that suggests that the SNP is not really operating through the common factor, that this single regression relationship is not sufficient for capturing the pattern of the relationships across these five indicators here. Instances where you might expect QSNP to be significant include when, for example, there are directionally opposing effects of the SNP on different indicators. So let’s say the SNP is risk-conferring for the first two indicators and is actually protective for the last three indicators. In that case, it’s clearly not operating through some general common factor, and we would expect QSNP to be significant. Another instance might be if the SNP has a really strong effect on one of the indicators and a null or at least a much weaker effect across the remaining indicators. As a canonical example of this, if we ever include alcohol use disorder or any alcohol use phenotype within a genomic structural equation model, we often find that the variants that exist within the alcohol dehydrogenase genes will pop as significant for QSNP. And that’s because those tend to be genetic variants that are not associated with the general factor that alcohol use is loading on, but are instead highly specific to that alcohol use phenotype. And what is cool about QSNP is that when you’ve got a set of really highly correlated traits. In fact, what might be more interesting is what actually causes these traits to diverge. Is to identify via this QSNP statistic, what it is that really genetically differentiates these disorders. Wouldn’t it be nice if we could use QSNP to gain some novel insight about what causes these things to have a slightly different presentation?\nIf you’re specifying a model that is something that is not a common factor model, then you’re going to want to use userGWAS, which allows the user to specify any model that includes the effect of individual SNPs, such as a SNP predicting multiple correlated factors in the model. userGWAS takes all of the same arguments that I just showed you for commonfactorGWAS, along with two additional arguments. One of those is the model that you’re estimating, written using lavaan syntax. And for this model, the way that you’re going to include the SNP in the model is just to literally refer to it as S-N-P or SNP in all capital letters. And I’ll show that model over in the R script in just a second. The second is the sub argument, and this is an optional argument specifying whether or not you’re going to request to save only specific components of the model output. The reason I would recommend almost always setting this argument to something is that there’s a lot of different rows for any given model that lavaan is going to produce. So for example, for the common factor model, there’s the five factor loadings, the five residual variances of the indicators, all of which are going to be fairly similar across all of the SNPs. And it would take up a lot of space to save all of that output for each individual SNP. And what we’re really interested in is just the effect of the SNP on the common factor. So what sub allows us to do is say, “I just want you to save that output for the SNP effect on the common factor.” And if you’re specifying a model in which you’re interested in multiple different parameters, you can also set sub to include more than just one parameter. However, it’s rarely going to be the case that you’re interested in saving every single piece of the model output for each SNP. Instead, you should think about just saving those pieces that you’re interested in.\nIf we switch back over to RStudio to run userGWAS, what I’m going to do is run the userGWAS for the exact same model that common factor GWAS is automatically specifying. So here, we’ve written the common factor model with the factor regressed on the SNP here, and we’re also setting that sub argument to just save the effect of the SNP on the factor. Let’s set that up to run. And what this should do is produce the exact same estimates that we saw for common factor GWAS. I’ll show that to you in just a second when this finishes up.\nSo if we look at the first five rows from the userGWAS output and the first five rows from the commonfactorGWAS output, you can see that these are exactly the same, 0.413 and so on. The userGWAS output is going to look a little bit different. It’s not going to include the Q statistic, but instead, it’s going to include model fit statistics. What’s the overall fit of that model. So it’s going to include Chi-square, Chi-square degrees of freedom, chi-square P value, and AIC. And those can be used to compare to what are referred to as nested models. So you could examine an independent pathways model where that SNP is predicting each of the five indicators. If you did a model comparison across those two models, you would find that that produces the same thing as Q SNP down here.\nSo if we take a look at the run times across this, I know for the first two steps, I didn’t run them now. But if you look at the output files, you’ll find that for munge on my own laptop, it took about eight minutes. LDSC took a little over a minute. Sumstats took about seven minutes. The commonfactorGWAS took about 17 seconds, and the userGWAS took 10 seconds. For those GWAS functions, of course, we only ran it on the first hundred SNPs, and we did run it in parallel. This is not necessarily indicative of how long it would take for a million SNPs. You wouldn’t just multiply these numbers by a certain amount because there are certain initial steps that the GWAS functions need to go through. At the same time, the GWAS functions do take a while. How long that takes exactly is going to depend on the number of traits and how complicated your model is. So I never know exact runtime considerations, but these are things that you are going to ideally be running on a computing cluster.\nFinally, I want to talk about how to really speed this up so that you can get results as quickly as possible. Both parallel and MPI processing are available for userGWAS and commonfactorGWAS, where parallel and serial processing are doing the exact same thing, with the exception that parallel processing is utilizing the cores available in your computing environment to send off different chunks of SNPs to the different cores to then run the GWAS on those cores. MPI takes advantage of a multi-node computing environment. It does require that Rmpi is already installed on the computing cluster, but that then adds this additional layer where it sends the output off to multiple nodes and often multiple cores within those nodes. Finally, you can speed this up just that much more by sending off separate jobs that then themselves use MPI processing, where they send it off to different nodes and different cores within those nodes. The important thing to know is that all runs are independent of one another, so you can dice up that sumstats output however you want, and you’ll still get the same output. For me, anytime I run a GWAS on the computing cluster, I will send off 40 jobs that then run in MPI, and for this commonfactorGWAS example for two-ish million SNPs, that only ends up taking about two to three hours. So again, if you reach out to me and ask, “What’s the exact runtime I should expect for this model?”, I’m not going to know because it’s really going to depend on the type of model you’re running. For sure, there are indicators that something is going wrong, like if you submit a hundred SNPs and it’s taking 12 hours to run, that suggests that something is just breaking apart on the computing cluster. You’re certainly welcome to reach out on the Google group to see if we have any input about how best to speed things up.\nWith that, I’ll just end here, and in the next videos, we’ll talk about some of the newer functionality available in genomic SEM, including the ability to examine multi-variate enrichment using what we call stratified genomic SEM.\n\n\n\nUsing Genomic SEM to Understand Psychiatric Comorbidity\nTitle: Using Genomic SEM to Understand Psychiatric Comorbidity\nPresenter(s): Andrew Grotzinger\nJason Fletcher [Host]:\nSo Andrew, thanks so much for coming. We are just super excited to hear about your work. Many of us know about genomics, sound, but really want to get a more in-depth knowledge. So, very, very happy to have you here and welcome.\nAndrew Grotzinger:\nThank you. It’s a real pleasure to be speaking to this group in particular. Today I’m gonna be talking about genomic SEM overall, but I really want to focus the second half of my talk on the application of genomic SEM to psychiatric traits. I know a number of you have heard me talk about genomic SEM before, so I’ll try to keep this first part relatively brief and more just a refresher, but there is a new package update within the last couple of months that I’ve put on the GitHub for stratified genomic SEM, so hopefully that’ll be kind of a new thing for some people here.\nSo just to start, giving an overview of genomic SEM and kind of the motivation for developing this package. I’m just showing here an old plot from Kendler et al. 2012 showing GWAS hits on the y-axis and different traits on the x-axis, and what this is demonstrating is something that’s well known to everyone here, that as the sample sizes have increased for different GWAS studies, the number of hits that we’ve identified has increased in a corresponding way, pretty rapidly, where we’re now identifying hundreds of different genetic variants underlying complex traits that are of interest to people. And so, for me, as someone who’s interested in psychiatric outcomes like schizophrenia and depression, if we look at these two Manhattan plots from some of the more recent efforts from these groups, these traits are so polygenic that if you’re trying to figure out what actually is shared across these two traits, it’s not just a matter of identifying five or ten overlapping genes or loci that are shared across these Manhattan plots. And it really required people to think about how we could actually quantify the level of genetic overlap, and that was done really beautifully in 2015 when the group from the Broad introduced LD score regression, and more specifically, bivariate LD score regression, for examining shared genetic overlap across traits, which allows you to estimate genetic correlations between samples with varying degrees of sample overlap using what is often publicly available GWAS summary data.\nAnd when people apply genomic SEM like they did in the Brainstorm Consortium paper in 2018, you can produce these genetic heat maps across different sets of traits where, on the left, you’ve got psychiatric phenotypes, and on the right, across a wider range of brain disorders and behavioral cognitive phenotypes, where the darker shading indicates higher levels of genetic overlap. Unsurprisingly, we see that there are certain clusters of traits within these heat maps where, for example, psychiatric disorders tend to show a pretty strong level of genetic overlap, which is something we might expect based on high levels of comorbidity that we observe among psychiatric disorders. While, at the same time, some genetic correlations were maybe particularly or even surprisingly high, such as genetic correlations between bipolar disorder and schizophrenia in the range of 0.6 or 0.8, as you can kind of see indicated by this particularly dark blue box between those two disorders.\nSo when these sorts of papers were coming out, I was in the midst of running twin models and helping navigate and manage a twin study down at UT Austin, and running different multivariate models of twin correlation matrices. And so, seeing these correlation matrices coming out based on genomic patterns of convergence, there seemed to be this kind of strong need to really develop and apply multivariate methods to actually model these constellations of genetic overlap. And so, that led us to develop and publish this paper that introduced genomic structural equation modeling in 2019 in Nature Human Behavior, which is kind of broadly speaking a framework for modeling patterns of genetic associations across wide constellations of traits.\nAnd so, in general, genomic SEM uses a two-stage estimation procedure where in stage one, you estimate the genetic covariance matrix and associated sampling covariance matrix of standard errors and their codependencies. We use LD score regression within the context of the package, but you could hypothetically use any sort of genetic covariance matrix, such as you might get from GCTA-GREML. Um, and then in stage two, we actually fit the structural equation model to the matrices that are estimated from stage one.\nAnd so, just to kind of show you visually what those stages look like, and so in stage one, we’d create this genetic covariance matrix or what we call S, that has the heritabilities estimated from the genome-wide data on the diagonal, and those genetic covariances are coheritabilities on the off-diagonal. And then critically, we’re also estimating this sampling covariance matrix V that contains the squared standard errors on the diagonal and the sampling dependencies on the off-diagonal, which indexes the dependencies between estimation errors, and that’s what allows us to apply genomic SEM for samples that have different levels of sample overlap. And that doesn’t need to be known; that is directly estimated from the data using a block jackknife procedure. But again, if you have different sets of summary statistics and you’re worried about some level of overlapping controls, this sampling covariance matrix is going to allow you to produce unbiased standard errors even in the presence of that sample overlap.\nAnd then in stage two, you take those two matrices and feed it into the genomicSEM R package and specify some sort of model like this genetic multiple regression model in which we have schizophrenia and bipolar disorder as correlated predictors of educational attainment. And then parameters are estimated that fit the observed genetic covariance matrix as close as possible. And since this is a fully saturated model, the model parameters are simply a transformation of the observed matrix.\nOne thing that I like to highlight when I’m talking about this (and I know that this kind of first part of this phrase is not applicable to this group), but even if you are not interested in genetics, I think genomic SEM offers some valuable tools, because it allows you to look at systems of relationships across a wide array of rare traits that could not be measured in the same sample. And so, if we think about just a case example of what that might look like, let’s say that as someone interested in clinical phenotypes, you have a real interest in the relationships between schizophrenia and bipolar disorder. You’ve read the research literature and see a lot of convergence across different risk factors for these disorders. You have a pretty good understanding that these phenotypically can often look pretty similar, and you also notice pretty similar kind of age of onset distributions for these two disorders. So in that case, you might be interested in looking and quantifying, in a sort of cross-leg panel like this, the relationships between both early and late onset versions of schizophrenia and bipolar disorder. The issue is that these two disorders are actually —you can’t diagnose them together. In the DSM, these would be rule outs of one another. So you couldn’t in a phenotypic sample actually observe these two disorders within the same individual. And so you wouldn’t be able to estimate this part of the model using phenotypic data. And of course, you also can’t observe both early and late onset versions of a disorder within the same person. So you couldn’t look at this part of the model.\nBut with genomics SEM, you can actually stratify the GWAS summary statistics by early and late onset versions of these disorders, and you could actually fit this sort of model and actually start to test the sorts of relationships that we’ve been left to really just hypothesize about and make qualitative conclusions around, based on convergence from different separate univariate studies.\nAt this point, stratified genomic SEM can be used to look at convergence at three main levels of analysis, and I just want to walk through those. So at the genome-wide level, this is actually just a recapitulation of that multiple regression model I showed earlier as an example of stage two estimation, where we’re just looking at the system of relationships between schizophrenia and bipolar disorder and educational attainment. But this is just to highlight that, for people who are interested in sort of mediation-type hypotheses, that within genomic SEM, you can kind of quantify things like a total indirect and direct effect between different disorders.\nWithin genomic SEM, you can also get standard model fit indices like AIC, model chi-square, CFI, and SRMR, and that allows you to go through kind of classic model fitting procedures to try and decide between different competing models, and decide what kind of patterns of relationships best fit the data. So, in that original paper, we looked at a series of models fit to the covariance matrix estimated across these 12 neuroticism items. And what we found is that a common factor model fit the data pretty well, a two-factor model did a little bit better, but a three-factor model really seemed to balance that relationship between fit and parsimony in terms of how we were representing the data. And what I would highlight here is that the way that the items kind of segregated across these factors actually does make a lot of post-hoc theoretical sense. So you see on factor one, this kind of mood, misery, and irritability items, maybe on this kind of negative affect factor, guilt, hurt, and embarrassment around maybe this kind of social reactivity factor, and nervous, worry, tense, and nerves maybe this kind of more anxiety-type factor. And while these factors are still highly correlated, they are somewhat dissociable. And in fact, when we run multivariate GWAS to look at SNP effects on these factors, we do find somewhat divergent biological pathways that we might miss if we were to employ an approach where, say, we just kind of sum across these items and run a GWAS on a sum score of the 12 neuroticism items. So I think this really highlights the ability of these sort of model-fitting procedures to pull out some pretty interesting patterns in the data.\nMoving on to this new level of analysis, stratified genomic SEM, that we introduced in a paper that’s currently on MedRxiv and out for review right now, but the code for that is live on the GitHub along with a tutorial page for that. It is really designed to think about how we can start to make sense of GWAS findings characterized by thousands of genes that really individually explain only a very small portion of the phenotypic variance. So, just as a sort of hypothetical example, let’s think about these future Manhattan plots where the GWAS sample sizes are getting into the millions and now we’re starting to see that basically the entire genome is somehow associated with the phenotype. Again, I know this is just sort of a case example, but it just highlights that at an individual SNP level, we’re going to get to a point where the picture is so complicated that it’s really hard to make sense of what’s going on just based on a Manhattan plot.\nThere’s already a number of methods out there that, in the univariate case, can be used to basically partition heritability by using collateral gene expression data, such as what you might get from RNA sequencing methods, to try to lump associated genetic variants and portions of heritability into meaningful categories. So, in this Partitioned LD Score Regression paper from 2015, they showed that disorders like schizophrenia and bipolar disorder are enriched in the central nervous system, which you can see here based on these orange bars for that annotation, including for years of education as well, which, of course, makes a lot of sense.\nSo we extend that same model to be able to look at partitioned coheritability. This will look familiar to anyone who’s worked with the bivariate LD score regression before, and really all we’re doing now is that, instead of using the LD scores here, we’re using the partitioned LD scores, so the LD scores within a particular functional annotation. So, we develop this and validate it as a means to an end for really being able to then feed these partitioned covariance matrices into genomic SEM to then be able to examine genetic enrichment of any model parameter estimated in genomic SEM for this kind of new extension that we’re calling stratified genomics SEM. So you can look at enrichment of residuals in a structural equation model, enrichment of correlations between factors, and I think what people would typically be interested in, enrichment of the factor variances to see where these kind of overarching factors that explain variance in the indicators are really enriched.\nSo, in that way, we can take a Manhattan plot of these common factors and start to look at these kind of top peaks and ask whether or not these hits are really enriched within genes that are expressed during certain developmental periods, such as during early, even prenatal periods, or later in life. Whether they’re enriching specific brain regions or even in certain neuronal subtypes. And this method really starts to, and, you know, any kind of partitioning method starts to become increasingly exciting as the gene expression work starts to move at a rapid pace and our ability to build these categories in meaningful ways starts to also really increase and become quite exciting.\nSo again, this method is about asking whether there are certain biological functions that can characterize genetic variants with pleiotropic effects, which, you know, there’s a lot of kind of convergence of findings across disorders that tend to cluster together. Unsurprisingly, psychiatric disorders tend to be enriched within brain regions, but now we’re really trying to quantify that using this multivariate method for stratified analyses.\nAnd at the most fine-grained level of analysis, genomic SEM can be used to perform multivariate GWAS to look at SNP effects on any model parameter that you would estimate in genomic SEM. And the way we do that is that we extend that S matrix I showed you earlier when walking through the two-stage process. We’ve got that same genetic covariance matrix from LD score regression here in blue, and then we append this SNP column that includes the SNP variance from a reference panel and the betas from the GWAS summary statistic scaled to covariances using that same SNP variance from the reference panel. And so what you would do is the package builds this matrix as many times as there are SNPs present across the indicators and then runs any particular model that you might specify that includes an individual SNP.\nSo for example, we look at SNP effects on a P factor that’s defined by these five psychiatric indicators of schizophrenia, bipolar, major depression, PTSD, and anxiety. We take this LD score regression matrix and append the SNP column, and then we’re able to look at the effects of an individual SNP, such as this particular SNP that we find is genome-wide significant with respect to its effect on the P-factor in the context of multivariate GWAS.\nWe also have developed this SNP heterogeneity metric, which we call QSNP because of its similarity to the meta-analytic Q statistic, that really asks whether or not the associations between a given SNP and the individual disorders, like I’m showing here in panel A, is sufficiently captured by a model that posits a single association between the SNP and the factor. So really, is it, kind of, if you just fit this one relationship, is that obscuring the relationships that you might get if you fit all of these individual relationships? We compare this common and independent pathways model, and if this model in panel A fits much better, then that suggests that this SNP is not really operating through the factor. An instance where you might see that is if, for example, there’s highly disorder-specific effects. So if a SNP really is specific to one of the indicators and has a null effect or an opposite effect on the other indicators, then you would expect to get a significant QSNP metric. And this really highlights that genomic SEM is not about boosting power for the individual traits in the way that some other multivariate methods are designed, like MTAG, but it’s really about finding the SNPs that operate through the factors and the SNPs that are highly specific to the disorders. So we can start to get a sense of the multivariate picture across the different disorders that we include in the model.\nThis is one particular application of a model that you could fit in genomic SEM that people have shown some interest in, namely GWAS-by-subtraction. Where you fit this sort of Cholesky model for two traits and then you have the SNP predict the two latent factors within the Cholesky model. So in this particular example, we’re looking at the effects of a SNP on cognitive performance and on educational attainment minus the genetic overlap with cognitive performance for what we’ve called this kind of non-cognitive element. So that then you can start to kind of break apart something that’s really multifaceted like educational attainment and ask what SNPs underlie this overall genetic signal that are separate from the cognitive component.\nThis is in a paper that is currently on BioRxiv but is forthcoming [Demange, et. al 2019], and this is just a highlight that then you can produce these Manhattan plots for the cognitive phenotype and the non-cognitive phenotype down here and identify these kind of dissociable genetic signals. And then in the paper, they also look at kind of polygenic score prediction for these two different phenotypes and find divergent patterns of prediction and also genetic correlations with outside traits. So this is just kind of one way that you can apply genomic SEM in interesting ways.\nIn general, this is just a kind of sales pitch for genomic SEM. The different groups that have used it and using the open-source R package have been fairly successful in publishing in a lot of different outlets. So this is just kind of a flavor of the different ways that people have been using it and the outlets that they have been publishing in using genomic SEM.\nSo with that, I want to transition as someone who’s trained in clinical psychology to my main interest in psychiatric phenotypes and how I’ve used genomic SEM to really understand the multivariate genomic architecture across psychiatric traits.\nSo I just want to kind of start by explaining why I think multivariate work in psychiatric disorders is so important to begin with, even ignoring the genomic piece. So in this orange circle, I’m just showing kind of all individuals with mental disorders that will meet criteria for a disorder in their lifetime, and among that group of people, about two-thirds are going to meet criteria for a second disorder, half will meet criteria for a third disorder, and 41% will meet criteria for a fourth disorder. And while we know that our categories are not perfect, we do kind of think of them as sort of these mutually exclusive things, where, I think when clinicians sometimes talk about people having multiple disorders, there’s this sense that they’ve just kind of had bad luck in terms of maybe the environments or genetic risk factors that they’ve been exposed to. But really, at the end of the day, I think this speaks to how much our categories overlap pretty substantially and how much we should think about kind of refining future versions of our diagnostic manuals so that we can create more mutually exclusive categories.\nAnd I think there’s a lot of really compelling reasons to think about why we would want to do that, but before I get to that, this is just kind of depicting the extent to which we might kind of loosely think about these patterns of comorbidity and kind of the loose rough borders that we’ve drawn between the disorders as being somewhat genetic in nature. So, on the x-axis here, we have parent disorders across depression, generalized anxiety, panic, substance use, and antisocial personality disorder. And on the y-axis, the odds ratio for their child developing a particular disorder. And what we see is that the children are really at risk for any disorder and not just the disorder present in the parent. Now, of course, that could just mean that a parent having a mental disorder is sort of generally stressful through an environmental component that kind of generally leads you at risk to any disorder, or it could mean that the genetics passed down from the parents are really kind of unspecific in terms of how they convey risk. But at the very least, that suggests that the boundaries between the disorders are pretty blurry at the end of the day.\nAnd so again, that’s important because, as someone who also currently practices, it can be very stressful to meet with someone and tell them that they’ve got every disorder in the book. I think this has real clinical implications in terms of the message that sends to the patient in the room and also what it means for a treatment provider to think about what kind of intervention to give somebody when they’re meeting criteria for multiple categories.\nAnd from a scientific perspective, I think this also has a lot of important ramifications. Where, if you have scientists A studying bipolar disorder, scientists B studying schizophrenia, and this fellow studying OCD, but at the end of the day, we know that these disorders are all related in some pretty substantial way, it probably isn’t the best use of grant funding money to give all these people a separate pool of money and then just kind of send them off in separate directions when they’re really studying these phenomena that are actually interrelated in pretty substantial ways at some level.\nAnd so, that is why, as a kind of backstory, I’m particularly interested in understanding the genomic architecture across different disorders. Which, I think, doing it in a genomic space has a number of advantages. One, it can give us insight into what we know is a pretty important component of these disorders since they’re all estimated to be pretty heritable. But one limitation when people are understanding what’s called this general psychopathology factor, a trans-diagnostic P factor, is family-based research on P is inherently limited to relatively common disorders because it’s going to be next to impossible to obtain genetically informed samples on rare disorders and in just a basic cross-sectional sample, again, I noted that some of the disorders actually disallow one another. So it would be not possible to actually examine disorders within the same sample. So, for the first time, methods like genomic SEM, building off LD score regression, allow us to look at the genetic structure across both common and rare disorders, because LD score regression is able to estimate the genetic relationships across samples that are potentially independent at the end of the day.\nI want to walk through the different iterations of these factor models that we’ve done now. So, I showed you this earlier in the context of the SNP model, but at the base model we modeled in that initial paper the relationships across these five disorders and pulled out what we called, at the time, this kind of general psychopathology factor. I kind of am referring to it as a so-called general psychopathology factor because we also only had five disorders at the time that were sufficiently powered to put in the model. And as we started to include more disorders, this model started to look a little bit more nuanced than just a single common factor.\nSo, in the second major iteration of this, we worked with the cross-disorder group from the Psychiatric Genomics Consortium to publish this paper in Cell where we looked at the genomic relationships across these eight disorders of anorexia, OCD, Tourette’s, schizophrenia, bipolar, MDD, ADHD, and autism. And what we found is that these kind of segregated into these three factors that we loosely call a “compulsive disorders factor”, a “psychotic disorders factor”, and a “neurodevelopmental disorders” factor. And let’s briefly mention too that, um, you know, things like OCD and Tourette’s loading under the same factor is very consistent with what we might expect based on comorbidity rates and kind of convergent patterns across different research groups. Same thing for schizophrenia and bipolar. One kind of lone wolf here is MDD.\nAnd we were able to improve on that when we, in our most recent iteration of this model, now include 11 major disorders—now with alcohol use disorder here in red, PTSD, and anxiety. And with the inclusion of PTSD and anxiety, we’re now pulling this fourth “internalizing disorders” factor. But I would highlight also that for a number of these disorders, the sample size is updated pretty substantially relative to the previous iteration, and even with that update, we were still pulling out the same three factors when we were going through the exploratory factor analyses. So again, this kind of indicates that there are a lot of intercorrelations across these disorders, but there is some kind of nuance in terms of how these disorders kind of segregate into these subgroups.\nAnd in fact, if you just kind of look at the genetic correlation matrix across these 11 disorders, you can kind of see these factors pop out when you order the matrix according to the factors that we modeled. Where you’ve got this kind of subcluster up here, Factor One. This really tight cluster between schizophrenia and bipolar in Factor Two, and in particular, this really apparent cluster between PTSD and MDD and anxiety on that internalizing fourth factor.\nBecause there were, you know, correlations across these factors and because of the kind of growing interest in this overall P factor, we did model an overarching P factor that explained the genetic relationships between these four different factors and found that this fit the data pretty well. But given these kind of partially competing models of either kind of four correlated factors or an overarching factor, we wanted to go on to do some other analyses to really evaluate the utility of each of these factors for understanding shared biology across their indicators.\nAnd we did that in a number of ways. One is, we took the same logic that we developed for QSNP and applied it to examining the relationships between relevant external correlates for psychiatric disorders. And so, again, in a very similar manner to QSNP, we asked whether or not the associations between an external trait and the individual disorder shown in Panel A is sufficiently captured by a model that just shows a single relationship between the external trait and Factor One. And we compare the fit of these models and would expect there to be a really significant decrease in fit by just modeling this common pathways model when these associations are really discrepant across one another. We fit these sorts of models for 49 different external traits and I just want to walk through a number of these findings.\nSo, I’m showing a couple of different things here, and color-coded at the top are the four different factors from the correlated factors model, and then the P factor in turquoise. And then whether or not the bar is shown with a solid or dashed outline indicates whether or not it was significant for that Qtrait metric. Where bars with a dashed outline we found to be significantly heterogeneous across the indicators. Within socioeconomic outcomes, we see that there’s a relatively homogeneous relationship between a lot of these outcomes and the compulsive disorders factor in the positive direction.\nWhen we look at health and disease, we see a lot of positive correlations with the internalizing disorders factor and these health disease outcomes, which is very consistent with phenotypic work. And within anthropomorphic traits, we do see, I think, a particularly interesting finding for the compulsive disorders factor of a negative correlation between body mass index and waist-to-hip ratio. And I mark that as interesting because anorexia loads on that factor, and you might think that that is really driving the relationship between BMI and waist-to-hip ratio, given low weight status as a diagnostic prerequisite for meeting criteria for anorexia. But in fact, we see that there are actually shared pathways with OCD and Tourette’s between these external correlates, indicating that there’s some sort of shared general risk between these anthropomorphic traits and this compulsive disorders factor. And then finally, we see that there’s a lot of homogeneous relationships with neuroticism but not with this internalizing factor in neuroticism, which might come as some surprise but it’s largely driven by a much higher genetic correlation with the MDD indicator.\nAnd if we look at these correlations overall, we see that in particular there’s the significant Qtrait metric for the neurodevelopmental and P factor, indicating that a lot of the relationships with external traits are not operating through these factors and suggesting somewhat limited clinical utility for these two factors relative to these compulsive, psychotic, and internalizing disorders factor.\nWe then went on to perform multivariate GWAS in sort of two main steps. In the first one, we looked at the SNP effect as it concurrently predicted these four correlated factors and then a SNP predicting this overarching general psychopathology factor.\nJust to orient you to these Miami plots, I’m showing on the top half the factor SNP effects, and on the bottom half the QSNP effects that I’ve talked about earlier in this talk. In black triangles, I’m showing the 132 hits across these four factors that were in LD with hits that were identified for the individual disorders that defined the factors. In red, I’m showing 20 novel hits that were not significant for any of the individual disorders, which highlights the ability of genomic SEM to make novel discoveries even without collecting any new data. And again, I’ll say that with the caveat that I really try to be careful to not frame genomic SEM as a method for boosting power, but for ultimately looking at shared genetic signal and divergent genetic signal across different traits. But at the same time, because you are leveraging shared power, you are going to get some kind of boost in signal for a lot of these disorders. And then in purple, I’m showing the purple diamonds, the significant QSNP effects. So for the compulsive disorders factor, these particular disorders were not super well-powered, so there’s not much signal. We see a lot of shared signal for the psychotic disorders factor. For the neurodevelopmental disorders factor, we see a sort of, somewhat even balance of factor and QSNP signal. And internalizing disorders factor, a much stronger signal for the factor.\nIf we look at the QQ plots for these different disorders, what I’m showing here in blue is the signal for the factor, and in pink, the signal for QSNP. And what we would expect if the factor is generally capturing the relationship between the individual SNPs, and the individual disorders that define the factor, is that this blue line would sit kind of nicely above this pink line. And that is what we observe for the compulsive, psychotic, and internalizing disorders factor. But, on the other end, we see that the QSNP metric is actually much stronger than the factor signal for the neurodevelopmental disorders factor, which indicates that a lot of the genetic signal is not operating through the factors. So, similar to what we observed for the patterns with external traits for individual SNPs, we find that they are often not operating through that neurodevelopmental factor. And if we look at the individual SNP effects for some of the SNPs that were particularly QSNP significant, we see that a lot of that is due to divergent effects for autism, which also loads on that factor and may come as no surprise. So, that’s not to say that a neurodevelopmental factor in a kind of hypothetical sense might not prove to be useful, but at least in the way that we’ve defined it based on the data that we had, it doesn’t seem to be sufficiently explaining the shared signal across its disorders.\nAnd then, if we look at the signal for the P factor, we see a really striking imbalance between the factor signal and the QSNP signal, where we see only one hit that was in LD with an individual disorder hit, no new loci, and 69 QSNP hits, and just a really elevated pink line relative to that blue line, indicating that almost none of the SNP effects are operating through that P factor, which really suggests that there is limited utility to a P factor for actually understanding shared biology across disorders.\nFinally, I just want to kind of go over the stratified genomic SEM findings for this particular 11 disorders model, and I want to focus on some findings for enrichment of brain cell types. We know that psychiatric disorders are generally enriched for genes expressed in the brain, which is sort of, I feel like, more of a sanity check to see that it’s not expressed in the spleen or the stomach but is really in the central nervous system like we’d expect. But with more recent single-cell sequencing efforts coming out, people have been able to pair that up with GWAS summary data to actually look at enrichment of specific brain cell subtypes, which gives us maybe a little bit better target in terms of where that signal is coming from within the brain. We also know that protein-truncating variant intolerant genes, which is referring to specific genes that generally do not show mutations, are enriched across disorders. So, these genes might be particularly relevant for just kind of functioning in general. And so, what we did is we created annotations for PI genes, brain cell subtypes, and their intersection, to examine whether or not prior enrichment patterns actually reflect some sort of pleiotropic signal across disorders.\nAnd so, here I’m showing those stratified genomic SEM findings for examining enrichment of the factors. In orange, I’m showing the glial cell category; in dark blue/purple, the excitatory brain cell subtypes; and then the GABAergic subtypes, the PI genes, and then their intersection over here in the rightmost part of the plot for the different factors. What we see is that there’s a really unique signal for the psychotic disorders factor within excitatory and GABAergic genes, within the PI genes, and a signal that is particularly enriched for their intersection. And so, I think this starts to give us some real traction in terms of being able to understand what it is that the psychotic disorders factor is capturing in terms of where that genetic signal is coming from. It also starts to point towards some reasons that these disorders might actually diverge and look both phenotypically different and not share genetic signal at the level of this P factor over here. I also would just highlight that if you were to go into Google Scholar right now and type in any of these neuronal sub- cell types for bipolar disorder or schizophrenia, you could really make a case based on the prior literature that really any of these are relevant to the factor. And of course, this is just one study among many, but we’re using a pretty high-powered set of GWAS summary statistics paired with some pretty cutting-edge RNA-seq data that shows that really these glial cells are not particularly relevant for the psychotic disorders factor as we’ve defined it here.\nIn conclusion, we’ve really expanded the genetic factor model of psychiatric risk to identify four major factors of genetic risk sharing. We find relatively high utility of the psychotic and internalizing disorders factor. When we go on to really kind of stress test these factors using different follow-up analyses, including those QSNP and Qtrait findings I talked about earlier, where we find that the associations between relevant external traits and individual SNPs are generally captured by the factor. For the compulsive disorders factor, I would really frame this as a factor where the jury is still out in terms of its utility, namely because the GWAS summary statistics that define that factor are still relatively low powered. We do see that it does a pretty good job of explaining relationships with external traits, but at the level of individual SNPs, there’s just not really the signal there to see whether or not it’s operating through the factor or through the individual disorders. And for the neurodevelopmental factor, we really see some pretty low utility, where a lot of the relationships and the disorders that define this factor are relatively unique in terms of their patterns of genetic correlations with external traits, and a lot of the SNP associations did not operate through the factor in a way that seemed largely attributable to a signal that was unique to autism relative to PTSD and ADHD that also loaded on this factor.\nWe also know that we can model a P factor using a genetic correlation matrix, in line with a lot of phenotypic and family-based work that’s been done, but when we stress test this factor, we find that this, in particular, has incredibly low utility, to the extent that it obscures relationships with external correlates and SNP associations. This might be due to the fact that there are sort of kind of unique bivariate associations between different factors that are not captured across the factors as a whole. So, for example, there might be some sort of shared signal between the psychotic disorders factor and the compulsive disorders factor, that results in that genetic correlation between those two factors, that is really dissociable from the signal between a psychotic disorders factor and an internalizing disorders factor. So it’s kind of this complex Venn diagram across these factors that does not include this kind of P factor at the center, at least in the way that we’ve examined it here, which we think has pretty broad implications for a pretty rapidly expanding P factor literature with a lot of articles coming out all the time about this P factor.\nUsing stratified genomic SEM, which is that new genomic SEM edition that is live on our GitHub, we find that prior enrichment findings generally reflect broad pathways of risk. And we find, in particular, the intersection of PI, excitatory, and GABAergic genes are enriched for this psychotic disorders factor, which gives some real insight into the biological pathways that might underlie the really high genetic correlation between these two very debilitating disorders of bipolar disorder and schizophrenia. And again, as RNA-seq methods get even better and the corresponding univariate GWAS become even better powered, we’re going to be able to make these categories even more refined and even put them within developmentally specific windows, such as excitatory neurons expressed during a specific period of development, which I think is, you know, just at face value, could be a really exciting set of findings.\nAnd just to end on a kind of sales pitch note, a lot of you have heard me talk about genomic SEM, but I hope that it’s clear that this is a pretty flexible method in terms of its ability to ask a number of different interesting questions. Whether you’re looking at something like GWAS-by-subtraction, just doing some general factor modeling, or trying to examine systems with relationships across traits that you might not be able to otherwise examine because of how rare or mutually exclusive they are. It is an open-source R package that’s publicly available with a GitHub and a Google Group that you can ask questions on. And stratify genomic SEM is now live on the GitHub and, you know, one of the reasons that it’s exciting to talk to a group like this is not just to talk about the work that I’m doing, which in and of itself is, you know, fun to do, but it’s also great to hear what kind of questions people have and also potentially develop collaborations, projects, or grant ideas. Just as an aside, I’m on my internship, my clinical internship year at Mass General Hospital in Boston, but I will be starting as an assistant professor at CU Boulder in the fall. So again, if people have projects or grants that they want to work on, this is obviously a group that I would be particularly interested in collaborating with.\nAnd so I’ll just end by naming and thanking a number of different people, in particular, Elliot Tucker-Drob and Michel Nivard, who have been really central to working with me to develop genomic SEM and its extensions. And a number of different people named here, and of course, also thanking groups like the Psychiatric Genomics Consortium, iPSYCH, and UK Biobank that really contributed to the datasets that I presented here in terms of the application of genomic SEM to psychiatric traits. That’s all I have, but again, I just want to thank everyone for inviting me and for your time. And curious to hear what questions people may have.\nHost: Andrew, thank you for a fascinating talk. How I’d like to structure the Q&A is that Sam Trejo, who suggested you come talk to us, and we really appreciate that suggestion, will give us the first question. And for those who have follow-up questions in that conversation, I suggest you jump right in. But for those who have other questions, if you could just queue up in the chat, I’ll moderate that to tell who’s next. So, Sam’s going to give us the first question, and then we’ll go from there.\nAndrew: Sounds great.\nAudience member [Sam Trejo]: Hey, Andrew, really cool talk.\nAndrew: Hi Sam. Thank you.\nAudience member [Sam Trejo]: My question is kind of about this idea that I think is true for this earlier stuff you talked about with the GWAS-by-subtraction and the sort of non-cognitive/cognitive parts of educational attainment. But it seems like it extends later on into the stuff that you’re doing with, like, the P factor and all these different kind of psychiatric factors. So the first question is: with GWAS-by-subtraction, if I were just to take the linear difference between the EA sumstats, like, you know, beta weights and the IQ beta weights, you know, I would do, like, a less sophisticated version of, I think, what you guys do in that paper, right? Like, I’m just sort of taking the bits in one GWAS that aren’t in the other GWAS. And I was kind of curious, like, how similar of an answer would I get, do you think, or do you know, to, you know, what the genomic SEM model fits? And then what are the advantages using genomic SEM? And then my kind of second question that’s related to all this is like, well, actually, I’ll just let you answer that one first.\nAndrew: Yeah, now that’s a great question. Michel Nivard actually has an alternative method that people are probably aware of: GWIS, you know, genome-wide inferred summary statistics, that does something very similar to what you’re describing. It just kind of takes the summary statistics, and at that level, just kind of pulls out the shared and unique genetic signal. I think the advantage of genomic SEM is that, there are two things. One, that you can kind of actually depict the relationship between these two different sets of traits within a classic Cholesky model in a way that’s sort of intuitive in terms of how the genetic relationships are shared across these different traits that you’re including. And the other is that my sense is that if you do just kind of what you’re describing, you’re potentially gonna get biased results because of that sample overlap piece that genomic SEM is able to account for using the V matrix. So, if you know that you have two entirely independent samples and you have no concern about that, then I don’t know, I think it’s kind of an open question whether or not the answer would be pretty similar. But my sense is that people are generally pretty concerned about some level of unknown sample overlap, and genomic SEM is going to give you some ease of mind in that case that you’re actually appropriately accounting for that.\nAudience member [Sam Trejo]: Yeah, that’s helpful. That’s helpful. And so then kind of the next question is like, in both cases, I think with the cognitive/non-cognitive and then all these psychiatric factors, like, we’re now able to basically, you know, isolate and generate polygenic scores for traits that just don’t exist. And, I guess, I mean by that, I mean they don’t exist; it’s like they don’t—they’ve never been measured, and it’s not clear that we would ever be able to measure them. And I was just curious if you—you know, what you thought about that and whether those sort of polygenic scores, um, should we, like, you know, use them differently or think about them differently.\nAndrew: Yeah, that’s another great question. I mean, I think about that too in terms of the factors that we pull out too. I mean, this reflects some sort of shared genetic signal across these disorders that isn’t actually directly observed. And so, I think it’s always really important to kind of do some follow-up, just for yourself, and for the people who are reading this kind of hypothetical article that you’re putting together, to actually characterize what that kind of new polygenic score looks like. So, for example, by taking non-cognitive summary statistics and looking at the genetic correlation between a bunch of external correlates, you get a sense of what that signal is actually picking up on in a sort of multi-dimensional space. So, I totally agree. I don’t think we should just start, like, kind of removing things from one another and just start kind of kitchen sinking things without any kind of hypothetical guiding force or sense of what these new summary statistics are picking up on at the end of the day. But I do think there’s a number of things within genomic SEM that you can do to kind of clarify that.\nAudience member [Sam Trejo]: Thanks, super helpful. Yeah, thank you, everyone.\nAudience member [Qiongshi Lu]: Thank you for the talk. It’s really comprehensive, pretty much covered everything about genomic SEM. So, I have questions about some technical details, but for the follow-ups, probably you can continue in our individual session after this. But for now, I’m very interested in this annotation-stratified genomic SEM. So, I’m wondering, because if you’re interested in the heritability enrichment of factors in certain regions, you actually don’t have to fit the annotation-stratified version of genomic SEM, right? You can just run the standard genomic SEM and get the GWAS summary stats for those factors, and then test if their heritability is enriched in certain annotation categories. I wonder empirically, do you actually see differences when you run the annotation-stratified version of GSEM and then characterize annotation enrichment?”\nAndrew: You do see some differences in general, you just see kind of deflation of signal. I think that’s a great question. You know, I think that’s one thing that comes up a lot is just this general idea of, like, couldn’t you do this in a much simpler, kind of, more straightforward way by just taking the summary statistics for the factors and running partitioned heritability on that? The reason that you see deflated signal is that the factor summary statistics are estimated with error, and what I mean by that is those summary statistics are going to include some of that Q signal or signal that is not actually operating through the factor itself. So, you could think about kind of pruning based on significant Q SNPs and then feeding those summary statistics into partitioned heritability, but I think it’s kind of unclear what threshold to use when you do that, whereas if you’re doing it strictly within a genomic SEM framework by looking at enrichment using partition covariance matrices, you’re not including that kind of error that gets introduced into the summary statistics in that way. Another thing I would say is that you can also look at enrichment of the uniquenesses and enrichment of things like factor correlations. So, that is sort of a tangent to the question that you’re asking, but just to say that stratified genomic SEM, I think, is kind of more broadly useful in the sense that you can look at things that you couldn’t really feed into a partitioned heritability analysis, like enrichment of factor correlations, or kind of concurrently looking at enrichment of the uniquenesses within the same model.\nAudience member [Qiongshi Lu]: Yeah, this is very helpful. So, a quick follow-up would be that, do you have a sense about how big the annotation need to be for this to work? Because my intuition would be that if you have a very small functional annotation, the annotation-stratified genetic covariance estimate will become very noisy, so then when you fit a separate genomic SEM in that genomic region alone, maybe it will be challenging to converge, right? So, empirically based on the sample size in your analyzed summary stats, how big can annotation be?\nAndrew: One thing, as far as the model converging, is that, the way that the estimation procedure is coded, is that we kind of fix the estimates from the annotation that includes all SNPs and then re-estimate the model with those fixed estimates and the partition-specific covariance matrices. So, that helps a little bit with just kind of wonky model estimation. In terms of just, like, genetic covariance estimates that are really imprecise. You know, I know that this is unclear to you, but just to sort of put out there that, you know, we have the partitioned sampling covariance matrices, too, so you don’t run a danger of false positives in the sense that if a covariance estimate in the partition space is really imprecise, then it’s also going to have a huge standard error that ports over to the enrichment estimate. So, there are certain annotations that are really small where you get, like, a huge enrichment estimate but with a confidence interval around it that is humongous at the same time. So, big point estimate, but not significant in that sense. In terms of annotation size, I don’t have, like, a great sense of how small it needs to be. I mean, these PI-by-brain cell subtype annotations are not humongous. Yeah, I don’t have a concrete answer for that. The other thing I’ll say is that when we first started this project, my interest was in actually looking at whether or not the factor structure changes across annotations, and that we are not powered to do for the reason that you’re kind of highlighting, that there’s a lot of just kind of random noise in the covariance estimates, so that you get these kind of fluctuating factor structures that don’t actually reflect something that seems to actually be changing in the population. So, that’s why we’re not doing kind of partition-specific factor structures, but more kind of fixing the model in the genome-wide sense and then looking at enrichment of particular model parameters.\nHost: We had an overlapping question that Philip and James asked about model fit. I think the general question is how you think about model fit, but maybe James could fill in a more precise targeted question.\nAudience member [James Li]: Sure, yeah. Thanks for the talk, Andrew. That was extremely helpful and informative and exciting. So my question, I think it’s similar to Philip’s, although he can certainly jump in here, is that when we’ve tried to fit these more complicated models in GSEM, where we have more than just the five, right, like you did with the 11, the problem that we ran into was that the more complicated your model gets, the harder it is to actually produce a strong fitting model. So I’m just wondering, since you didn’t really talk about it in your slide, is when you were able to successfully extract these four different latent factors, how well would you say that the overall model actually fit the data, to the extent that you feel confident that this neurodevelopmental factor is, in fact, a truly unique factor? Can you speak a little bit about that?\nAndrew: Yeah, so, that four-factor model does fit the data well by conventional standards. So CFI, I think, is like .96. SRMR is, I think, .06 for that model. So using these kind of arbitrary cutoffs, we find that the model fits pretty well. We see a really clear increase in model fit as we kind of move from a common factor model to this more nuanced four-factor model. Of course, you know, there’s the trade-off of a more complicated factor model is always going to fit the data better, but it hit this point where hypothetically the factors that we were pulling out made a lot of sense, it seemed to fit the data pretty well, and as we kind of included, you know, updated summary statistics, we were consistently pulling out the same factors using a kind of restarted exploratory procedure. So, you know, at the same time, I think that you can have genetic correlations that are such a broad kind of 10,000-foot metric that, in the way that we showed with these kind of Q follow-up analyses, that’s important to really stress test these factors. Because, you know, we can model a P factor, we can model a neurodevelopmental factor, but that might be that it’s kind of aggregating across these really dissociable pathways at different levels of analysis. So, I think that model fit is just one piece of this puzzle.\nAudience member [James Li]: I agree entirely, and I’ll follow up with you in our individual meeting. But one additional thing I just wanted to add was the alignment of the models that you extracted and with theoretical alignment with our theory of what these factors should be. So, for instance, in your neurodevelopmental factor disorders, you had PTSD, autism, ADHD, and I think Tourette’s, right? So, you know, to what extent does that align with, for instance, high-top models of neurodevelopmental disorders or even DSM perspectives of what neurodevelopmental disorders are classified as, not that we necessarily agree with the DSM, but I’m just saying that there’s this quantitative piece in which the disorders fit this type of structure, but then there’s also how well does that fit with our theoretical understanding of how these disorders should look or how they present.\nAndrew: Yeah, I mean, it’s sort of a mix of both, and I think that, you know, in some ways, kind of nice. Like, you would hope that after spending millions of dollars to do genomics, we don’t just recapitulate what we already knew in some way but actually get some kind of novel insight. And the real interesting thing within this neurodevelopmental factor is PTSD, and the reason I highlight that is we actually see that PTSD and ADHD are correlated greater than 0.7 across multiple separate cohorts, which is not something that you would get from reading the DSM or just from practicing clinically, I don’t think. And so there’s sort of a separate project going on that’s really trying to tease that apart. ADHD and autism, I would argue, you know, there’s some good reason to think they would load together, but in a purely kind of exploratory sense, that particular factor, I think, is the odd one out, particularly because PTSD is loaded so strongly due to its relationship with ADHD, but that relationship is there, and we feel pretty confident that it’s there because of how consistently we see that across independent cohorts.\nHost: Phillip, Is one of your questions short enough that it would be a minute or two long?\nAudience Member [Phillip Koelinger]: Oh, my god. I’m not really sure, but, but anyways, let me just briefly say, Andrew, I’m so glad that I heard that you accepted this job offer from Boulder. So, I know that Matt Keller is like super excited about, you know, that you’re coming there, and I only heard it from him very recently that this is finally working out. So, thumbs up, congratulations. So, actually, I had a lot of, like, very far-drifting comments and questions that may actually be really better if I talk to you separately about this, but maybe just one very concrete thing. So, I was just wondering if genomic SEM actually makes use of the standard errors that LD score regression rg estimates spit out, or are you only using the point estimates themselves?\nAndrew: We’re also using the standard errors in that V matrix.\nAudience Member [Phillip Koelinger]: Okay, well, so then there is this slightly weird thing that if you construct the genetic correlation matrix with a bivariate method that doesn’t take the entire multivariate structure into account, like LDSC, right? So that’s really just bivariate - bivariate. Then the resulting matrix may actually not be positive semi-definite, and the standard errors are actually not necessarily theoretically correct. So, I mean, there is a way how you can actually get the correct standard errors, but it would be a completely different method. It will basically be a multivariate version of GREML, which would have some advantages and some disadvantages, but I’m just thinking, is it theoretically possible that you fit that, that you built your genomic SEM model based on standard errors and rg estimates that are not coming from LDSC, but let’s say from multivariate GREML?\nAndrew: It is, and I think at one point, Ronald was maybe working on that even.\nAudience Member [Phillip Koelinger]: Yes, this is where this is coming from, exactly.\nAndrew: Yeah, yeah. And, you know, there’s, it’s sort of like this kind of constant thing that is like, you know, these different kind of bivariate methods come out, and, you know, how we could maybe incorporate that to kind of build out these matrices. I will say that we, you know, the sampling covariance matrix is estimated within our multivariable version of LDSC, and the block jackknife kind of precedes in a way that is similar but different in that we’re also populating those off-diagonal elements, but that actually doesn’t answer your question. So, your point still holds. And I think, yeah, trying to incorporate some of the stuff that Ronald’s been working on as a real interest.\nAudience Member [Phillip Koelinger]: Cool, all right, thanks.\nAndrew: Of course.\nHost: I think we could keep you for a long time with these questions and answers, Andrew, but we thank you again for coming. I know you’re going to stick around for the beginning of your individual meetings.\nAndrew: Yes.\nHost: I want to just, on behalf of the group, thanks for such a clear and interesting discussion and presentation.\nAndrew: Yeah, thanks, everyone. Really great questions, and it was a real privilege to present to you all."
  },
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Table of Contents",
    "section": "",
    "text": "Welcome and Introduction\n\n\nChapter 1: Introduction\n\n1.1: What are psychiatric disorders?\n1.2: Epidemiology\n1.3 History\n1.4 Psychiatric Genomics: State-of-the-Science\n\n\n\nChapter 2: The Genome\n\n2.1: Organization of the genome\n2.2: Types of genetic variation\n2.3: Evolutionary signatures\n2.4: Linkage disequilibrium\n\n\n\nChapter 3: Technologies\n\n3.1: SNP array genotyping\n3.2: Next Generation Sequencing\n\n\n\nChapter 4: Study designs\n\n4.1: Epidemiological study design\n4.2: Confounding, Chance, and Bias\n4.3: Genetic study designs\n\n\n\nChapter 5: GWAS analysis\n\n5.1: Genotyping Quality Control\n5.2: Imputation\n5.3: Association testing\n5.4: Meta-analysis\n\n\n\nChapter 6: Polygenic Scores\n\n6.1: Polygenic Risk Scores\n\n\n\nChapter 7: Ancestry-specific analysis\n\n7.1: Cross-ancestry analysis\n7.2: Ancestry-specific PRS\n7.3: Local ancestry and Admixed populations\n\n\n\nChapter 8: Post-GWAS Bioinformatics\n\n8.1: SNP Heritability\n8.2: Genetic correlations and partitioned LDSC\n8.3: Gene-association analysis\n8.4: Gene-set analysis\n8.5: Fine-mapping\n8.6: Quantitative Trait Loci (QTLs)\n8.7: TWAS\n8.8: pheWAS\n\n\n\nChapter 9: Advanced Topics\n\n9.1: Copy Number Variation\n9.2: Mendelian Randomization\n9.3: Genomic Structural Equation Modeling\n9.4: Interactions with Environmental Factors\n9.5: Family-based analysis\n9.6: Therapeutic Implications\n\n\n\nChapter 10: Other considerations\n\n10.1: A Career in Psychiatric Genetics\n10.2: Caution in Genetic Prediction\n10.3 Small Effect Sizes\n10.6 GDPR for Dummies\n\n\n\nSoftware tutorials\n\nCNVs\nConditional Analysis\nCross-Disorder Analysis\nDatasets\nEpigenome-Wide Association Studies\nGene Set Identification\nGenome-Wide Association Studies\nGenomic SEM\nMendelian Randomization\nMulti-Trait Analysis of GWAS\nPolygenic Risk Scores\nSNP Heritability and Genetic Correlation\n\n\n\nGlossary\n\n\nSoftware Resources\n\n\nAdditional Reading"
  },
  {
    "objectID": "software_conditional_transcript.html",
    "href": "software_conditional_transcript.html",
    "title": "Software Tutorials: Conditional Analysis (Video Transcript)",
    "section": "",
    "text": "mtCOJO\nTitle: How to perform mtCOJO\nPresenter(s): Zhihong Zhu\nZhihong Zhu:\nHi everyone, I’m Zhihong Zhu. Today, I’m going to talk about how to perform mtCOJO. In observational studies, we can directly measure relationships between traits. For example, people with schizophrenia have low levels of vitamin D, which has led to hypotheses that low vitamin D is a risk factor for psychiatric disorders. But this is very difficult to test.\nMendelian randomization provides a way to test for causality using GWAS results from the putative causal trait and from the outcome. However, in testing these relationships, we may wish to account for other confounding factors. If we had individual-level data, we could account for confounding factors through their inclusion as covariates. mtCOJO is a method that allows us to condition on confounding factors when we only have GWAS summary statistics. The conditional GWAS allows us to investigate causal relationships free from the bias of confounding factors.\nAs I mentioned, accounting for confounding factors can be achieved by their inclusion as covariates. Due to the stringent assumptions of Mendelian Randomization methods, it is not straightforward to fit genetic variables jointly. Instead, we can achieve our goal by a two-step approach. The first step is to adjust both focal traits and risk factor for covariates. The second step is to investigate the causality. The estimated causal relationship is then free of confounding with covariates. The mtCOJO estimates causal effects of covariates on focal traits and the risk factors, and then performs conditional Mendelian randomization on those traits. The Mendelian randomization method uses genetic summary statistics, which can be from a single cohort or multiple studies. Let me talk about some details of mtCOJO.\nThe first step of mtCOJO is to estimate the causal effects of covariates on focal traits and the risk factors. This step is conducted by GSMR, a Mendelian randomization method which has similar concepts to randomized controlled trials, the gold standard to test for causality. The GSMR method uses SNPs as instruments to test the causal effect, taking the correlation between SNPs into account. Pleiotropic SNPs for two traits commonly effect causal relationships between two traits. The GSMR method excludes those pleiotropic SNPs which have effect sizes that deviates the relationship at causal SNPs. The GSMR method is highly robust and investigates causality, and the causal effect is unbiased in the presence of non-genetic factors. Because of this, the mtCOJO result is free of bias. The GSMR method uses multiple SNP instruments, therefore, we need to incorporate the correlation between SNPs. So we need a cohort with individual-level data to provide an LD reference.\nThe second step of mtCOJO is to conduct the conditional GWAS. We derived the formula to estimate the conditional effects, as shown in the equation. The effects of SNPs are directly from GWAS studies. The mtCOJO effects also need estimates from LD score regression analysis. Due to the limited time, I will not describe the details here. We compare the mtCOJO result to the traditional method if we have individual-level data. The two results are identical. Furthermore, mtCOJO accounts for distinctive sample sizes and overlapping individuals when there are multiple GWAS studies. Lastly, the mtCOJO method is free from bias. I will talk about it in detail.\nThe mtCOJO method is free of bias. There are two sources of bias: environmental and genetic factors. Suppose a covariate does not have a direct effect on focal traits. We may still be able to observe an association between covariate and focal traits because of non-genetic and genetic factors. So when we perform GWAS of focal traits conditioned on covariates with observed data, we are likely to detect spurious GWAS hits. mtCOJO can address these issues. Firstly, mtCOJO can unbias-ly estimate the effect of covariates on focal traits in the presence of non-genetic factors. Secondly, because of the causal effect, mtCOJO is free of pleiotropic effects. In general, the mtCOJO method is free of bias.\nThe mtCOJO method has been implemented in software that is free to download and easy to use. The mtCOJO software only needs GWAS summary statistics. The command for mtCOJO is very simple; press enter and the analysis will be finished in a few minutes. This is the link to the mtCOJO webpage. When you open the page, you can easily find the commands.\nTo perform the mtCOJO analyses, we will need GWAS summary statistic, an LD reference sample and LD scores.\nToday, I will use the GWAS of vitamin D conditioned on BMI as an example. In this example, firstly, we will need the original GWASs of vitamin D and BMI.\nGWAS summary statistics in mtCOJO format: There are eight columns in the mtCOJO format file. The first row is the header. The mtCOJO will use the SNP ID column to match the GWAS summary stats with LD reference. The SNPs which are not matched will be excluded. The allele frequency is the one of the effect allele A1. Likewise, the effect of SNP beta is that of the effect allele A1. So please double-check the SNP ID, A1, A2, allele frequency, and beta before performing your mtCOJO analysis.\nSecondly, we need our LD reference sample. If we have the cohort where we performed the GWAS, we can simply use the subset of the GWAS cohort as an LD reference. Usually, the GWAS cohort is not available, so we need a cohort of the same ancestry as the GWAS cohort. We recommend using a relatively large cohort. When the LD reference sample is large, the LDs in the LD reference are likely to be consistent with the GWAS cohort. In the demonstration, we will use a cohort with 10,000 individuals as LD reference. The genotypes of the LD reference sample are saved in PLINK binary format. Thirdly, we will download the LD scores from LDSC GitHub.\nNow let’s perform the mtCOJO analysis of vitamin D together. We will download the software and the datasets used in the analysis: the GCTA software, the GWAS summary stats of the focal trait, vitamin D, and the covariate BMI, the LD reference sample, and LD scores.\nWhen all the datasets are ready, we can perform the mtCOJO analysis of vitamin D. Now the datasets are ready, let’s have a look. This is the original GWAS of vitamin D in mtCOJO format. This is the GWAS of BMI in mtCOJO format. We create a list of GWAS summary stats for the mtCOJO analysis. The first row is the focal trait, vitamin D. The next rows are the covariates. We only have a single covariate, BMI. If there are multiple covariates, you can simply add them into the list. This is the LD reference sample in PLINK binary format. These are the LD scores for 22 chromosomes. The LD scores are downloaded from LDSC GitHub.\nWhen all the datasets are ready, we can perform the mtCOJO analysis. The command is very simple. Copy the command and press enter. Then, mtCOJO analysis starts.\nNow the mtCOJO analysis is finished. It is very quick, only around three minutes. The mtCOJO analysis uses the causal effect of BMI on vitamin D. The estimated causal effect is free of bias; therefore, it is less likely that the mtCOJO caused spurious conditional GWAS hits.\nLet’s have a look at the mtCOJO results. The “.mtcojo.cma” file contains the original GWAS estimates of vitamin D and the GWAS estimates as adjusted for BMI. The first eight columns are the original GWAS estimates, and the last three columns are the GWAS estimates of vitamin D adjusted for BMI.\nWe plot the conditional GWAS results of vitamin D. There are more than 100 significant loci in the conditional GWAS. Those loci are vitamin D-specific and free of confounding with BMI.\nLikewise, we perform the mtCOJO analysis for schizophrenia conditional on BMI, and then conduct the GSMR analysis to test for causality. Conditioned on BMI, the association between vitamin D and schizophrenia remains significant.\nTake-home message: mtCOJO has easy-to-use software; just a simple command and press enter. It only uses GWAS summary statistics, and the mtCOJO method is free of bias. These are the references of mtCOJO analysis in the demonstration. I would like to thank my supervisors, Naomi [Wray], Peter [Visscher], and Jian [Yang], who supervised me and helped me with tasks. I would also like to thank Joana [Revez], Naomi [Wray], and John [McGrath], who offered me a quick example of mtCOJO analysis. Finally, I will give special acknowledgments to all PGC group members and collaborators who worked on these projects. Thanks for watching."
  },
  {
    "objectID": "chapter2.3_transcript.html",
    "href": "chapter2.3_transcript.html",
    "title": "Chapter 2.3: Evolutionary signatures (Video Transcript)",
    "section": "",
    "text": "Origins of Genetic Variation\nTitle: Origins of Genetic Variation\nPresenter(s): Jessica Pamment, Professor, DePaul University\n[Music]\nNext time you’re in the classroom, look around you, and you’ll see that although all your peers are humans, the same species as you, no two individuals in the class will look exactly the same, unless you have identical twins in the room. This variation in traits is true not only for human population but for any species. Some of the differences observed within a population are caused by the environment and experiences of each individual. For example, hormonal changes brought on by cooler temperatures result in the fur of an arctic fox turning from brown to white. Although the environment definitely plays a role in introducing variation in a population, most of the variations seen in populations are caused by differences in genes. For example, one gene is responsible for determining whether the rats will have brown or black fur. With the exception of clones, such as identical twins, each individual within a population carries a unique set of genes, half of which were received from one parent and half from the other. The total set of genes of all individuals in a given population is called the gene pool.\nA gene is a discrete unit of hereditary information consisting of a specific nucleotide sequence in DNA. So, nucleotides are the building blocks of DNA and, therefore, of genes. Differences between individuals can be measured all the way down to the level of individual nucleotides. However, measuring differences within a gene pool at this level is not particularly useful because much of the variation lies within non-coding regions of the DNA, meaning that these variations don’t result in an observable difference. It’s often better to measure variation at the gene level because it is at this level that both quantitative and discrete traits are coded.\nSo, how does genetic variation arise in a population? Well, one of the ways is as a result of mutations, which results in a change in the original DNA sequence. Mutations can occur as mistakes during DNA replication. However, if the mutation does not happen in a cell that is passed down to offspring, such as an egg or sperm cell, the change cannot lead to a new allele, which is an alternative version of a gene. Variation can also arise at the chromosome level during the process of meiosis. This is a modified type of cell division found only in sexually reproducing organisms, which results in the production of gametes.\nThe two ways in which variation is introduced in meiosis: crossing over and independent assortment. Crossing over happens early on in meiosis in prophase one and results in the exchange of DNA between homologous chromosomes, so between the paternal and maternal chromosome of each chromosome pair. This results in recombinant chromosomes. The second way in which variation is introduced is as a result of the random arrangement of chromosome pairs on the cell plate during metaphase one. In humans, the random assortment of chromosomes gives rise to over 8.4 million possible combinations of chromosomes, and this is without taking crossing-over into account, which introduces even more variation.\nAnother mechanism that contributes to genetic variation in sexually reproducing organisms is random fertilization. As I just mentioned in humans, each male and female gamete represents one of about 8.4 million possible chromosome combinations due to independent assortment. The fusion of a male gamete with a female gamete during fertilization is completely random and will produce a cell with any of about 70 trillion chromosome combinations. If we factor in variation brought in by crossing-over, the number of combinations is even higher.\nHopefully, you can see how unique you really are. Now that we’ve learned how genetic variation is introduced into a population of sexually reproducing organisms, it is important to remember the evolutionary significance of this. Natural selection is a driving force behind evolution, and natural selection results in the accumulation of genetic variations favored by the environment. Another way of thinking about this is that genetic variation is the raw material needed for evolution to occur.\n[Music]\n\n\n\nMPG Primer: Natural selection & human genetic variation\nTitle: MPG Primer: Natural selection & human genetic variation\nDescription:\nPresenter(s): Stephen Schaffner, Computational Biologist, Broad Institute\nStephen Schaffner:\nIntro\nGood morning, hello everyone. Thank you for coming. I have the privilege of introducing myself. I’m Steve Schaffner, a staff scientist here, a computational biologist, and I’ve been here for a long time. I used to be part of MPG back in the Thousand Genomes and HapMap era and even before that. These days, I work more in pathogens, malaria, and viruses, but I’m still interested in human genetics and natural selection in humans. That’s why I’m talking to you today about natural selection in humans and in particular its effect on human genetic variation and what we can learn from this and how we can detect it.\nNatural selection\nSo, I think you probably all know what natural selection is. I’ll just state it for the record that it’s the principle that alleles that make an organism more successful in terms of survival and reproduction are likely to be transmitted more to the next generation and therefore likely to increase in frequency while they’re being selected. We can, for convenience, divide the kinds of selection into several different types. First is balancing selection, which is selection that maintains multiple alleles in the population at some intermediate frequency. Then there’s purifying selection, which is the elimination of new mutations that are deleterious. Finally, there’s positive selection, which is selection for some beneficial trait. Actually, the last two are really flip sides of the same thing – if you’re selecting for one trait, you’re selecting against some other trait. You have to be choosing one, but it’s convenient to distinguish them based on what’s the starting out as rare; you can think of purifying selection as eliminating new rare things.\nSickle cell\nI’ll start by talking about balancing selection. This is probably the rarest kind of selection. It’s kind of cool, if you can find it, but it’s a little difficult to detect and probably doesn’t happen very often. There is one very well-known case in humans, the sickle cell trait. The sickle cell allele was one of the first cases, probably I think the first case, of identified natural selection in humans. It was Haldane in the ’50s, who noticed that certain diseases involving hemoglobin were much more common in places where there was a lot of malaria, and he hypothesized that natural selection was playing some sort of role and he was correct. These two maps: the top map shows where malaria occurs worldwide and the bottom map shows where there’s a high prevalence of the sickle cell variant, the sickle cell trait. The reason is quite clear and it's easy to find out.\nHeterozygous individuals survive best\nTo check, you can actually measure the difference in fitness. If you have one copy of the allele, you’re a heterozygote, then you have considerable protection against malaria. So, if you have no copies, you’re exposed to malaria and more likely to die younger because malaria is a big killer. If you have one copy, you have a real benefit. If you have two copies, then you get sickle cell disease and you’re also likely to die young, because without modern medical care, it’s a very serious disease. So, you could look at the survival and it’s very clear that heterozygotes have an advantage. That means that there’s selection pressure to maintain this allele at some intermediate level, so that there’s a maximum number of heterozygotes in the population. It’s not a very pleasant solution, but it is an evolutionarily stable solution to a severe selective pressure.\nBalancing selection for diversity\nYou can get balancing selection in other ways by selection for diversity. There are a lot of cases in which it’s good to be different from other members of your species, like escaping from predators. If the predator is used to finding purple people and eating them, if you’re green, it can be kind of good to be green. Then, it’s quite common in resistance to disease. If a new virus enters the village and it can infect most of the people in the village, and you’re different and you know you have a different genotype than most of the people, then you have an advantage. So, the whole thing that’s spreading through the rest of the population, you’re immune from, and you can see the effects of this, the selection for diversity in terms of disease resistance in the HLA region, which is critical for immune response to pathogens.\nSelection for diversity: HLA\nAnd if you just look on chromosome 6, the density of SNPs in that region is much higher there than elsewhere in the genome because there’s a lot of selection. Obviously, we’re constantly exposed to different kinds of infectious diseases, so there’s a lot of selection for having diversity there. And there’s the HLA region – if you couldn’t guess where it was.\nAnd this is actually an example of frequency-dependent selection, that is, whether the frequency of an allele is advantageous affects how frequent it is. When it’s rare, it’s good to be different, so that’s advantageous. So, if it increases in frequency, then it can become less advantageous. And also, it obviously can vary quite a lot with your local environment, what pathogens happen to be nearby or what predators, and in humans, it’s mostly pathogens we worry about, not so much predators these days. So, it can fluctuate quite a bit on small geographic scales and on short temporal timescales.\nAs a side note, pathogens also evolve and they do some similar things. This is one of the chromosomes of Plasmodium falciparum, which causes the most severe kind of malaria, and where that sharp spike in diversity is showing diversity across the chromosome, that short, sharp spike in diversity is a gene that codes for a protein that’s exposed to the immune system. A lot of malaria proteins are not exposed because they hide in red blood cells, but this protein is exposed to the human immune system. And again, there’s a lot of pressure to be diverse, so that if you are the first parasite entering a village, you want to be different from all the other parasites that the population has been exposed to, so that you can happily infect people without having that unpleasant immune system triggered immediately. And they evolve faster than we do, by the way. So, they’re constantly evolving as well. So that’s balancing selection.\nPurifying selection\nPurifying selection is the most common kind of selection. It’s sort of a little dull, because all it is the removal of new deleterious mutations. Most organisms are pretty well adapted to their environment, and a functional change, something that changes their phenotype in an important way, is probably bad for them. It’s going to be eliminated by natural selection. It’s not going to be passed on for very long. And it’s very clear to see this in human genetic data. This is a plot of the top figure here from Thousand Genomes data. It’s the diversity across the whole genome, looking at different parts of genes. And I’ve marked where some exons are, the first exon, the middle exon, and the last exon within genes throughout the genome. And you can see the diversity is much lower within these coding exons, because changes to the protein are probably bad and they tend to be eliminated. And you can see the effect is strong and just read off from there how many mutations have occurred in these exons and that have been eliminated by selection over time. And although they’re eliminated, they may not be eliminated immediately. Obviously, if something makes you non-viable, then you’re not going to see it in the population. But lots of mutations are mildly deleterious. They might make you more likely to be sick or stupid or less attractive, whatever, which might be bad in certain circumstances.\nPurifying selection eliminates deleterious mutations\nAnd so, you can see these alleles are hanging around the population, but they won’t rise to very high frequency, because they are bad and they’re less successful. So, if you just plot the allele frequency of nonsynonymous mutations and compare it to synonymous mutations, that’s when I show here on the left as a function of frequency. In the 1% being that first bin, you see there’s an excess of nonsynonymous mutations compared to the synonymous mutations, because more of these are functional; so, that excess represents mildly deleterious alleles that are going to be eliminated eventually by selection, but haven’t been eliminated yet.\nEven within the nonsynonymous, you can break it down further. On the right, I’ve plotted different categorizations of non-synonymous mutations as to whether or not they’re likely to damage the protein – change the effect of the protein. So, in pink and red, those are changes that have a pretty good chance of changing the protein’s function, and those are the ones that you see more of them at a low frequency, so those are the ones that are going to be eliminated.\nIn terms of evolutionary biology, these are not interesting; this is going to sludge, it's being eliminated by natural selection all the time. In terms of medical genetics or human genetics of disease, these are probably a lot of the ones that we’re interested in, because one of the ways of being deleterious is it makes you more likely to get sick. And so, these are some of the things that are causing genetic diseases or increasing your risk for early onset of diseases of various kinds. So, there is great medical interest in some cases but not of tremendous interest to evolutionary biologists.\nThe effect of purifying selection can be seen not only in the allele itself that’s being selected against – it can also be seen in some of the surrounding variation. The effect of purifying selection is that it reduces diversity in that region. You can think of it like this: Say there’s a gene where deleterious mutations keep happening. When a deleterious mutation happens, the chromosome it happens on is going to be removed from the population eventually. So effectively, it’s not part of the population now in terms of the long-term success. So, you have right around genes or other functional elements, you effectively have a smaller population size, and that means you can sustain less diversity because some of the diversity gets taken. Any diversity that’s on that chromosome that gets the bad mutation is going to be removed from the population.\nAnd it’s very easy to see this, too. This again a 1000 Genomes data. Here they plotted the distance from the start or the stop of a gene across the whole genome. The three colors are three different populations, and what’s plotted is diversity. And there’s a dip, a significant dip, in diversity around a gene. It’s on the order of 50 or 100 kb, so it’s a substantial stretch where there’s notable reduction in diversity and this is affecting the distribution of diversity throughout the genome. This is an ongoing effect.\nAudience question: [Unable to hear on video].\nStephen: Why? Why are the absolute levels different? Well, there’s just less diversity. The red and the blue are the two non-African populations, and there’s just less diversity outside of Africa because they passed through a bottleneck leaving Africa and so there isn’t that much there. There may also be a small difference in how much diversity is reduced there. There’s a question of whether purifying selection has been less effective outside of Africa because the effective population size was smaller, but that’s a pretty minor effect.\nFinally, there’s positive selection, which is what we usually think of as natural selection. It's what Darwin was famous for. It’s the basis for adaptation, pretty much all adaptation we think. And so, it’s kind of a sexy thing to look for, and people have looked for it.\nI’m going to focus it initially on selective sweeps, which is selection where the mutation starts out basically as a new mutation and increases in frequency. I’ll add a few complications later. And so, the question is: what's the effect of positive selection on genetic variation? And basically that means: How can we detect it just by looking at genetic variation?\nSo, let’s take the case where selection doesn’t happen, this is neutral evolution. So, suppose there’s a mutation that happens in this blue guy here, and as time goes on, the frequency of that allele may increase, it may decrease, it kind of bops around a little bit. It drifts, and the technical term is \"it drifts\", it’s genetic drift. But on average, it doesn’t actually change in frequency. So very slowly, it might change over time.\nIf there’s positive selection for something. If this new allele, this mutation, provides a benefit, well then over time, it can rapidly increase in frequency. And that rapid increase is what leaves the genetic signature that we can look for. It actually leaves a number of different genetic signatures, and I’ll kind of describe some of them.\nSo, let’s consider what the genetic situation looks like before selection happens and after. So, if we have this cartoon version of some chromosomes in the population, this new red mutation is beneficial, and there’s a lot of diversity there. There are different alleles, you have different combinations of those different haplotypes present in the population, because it’s just been sitting and behaving normally. After selection has happened, this allele has increased rapidly in frequency and is present in a large fraction of the population. And so, this signature we can look for… well, one signature happens. Suppose this selection has happened only in one geographic region, like it happened in the Boston area, like being a Red Sox fan means or maybe even Yankee fan, there’s an allele for that might make you more reproductively successful here. Let’s compare it to New York. If that’s the case, then you will find that allele at very high frequency in that region and very low frequency elsewhere. So, it’ll be unusual. One signature of selection then is that there’s an unusually large difference in frequency between populations at that locus, and this can indeed be seen. I'll mention a common way of measuring frequency differences in populations is a statistic called FST, but there are other statistics, too, you can use.\nAnd a classic example of this is the Duffy null allele. The Duffy protein is a blood antigen, one of the many blood antigens, it sits on the outside of red blood cells and does something or other – not entirely clear what. But one of its roles that it’s not intended for it is that it’s also the way by which Plasmodium vivax, another cause of malaria, enters red blood cells. And so, it’s the only way of entering the red blood cell, so it’s critical for invasion and for infection. And if you lose that protein, then you are pretty much immune to vivax malaria. And if there is a mutation that knocks out that gene, it’s called the Duffy null allele. The plot that's shown there is the density, the frequency of that mutation around the world. And you see it’s in very high frequency in sub-Saharan Africa, and as a consequence, people in sub-Saharan Africa are largely immune to the effects of vivax malaria. So much so that there’s almost no vivax malaria across Africa. So, this was a highly successful case of natural selection in humans, suggesting that there was a very large cost from vivax malaria at some point. And outside of Africa, it’s basically not there at all. And so, this is sort of a classic case. In fact, I believe this is how it was discovered that this protein was the invasion route for this parasite. So, it can be very useful to be looking at natural selection.\nHigh altitude adaptation in Tibet\nA more recent case in terms of studying it came from people who are looking for adaptation for handling high altitude, where there’s very low oxygen. So, what this research group did was they compared allele frequencies between a Tibetan population, a sample from Tibet, and a Han Chinese sample, very closely related populations, very similarly low frequencies. So, the bottom of the two axes are the Tibetan frequency and the Han Chinese frequency, and you can see almost everything has this pretty much the same frequency in both populations. And there are two alleles that are very high frequency in Tibet and not at a very high frequency in the Han Chinese, and they’re on the lower right-hand corner. There are two alleles in the same gene EPAS1 and it turns out this does indeed confer adaptation to handling low oxygen levels present there. So, this was a very easy way of finding this particular gene.\nSo, in principle, this is a powerful way of detecting where selection has happened regionally. In practice, there aren’t very many low-hanging fruit like that. Somebody took the trouble of plotting, of comparing, how many real outliers are there. They took a whole bunch of different population pairs, so each one of these dots is a pair of populations. On the x-axis is the average difference in the allele frequency measured by FST between those two populations, and on the y-axis is the most extreme single allele in that pair of populations. And most of the time, you can predict really well what the extreme is going to be just from the averages. So, all you’re seeing is sort of the tail, the distribution up at the high end. There are a few cases here up here where there are clearly outliers. The one I was just talking about, EPAS1, is one of them. All the other colored dots here are known pigmentation genes, so there's information in there, but by itself, it may not be a very easy way of finding out what’s been selected.\nAlright, so that’s just the first signature. I won’t focus much on the others. Another signature is low diversity. If everyone or a large fraction of the population is the same now around this new selected allele, that means if you’re the same, you’re not different. There’s not a lot of genetic diversity there. And so, if you just plot genetic diversity across the chromosome, you’ll find regions where diversity has dips because there’s been a selective sweep there. So, I said purifying selection produces reduced diversity around functional elements. Positive selection can also produce reduced diversity. So, they have somewhat similar signatures here, which is inconvenient. The signature, the loss of diversity, can be more profound in the case of positive selection because if a sweep goes all the way to fixation, everybody is the same and so there’s virtually no diversity present.\nAnother thing that’s a little bit easier to look for and to serve as a different way of looking at the same thing is that if everyone is the same, you have the same haplotype there. They’re basically if you can predict if you have that red allele, you can predict what other alleles everyone will have in the population up to the point where recombination has broken it down. If it’s if this selection happened recently, then recombination hasn’t had time to break it down yet. And so, you can look for long haplotypes that are at high frequency in the population. And there’s a whole series of statistical tests that have been developed for detecting that there’s an unbroken long haplotype present.\nLong haplotype: LCT\nAnd this turns out to be a very powerful way of detecting selection that’s happened within the last 20,000 years, roughly. Here, a very clear signature for positive selection can be seen at one of the now poster children for selection in humans, which is lactase persistence. The normal state for mammals is that as you get older, you lose the ability to digest lactose, because lactose is present mostly only in milk and normal mammals don’t drink milk as adults. But in human populations where they practiced herding for a long time, many adults can, in fact, digest lactose; they’re lactose tolerant. Most Northern Europeans are… I'm lactose tolerant because my ancestors are from Northern Europe. If you look around the lactase gene, there is an enormously long haplotype that’s hardly broken at all. And in Northern Europe, it’s around 70% frequency. It extends for more than a megabase. It is the result of a very strong natural selection, positive selection for this trait. The mutation that gives the capacity for digesting lactose is, in fact, on that haplotype. The plot just shows that if you plot the length of the haplotype versus the frequency; lactase sits way up here on the right. This is the strongest signal of selection by this test in Western Europe.\nFinally, one other signature that’s a little bit harder to visualize is that in regions that have been undergoing recent positive selection, there will be an excess of high-frequency derived alleles. Normally, the derived allele, the newer allele, is at low frequency. It tends to stay at low frequency. But when this kind of sweep happens towards higher frequency, it can bring any other mutations that are nearby to higher frequency. So, it can bring more of these rare mutations up to high frequency along with it. That’s just a different sort of thing you can look at in the data. All these signatures have been looked at, they’ve been known for a while, and they’ve been looked at in various scans across the genome, looking for places where selection has happened. You can do some interesting things with them.\nComposite of Multiple Signals (CMS)\nIt turns out that there’s independent information in each of these signatures. So, one thing you can do, an approach that was pioneered by Pritchard and Sabeti, is to combine the information from the different signals. Here, in a cartoon version, we’ve got all the evidence from long haplotype, evidence from derived allele frequency, and evidence from differentiation in populations and they’re giving you different information, but the actual selected one, where selection actually happened, you can get enhanced information about that, which is important because some of these signatures tend to cover very large regions of a chromosome and not really tell you much about which allele or gene was actually selected for. So, if you want to get down to details, then it’s better to combine information.\nMS pinpoints candidate variants\nHere’s an actual case. I think this is chromosome 5 in humans, and I don’t remember what dataset this is – it might be a 1000 Genomes. These are different signatures of selection. The top one is a long haplotype, the second one is population differentiation, and then the bottom one is derived alleles. A lot of these are raised or elevated, so somewhere in here, there’s evidence that selection happened. But they’re very noisy, and it’s very hard to know exactly where the selection happens. But if you combine them, then, as if by magic, the bottom distribution shows the score for where you think selection happened. And I don't know if you can actually see it, but there are only a handful that have an elevated score, indicating that right here is where selection probably happened. It turns out the biggest signal there is a nonsynonymous mutation, so there’s a good chance it’s functional. It’s sitting in a gene that’s important for skin pigmentation. This, in fact, is an allele that contributes to European skin color; it’s one of the major alleles for that.\nAudience question: What kind of sample size do you need to detect these kinds of changes?\nStephen: Good question. Depends on how strong it is. I’d say, for really strong signals, 100 is fine. As you’re getting to more subtle things, thousands are better. Above that, the problem isn’t so much sample size as knowing how to distinguish between background stuff and what’s actually selection. It’s not just a statistical problem. Basically, what we’re doing is looking for the weirdest part of the genome, and lots of weird stuff can be happening in the genome. There are other various things that confound these. For instance, the long haplotype test can be confounded if there’s an inversion where that suppresses recombination in that region. I think that’s one of those sorts of things that can confuse you. So, there’s a variety of things that can affect you.\nSo I said, these tests have been known for a while. There was a big spate of genome scans about ten or twelve years ago as genome-wide data became available. But there is still work going on in this area.\nA new signature singleton density\nThis is a figure from a paper that was published two weeks ago in Science, which introduces a new signature for recent selection. I thought it was cool enough that I will try to explain it to you. The basic idea is – we've got two alleles here at a site. Along the bottom, these are all the samples. If you just construct a gene tree of which samples are related to which at this particular locus, this is the gene tree. In blue, here is a derived allele, a mutation that happened at some point in the past and has been selected for. If it’s selected for, it increases in frequency, which means it’s younger. On average, you have more recent common ancestors. If you compare people, they have a recent common ancestor; that’s what it means for it to have increased in frequency recently. So, if you just look at the terminal branches, the terminal lines here leading to all of these samples, they tend to be longer for the one that wasn’t selected for, because they're older. Any mutation that occurs on one of these is going to appear as a singleton in your sample. It’s just one mutation.\nSo, all this test does is count how many singletons there are around each allele from the genome. Just count how many singletons there are nearby and look at the density of singletons. The density will be lower, fewer singletons around recently selected things. One of the nice things about this test is that, unlike some of the other tests I’ll mention in a minute, it works not just on selective sweeps, but it works in some other more complicated situations. According to the authors, they figured out what is this sensitive to; if you have a reasonable sample size of a few thousand, then it’s probably sensitive to selection that occurred in the last 2000 years. In genetic terms, that’s very recent selection. So, this is cool, and it’s worth reading that paper.\nOkay, so I said there are some complications. A selective sweep is nice and pure if you have one, but it’s making some assumptions. It assumes that the beneficial mutation occurred once or at least was so rare that you treat it as just one copy, and then it rises to frequency and has all these effects. There's just that one mutation of pretty large effect.\nSelection can happen in other ways. Selection can occur on standing variation, which is variation, variants that have been in the population for a long time. If it’s been there for half a million years or a million years, recombination has been happening all the time, so that variant is hitching on all lots of different haplotype backgrounds, and they’re all going to rise in frequency. You’re not going to see most of these signals. The same mutation can happen more than once. That’s happened in the sickle cell trait, where this identical mutation has occurred on multiple occasions and been selected for. But you’re in a scenario where you have multiple haplotypes increasing in frequency.\nFinally, there may be lots of different alleles that contribute to the same trait, and each one may only shift a little bit in frequency, but you can still have a substantial effect on that trait. So, these signatures of selection are pretty much hopeless. There are attempts to use modifications of these tests, like looking at allele frequency distributions in the case of selection on the standing variation, but it’s just harder. The signal isn’t as easy to find.\nFocus on traits rather than alleles\nYou can still do some interesting things, though, and find some interesting stuff if you stop thinking just about alleles but rather think about the trait. If you know what genes or what alleles are contributing to variation in a trait, then you can aggregate different alleles that are involved in that trait.\nThis is an example that was carried out by Joel Hirschhorn’s group here at Harvard, looking at height variation in Europeans. We know from genome-wide association studies (GWAS) many of the variants that affect stature. We also know just from observing Europeans that there’s a cline in height across Europe from north to south; northern Europeans tend to be taller than southern Europeans. So, what they did in this figure, they plotted and ranked the SNPs from the GWAS in terms of how large an effect each SNP had on stature; then, on the y-axis, they plotted the difference in frequency between northern Europe and southern Europe. What they find is that the alleles with the biggest effect on stature are also the ones with the biggest frequency difference in Europe. This is consistent across all the major alleles here. They all have higher frequency. This isn't randomly a few alleles happen to be higher. All of them have higher frequency in the increased height direction in northern Europe or, alternatively, they all have the lower stature allele in southern Europe. This provides pretty good evidence – and it’s been supported by further studies by others – that selection was acting on stature in Europeans in some way. It’s not exactly clear how, but in some way.\nThere have been other studies. This is a similar sort of study looking at a variety of traits correlated with geography. In this case, finding a significant selection for a stronger response to damage from ultraviolet radiation if you live near the equator, which maybe isn't too surprising. Different colors are different continents, and on different continents, the same sort of selection pressure has been happening. So, you can indeed extract a fair bit of information about the trait that’s been selected for, even if you may not necessarily know which particular allele has been selected.\nSo, that leaves the question: what traits have been selected for? What have we actually found? What’s been going on in humans in the last 20,000 years or so? Because that’s pretty much all the data I’m talking about are from selection that’s occurred within the last twenty to forty thousand years, because that’s the easy place to look. Lots of interesting selection happened before, like what made us anatomically modern humans, but that’s a lot harder to find, harder to study. So, we’re studying all the relatively easier things.\nResults of a selection scan\nSo, as I said, you can scan the genome for these signatures of selection, and many people did. Here’s the result of one of those scans, guess it was done here. In this figure, there are many places in the genome where there’s evidence that selection happened. There are probably some false positives in there, but there are lots of actual cases of selection that occurred there. The question is: What do they do? What was involved? The answer is mostly, we have no idea. Something happened there, somewhere. And we don’t know what the trait was. We don’t know what the selection pressure was. It’s going from “something happened at this locus” to “what’s the phenotype?” That’s hard work; that’s biology. That’s not just sitting around looking at genome data. You actually have to do some biology then.\nFrom candidate to function: EDAR\nI’ll provide one example here of the kind of work that can elucidate this sort of thing. This is a little segment of one of those genome scans. This is chromosome 2 in East Asians. This is a test for long haplotype, so there’s a very high score here across this entire region, indicating that selection happened here. Where exactly? Probably somewhere in here, but you can’t really tell.\nAudience question: [Unable to hear on video].\nStephen: Relative to size of a gene. Also, the sheer number of variants here is a problem if you want to see which one of these variants had a phenotypic effect that actually drove this. Because one of these variants was probably actually important, and the rest didn’t. So figuring out what to test for, if you want to do some functional work. This by itself is kind of daunting. There are just too many variants here to put into a model organism, say, and see it in the cell line.\nBut you can use the trick I mentioned before of combining information from multiple signal signatures, and that actually cleans up this particular locus very nicely. There are only a handful of candidate variants you’d want to look at, and one of those turns out to be a nonsynonymous change. As you can see here, there are, in fact, multiple genes across this region. It was a nonsynonymous change in a particular gene known to be involved in the development of hair and sweat glands. It’s known to have been under selection in other organisms as well. That then gives you something you can look at in more detail.\nMouse model EDARV370A\nYou can take that variant and study it in the lab. Somebody did a postdoc at Harvard, working with the Broad Institute, and said this was selection that apparently happened in East Asia. This variant increased in frequency, and she stuck it, this particular variant, in a mouse to see what it did. Well, it did several different things. It produced thicker hair, smaller and denser mammary glands, and a higher density of eccrine sweat glands. It turns out that at least the first and third effects, the hair and the sweat glands, have the same phenotypic effect in humans. The thicker hair is something you can actually see; East Asian hair doesn't look the same as European hair, typically. So, this is a mutation that was selected for. It's illustrative in that by focusing on something that was selected for, we found something that clearly has a notable phenotypic effect on humans, distinguishing between humans.\nIt also illustrates one of the problems, which is that mutations don’t always have one effect. So, it affected hair and it affected sweat glands – and we don’t know why. You can guess maybe it’s something about temperature regulation, and sweat glands are important. Or maybe hair was important. We don’t know. So, it’s not a final answer, but it was a major clue to finding something that’s phenotypically important and that distinguishes humans from one another.\nIf you look more broadly, this is from a very recent review article by Sarah Tishkoff’s group in Science about what we’ve learned about regional selection in regional human populations. You can see, on the map, we’ve learned a number of different things, and several of the cases I’ve talked about are on this map. I’ll just mention them: selection for changes to diet – the lactase persistence case up there in Europe – that’s one of the classic ones. If you can’t find lactase, then you’re doing something wrong when you’re studying selection. But that’s also been studied in East Africa and the Middle East, where there have been other herding populations. Turns out to be independent mutations in the same regulatory region that have the same phenotypic effect. So, it’s exactly the same pathway, exactly the same mechanism, but that's occurred multiple times in different places. There are other populations around the world that have lactase persistence. In South Asia and West Africa, these have not been studied in any depth at this point. So, exactly what the mechanisms are there is yet unknown. I mentioned skin pigmentation. There are a bunch of genes where alleles are known to contribute to the paler skin color that Europeans have, which varies with latitude. So they've been well-studied. Other genes are known to have mutations in Asia – some of them independent, some of them shared. So, a partially different set of mutations. Other parts of the world where there's also pigmentation. Like, within Africa, there’s quite a lot of variation and, probably, some have been under selection, but again, the studies haven’t been done there. There’s plenty of things still to study. I mentioned the polygenic selection on stature in Europe. Turns out, there’s also been selection on stature elsewhere, particularly in rainforest environments. The selection for smaller stature in humans – pygmies that tend to have small stature – is well-known in the Central African rainforest. Selection there has operated in a rather different way. In Europe, it was very polygenic – selection on lots of different alleles. In Central Africa, it apparently was strong selection operating on a handful of loci for shorter stature. It might be because the pressure was of a different kind, or it may just be that’s the way it happened to work. Don’t really know. I think the last one is the circle here for high altitude adaptation. That's been studied in the Himalayas, and it's also been studied in other high places, in the Ethiopian highlands and in the Andes. In these cases, you find independent mutations, most of them in the same pathway leading to a similar phenotype. So, the same selection pressure, different mutations, but similar outcome.\nAudience question: What's the thinking behind the selection for short stature and the advantageous nature of that?\nStephen:I don’t know much about the thinking. It might just be a matter of resources – you’re better off if you need to eat less because it’s not a very nourishing environment. But it's not something I've looked at in any detail.\nSo, those are the cases we’ve understood something about. We don’t always necessarily know exactly what the selection pressure was, but at least we have some idea what the phenotype might be.\nHow much positive selection is there, anyway?\nOne question that might occur to people is, well, how much of this is there? How much has natural selection been operating in humans recently? It turns out to be kind of tricky to figure out because there are frequency differences between populations. It varies across the genome. How much of that is naturally occurring and how much of that is, I mean, neutrally occurring? How much of that is the result of selection? Especially since we don’t really know the demographic history of humans in detail, we can’t exactly model it and tell you what the distribution should look like. So, it’s kind of an open question of how much positive selection has happened.\nOne way of trying to address this is to look at that reduction in diversity, particularly the reduction of diversity around the genes. And I said that can be caused either by selective sweeps happening repeatedly in positive selection or by background selection. So, you need to find a way to distinguish them. So, one group, what they did, was they looked at genes. We’re only looking at genes. Presumably, background selection is happening all the time, and they compared cases where there’s been a nonsynonymous change in humans over the last million years or so to places where there’s only been a nonsynonymous [synonymous] change. The idea is if they’ve been a lot of selective sweeps, you should see reduced diversity around the nonsynonymous case. Here’s their plot from their paper. This is Hernandez et al. from 2011, a few years ago now. Here’s where the substitution happened at some point, and they plot diversity for synonymous in blue, the substitutions, and for around non-synonymous substitutions in red. Those distributions are basically the same. There’s no obvious difference between the non-synonymous and the synonymous.\nConclusion 1\nAnd their conclusion was, “We don’t see any evidence for lower diversity around functional changes.\" Classic selective sweeps, sure, there's been some, but they’re not a major factor. Maybe it’s been, you know, this polygenic selection, other kinds of selection are happening. Basically, they said the conclusion is,”You should stop wasting your time looking for these things,” which was a little annoying for those of us who were looking for them, particularly those of us who wanted to get funding to look for them. Because then, you know, the reviewers of the grant proposal say, “Why are you looking for this? Hernandez et al. just showed that there aren’t any selective sweeps, or you know, we’ve already found them all. Don’t waste your time.”\nAnd then a couple of years later, there was another paper from a different group, David Enard, Dmitri Petrov's group, and they came to a very different conclusion. I think I’ve time to go through this. And they said they looked at the same data and they said, “No, you’ve drawn the wrong conclusion. Because the problem is, you’re looking at nonsynonymous mutations.\" Well, let’s think about that a little bit. Let’s think about you’ve got two genes: Gene A is highly constrained, every amino acid is a precious jewel, and if you change one of these, it’s bad. So, anytime there’s a missense mutation, it’s deleterious. You will find very few missense mutations that happen neutrally. So, if there’s any mutations in missense mutations, they will have been beneficial. And what that means is, lots of background selection happens; there’s lots of purifying selection going on all the time, because all of these mutations are bad. So, if you look at diversity around that gene, well, there’s going to be very low diversity around that gene. Now, Gene B, on the other hand, this is a weakly constrained gene; a lot of the amino acids don’t do anything, so a lot of neutral changes. And the consequence is there's got to be less background selection because there are fewer opportunities for deleterious mutations. And so, around those genes, there’s going to be a smaller reduction in diversity. And what they say is that when you look at nonsynonymous mutations and nonsynonymous substitutions that have happened, which is what the Hernandez et al. group did, you’re picking primarily from this group, from these genes. On the other hand, when you’re picking synonymous mutations, you’re picking from both of them. So, you’re biasing yourself to finding places where there’s not a lot of change. And so, if there are cases, a lot of cases, of positive selection going on, selective sweeps, you’re going to completely lose it because of this bias.\nConclusion 2\nAnd so, their conclusion was that there’s actually strong evidence for lots of selective sweeps in recent human history. There’s been a high rate of strongly adaptive substitutions near amino acid changes, and there have been even more sweeps driven around regulatory changes, which I think we know independently. So, that same data, very different conclusions, and a little heartening to those of us who are interested in selection.\nI mentioned that I introduced that new test for selection, this density of singletons. They conclude that lots of different traits, if you just accept their data at face value, lots of different quantitative traits in humans that have been studied by GWAS, a lot of them show evidence for having been undergoing selection in the last 2000 years – like more than half. I don’t know if I believe that, but certainly their evidence has suggested that lots of slow selection of some kind is going on all the time. Okay, so that’s where we are in terms of studying selection from just looking at genetic variation and what its effect is.\nBut there’s one other topic that’s really interesting because it gives us a whole new way of looking at this, and that’s ancient DNA. Because in all these things, we’re inferring what happened in the past. All these kinds of studies, we look at genetic variations in a bunch of Norwegians, and you try to figure out what happened 10,000 years ago. But now we can actually look at DNA from 10,000 years ago, 5,000 years ago, and see directly what’s changed between then and now. People have done this. We now at the point where we’ve sequenced enough ancient genomes; we can compare allele frequencies in the past to allele frequencies now.\n230 ancient genomes\nAnd this is data from a paper of last year, David Reich’s group at Harvard. And the colored dots are the frequency of several particular alleles that are of interest in several ancient populations. And the dashed lines are what the frequency is in modern European populations – this is basically Europe and West Asia there are these ancient populations. And I’ll point out a couple of cases. The top-left one is lactase, again, this classic example. You can see that the frequencies in these ancient populations were at or near zero, the allele that gives you the ability to digest lactose. Whereas in modern southern European populations, it’s still fairly low, but in northern European populations, it’s very high. So, this is selection that’s happened just in the last few thousand years. You can see that it's dramatically increased in frequency over that time.\nHere on the other hand is a pigmentation allele, one of the major ones that contributes to European paleness. You can see that it was at different levels in different populations. The steppe peoples, these are the people of the Western steppes in Asia, were apparently paler than these other Europeans down here. But they’re all at a lower frequency than in modern Europeans. So, this is a selection that was ongoing at this time and has continued into the present.\nIf you look across the genome – I don’t know if you’re used to Manhattan plots but this is the entire genome spread out – and the places where there are signals that selection has happened, just from comparing the ancient DNA to the modern DNA, you find a lot of the same things. You find the skin pigmentation genes that are already known. They find the genes including lactase and fatty acid dehydrogenase, which has been seen to be under selection in other populations. Selection for resistance to infectious disease at the HLA toll-like receptors.\nAncient DNA tells a similar selection story\nSo, the basic story we get from ancient DNA is very similar, which is heartening, because we were reconstructing the past based on computer models and it’s nice to see that when you actually can look at the ancient DNA, it tells you we were right in a lot of these cases. We were really correctly inferring that selection happened. But you get a lot more detail when you look at the ancient DNA because our computer models are simple. So, I’ll give you a few examples of the difference in the story and to end here.\nI talked about this gradient of stature in Europe, and there’s this north-south gradient. So, we concluded selection happened. Well, when looking at the ancient DNA, they concluded a little more detail. They conclude that selection happened for shorter stature in southern Europe. That selection for taller stature happened in West Asia, in these steppe populations. It seems that’s where we see it, and that we see this greater height in northern Europe because those people then moved into northern Europe. It wasn’t necessarily selection happening in northern Europe for greater height – it was just people having to migrate in. And looking at modern populations, we would have no idea. We would have no idea about this. It’s very hard to figure out all of these, where all these people have been moving around. We tend to assume that if we’re looking at Chinese people today, their ancestors were living there 10,000 years ago. No, people move, they move a lot it turns out – from looking at ancient DNA sometimes in ways that we couldn’t tell at all from modern DNA.\nAlright, a second case, pigmentation. There are two main genes that contribute to European pigmentation, typical color, with very similar names: SLC24A5 and SLC45A2. And we know from the genetic evidence in modern Europeans that they’ve been both under strong selection. There’s an allele that’s risen to high frequency in both cases. Turns out the two genes have somewhat different histories, looking at the ancient DNA. One of them – actually, it’s the one I just showed – this skin pigmentation gene, you can see rising in frequency within Europe as selection was occurring for lighter skin. The other allele actually entered Europe at a very high frequency with farmers when the first farmers moved in from Anatolia, which is modern-day Turkey. They largely replaced the European population, and they already had lighter skin, presumably as a result of earlier selection. But you get a much more detailed picture of the history this way.\nAnd finally, the last case is the case of EDAR, the one thing that gives you more sweat glands and thicker hair. When we were studying it, the story seemed pretty simple. This plot shows where that allele is present, it's present in East Asia and it's present the New World, because people from eastern Asia populated the New World. And so, it was clear, they did very detailed modeling for this. And this is estimating where this allele originated, and it originated in central China 30,000 years ago. So, it was a nice, simple story. There was a Cell paper, and it’s a great paper, but this particular conclusion – the problem is, you look at the ancient DNA, it turns out this allele was at high frequency in Swedish hunter-gatherers 6,000 years ago, which is not something you would guess from looking at the modern distribution. It was still under selection in East Asia and that story hasn’t changed, but the details of what happened historically are more complicated. It probably arose in western Asia and happened to be selected for in eastern Asia. So, this is the kind of information you get from it. Really, it’s a tiny snapshot of ancient DNA. As we get more DNA, we’ll be able to learn a great deal more, at least from places where there is ancient DNA – a lot of the world, you know tropics, DNA does not preserve well.\nI’m going to conclude with a couple of comments. So, recent positive selection has clearly had an impact, a significant impact on human phenotypic diversity, both within individuals and within groups. Exactly how much? You know, some traits – many traits, at least two traits – have been changed by this. And many of these traits are of medical interest or biological interest. It’s a great way of finding out, one way of finding out where these important phenotypic changes are. So, if you study natural selection, you can learn and identify places where things have changed. It’s not, by itself, an all-purpose tool. It’s a clue. It has to be combined with functional work, with GWAS, with association studies, with all kinds of other things. But it is one tool in the toolkit. And I'll stop there, thanks."
  },
  {
    "objectID": "chapter10.html",
    "href": "chapter10.html",
    "title": "Chapter 10: Other Considerations",
    "section": "",
    "text": "Chapter goals:\n\n10.1 A Career in Psychiatric Genetics\nTitle: How does Genetics Affect our Mental Health?\nPresenter(s): Cathryn Lewis, Alex Curmi\nLevel: Beginner\nLength: 58:54\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.2 Caution in Genetic Prediction\nTitle: Predicting the likelihood of future psychiatric disorders: a closer look, and some cautions.\nPresenter(s): Howard Edenberg\nLevel: Beginner\nLength: 4:30\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.3 Small Effect Sizes\nTitle: Small effect sizes\nPresenter(s): Howard Edenberg\nLevel: Beginner\nLength: 4:05\n\n\n\n\n\n\n\n\n\nLink to video transcript here.\n\n\n\n10.4 Language in Genetics Research\nTitle:\nPresenter(s): Carina Seah, Kayla Townsley\nLevel: Beginner\nLength:\n\n\n\n\n\n\n\n\n\nLink to video transcript\n\n\n\n10.5 Equitable Collaboration for LMIC Researchers\n\n\n\n10.6 GDPR for Dummies: A Survival Guide for Genetics Research\nTitle: GDPR for dummies: A survival guide for genetics research\nPresenter(s): Heidi Beate Bentzen\nLevel: Beginner\nLength: 23:46\n\n\n\n\n\n\n\n\n\nLink to video transcript here."
  },
  {
    "objectID": "chapter9.4_transcript.html#sec-video1",
    "href": "chapter9.4_transcript.html#sec-video1",
    "title": "Chapter 9.4: Interactions with Environmental Factors (Video Transcript)",
    "section": "Dummy variables: interaction terms explanation",
    "text": "Dummy variables: interaction terms explanation\nTitle: Dummy variables: interaction terms explanation\nPresenter(s): Ben Lambert\nBen Lambert:\nSo, let’s think back to our example we had in the last video. Let’s say we were interested in how wage rates varied between, let’s say, male and female individuals. So, the idea is that we regress wage on, let’s say, now that we sort of implicitly assume that we have all these other variables. We’re not going to include them explicitly. We’re just going to have alpha plus, let’s say, beta 1 times the number of years of education plus beta 2 times our sort of sex variable. Our sex variable takes on the value of 1 if the individual is female and it takes on a value of 0 if the individual is male. But then we included a further term, which was, let’s say, beta 2 3 where we multiplied sex times education. So, the idea is that we have collected all these variables across our individuals or in the population or in our sample, rather, and we have included a multiplicative term in our regression specification. So, what does this multiplicative term mean? How do we interpret this beta 3?\nWell, let’s think about again what this average wage rate would be for a female and compare it with the average wage rate for a male. The average wage rate for a female, if they had a given number of years of education, would be alpha plus beta 1 times the number of years of education which they had, plus, well, this sex variable now takes on the value of 1, so I’ve just got plus beta 2, and now our sex variable here is taking on a value of 1 as well, so I’ve got plus beta 3 times the number of years of education. Okay, and then we can sort of simplify this. If we should have noticed that our alpha and our beta 2 are those constants here, so you’re writing those both the stars of the model, we get sort of alpha plus beta 2, let’s say. And then we recognize that we have essentially got two education terms, we’ve got this one and this one, so I could simplify these as well by writing them or by combining them. I just get beta 1 plus beta 3 times the number of years of education. Okay, so that’s for the female case.\nWhat do we have for the male? So, the idea for the males in our sample is that the average wage rate is given by alpha plus beta 1 times the number of years of education because our sex variable takes on a value of 0. So, these second two terms actually cancel or don’t exist for the male. So now we can think about what the effects of our sex variable have been on our specification and our interpretation. So, what does beta 2 3 represent here? Well, beta 2 represents the additional premium which females would have over males if they had zero years’ worth of education because if they had two years’ worth of education then both of these two terms would disappear and the only difference between males and females, in fact, and when it would, in fact, actually be our beta 2. So just like we proved in the last video, that is actually the wage premium which females have over males in the case, in this case, of having zero years’ worth of education.\nOkay, so what does beta 2 3 represent? Well, notice that the only difference between these two specifications in terms of the education variable is that essentially the partial effect of education for females has been boosted by an amount beta 2 3 relative to the males. So, what does that mean? If beta 3 was greater than zero, it means that the additional effect of one more year of education for females was, in fact, greater than that for males. If it was less than zero, then it would be the other way around. So, the additional effects of having one more year of education, on average, would tend to cause a smaller increase in female wage than it would do for males.\nSo, we can sort of think about what these cross terms mean in our regression specification. Essentially, what they mean is if I’m interacting a dummy variable with a continuous variable, it allows us to have different slopes of that particular continuous variable across the two different values which our dummy variable can take on. So, that’s quite an inappropriate assumption to make in a whole host of different situations. In this particular situation, you might suppose that there might be a different effect of education between males and females, but there are a whole host of other ways in which this could be true across other types of models."
  },
  {
    "objectID": "chapter9.4_transcript.html#sec-video2",
    "href": "chapter9.4_transcript.html#sec-video2",
    "title": "Chapter 9.4: Interactions with Environmental Factors (Video Transcript)",
    "section": "Continuous variables: interaction term interpretation",
    "text": "Continuous variables: interaction term interpretation\nTitle: Continuous variables: interaction term interpretation\nPresenter(s): Ben Lambert\nBen Lambert:\nHi there! In this video, I wanted to explain the interpretation when we have two continuous variables multiplied together in a regression model. Okay, so let’s think about a particular example. Let’s say we were trying to explain a company’s level of sales, but we’re trying to do that in terms of the effective price and the effect of advertising. Yes, this might be the company’s level of advertising spend, and this is just the company’s price set for a particular product. So, traditional theory would expect us to have a downward-sloping demand curve. We would expect beta 1 to be less than zero because if you lower the price, then sales increase. And we would expect that if we spend more on advertising, then sales tend to be higher as well. So, we’ve got beta 1 being less than 0 and beta 2 being greater than zero.\nBut let’s say we included a third term here, which is beta 3, and now included the product of price and the company’s spend on advertising. What interpretation can we actually give to this beta 3? Well, let’s think about this in two different situations. Let’s say that the company was spending a hundred thousand on advertising, and let’s think about what the company’s expected sales would be under that situation. The idea is that the company’s level of sales we would expect if advertising was 100 would be equal to alpha plus beta 1 times the price plus now we’re going to get 100 times beta 2 for this third term and then we’re going to get plus 100 times beta 2 3 times the price.\nOkay, so what does this show us? Well, we can actually think about the effective price because price is appearing twice in our model here. We can combine the price variables to create an aggregate effective price. So here we would have the aggregate effective price would be beta 1 plus 100 times beta 3, and then that would all be multiplied by the price. One interpretation is beta 3, and what sign would we expect beta 3 to have in this case? Well, we would actually expect that beta 3 would be greater than zero. Why would we expect that? Well, the idea here is that if you spend more money on advertising, then that tends to decrease the sensitivity of your consumers to price changes in that product. Notice that this appears because beta 1 is less than zero. So if we’re adding 100 times beta 3 where beta 3 is greater than zero, then the idea is actually we have decreased the sensitivity of consumers to price changes or we have made our customers less reactive to price changes, which is something which you might expect companies to exhibit. You might expect if you spend more money on advertising, you increase the intangible brand value or the non-tangible effect which consumers consider when they’re thinking about your brand. That might make them less price-sensitive. Okay, so that’s kind of what beta 3 is representing in this case.\nLet’s think about another example whereby let’s say we had the price level set to ten, and let’s see what we might predict the company’s sales to be in that case. The idea is that the company’s sales on average when the price was ten would be equal to alpha plus ten times beta 1 plus beta 2 times the level of advertising, which we haven’t specified. Plus now we can have 10 times beta 2 3 times the level of advertising. Notice that again here, we have two terms which essentially have the same variable. So, we can combine these. So now we have an aggregate effective advertising being denoted by beta 2 plus 10 times beta 3. What does beta 3 represent in this case? Well, remember that we found from the first example that beta 3 by theory should be greater than zero. Well, what does it say in this case? It says if your price is higher, represented by this 10 here, then the effect of advertising tends to be greater. So that might be the case. If you have a higher premium price product, you might have to demonstrate to consumers that it’s worthwhile buying, so the effect of advertising is greater than if you have, let’s say, a low-price product which consumers would flock towards anyway.\nSo, beta 3 generally, what does it mean? What have we learned from considering these two cases? It shows that the effect of price depends on the level of advertising spend, and the effect of advertising tends to be determined by or affected by the level of price. So, beta 3 is a way of adjusting the effect of price and advertising to take into account their multiplicative effects on one another."
  },
  {
    "objectID": "chapter8.7_transcript.html",
    "href": "chapter8.7_transcript.html",
    "title": "Chapter 8.8: TWAS (Video Transcript)",
    "section": "",
    "text": "Transcriptome-Wide Association Studies\nTitle: Understanding GWAS mechanisms with Transcriptome-Wide Association Studies\nPresenter(s): Sasha Gusev\nI am Sasha Gusev. This is my first time at CGSI, so thanks, everybody, for having me and giving me the opportunity to give this tutorial. As with the other ones, please feel free to interrupt or ask questions throughout, and I’ll try to sort of break things down in a way that’s accessible. I’m going to be talking about genome-wide association studies and specifically trying to make sense of genome-wide association studies as a way to understand human disease and complex traits.\nSo just to sort of start it at the very basic level, this is the output of a genome-wide association study or GWAS. The procedure is very straightforward - you collect a lot of genetic data on individuals with the disease and without the disease or with a quantitative trait, and then you test each genetic variant (and that’s what each of these dots is here) for association with the phenotype. The variants that are significantly associated are above a predefined threshold here, and if they replicate, we treat those as genetic variants that are causal for the disease. This is sort of a study design that I think initially almost seemed too simple to work, but now over time and with very large sample sizes has produced thousands, if not hundreds of thousands, of associations for nearly every complex trait that it’s been applied to when there was sufficient sample size.\nIn fact, the challenge is now that these association studies are almost producing too many results, and what we would rather have than sort of this figure, which is a real plot from a GWAS in prostate cancer, is something more like this, which is a systemic or systematic understanding of the disease, of which genes are involved in the disease, how they interact, what contexts they’re relevant in, and so forth. So, whereas initially there is sort of a challenge of just fleshing out this side of the plot, getting these associations, I think a key challenge now is in connecting from this side of the plot over here to an actual understanding of the disease.\nOne of the most basic pieces of getting to that understanding is connecting variants to the genes, associated variants to the genes that they likely operate through and then operate on the trait. So, we can break it down into this very simple structure. We have a variant; we want to know its target gene and the effect that it has on that disease. So, in particular, we can break this down even further and first just ask whether we can identify variants that influence the expression of genes in a systematic way.\nThis is something that was observed some time ago is that, in fact, if you take gene expression and you basically run a GWAS but on expression as your outcome (gene expression measured in the past through microarrays or now through RNA-seq and test variants), typically near the gene in cis with the gene for association with expression across individuals, you will find that the expression of many genes is often highly heritable. So, there’s an estimate here, in 2011, that the cis locus for an average gene contributed to between 37% and 24% of the variance of expression. And again, once you have a heritable phenotype in a population, you can sort of apply the GWAS paradigm to that phenotype and, instead, we call that an eQTL analysis.\nI’m sure you folks have seen work from the GTEx Consortium over many years, applying eQTL studies and identifying thousands of variants associated with the expression of many genes in many tissues. And in fact, again, this is one of those cases, where as the sample sizes have grown, this study design has actually yielded a very large number of associations that are almost like too difficult, too many to fully process.\nAnd I think the most recent GTEx study showed that if you sort of relax the significance threshold for these associations, nearly every gene has at least one eQTL in some tissue. And in fact, I think that if you continue as the sample sizes have grown even further, we see that genes then start to have secondary eQTLs and tertiary eQTLs, and this sort of curve does not even hit diminishing returns. So that’s the piece about identifying genetic variants that influence gene expression.\nThen there’s been a lot of work in trying to understand how these eQTLs connect to disease, and I’ll highlight a couple of studies in particular, which basically asked in a couple of different ways whether an eQTL is more likely to be a GWAS variant or is more likely to be associated with a complex trait. So, the results on the left show that eQTLs, specifically as you get more confident about them being the causal eQTL, are more enriched for heritability across many complex traits from GWAS. And then this figure on the right from Gamazon et al. showing that if you just sort of try to partition the amount of disease heritability that could be explained by eQTLs, those estimates are also quite high across a large number of complex traits, again ranging from maybe 10% up to 35%. So there’s this sort of incidental evidence that eQTLs are enriched for disease heritability and may therefore give us an instrument to understand the likely causal genes and eventually go back to that big system-wide understanding of the phenotype.\nSo that’s the first part of the arrow. The other part of this network is we want to understand how this genetic mechanism of gene expression actually goes on to influence the trait and for which traits, and this is where the approach of a transcriptome-wide association study or a TWAS comes in.\nI’ll just start with a very basic sort of thought experiment of what would we want to do if we had the ideal data set. How would we, with infinite resources, try to relate gene expression, genetics, and disease together? I think one way that we could do this is we could estimate expression in the hundreds of thousands of individuals that we have genetics and case-control status in. Here, like this represents case-control status. And then we could ask what genes are genetically correlated, meaning the effect sizes on expression are also shared with the effect sizes on disease. We could do this for every single gene across the genome, and that would give us an estimate of the genes that, in principle, could be linked to this phenotype. The hurdle here is that we very rarely or pretty much never have data at this scale. What we typically have is a relatively small study of genotypes and measured gene expression, usually as in the case of the GTEx, in a sort of healthy, relatively healthy, population that was convenient to sample. And then we also have very large disease studies that also have genotypes but no gene expression measured. So the basic insight of the transcriptome-wide association study or TWAS is kind of thinking about the fact that what is shared across these two studies is the genetics, and we know previously that gene expression is itself a heritable trait. And if it’s a heritable trait, then in principle, it should be a predictable trait.\nSo what we want to do is use the genetics to predict expression into this study over here where we haven’t measured it and then use the predicted expression as a sort of proxy to estimate the relationship between the genetic component or the predicted component of expression and the phenotype. Again, I’m sort of presenting everything in the context of a single gene, but the idea is to use this methodology and scan across every gene in the genome and identify the set of genes that are significantly genetically correlated or for whom the predicted expression is significantly associated with the phenotype.\nAnd so right, then we do the test. So the first question is: can we actually predict gene expression in this way? And the fact that we’ve observed significant eQTLs or individual variants that affect expression basically tells us that we can. And in work that we’ve done and others have done, we’ve shown using a number of different prediction schemes that I sort of won’t go into but that are various forms of penalized or Bayesian regression that you can, in fact, predict gene expression with a substantial degree of accuracy. In particular, when you use models that incorporate all of the genetic variation around the gene, you typically have substantial gains in the predictive accuracy. So even though the single-topic eQTL explains a large fraction of the cis effect or of sort of the total heritability near the gene, there is a very large number of genes for which additional variants contribute substantially to the predictive accuracy. Simply going from a single SNP paradigm to a sort of locus-wide paradigm increases our predictive accuracy, and that’s going to translate into better association statistics in the eventual GWAS study.\nNow, one additional constraint is that we typically don’t really even have this design where there’s individual-level data in both studies. What we actually have more frequently is this design where we have individual-level data for the gene expression study, and then we have summary statistics for the GWAS. The summary statistics are basically for every SNP, the marginal association statistics for every variant. And what we want to know from this kind of data is what would the gene-trait association have been if we could get to the individual-level data and measure it. And so this is really where the TWAS methodology comes in. Again because this is the type of data we have most of the time.\nI’ll just sort of sketch out how this parameter is estimated, and the basic idea is that we think about what we would want to do with individual-level data and then we kind of move terms around and try to identify pieces that can be estimated from the summary level data. So, we start with predicted expression over here (X are the genotypes that we use for the prediction, w are the weights that we’ve trained in the gene expression data that gives us this term G, that’s the predicted expression), and then what we want to know is the association between Y (the phenotype) and G (the predicted expression). So specifically, we want to know this orange βTWAS. So we can kind of plug in the terms into a basic ordinary least squares regression and then decompose these terms, and you can start to see pieces here that you can actually estimate from summary level data. In particular, you’ll see that this covariance between the genotype and the phenotype actually corresponds to these GWAS summary statistics that we get, the association between each SNP and the phenotype. And then this term down here, the covariance between the SNPs themselves is also something that we, in principle, can get from reference panels because it doesn’t rely on knowing the phenotype. So these two pieces we can get externally, we plug them back in, and now this is a summary-based estimate of the βTWAS that only requires the Z-scores, the reference LD, and then these weights which we have (we sort of assume that we have a priori). And then I won’t go into the details of how we derive the variance for this statistic; it’s very, very similar. And the final association statistic that we get looks like this, where again in the numerator you have, you can think of this, as a weighted sum of the GWAS scores that’s weighted by the predictors of expression, and then in the denominator, we have essentially the variance of that predicted expression that accounts for the correlation across these SNPs – so SNPs that are correlated are going to add to the variance and SNPs that are independent are not. So this is basically the score, and I think this is also kind of a useful framework to think about how you can go from individual-level data to estimates of quantities we’re interested in with summary-level data.\nWhen we apply this technique to summary-based data and individual-based data, it works really well. Correlation is nearly perfect, and again, we didn’t really make any assumptions going through that previous derivation except for the fact that the LD is well-matched to the target population, and also there’s sort of a hidden assumption that the effect sizes can’t be so enormous that we need to account for changes in the environmental variants and those assumptions are very easily satisfied in most studies.\nNow, thinking about when does this approach actually lead to associations, we ran some simulations where we considered three different study designs under the model where there is a causal gene, and we’ve observed the predictors of that causal gene. So you could imagine, in that scenario, just running your standard GWAS to try to identify the association. You could imagine testing only the top SNP, the top eQTL that’s associated with expression, or you can imagine running a full TWAS test. And when we do that, we see that in this scenario, because we’re testing fewer features, we’re only testing each gene instead of each SNP, then the power of the TWAS or the eQTL-only approach is higher than the GWAS approach. So, this is one case where not only are we getting a parameter that we’re interested in on its own, we also have some increase in power because the multiple testing burden is effectively lower.\nFurthermore, if we expand the model and say, additionally consider genes with multiple causal variants, where now the TWAS approach of applying a penalized model to the entire locus is giving us more signals, more predictive accuracy than the top eQTL, we see that the power of these single SNP approaches drops, but the power of the TWAS locus-wide approach remains effectively the same. So again, this is another scenario where when we have many causal variants for expression that all lead to disease, then we can substantially boost power. And where the truth is in between or maybe a little bit off the page, there’s going to be some loci where we don’t have the measured expression at all, so these expression-based approaches will just fail. There’s going to be some loci where there’s only a single variant for the gene, and we’ll be up here, and there’s going to be some loci where there are many causal variants for the gene, and the TWAS will then maximize power relative to other approaches.\nSo this is all in simulations under very specific presumed models. We can also ask how well does this approach perform in real data. And this has actually been quite a challenging question to answer because as it stands, we have very few well-established causal genes for disease. So I showed you that plot at the beginning that had over 200 known associations for prostate cancer, but the number of well-established, really definitively established causal genes for prostate cancer for that study is extremely small, and that’s sort of the case for most complex traits. So we don’t actually have a kind of working in a regime where we don’t really have a ground truth. There was a study that was done in this pre-print by Weeks et al., from the Finucane lab, which I thought was an interesting attempt to try to get at a ground truth. The basic idea was that if we look at data, in their case, they looked at data from the UK Biobank where you had associations both with common, sort of standard GWAS and also a rare variant, coding variant-based set of tests, and you identify a locus where there’s both common non-coding associations and rare coding variant associations. You can assume, maybe it’s not a safe assumption, but they assume that the rare coding variant is telling you the right causal gene. And so under this model, they basically have a kind of ground truth, which is what does the rare coding variant tell you the causal gene is, and then they can ask how various other approaches do based on just the blue stuff, just the common variant associations for identifying that causal gene.\nAnd so, now they have a ground truth. They can plot precision-recall curves, and they used this approach to evaluate a bunch of different methods listed here and then also to propose their method, which is, conceptually, quite different. I won’t go into it, but it’s sort of like an ensemble that integrates many different features at the locus to make the predictions. But I think what’s relevant here is how these other approaches perform, and what you can see is that there’s quite a lot of heterogeneity in their performance. The TWAS is here in blue, and at one point, it has the highest recall in this model relative to the other approaches, aside from their ensemble-based approach. And then, additionally, they integrated each of these methods together with their model, and in that scenario, the TWAS had the highest precision together with their approach. But again, I think an important takeaway here is that this is far from a solved model with a clear optimal method. The TWAS provides you an estimate of a certain statistical quantity, but this is biology and biology is complicated. So, lots of different approaches have different trade-offs for what they’re able to identify and at what levels of precision and recall. And then, you know, one thing I should mention that maybe people are noticing is that if you take a very simple model of just what is the nearest gene or what’s the distance or how far away is the potential causal gene, that actually performs really well. And, in fact, it performs about as well as the method that they developed and also, when combined, has very good precision. So, again, I think that there are many explanations for this. One is that, in fact, it may be that the nearest gene, oftentimes, is the correct gene. It may also be the case that this specific model tends to emphasize genes that are close to the association statistics. But, again, I think it’s also important to keep in mind that probably some hybrid of all of these methods that also consider proximity is going to eventually be the optimal solution.\nOkay, so that’s kind of where we stand with TWAS applications. Coming back to this figure, you can sort of wonder why the precision of TWAS is relatively low compared to these other methods, and I think, again, there’s an important set of caveats which were sort of highlighted in this paper from Mike Weinberg et al. a couple of years ago, in Nature Genetics, which essentially come down to the fact that TWAS is an association study. It’s not a causal inference technique. And as an association study, it’s going to be susceptible to tagging and correlation in the same way that genome-wide association studies are.\nAnd so, in this paper, they proposed a number of alternative models, which could still identify a significant TWAS hit. One alternative model is that you can have co-regulation at a locus, where the same genetic variant or set of genetic variants drive the expression of multiple genes, both the causal and non-causal genes. And this is a real phenomenon that it’s not that uncommon that you will identify loci with multiple genes with very high cis genetic correlation and high genetic correlation to the trait.\nAnother case is you can imagine some part of the genetic effect on a non-causal gene is tagged because of LD between variants with the effect on the causal gene. And this would produce a false positive association or it would sort of induce some effect on both the causal and non-causal gene and the causal gene.\nAnd then, likewise, you can imagine a scenario where the effect on the causal gene was missed in the gene expression study because it was not sufficiently powered or it didn’t get the right context. And so this would lead to a false negative association. And so, again, I think this is important to keep in mind that this is a test that is expected to tag the causal mechanism when these assumptions are met, but in the real world, these assumptions should also be interrogated.\nThe other, I think, important limitation and one that’s potentially solvable is to consider is the fact that as we’ve seen with other genetic predictors, the predicted expression models do not generalize well to other populations. And really, because most of the data has so far been collected in individuals of European ancestry, this is particularly a problem for generalizing to data from non-European populations or in admixed populations with low European ancestry. And so this paper showed models that were trained in European individuals that had high accuracy predicted into held-out European individuals and had significant drops in accuracy when predicting into individuals of African ancestry. And, again, I think that there are potentially interesting ways to address this problem. Probably the most basic is just to start collecting more data in other populations; we should definitely be doing that. But also, there’s methodological approaches that could potentially leverage all of the training data that we have available or think about the differences between populations to improve the prediction of these models.\nSo I also wanted to talk a little bit about methods. I think these are all methods that we did not develop and had no hand in, but that I think are interesting approaches to moving beyond just that βTWAS that I described for the association between expression and disease. And I’ll sort of walk through them briefly, you know, to give you guys a flavor of methodologically what else can be done in this space.\nThere’s a great method called UTMOST that came out a couple of years ago in Nature Genetics, which thought about how gene expression data that’s measured in multiple tissues in the same individuals could potentially be used to improve these predictive models. So everything that I’ve been talking about so far sort of assumed that there was a population with some single modality of gene expression. But you could imagine, and this is exactly how the GTEx was designed, that you’ve measured multiple tissues for every individual. This Y is now a matrix instead of a vector for a given gene. And then, the approach that UTMOST proposes is to actually try to learn the expression for each tissue together with all of the other tissues observed. And so, again, you see some similarities here. These B’s, what they’re learning, are sort of the w’s that I talked about earlier, now are being learned for all tissues at once. And they do that by using again a form of penalized regression where they have a penalty within each tissue where they want the weights to generally be sparse. And then they also have a penalty across the tissues where they don’t want to see a lot of differences between tissues. They sort of assume that if a SNP is important for one tissue, it should also be important for another tissue. And this approach, particularly for tissues that had relatively small sample sizes, substantially increased the prediction accuracy, basically by borrowing signal from other tissues that were available. And that’s sort of shown here in purple is the increase in prediction accuracy and held-out data.\nAnother approach thinking about this sort of multi-tissue framework is instead of learning weights using multiple tissues, we may be interested in testing multiple tissues where each set of weights were learned individually. And so there was this work, a method called MultiXcan, by Barbeira et al. in 2019, which essentially showed that if what we’re interested in is this relationship here between – now we have many G’s for a single gene, we have the predictive model from one tissue, a second tissue, a third tissue, and we want to know if there’s an association for any of these features in a joint model, so a multi-degree of freedom test for association, what we actually have, again, because we don’t have the individual level data, we’ve actually observed is these marginal TWAS, individual TWAS, statistics. But if we know the correlation between these statistics, then we can actually approximate the relationship or the effect under the three-degree of freedom or n-degree of freedom test from these marginal effects. The MultiXcan paper also did some clever stuff where you have many tissues with highly correlated expression, and you don’t want to just throw them all into this model by using principal components analysis to first reduce the dimensionality of the expression down, then just test the leading components of expression for association in this P degree of freedom test. And this also, in practice, showed that it produces a much larger number of significant gene-trait associations. Again, now we’re sort of saying that if there’s a little bit of signal here and a little bit of signal here and here, then that can add up to a lot of signal across the three degrees of freedom.\nThe other, I think, interesting method or any other interesting method in this space is now thinking about how to integrate together many TWAS associations across a given locus. And so, you’ve probably seen methods for GWAS fine-mapping that try to identify the set of causal variants or variants that contain the causal variant with some predefined probability. The same kind of methodology or the same sort of concept can be applied to TWAS statistics. And so, instead, this work of Mancuso et al. in 2019 reformulated this problem in terms of having multiple TWAS associations at a single locus and then fine-mapping these down to the set of likely causal genes. And this is actually starting to address some of the caveats that I outlined earlier when you have co-regulation of multiple genes, or you have some tagging across multiple genes, this is now an approach to put probabilities on which genes out of many are likely to be causal, whereas which are likely to just be tags, and to sort of estimate posterior probabilities of causality for a given gene.\nSo just in the last couple of minutes, I wanted to mention a bit about what else can be done with this framework. And so, everything so far that I’ve been talking about has involved gene expression or transcription, but really the idea is that any molecular trait that is heritable and that can be predicted from data that we’ve measured is amenable to this sort of approach and this way of integrating with GWAS.\nAnd, in particular, we can go back to this model which maybe we’ve solved now in some sense and observe that this is also an oversimplification. In that most of the time for non-coding variants, what we expect is that there’s some regulatory element that sits in between the variant and the expressed gene that maybe is the modifier or is the mediator of this gene expression. So really, there’s probably an enhancer or a transcription factor or a combination of those features through which this SNP has an effect on the expression of the gene, which then goes on to have an effect on the trait. And with sufficient data, we can actually start modeling these regulatory elements and the genetic predictors of these regulatory elements.\nAnd we have some recent work to that end, which we call a regulome or a system-wide association study. So, we’re sort of padding out the letters of the alphabet here, but the idea is that instead of learning predictors of a given gene, you can learn predictors of some biochemical activity, including transcription factor binding, chromatin state, or chromatin accessibility. And additionally, in this regime, we can also leverage some allele-specific information of variants that are inside these peaks that we suspect to be modifying their activity. And so, we can boost power even further because we can leverage signal within each individual in addition to across the individuals. And again, we’ve shown that this approach is fairly robust, that you can identify a very large number of predictive models.\nAnd when we’ve applied this approach specifically to cancer GWAS phenotypes, so again, this is cancer risk we’re just talking about the predisposition to develop cancer, we see, going back to this plot that I started with, we’ve now characterized each of these loci where we see that there was this inner circle here is the number of loci that had a significant TWAS association with a gene. But then actually, when we incorporate these epigenetic features, we see a much larger number of loci that additionally have associations through chromatin accessibility in this case, many of which do not actually exhibit a direct transcriptomic association. And so, this is actually sort of interesting and somewhat mysterious in that we’re able to identify loci where there seems to be a genetic regulatory effect that we don’t see have a downstream cascade on expression. We do capture most of the loci that have the TWAS association, those we’re able to characterize, but then we have this number of additional loci.\nAnd we’ve sort of started to think about what those loci could be telling us. One observation is that if you look at the distribution of evolutionary constraint across the genome, you will see that in regions with higher evolutionary constraint, we see fewer TWAS models that can be built, probably because selection is making it more difficult to detect or is decreasing the observed effect on expression, making it harder to pick up the eQTLs. But we actually see more of these RWAS or chromatin-WAS models observed in those loci. So, this is maybe this gap could potentially explain that sliver in the previous figure. These are loci that are very difficult to pick up in the expression framework but are not as difficult to pick up when we look directly at the intermediate chromatin phenotype. And a sort of related observation that we’ve made is if you look at genes in terms of their tissue specificity, so as we move from here to here from the left to the right, these genes are more specifically expressed in prostate tissue, we’re looking at a prostate cancer GWAS again, we see that the TWAS models, there’s fewer of them, they’re harder to fit for more tissue-specific expression, but for the chromatin-based models and the transcription factor-based models, there’s more of them and they’re easier to fit. And so again, this could be pointing towards a phenomenon where more tissue-specific expression has lower power for the sort of eQTL and transcriptome-based models but higher power for these epigenetic-based models.\nAnd so with that, I’ll conclude. I hope I’ve been able to convince you at the very least that gene expression is a complex, heritable, and predictable trait, and this predictability is something that we can leverage to integrate that trait into other datasets where we don’t have it measured. And specifically, we derived this TWAS statistic, which is a measure of the cis genetic correlation between the gene expression and the disease. As I noted at the end, this is not just limited to transcription; other molecular phenotypes can be used within the same framework. And again, I want to sort of emphasize the caveats that go along with any kind of association study. It’s not a causal inference, and in fact, causal inference in this space is I think a really interesting and sort of ongoing open problem. How do we disentangle all of those different arrows that I was showing earlier? And then also, just to remind you that all the prediction here has been within the cis locus of the gene. That’s where we have power at the current sample size. But there’s a whole world of trans effects, which we haven’t really scratched the surface in understanding. And so that’s something that I think as studies get larger and as we have more experimental data, we’ll also be able to fold into this framework. And so with that, I’ll take your questions. Thanks. Thank you.\n\n\n\nTWAS Primer\nTitle: PGC TWAS Primer\nPresenter(s): Sasha Gusev\nSasha Gusev:\nHello! Thank you for having me for PGC Day to talk about transcriptome wide association studies and our method, FUSION. I’m very excited to give a little bit of background on this methodology and also some examples of how to use it and how to interpret the results.\nSo I will start by talking a little bit more generally about what the transcriptome-wide association study, or TWAS, is, and then I’ll get into how to actually run it yourself with your own data. So, and this is going to be very brief. The basic idea of TWAS is that oftentimes when we’re performing a genome-wide association study (GWAS), we’re interested in understanding the potential mechanisms of an associated locus or identifying novel mechanisms that we haven’t discovered yet. One way that we can make those associations is by integrating gene expression data. So, under the assumption that genetic variants modify transcriptional activity and then lead to disease via that transformational activity, what we would like to have is a study with genetic information, gene expression information, and disease all measured in the same individuals, for which we could sort of investigate the relationships between all of these modalities and additionally compute the genetic component of expression and ask whether that component is associated with the phenotype and how strongly.\nUnfortunately, in a typical GWAS, we don’t have this data. We usually don’t have gene expression measured in our cases and controls, and so we can’t probe these questions directly. But the sort of insights that TWAS makes are essentially that we can probe these questions using prediction of this genetic component of expression, and that’s something we can do with summary level data from the GWAS only without even requiring the individual-level phenotype information. So the way that the TWAS works is by constructing predictive models that relate genotypes to expression in some training data, for example, in the GTEx cohort or in the Common Mind Consortium datasets that don’t necessarily have the phenotype of interest measured in them. Then we predict this expression into our GWAS study, and now we can associate it with the phenotype directly, and we can infer associations between the genetic effect on expression and the disease. Alternatively, you can think about this as inferring the genetic correlation between the gene expression and the phenotype. And in all of the applications I’m going to be talking about, we train this using just the cis locus, just a megabase around the gene. So this is all the cis genetic correlation or the local genetic correlation between gene expression and disease.\nNow, the one other point that’s important here is that in the cases where we don’t have the underlying phenotype information but we do have summary GWAS data, so association statistics, p-values, Z-scores, and effect sizes, we can still perform this analysis by using an LD reference panel with the summary-level data and estimating what the predicted gene-trait association would have been if we had the underlying data. And for more information and the derivation of how we do this, how accurate it is relative to individual level data, I would refer you to the references here.\nOkay, so a little bit more context now that we have a sense of what TWAS is doing, how do we interpret what it’s doing, how do we interpret those results? So, one point to think about is how does TWAS compare to other approaches that integrate molecular data with GWAS, and I think, you know, a main analysis type is co-localization analysis, and I think that it’s important to keep in mind how TWAS differs from co-localization. So, as I mentioned, TWAS is estimating essentially the genetic correlation between the cis component of expression and the disease that we’re interested in. Co-localization is testing a specific hypothesis or estimating a specific probability that the disease and the molecular phenotype have the same causal variant. So, this is an association test, and co-localization is evaluating the probability of a shared causal variance. That’s a little bit different. The rest of the differences are kind of mechanistic, so TWAS is a frequentist test, and it provides a signed test statistic because it’s estimating a genetic correlation, whereas co-localization is typically implemented in a Bayesian framework, and it estimates a posterior between zero and one of the probability of the shared causal effect.\nTWAS doesn’t make any assumptions or does not have to make any assumptions on the causal variants either in the disease or in the expression trait. So, you can have allelic heterogeneity; you can have complex relationships, as long as the causal effects are linear within the summary-based model. There’s no assumption on how those causal effects have to be distributed, whereas typically co-localization requires some assumption on the number or the relative relationship of the causal effect sizes to estimate this probability.\nAnd then, lastly, because TWAS is using individual level data to train predictive models, it can train all sorts of fancy predictors of gene expression and then impute those into the target GWAS study. Whereas typically co-localization is using marginal eQTL association statistics and marginal GWAS association statistics, and so it does not inherently allow for these kinds of fancy predictive models. And this means that, in some cases, TWAS maybe can squeeze out a bit more power by modeling complex genotype-phenotype relationships.\nOne other important point to think about is how not to interpret TWAS or ways in which TWAS can provide you with non-causal associations, which is what we don’t want. And this was covered in detail in this great paper from Weinberg et al. in 2019 in Nature Genetics. But the basic idea is that because TWAS is an association statistic, it can pick up associations due to all sorts of tagging, in the same way that GWAS can pick up associations due to SNPs being tagged or correlated with a causal variant. TWAS can pick up associations due to genes or eQTLs being correlated with the causal variant, and so this example from Weinberg et al. shows that if you have a non-causal gene that’s correlated and co-regulated, meaning the same variant drives the non-causal gene and the causal gene, you may observe a TWAS association with the non-causal gene that’s just a product of this co-regulation. And likewise, if you have eQTLs that are influencing both a causal gene and maybe partially a non-causal gene, that may induce some TWAS association. And if you are missing an eQTL for the causal gene, but you have an eQTL for the non-causal gene in the same locus, that can induce a missing TWAS association. So these are all things to keep in mind. Essentially, the same sort of limitations that apply to integrating eQTL studies of any kind apply to the TWAS framework, which is building on top of the eQTLs.\nOkay, so now getting to the heart of the problem, how do we actually run these methods and what do we get out of them? So, I’m just going to run through a specific example. I would urge everybody to go to the TWAS website (the link is here) and sort of work through the outline and download the code. I’m going to show you what it looks like for a single gene. The inputs that we need for this analysis are the FUSION software, which implements the TWAS association test; LD reference data, as I mentioned; GWAS summary statistics in a prepared format; and gene expression weights. These are the predictive models that we’re going to be using that are the key component of the TWAS analysis.\nSo, let’s walk through each of these one at a time. The LD reference data is essentially a directory of PLINK-formatted genotype files from the 1000 Genomes in this case. You can substitute your own LD reference panels if you’re interested in or if you’re working with other populations, just broken up by chromosome - basic genotype files.\nThe summary statistics I’m going to use schizophrenia summary statistics from the PGC2. They follow the LD score format. If you’ve ever used LDSC for heritability analyses, the format is essentially the same. We need to know the SNP, the alleles for that SNP, and the Z-score field - the direction and significance of the association - all we need for this analysis.\nLastly, the gene expression weights. Again, you can download these extensively from the FUSION website. We’ve compiled these for GTEx, for Common Mind, for various PsychENCODE studies, as well as actually for other phenotypes besides gene expression. Those weights, when you download them, you get an annotation file which is this .pos file that contains information about the underlying weight. The weight is in an R data format. You can just load this in R and see what the weights are for predicting expression. We also have the information on the gene and the position of the gene, and this is again this file and pointers to each of these weights is all that’s required.\nOkay, so let’s run a TWAS analysis now that we have all of those components. TWAS is a series of FUSION implements, a series of R scripts for doing this analysis. To perform a test, you run this FUSION association test script. It takes as inputs the summary statistics, a pointer to the weights file, a reference to where the weights actually are so that it can read them in, a reference to the LD reference panel (which has all the chromosome information), the specific chromosome that we’re going to be running on (chromosome 3), and where to print the output. This is it. We run this.\nAnd the output that we get is this .dat file. I’m just going to look at a couple of lines from that file, the significant associations from that file. And they look like this. This isn’t all that readable, but you can see each line is one result from a TWAS test analysis. Just to break this down a little bit further, let’s take a look at one of these lines. So it looks something like this. You have entries for which weights file was used. In this case, the CNTN4 gene was used for prediction. The name of the gene, where it is, the heritability of the gene. We have some information on the best GWAS SNP in the locus and the Z-score for that GWAS SNP. We also have information on the best eQTL for that gene and the R-squared and Z-score for that eQTL. So, you can see that the eQTL, the best eQTL, is actually quite a bit less significant in the GWAS study than the best SNP in the locus.\nWe have information on how many SNPs were used to train the model and how many were actually retained for prediction. This NWGT is the number of non-zero weights, the best predictive model of many different models trained. The cross-validation R-squared of that model and, again, something to keep in mind here, for example, is that the R-squared from the predictive model from the Elastic Net model is much higher than the R-squared that we got from the top eQTL. So this suggests that there are some additional variants that are contributing to the predictive accuracy of this test.\nThe cross-validation model. The cross-validation model p-value, this is just the p-value on this R-squared. And then finally, the statistics that we’re interested in, the TWAS association Z-score and the p-value on that Z-score. So this is the key statistic that we want: how strongly is this CNTN4 predictor associated with the disease?\nOkay, now in addition to getting these kinds of outputs, something else that we may be interested in is visualizing these kinds of results. So, I’m going to take a look at again some of the top significant TWAS associations from our analysis. We’ll put them in this file .top. Then we run this script in FUSION for post-processing the data. The script takes some of the same inputs, essentially the summary statistics and the LD reference data, this .top file that we just generated, as well as some information on what we want to do. We want to plot these loci within 100 kb of the gene and generate the outputs.\nAnd this produces figures that look like this, which I think are very useful to make sense of these analyses. What this figure is showing is a Manhattan plot of the significant locus from this .top file. In gray is the original GWAS association statistic, and then in blue is the GWAS association statistic after conditioning on the TWAS-predicted gene, which is shown in green here. This is a locus with the\nTHOC7 gene; after conditioning on the predictor of THOC7, you see that the GWAS Manhattan plot goes from significant associations beyond a p-value of 10-8 to essentially no significant associations in this locus. So, this is a very good sign visually that things are kind of working as we expect. And when we condition on the predictor, the association goes away, which is consistent with the predictor being the mediator. You can do these analyses when you have multiple genes in the locus, you can do all sorts of fancy pairwise or individual model conditioning, but I think that this is, in addition to looking at just the raw statistics, this is a useful visual output to understand what’s going on at the locus.\nOkay, so hopefully, that was a useful primer on running a TWAS analysis with FUSION. I’ll just mention that there are a couple of other analyses that you can do. We’ve worked with Nick Mancuso in this paper in 2019 for fine-mapping TWAS associations in a similar way as how you would fine-map SNPs. I would also refer you to this paper of Yao et al. in Nature Genetics looking at estimating the fraction of heritability that’s mediated by gene expression rather than just associated. Additionally, there are a lot of scripts on the Fusion website that let you predict into individual-level data, perform co-localization analyses in addition to TWAS analyses, and do more detailed comparisons and visualizations of the ones that I described, as well as cross-model correlations. What are the weights that are contributing to each model, how do the eQTLs for a model look like, and so forth, and let you really dig into an association in a visual way.\nThe one other thing I mentioned is that I’ll mention is that we’ve put together an interactive website called twas-hub.org, where you can go and look at all of these associations. You can search for individual phenotypes for genes and look at the TWAS associations and investigate which models are predictive and do all this in the web interface. Um, which I think is also very useful to get a sense of how these methods perform. And with that, thank you very much."
  }
]